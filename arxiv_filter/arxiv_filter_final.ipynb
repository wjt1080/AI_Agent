{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cafd31",
   "metadata": {},
   "source": [
    "# ArXiv Dataset Filtration for AI PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829957f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d824e56",
   "metadata": {},
   "source": [
    "The ArXiv meta-dataset is downloaded from Kaggle: https://www.kaggle.com/datasets/Cornell-University/arxiv![image.png](attachment:image.png)<br>\n",
    "date of download: 21 January 2024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "779efde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open('arxiv-metadata-oai-snapshot.json','r') as file:\n",
    "    for line in file:\n",
    "        json_data = json.loads(line)\n",
    "        data.append(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3129c3f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of papers in metadata:  2403902\n"
     ]
    }
   ],
   "source": [
    "print('total number of papers in metadata: ',len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e3fee",
   "metadata": {},
   "source": [
    "## Filter on math/cs/stat categorized papers \n",
    "We only want to focus on papers under  math/cs/stat categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6cfa7be3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cate_data=[]\n",
    "for paper in data:\n",
    "    categories = paper.get('categories','').split()\n",
    "    if any(category.startswith((\"math\", \"cs\", \"stat\")) for category in categories):\n",
    "        cate_data.append(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67ca3293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of papers in metadata:  1138099\n"
     ]
    }
   ],
   "source": [
    "print('total number of papers in metadata: ',len(cate_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ed950",
   "metadata": {},
   "source": [
    "## Extract target text\n",
    "Here we want to use the title + abstract as the target text and perform keywords matching through Regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7cac9ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_title_abstract(paper):\n",
    "    combined = paper.get('title').replace('\\n',' ') + '.' + paper.get('abstract').replace('\\n',' ')\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0772bb",
   "metadata": {},
   "source": [
    "## First round of general filtration\n",
    "First round of general filtration, make sure those papers are all AI related or at least mentioned AI. \n",
    "* Used \"\\b\" the word boundary here, because we want to match the exact terms here, not any substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3088f816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(A|a)rtificial (I|i)ntelligence|AI|LLMs?|(L|l)arge (L|l)anguage (M|m)odels?|(c|C)hat(gpt|GPT)|(gpt|GPT)4|NLP|(n|N)atural (L|l)anguage|(d|D)eep (L|l)earning|(m|M)achine (L|l)earning\n"
     ]
    }
   ],
   "source": [
    "AI_word = ['(A|a)rtificial (I|i)ntelligence',\n",
    "            'AI',\n",
    "            'LLMs?',\n",
    "            '(L|l)arge (L|l)anguage (M|m)odels?',\n",
    "            '(c|C)hat(gpt|GPT)',\n",
    "            '(gpt|GPT)4',\n",
    "            'NLP',\n",
    "            '(n|N)atural (L|l)anguage',\n",
    "            '(d|D)eep (L|l)earning',\n",
    "            '(m|M)achine (L|l)earning']\n",
    "temp = '|'.join(AI_word)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c7ff7db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent: 62.0 seconds\n",
      "number of papers filtered:  111783\n"
     ]
    }
   ],
   "source": [
    "pattern_whole = r'\\b(?:{})\\b'.format(temp)\n",
    "# pattern_initial = r'\\bAI\\b'\n",
    "general_filter = [] \n",
    "\n",
    "start_time = time.time()\n",
    "for paper in cate_data:\n",
    "    try:\n",
    "        title_abstract = extract_title_abstract(paper)\n",
    "        \n",
    "        matches_whole = re.findall(pattern_whole, title_abstract,flags=re.NOFLAG)\n",
    "        \n",
    "        if matches_whole:\n",
    "            #print(matches_whole)\n",
    "            general_filter.append(paper)\n",
    "            \n",
    "        # else:\n",
    "        #     matches_intial = re.findall(pattern_initial,title_abstract,flags=re.NOFLAG)\n",
    "          #   if matches_intial:\n",
    "            #     general_filter.append(paper)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "        \n",
    "end_time = time.time()\n",
    "time_spent = end_time-start_time\n",
    "print(f'time spent: {time_spenth} seconds')\n",
    "print('number of papers filtered: ',len(general_filter))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f789ad92",
   "metadata": {},
   "source": [
    "## determine keywords for regex matching\n",
    "Target keywords, selected from Mitre Atlas Attack Classes and some other commonly used ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b93989f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "important_word = ['poison|poisoning|poisoned',\n",
    "                    'evade|evasion',\n",
    "                    'prompt injection',\n",
    "                    'backdoor|backdoored',\n",
    "                    'jailbreak',\n",
    "                    'hijacking',\n",
    "                    'extraction attacks?',\n",
    "                    'privacy leakage',\n",
    "                    'data leak|data leaking|data leakage',\n",
    "                    'denial of service|DDOS',\n",
    "                    'deepfake',\n",
    "                    'adversarial examples?|adversarial attacks?|adversarial samples?',\n",
    "                    'ransomwares?']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86573ad4",
   "metadata": {},
   "source": [
    "## Regex matching with detailed keywords\n",
    "Run regex on each keyword individually to see the hit performance, in order to adjust quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "08ecd7c4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching keyword:  poison|poisoning|poisoned\n",
      "\\b(poison|poisoning|poisoned)\\b\n",
      "number of papers found:  519\n",
      "time spent: 1.5 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  evade|evasion\n",
      "\\b(evade|evasion)\\b\n",
      "number of papers found:  325\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  prompt injection\n",
      "\\b(prompt injection)\\b\n",
      "number of papers found:  19\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  backdoor|backdoored\n",
      "\\b(backdoor|backdoored)\\b\n",
      "number of papers found:  170\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  jailbreak\n",
      "\\b(jailbreak)\\b\n",
      "number of papers found:  43\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  hijacking\n",
      "\\b(hijacking)\\b\n",
      "number of papers found:  24\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  extraction attacks?\n",
      "\\b(extraction attacks?)\\b\n",
      "number of papers found:  57\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  privacy leakage\n",
      "\\b(privacy leakage)\\b\n",
      "number of papers found:  154\n",
      "time spent: 1.5 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  data leak|data leaking|data leakage\n",
      "\\b(data leak|data leaking|data leakage)\\b\n",
      "number of papers found:  125\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  denial of service|DDOS\n",
      "\\b(denial of service|DDOS)\\b\n",
      "number of papers found:  91\n",
      "time spent: 2.0 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  deepfake\n",
      "\\b(deepfake)\\b\n",
      "number of papers found:  158\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  adversarial examples?|adversarial attacks?|adversarial samples?\n",
      "\\b(adversarial examples?|adversarial attacks?|adversarial samples?)\\b\n",
      "number of papers found:  1947\n",
      "time spent: 1.7 seconds\n",
      "==================================================\n",
      "\n",
      "matching keyword:  ransomwares?\n",
      "\\b(ransomwares?)\\b\n",
      "number of papers found:  50\n",
      "time spent: 1.4 seconds\n",
      "==================================================\n",
      "\n",
      "total number of papers found:  3682\n"
     ]
    }
   ],
   "source": [
    "offence_result=[]\n",
    "\n",
    "for tech in important_word:\n",
    "    num_hits = 0\n",
    "    print('matching keyword: ',tech)\n",
    "    \n",
    "    pattern_offence = r'\\b({})\\b'.format(tech)\n",
    "    print(pattern_offence)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for paper in general_filter:\n",
    "        try:\n",
    "            title_abstract = extract_title_abstract(paper)\n",
    "\n",
    "            matches_key = re.findall(pattern_offence, title_abstract,flags=re.IGNORECASE)\n",
    "\n",
    "            if matches_key and paper not in offence_result:\n",
    "                # print(paper.get('id'))\n",
    "                # print(matches_key)\n",
    "                #if tech == 'exfiltration':\n",
    "                    # print(matches_key)\n",
    "                num_hits += 1\n",
    "                # print(matches_key)\n",
    "                paper['matches_key'] = set([key.lower() for key in matches_key])\n",
    "                offence_result.append(paper)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    \n",
    "    \n",
    "    end_time = time.time()\n",
    "    time_spent = end_time - start_time\n",
    "    \n",
    "    print('number of papers found: ',num_hits)\n",
    "    print(f\"time spent: {time_spent:.1f} seconds\")\n",
    "    print('='*50+'\\n')\n",
    "print('total number of papers found: ',len(offence_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f3805",
   "metadata": {},
   "source": [
    "Reformat the keyword list into a long regex expression, run the filtration to match the papers with any of them\n",
    "\n",
    "adding \"matches_key\" to paper data for easily manual check\n",
    "\n",
    "adding \"url\" to get direct link to PDF file\n",
    "\n",
    "adding \"date\" to get the latest update date in datestamp format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e223f91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent: 7.1 seconds\n",
      "total number of papers found:  3682\n"
     ]
    }
   ],
   "source": [
    "offence_result=[]\n",
    "tech_str = '|'.join(important_word)\n",
    "\n",
    "pattern_offence = r'\\b(?:{})\\b'.format(tech_str)\n",
    "# print(pattern_offence)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for paper in general_filter:\n",
    "    try:\n",
    "        title_abstract = extract_title_abstract(paper)\n",
    "\n",
    "        matches_key = re.findall(pattern_offence, title_abstract,flags=re.IGNORECASE)\n",
    "        # matches_key = [k if k is not \"\" for k in matches_key]\n",
    "\n",
    "        if matches_key:\n",
    "            # print(paper.get('id'))\n",
    "            # print(matches_key)\n",
    "            #if tech == 'exfiltration':\n",
    "                # print(matches_key)\n",
    "            latest_version = max(version['created'] for version in paper['versions'])\n",
    "            date_object = datetime.strptime(latest_version, \"%a, %d %b %Y %H:%M:%S %Z\")\n",
    "            datestamp_format = date_object.strftime('%Y-%m-%d')\n",
    "            \n",
    "            paper_id = paper.get('id','')\n",
    "            paper['matches_key'] = set([key.lower() for key in matches_key])\n",
    "            paper['url'] = f'https://arxiv.org/pdf/{paper_id}' \n",
    "            paper['publish_date'] = datestamp_format\n",
    "            #print(paper['matches_key'])\n",
    "            offence_result.append(paper)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "end_time = time.time()\n",
    "time_spent = end_time - start_time\n",
    "\n",
    "print(f\"time spent: {time_spent:.1f} seconds\")\n",
    "print('total number of papers found: ',len(offence_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b99038",
   "metadata": {},
   "source": [
    "## output the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9b3166",
   "metadata": {},
   "source": [
    "save to excel file for manual check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5f85b218",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to filtered_papers_eventual.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extracting id, title, and abstract from each paper\n",
    "excel = {'id': [], 'title': [], 'abstract': [],'categories':[],'matched_keyword':[]}\n",
    "\n",
    "for paper in offence_result:\n",
    "    excel['id'].append(paper['id'])\n",
    "    excel['title'].append(paper['title'])\n",
    "    excel['abstract'].append(paper['abstract'].replace('\\n',' '))\n",
    "    excel['categories'].append(paper['categories'])\n",
    "    excel['matched_keyword'].append(paper['matches_key'])\n",
    "    \n",
    "# Creating a DataFrame using pandas\n",
    "df = pd.DataFrame(excel)\n",
    "\n",
    "# Specifying the Excel file path\n",
    "excel_file_path = 'filtered_papers_eventual.xlsx'\n",
    "\n",
    "# Writing the DataFrame to an Excel file\n",
    "df.to_excel(excel_file_path, index=False)\n",
    "\n",
    "print(f'Data exported to {excel_file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3819d4",
   "metadata": {},
   "source": [
    "save to json file for uploading to openCTI (dropped \"matches_key\" since set cannot be in json file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9055196a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data exported to final_filter_results.json\n"
     ]
    }
   ],
   "source": [
    "output_json = 'final_filter_results.json'\n",
    "\n",
    "for paper in offence_result:\n",
    "    paper.pop('matches_key', None)\n",
    "\n",
    "with open(output_json,'w',encoding='utf-8') as output_file:\n",
    "    json.dump(offence_result,output_file,indent=2)\n",
    "\n",
    "print(f'Data exported to {output_json}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10b1f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
