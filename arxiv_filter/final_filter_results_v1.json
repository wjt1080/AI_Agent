[
  {
    "id": "1104.5566",
    "submitter": "Stefan Szeider",
    "authors": "Stefan Szeider",
    "title": "Limits of Preprocessing",
    "comments": "This is a slightly longer version of a paper that appeared in the\n  proceedings of AAAI 2011",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We present a first theoretical analysis of the power of polynomial-time\npreprocessing for important combinatorial problems from various areas in AI. We\nconsider problems from Constraint Satisfaction, Global Constraints,\nSatisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a\ncomplexity theoretic assumption, none of the considered problems can be reduced\nby polynomial-time preprocessing to a problem kernel whose size is polynomial\nin a structural problem parameter of the input, such as induced width or\nbackdoor size. Our results provide a firm theoretical boundary for the\nperformance of polynomial-time preprocessing algorithms for the considered\nproblems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Apr 2011 08:31:41 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Aug 2011 15:53:23 GMT"
      }
    ],
    "update_date": "2011-08-12",
    "authors_parsed": [
      [
        "Szeider",
        "Stefan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1104.5566",
    "publish_date": "2011-08-11"
  },
  {
    "id": "1202.1761",
    "submitter": "Mehdi Ebady Manaa",
    "authors": "Mehdi Ebady Manna and Angela Amphawan",
    "title": "Review of syn-flooding attack detection mechanism",
    "comments": null,
    "journal-ref": "International Journal of Distributed and Parallel Systems (IJDPS)\n  Vol.3, No.1, January 2012, 99-117",
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Denial of Service (DoS) is a security threat which compromises the\nconfidentiality of information stored in Local Area Networks (LANs) due to\nunauthorized access by spoofed IP addresses. SYN Flooding is a type of DoS\nwhich is harmful to network as the flooding of packets may delay other users\nfrom accessing the server and in severe cases, the server may need to be shut\ndown, wasting valuable resources, especially in critical real-time services\nsuch as in e-commerce and the medical field. The objective of this paper is to\nreview the state-of-the art of detection mechanisms for SYN flooding. The\ndetection schemes for SYN Flooding attacks have been classified broadly into\nthree categories - detection schemes based on the router data structure,\ndetection schemes based on statistical analysis of the packet flow and\ndetection schemes based on artificial intelligence. The advantages and\ndisadvantages for various detection schemes under each category have been\ncritically examined. The performance measures of the categories have also been\ncompared.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Feb 2012 16:42:54 GMT"
      }
    ],
    "update_date": "2012-02-09",
    "authors_parsed": [
      [
        "Manna",
        "Mehdi Ebady",
        ""
      ],
      [
        "Amphawan",
        "Angela",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1202.1761",
    "publish_date": "2012-02-08"
  },
  {
    "id": "1304.5961",
    "submitter": "Andreas Pfandler",
    "authors": "Andreas Pfandler, Stefan R\\\"ummele, Stefan Szeider",
    "title": "Backdoors to Abduction",
    "comments": "12 pages, a short version will appear in the proceedings of the 23rd\n  International Joint Conference on Artificial Intelligence (IJCAI 2013)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CC cs.LO",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Abductive reasoning (or Abduction, for short) is among the most fundamental\nAI reasoning methods, with a broad range of applications, including fault\ndiagnosis, belief revision, and automated planning. Unfortunately, Abduction is\nof high computational complexity; even propositional Abduction is\n\\Sigma_2^P-complete and thus harder than NP and coNP. This complexity barrier\nrules out the existence of a polynomial transformation to propositional\nsatisfiability (SAT). In this work we use structural properties of the\nAbduction instance to break this complexity barrier. We utilize the problem\nstructure in terms of small backdoor sets. We present fixed-parameter tractable\ntransformations from Abduction to SAT, which make the power of today's SAT\nsolvers available to Abduction.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 22 Apr 2013 14:23:11 GMT"
      }
    ],
    "update_date": "2013-04-23",
    "authors_parsed": [
      [
        "Pfandler",
        "Andreas",
        ""
      ],
      [
        "R\u00fcmmele",
        "Stefan",
        ""
      ],
      [
        "Szeider",
        "Stefan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1304.5961",
    "publish_date": "2013-04-22"
  },
  {
    "id": "1707.05173",
    "submitter": "Owain Evans",
    "authors": "William Saunders, Girish Sastry, Andreas Stuhlmueller, Owain Evans",
    "title": "Trial without Error: Towards Safe Reinforcement Learning via Human\n  Intervention",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG cs.NE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI systems are increasingly applied to complex tasks that involve interaction\nwith humans. During training, such systems are potentially dangerous, as they\nhaven't yet learned to avoid actions that could cause serious harm. How can an\nAI system explore and learn without making a single mistake that harms humans\nor otherwise causes serious damage? For model-free reinforcement learning,\nhaving a human \"in the loop\" and ready to intervene is currently the only way\nto prevent all catastrophes. We formalize human intervention for RL and show\nhow to reduce the human labor required by training a supervised learner to\nimitate the human's intervention decisions. We evaluate this scheme on Atari\ngames, with a Deep RL agent being overseen by a human for four hours. When the\nclass of catastrophes is simple, we are able to prevent all catastrophes\nwithout affecting the agent's learning (whereas an RL baseline fails due to\ncatastrophic forgetting). However, this scheme is less successful when\ncatastrophes are more complex: it reduces but does not eliminate catastrophes\nand the supervised learner fails on adversarial examples found by the agent.\nExtrapolating to more challenging environments, we show that our implementation\nwould not scale (due to the infeasible amount of human labor required). We\noutline extensions of the scheme that are necessary if we are to train\nmodel-free agents without a single catastrophe.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 17 Jul 2017 14:13:40 GMT"
      }
    ],
    "update_date": "2017-07-18",
    "authors_parsed": [
      [
        "Saunders",
        "William",
        ""
      ],
      [
        "Sastry",
        "Girish",
        ""
      ],
      [
        "Stuhlmueller",
        "Andreas",
        ""
      ],
      [
        "Evans",
        "Owain",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1707.05173",
    "publish_date": "2017-07-17"
  },
  {
    "id": "1709.05583",
    "submitter": "Xiaoyu Cao",
    "authors": "Xiaoyu Cao, Neil Zhenqiang Gong",
    "title": "Mitigating Evasion Attacks to Deep Neural Networks via Region-based\n  Classification",
    "comments": "33rd Annual Computer Security Applications Conference (ACSAC), 2017",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) have transformed several artificial intelligence\nresearch areas including computer vision, speech recognition, and natural\nlanguage processing. However, recent studies demonstrated that DNNs are\nvulnerable to adversarial manipulations at testing time. Specifically, suppose\nwe have a testing example, whose label can be correctly predicted by a DNN\nclassifier. An attacker can add a small carefully crafted noise to the testing\nexample such that the DNN classifier predicts an incorrect label, where the\ncrafted testing example is called adversarial example. Such attacks are called\nevasion attacks. Evasion attacks are one of the biggest challenges for\ndeploying DNNs in safety and security critical applications such as\nself-driving cars. In this work, we develop new methods to defend against\nevasion attacks. Our key observation is that adversarial examples are close to\nthe classification boundary. Therefore, we propose region-based classification\nto be robust to adversarial examples. For a benign/adversarial testing example,\nwe ensemble information in a hypercube centered at the example to predict its\nlabel. In contrast, traditional classifiers are point-based classification,\ni.e., given a testing example, the classifier predicts its label based on the\ntesting example alone. Our evaluation results on MNIST and CIFAR-10 datasets\ndemonstrate that our region-based classification can significantly mitigate\nevasion attacks without sacrificing classification accuracy on benign examples.\nSpecifically, our region-based classification achieves the same classification\naccuracy on testing benign examples as point-based classification, but our\nregion-based classification is significantly more robust than point-based\nclassification to various evasion attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Sep 2017 00:18:42 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 3 Jan 2018 06:45:33 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 11 Jan 2018 20:58:56 GMT"
      },
      {
        "version": "v4",
        "created": "Tue, 31 Dec 2019 14:36:29 GMT"
      }
    ],
    "update_date": "2020-01-01",
    "authors_parsed": [
      [
        "Cao",
        "Xiaoyu",
        ""
      ],
      [
        "Gong",
        "Neil Zhenqiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1709.05583",
    "publish_date": "2018-01-03"
  },
  {
    "id": "1801.00553",
    "submitter": "Naveed Akhtar Dr.",
    "authors": "Naveed Akhtar and Ajmal Mian",
    "title": "Threat of Adversarial Attacks on Deep Learning in Computer Vision: A\n  Survey",
    "comments": "Incorporates feedback provided by multiple researchers",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning is at the heart of the current rise of machine learning and\nartificial intelligence. In the field of Computer Vision, it has become the\nworkhorse for applications ranging from self-driving cars to surveillance and\nsecurity. Whereas deep neural networks have demonstrated phenomenal success\n(often beyond human capabilities) in solving complex problems, recent studies\nshow that they are vulnerable to adversarial attacks in the form of subtle\nperturbations to inputs that lead a model to predict incorrect outputs. For\nimages, such perturbations are often too small to be perceptible, yet they\ncompletely fool the deep learning models. Adversarial attacks pose a serious\nthreat to the success of deep learning in practice. This fact has lead to a\nlarge influx of contributions in this direction. This article presents the\nfirst comprehensive survey on adversarial attacks on deep learning in Computer\nVision. We review the works that design adversarial attacks, analyze the\nexistence of such attacks and propose defenses against them. To emphasize that\nadversarial attacks are possible in practical conditions, we separately review\nthe contributions that evaluate adversarial attacks in the real-world\nscenarios. Finally, we draw on the literature to provide a broader outlook of\nthe research direction.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 2 Jan 2018 05:22:06 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 4 Jan 2018 08:50:39 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 26 Feb 2018 06:18:58 GMT"
      }
    ],
    "update_date": "2018-02-27",
    "authors_parsed": [
      [
        "Akhtar",
        "Naveed",
        ""
      ],
      [
        "Mian",
        "Ajmal",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1801.00553",
    "publish_date": "2018-01-02"
  },
  {
    "id": "1801.09097",
    "submitter": "Yifei Fan",
    "authors": "Yifei Fan and Anthony Yezzi",
    "title": "Towards an Understanding of Neural Networks in Natural-Image Spaces",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Two major uncertainties, dataset bias and adversarial examples, prevail in\nstate-of-the-art AI algorithms with deep neural networks. In this paper, we\npresent an intuitive explanation for these issues as well as an interpretation\nof the performance of deep networks in a natural-image space. The explanation\nconsists of two parts: the philosophy of neural networks and a hypothetical\nmodel of natural-image spaces. Following the explanation, we 1) demonstrate\nthat the values of training samples differ, 2) provide incremental boost to the\naccuracy of a CIFAR-10 classifier by introducing an additional \"random-noise\"\ncategory during training, 3) alleviate over-fitting thereby enhancing the\nrobustness against adversarial examples by detecting and excluding illusive\ntraining samples that are consistently misclassified. Our overall contribution\nis therefore twofold. First, while most existing algorithms treat data equally\nand have a strong appetite for more data, we demonstrate in contrast that an\nindividual datum can sometimes have disproportionate and counterproductive\ninfluence and that it is not always better to train neural networks with more\ndata. Next, we consider more thoughtful strategies by taking into account the\ngeometric and topological properties of natural-image spaces to which deep\nnetworks are applied.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 27 Jan 2018 14:41:59 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 11 Feb 2019 05:18:00 GMT"
      }
    ],
    "update_date": "2019-02-12",
    "authors_parsed": [
      [
        "Fan",
        "Yifei",
        ""
      ],
      [
        "Yezzi",
        "Anthony",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1801.09097",
    "publish_date": "2018-01-27"
  },
  {
    "id": "1803.09468",
    "submitter": "Boussad Addad",
    "authors": "Boussad Addad, Jerome Kodjabachian, and Christophe Meyer",
    "title": "Clipping free attacks against artificial neural networks",
    "comments": "12 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  During the last years, a remarkable breakthrough has been made in AI domain\nthanks to artificial deep neural networks that achieved a great success in many\nmachine learning tasks in computer vision, natural language processing, speech\nrecognition, malware detection and so on. However, they are highly vulnerable\nto easily crafted adversarial examples. Many investigations have pointed out\nthis fact and different approaches have been proposed to generate attacks while\nadding a limited perturbation to the original data. The most robust known\nmethod so far is the so called C&W attack [1]. Nonetheless, a countermeasure\nknown as feature squeezing coupled with ensemble defense showed that most of\nthese attacks can be destroyed [6]. In this paper, we present a new method we\ncall Centered Initial Attack (CIA) whose advantage is twofold : first, it\ninsures by construction the maximum perturbation to be smaller than a threshold\nfixed beforehand, without the clipping process that degrades the quality of\nattacks. Second, it is robust against recently introduced defenses such as\nfeature squeezing, JPEG encoding and even against a voting ensemble of\ndefenses. While its application is not limited to images, we illustrate this\nusing five of the current best classifiers on ImageNet dataset among which two\nare adversarialy retrained on purpose to be robust against attacks. With a\nfixed maximum perturbation of only 1.5% on any pixel, around 80% of attacks\n(targeted) fool the voting ensemble defense and nearly 100% when the\nperturbation is only 6%. While this shows how it is difficult to defend against\nCIA attacks, the last section of the paper gives some guidelines to limit their\nimpact.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 26 Mar 2018 08:39:15 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 28 Mar 2018 07:44:38 GMT"
      }
    ],
    "update_date": "2018-03-29",
    "authors_parsed": [
      [
        "Addad",
        "Boussad",
        ""
      ],
      [
        "Kodjabachian",
        "Jerome",
        ""
      ],
      [
        "Meyer",
        "Christophe",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1803.09468",
    "publish_date": "2018-03-28"
  },
  {
    "id": "1804.02257",
    "submitter": "Sam Kriegman",
    "authors": "Sam Kriegman, Nick Cheney, Francesco Corucci, and Josh C. Bongard",
    "title": "Interoceptive robustness through environment-mediated morphological\n  development",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1145/3205455.3205529",
    "report-no": null,
    "categories": "cs.AI cs.NE cs.RO",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Typically, AI researchers and roboticists try to realize intelligent behavior\nin machines by tuning parameters of a predefined structure (body plan and/or\nneural network architecture) using evolutionary or learning algorithms. Another\nbut not unrelated longstanding property of these systems is their brittleness\nto slight aberrations, as highlighted by the growing deep learning literature\non adversarial examples. Here we show robustness can be achieved by evolving\nthe geometry of soft robots, their control systems, and how their material\nproperties develop in response to one particular interoceptive stimulus\n(engineering stress) during their lifetimes. By doing so we realized robots\nthat were equally fit but more robust to extreme material defects (such as\nmight occur during fabrication or by damage thereafter) than robots that did\nnot develop during their lifetimes, or developed in response to a different\ninteroceptive stimulus (pressure). This suggests that the interplay between\nchanges in the containing systems of agents (body plan and/or neural\narchitecture) at different temporal scales (evolutionary and developmental)\nalong different modalities (geometry, material properties, synaptic weights)\nand in response to different signals (interoceptive and external perception)\nall dictate those agents' abilities to evolve or learn capable and robust\nstrategies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 6 Apr 2018 13:33:37 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 20 Jun 2018 01:39:25 GMT"
      }
    ],
    "update_date": "2018-06-21",
    "authors_parsed": [
      [
        "Kriegman",
        "Sam",
        ""
      ],
      [
        "Cheney",
        "Nick",
        ""
      ],
      [
        "Corucci",
        "Francesco",
        ""
      ],
      [
        "Bongard",
        "Josh C.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1804.02257",
    "publish_date": "2018-06-20"
  },
  {
    "id": "1804.08757",
    "submitter": "Witold Oleszkiewicz",
    "authors": "Witold Oleszkiewicz, Peter Kairouz, Karol Piczak, Ram Rajagopal,\n  Tomasz Trzcinski",
    "title": "Siamese Generative Adversarial Privatizer for Biometric Data",
    "comments": "Paper accepted to ACCV 2018 (Asian Conference on Computer Vision)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  State-of-the-art machine learning algorithms can be fooled by carefully\ncrafted adversarial examples. As such, adversarial examples present a concrete\nproblem in AI safety. In this work we turn the tables and ask the following\nquestion: can we harness the power of adversarial examples to prevent malicious\nadversaries from learning identifying information from data while allowing\nnon-malicious entities to benefit from the utility of the same data? For\ninstance, can we use adversarial examples to anonymize biometric dataset of\nfaces while retaining usefulness of this data for other purposes, such as\nemotion recognition? To address this question, we propose a simple yet\neffective method, called Siamese Generative Adversarial Privatizer (SGAP), that\nexploits the properties of a Siamese neural network to find discriminative\nfeatures that convey identifying information. When coupled with a generative\nmodel, our approach is able to correctly locate and disguise identifying\ninformation, while minimally reducing the utility of the privatized dataset.\nExtensive evaluation on a biometric dataset of fingerprints and cartoon faces\nconfirms usefulness of our simple yet effective method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 23 Apr 2018 21:57:28 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 20 Jul 2018 18:36:20 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 8 Oct 2018 07:23:51 GMT"
      }
    ],
    "update_date": "2018-10-09",
    "authors_parsed": [
      [
        "Oleszkiewicz",
        "Witold",
        ""
      ],
      [
        "Kairouz",
        "Peter",
        ""
      ],
      [
        "Piczak",
        "Karol",
        ""
      ],
      [
        "Rajagopal",
        "Ram",
        ""
      ],
      [
        "Trzcinski",
        "Tomasz",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1804.08757",
    "publish_date": "2018-10-08"
  },
  {
    "id": "1806.02877",
    "submitter": "Siwei Lyu",
    "authors": "Yuezun Li, Ming-Ching Chang, Siwei Lyu",
    "title": "In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye\n  Blinking",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The new developments in deep generative networks have significantly improve\nthe quality and efficiency in generating realistically-looking fake face\nvideos. In this work, we describe a new method to expose fake face videos\ngenerated with neural networks. Our method is based on detection of eye\nblinking in the videos, which is a physiological signal that is not well\npresented in the synthesized fake videos. Our method is tested over benchmarks\nof eye-blinking detection datasets and also show promising performance on\ndetecting videos generated with DeepFake.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Jun 2018 19:36:09 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 11 Jun 2018 19:28:49 GMT"
      }
    ],
    "update_date": "2018-06-13",
    "authors_parsed": [
      [
        "Li",
        "Yuezun",
        ""
      ],
      [
        "Chang",
        "Ming-Ching",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1806.02877",
    "publish_date": "2018-06-07"
  },
  {
    "id": "1806.08970",
    "submitter": "Md Ashraful Alam Milton",
    "authors": "Md Ashraful Alam Milton",
    "title": "Evaluation of Momentum Diverse Input Iterative Fast Gradient Sign Method\n  (M-DI2-FGSM) Based Attack Method on MCS 2018 Adversarial Attacks on Black Box\n  Face Recognition System",
    "comments": "The Code is available for download in the following github link:\n  https://github.com/miltonbd/mcs_2018_adversarial_attack . arXiv admin note:\n  text overlap with arXiv:1803.06978 by other authors",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The convolutional neural network is the crucial tool for the recent success\nof deep learning based methods on various computer vision tasks like\nclassification, segmentation, and detection. Convolutional neural networks\nachieved state-of-the-art performance in these tasks and every day pushing the\nlimit of computer vision and AI. However, adversarial attack on computer vision\nsystems is threatening their application in the real life and in\nsafety-critical applications. Necessarily, Finding adversarial examples are\nimportant to detect susceptible models to attack and take safeguard measures to\novercome the adversarial attacks. In this regard, MCS 2018 Adversarial Attacks\non Black Box Face Recognition challenge aims to facilitate the research of\nfinding new adversarial attack techniques and their effectiveness in generating\nadversarial examples. In this challenge, the attack\"s nature is targeted-attack\non the black-box neural network where we have no knowledge about black-block\"s\ninner structure. The attacker must modify a set of five images of a single\nperson so that the neural network miss-classify them as target image which is a\nset of five images of another person. In this competition, we applied Momentum\nDiverse Input Iterative Fast Gradient Sign Method (M-DI2-FGSM) to make an\nadversarial attack on black-box face recognition system. We tested our method\non MCS 2018 Adversarial Attacks on Black Box Face Recognition challenge and\nfound competitive result. Our solution got validation score 1.404 which better\nthan baseline score 1.407 and stood 14 place among 132 teams in the\nleader-board. Further improvement can be achieved by finding improved feature\nextraction from source image, carefully chosen hyper-parameters, finding\nimproved substitute model of the black-box and better optimization method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 23 Jun 2018 14:05:16 GMT"
      }
    ],
    "update_date": "2019-03-28",
    "authors_parsed": [
      [
        "Milton",
        "Md Ashraful Alam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1806.08970",
    "publish_date": "2018-06-23"
  },
  {
    "id": "1807.01069",
    "submitter": "Mathieu Sinn",
    "authors": "Maria-Irina Nicolae and Mathieu Sinn and Minh Ngoc Tran and Beat\n  Buesser and Ambrish Rawat and Martin Wistuba and Valentina Zantedeschi and\n  Nathalie Baracaldo and Bryant Chen and Heiko Ludwig and Ian M. Molloy and Ben\n  Edwards",
    "title": "Adversarial Robustness Toolbox v1.0.0",
    "comments": "34 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial Robustness Toolbox (ART) is a Python library supporting\ndevelopers and researchers in defending Machine Learning models (Deep Neural\nNetworks, Gradient Boosted Decision Trees, Support Vector Machines, Random\nForests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn\nPipelines, etc.) against adversarial threats and helps making AI systems more\nsecure and trustworthy. Machine Learning models are vulnerable to adversarial\nexamples, which are inputs (images, texts, tabular data, etc.) deliberately\nmodified to produce a desired response by the Machine Learning model. ART\nprovides the tools to build and deploy defences and test them with adversarial\nattacks. Defending Machine Learning models involves certifying and verifying\nmodel robustness and model hardening with approaches such as pre-processing\ninputs, augmenting training data with adversarial samples, and leveraging\nruntime detection methods to flag any inputs that might have been modified by\nan adversary. The attacks implemented in ART allow creating adversarial attacks\nagainst Machine Learning models which is required to test defenses with\nstate-of-the-art threat models. Supported Machine Learning Libraries include\nTensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM,\nCatBoost, and GPy. The source code of ART is released with MIT license at\nhttps://github.com/IBM/adversarial-robustness-toolbox. The release includes\ncode examples, notebooks with tutorials and documentation\n(http://adversarial-robustness-toolbox.readthedocs.io).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Jul 2018 10:25:26 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 8 Aug 2018 22:17:25 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 11 Jan 2019 14:01:33 GMT"
      },
      {
        "version": "v4",
        "created": "Fri, 15 Nov 2019 15:05:57 GMT"
      }
    ],
    "update_date": "2019-11-18",
    "authors_parsed": [
      [
        "Nicolae",
        "Maria-Irina",
        ""
      ],
      [
        "Sinn",
        "Mathieu",
        ""
      ],
      [
        "Tran",
        "Minh Ngoc",
        ""
      ],
      [
        "Buesser",
        "Beat",
        ""
      ],
      [
        "Rawat",
        "Ambrish",
        ""
      ],
      [
        "Wistuba",
        "Martin",
        ""
      ],
      [
        "Zantedeschi",
        "Valentina",
        ""
      ],
      [
        "Baracaldo",
        "Nathalie",
        ""
      ],
      [
        "Chen",
        "Bryant",
        ""
      ],
      [
        "Ludwig",
        "Heiko",
        ""
      ],
      [
        "Molloy",
        "Ian M.",
        ""
      ],
      [
        "Edwards",
        "Ben",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1807.01069",
    "publish_date": "2018-08-08"
  },
  {
    "id": "1807.06752",
    "submitter": "Tong Chen",
    "authors": "Tong Chen and Wenjia Niu and Yingxiao Xiang and Xiaoxuan Bai and\n  Jiqiang Liu and Zhen Han and Gang Li",
    "title": "Gradient Band-based Adversarial Training for Generalized Attack Immunity\n  of A3C Path Finding",
    "comments": "25 pages 14 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As adversarial attacks pose a serious threat to the security of AI system in\npractice, such attacks have been extensively studied in the context of computer\nvision applications. However, few attentions have been paid to the adversarial\nresearch on automatic path finding. In this paper, we show dominant adversarial\nexamples are effective when targeting A3C path finding, and design a Common\nDominant Adversarial Examples Generation Method (CDG) to generate dominant\nadversarial examples against any given map. In addition, we propose Gradient\nBand-based Adversarial Training, which trained with a single randomly choose\ndominant adversarial example without taking any modification, to realize the\n\"1:N\" attack immunity for generalized dominant adversarial examples. Extensive\nexperimental results show that, the lowest generation precision for CDG\nalgorithm is 91.91%, and the lowest immune precision for Gradient Band-based\nAdversarial Training is 93.89%, which can prove that our method can realize the\ngeneralized attack immunity of A3C path finding with a high confidence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 18 Jul 2018 02:57:16 GMT"
      }
    ],
    "update_date": "2018-07-19",
    "authors_parsed": [
      [
        "Chen",
        "Tong",
        ""
      ],
      [
        "Niu",
        "Wenjia",
        ""
      ],
      [
        "Xiang",
        "Yingxiao",
        ""
      ],
      [
        "Bai",
        "Xiaoxuan",
        ""
      ],
      [
        "Liu",
        "Jiqiang",
        ""
      ],
      [
        "Han",
        "Zhen",
        ""
      ],
      [
        "Li",
        "Gang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1807.06752",
    "publish_date": "2018-07-18"
  },
  {
    "id": "1807.07418",
    "submitter": "Sudip Mittal",
    "authors": "Nitika Khurana, Sudip Mittal, Anupam Joshi",
    "title": "Preventing Poisoning Attacks on AI based Threat Intelligence Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As AI systems become more ubiquitous, securing them becomes an emerging\nchallenge. Over the years, with the surge in online social media use and the\ndata available for analysis, AI systems have been built to extract, represent\nand use this information. The credibility of this information extracted from\nopen sources, however, can often be questionable. Malicious or incorrect\ninformation can cause a loss of money, reputation, and resources; and in\ncertain situations, pose a threat to human life. In this paper, we use an\nensembled semi-supervised approach to determine the credibility of Reddit posts\nby estimating their reputation score to ensure the validity of information\ningested by AI systems. We demonstrate our approach in the cybersecurity\ndomain, where security analysts utilize these systems to determine possible\nthreats by analyzing the data scattered on social media websites, forums,\nblogs, etc.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Jul 2018 13:40:37 GMT"
      }
    ],
    "update_date": "2018-07-20",
    "authors_parsed": [
      [
        "Khurana",
        "Nitika",
        ""
      ],
      [
        "Mittal",
        "Sudip",
        ""
      ],
      [
        "Joshi",
        "Anupam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1807.07418",
    "publish_date": "2018-07-19"
  },
  {
    "id": "1807.11655",
    "submitter": "Ho Bae",
    "authors": "Ho Bae, Jaehee Jang, Dahuin Jung, Hyemi Jang, Heonseok Ha, Hyungyu\n  Lee, Sungroh Yoon",
    "title": "Security and Privacy Issues in Deep Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  To promote secure and private artificial intelligence (SPAI), we review\nstudies on the model security and data privacy of DNNs. Model security allows\nsystem to behave as intended without being affected by malicious external\ninfluences that can compromise its integrity and efficiency. Security attacks\ncan be divided based on when they occur: if an attack occurs during training,\nit is known as a poisoning attack, and if it occurs during inference (after\ntraining) it is termed an evasion attack. Poisoning attacks compromise the\ntraining process by corrupting the data with malicious examples, while evasion\nattacks use adversarial examples to disrupt entire classification process.\nDefenses proposed against such attacks include techniques to recognize and\nremove malicious data, train a model to be insensitive to such data, and mask\nthe model's structure and parameters to render attacks more challenging to\nimplement. Furthermore, the privacy of the data involved in model training is\nalso threatened by attacks such as the model-inversion attack, or by dishonest\nservice providers of AI applications. To maintain data privacy, several\nsolutions that combine existing data-privacy techniques have been proposed,\nincluding differential privacy and modern cryptography techniques. In this\npaper, we describe the notions of some of methods, e.g., homomorphic\nencryption, and review their advantages and challenges when implemented in\ndeep-learning models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 31 Jul 2018 04:18:26 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 6 Dec 2018 07:35:31 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 23 Nov 2019 17:25:45 GMT"
      },
      {
        "version": "v4",
        "created": "Wed, 10 Mar 2021 00:55:18 GMT"
      }
    ],
    "update_date": "2021-03-11",
    "authors_parsed": [
      [
        "Bae",
        "Ho",
        ""
      ],
      [
        "Jang",
        "Jaehee",
        ""
      ],
      [
        "Jung",
        "Dahuin",
        ""
      ],
      [
        "Jang",
        "Hyemi",
        ""
      ],
      [
        "Ha",
        "Heonseok",
        ""
      ],
      [
        "Lee",
        "Hyungyu",
        ""
      ],
      [
        "Yoon",
        "Sungroh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1807.11655",
    "publish_date": "2021-03-10"
  },
  {
    "id": "1809.04790",
    "submitter": "Jiliang Zhang",
    "authors": "Jiliang Zhang and Chen Li",
    "title": "Adversarial Examples: Opportunities and Challenges",
    "comments": "16 pages, 13 figures, 5 tables",
    "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (2019)",
    "doi": "10.1109/TNNLS.2019.2933524",
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) have shown huge superiority over humans in image\nrecognition, speech processing, autonomous vehicles and medical diagnosis.\nHowever, recent studies indicate that DNNs are vulnerable to adversarial\nexamples (AEs), which are designed by attackers to fool deep learning models.\nDifferent from real examples, AEs can mislead the model to predict incorrect\noutputs while hardly be distinguished by human eyes, therefore threaten\nsecurity-critical deep-learning applications. In recent years, the generation\nand defense of AEs have become a research hotspot in the field of artificial\nintelligence (AI) security. This article reviews the latest research progress\nof AEs. First, we introduce the concept, cause, characteristics and evaluation\nmetrics of AEs, then give a survey on the state-of-the-art AE generation\nmethods with the discussion of advantages and disadvantages. After that, we\nreview the existing defenses and discuss their limitations. Finally, future\nresearch opportunities and challenges on AEs are prospected.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 13 Sep 2018 06:09:32 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 29 Apr 2019 11:09:01 GMT"
      },
      {
        "version": "v3",
        "created": "Sun, 4 Aug 2019 04:54:27 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 23 Sep 2019 13:04:37 GMT"
      }
    ],
    "update_date": "2019-09-24",
    "authors_parsed": [
      [
        "Zhang",
        "Jiliang",
        ""
      ],
      [
        "Li",
        "Chen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1809.04790",
    "publish_date": "2018-09-13"
  },
  {
    "id": "1810.08070",
    "submitter": "Yingdi Wang",
    "authors": "Yingdi Wang, Wenjia Niu, Tong Chen, Yingxiao Xiang, Jingjing Liu, Gang\n  Li, and Jiqiang Liu",
    "title": "A Training-based Identification Approach to VIN Adversarial Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the rapid development of Artificial Intelligence (AI), the problem of AI\nsecurity has gradually emerged. Most existing machine learning algorithms may\nbe attacked by adversarial examples. An adversarial example is a slightly\nmodified input sample that can lead to a false result of machine learning\nalgorithms. The adversarial examples pose a potential security threat for many\nAI application areas, especially in the domain of robot path planning. In this\nfield, the adversarial examples obstruct the algorithm by adding obstacles to\nthe normal maps, resulting in multiple effects on the predicted path. However,\nthere is no suitable approach to automatically identify them. To our knowledge,\nall previous work uses manual observation method to estimate the attack results\nof adversarial maps, which is time-consuming. Aiming at the existing problem,\nthis paper explores a method to automatically identify the adversarial examples\nin Value Iteration Networks (VIN), which has a strong generalization ability.\nWe analyze the possible scenarios caused by the adversarial maps. We propose a\ntraining-based identification approach to VIN adversarial examples by combing\nthe path feature comparison and path image classification. We evaluate our\nmethod using the adversarial maps dataset, show that our method can achieve a\nhigh-accuracy and faster identification than manual observation method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 Oct 2018 14:17:12 GMT"
      }
    ],
    "update_date": "2018-10-19",
    "authors_parsed": [
      [
        "Wang",
        "Yingdi",
        ""
      ],
      [
        "Niu",
        "Wenjia",
        ""
      ],
      [
        "Chen",
        "Tong",
        ""
      ],
      [
        "Xiang",
        "Yingxiao",
        ""
      ],
      [
        "Liu",
        "Jingjing",
        ""
      ],
      [
        "Li",
        "Gang",
        ""
      ],
      [
        "Liu",
        "Jiqiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1810.08070",
    "publish_date": "2018-10-18"
  },
  {
    "id": "1811.00189",
    "submitter": "Jiayang Liu",
    "authors": "Jiayang Liu, Weiming Zhang, Kazuto Fukuchi, Youhei Akimoto, Jun Sakuma",
    "title": "Unauthorized AI cannot Recognize Me: Reversible Adversarial Example",
    "comments": "arXiv admin note: text overlap with arXiv:1806.09186",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this study, we propose a new methodology to control how user's data is\nrecognized and used by AI via exploiting the properties of adversarial\nexamples. For this purpose, we propose reversible adversarial example (RAE), a\nnew type of adversarial example. A remarkable feature of RAE is that the image\ncan be correctly recognized and used by the AI model specified by the user\nbecause the authorized AI can recover the original image from the RAE exactly\nby eliminating adversarial perturbation. On the other hand, other unauthorized\nAI models cannot recognize it correctly because it functions as an adversarial\nexample. Moreover, RAE can be considered as one type of encryption to computer\nvision since reversibility guarantees the decryption. To realize RAE, we\ncombine three technologies, adversarial example, reversible data hiding for\nexact recovery of adversarial perturbation, and encryption for selective\ncontrol of AIs who can remove adversarial perturbation. Experimental results\nshow that the proposed method can achieve comparable attack ability with the\ncorresponding adversarial attack method and similar visual quality with the\noriginal image, including white-box attacks and black-box attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 1 Nov 2018 02:28:31 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 28 Nov 2018 14:30:54 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 8 Oct 2021 17:42:59 GMT"
      }
    ],
    "update_date": "2021-10-12",
    "authors_parsed": [
      [
        "Liu",
        "Jiayang",
        ""
      ],
      [
        "Zhang",
        "Weiming",
        ""
      ],
      [
        "Fukuchi",
        "Kazuto",
        ""
      ],
      [
        "Akimoto",
        "Youhei",
        ""
      ],
      [
        "Sakuma",
        "Jun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1811.00189",
    "publish_date": "2018-11-28"
  },
  {
    "id": "1811.00656",
    "submitter": "Yuezun Li",
    "authors": "Yuezun Li and Siwei Lyu",
    "title": "Exposing DeepFake Videos By Detecting Face Warping Artifacts",
    "comments": "CVPRW 2019",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this work, we describe a new deep learning based method that can\neffectively distinguish AI-generated fake videos (referred to as {\\em DeepFake}\nvideos hereafter) from real videos. Our method is based on the observations\nthat current DeepFake algorithm can only generate images of limited\nresolutions, which need to be further warped to match the original faces in the\nsource video. Such transforms leave distinctive artifacts in the resulting\nDeepFake videos, and we show that they can be effectively captured by\nconvolutional neural networks (CNNs). Compared to previous methods which use a\nlarge amount of real and DeepFake generated images to train CNN classifier, our\nmethod does not need DeepFake generated images as negative training examples\nsince we target the artifacts in affine face warping as the distinctive feature\nto distinguish real and fake images. The advantages of our method are two-fold:\n(1) Such artifacts can be simulated directly using simple image processing\noperations on a image to make it as negative example. Since training a DeepFake\nmodel to generate negative examples is time-consuming and resource-demanding,\nour method saves a plenty of time and resources in training data collection;\n(2) Since such artifacts are general existed in DeepFake videos from different\nsources, our method is more robust compared to others. Our method is evaluated\non two sets of DeepFake video datasets for its effectiveness in practice.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 1 Nov 2018 22:13:55 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 29 Mar 2019 01:20:52 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 22 May 2019 15:06:39 GMT"
      }
    ],
    "update_date": "2019-05-23",
    "authors_parsed": [
      [
        "Li",
        "Yuezun",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1811.00656",
    "publish_date": "2019-05-22"
  },
  {
    "id": "1811.01431",
    "submitter": "Dianbo Liu Dr",
    "authors": "Shifa Zhang, Anne Kim, Dianbo Liu, Sandeep C. Nuckchady, Lauren Huang,\n  Aditya Masurkar, Jingwei Zhang, Lawrence Tseng, Pratheek Karnati, Laura\n  Martinez, Thomas Hardjono, Manolis Kellis, Zhizhuo Zhang",
    "title": "Genie: A Secure, Transparent Sharing and Services Platform for Genetic\n  and Health Data",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial Intelligence (AI) incorporating genetic and medical information\nhave been applied in disease risk prediction, unveiling disease mechanism, and\nadvancing therapeutics. However, AI training relies on highly sensitive and\nprivate data which significantly limit their applications and robustness\nevaluation. Moreover, the data access management after sharing across\norganization heavily relies on legal restriction, and there is no guarantee in\npreventing data leaking after sharing. Here, we present Genie, a secure AI\nplatform which allows AI models to be trained on medical data securely. The\nplatform combines the security of Intel Software Guarded eXtensions (SGX),\ntransparency of blockchain technology, and verifiability of open algorithms and\nsource codes. Genie shares insights of genetic and medical data without\nexposing anyone's raw data. All data is instantly encrypted upon upload and\ncontributed to the models that the user chooses. The usage of the model and the\nvalue generated from the genetic and health data will be tracked via a\nblockchain, giving the data transparent and immutable ownership.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 4 Nov 2018 20:43:53 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 30 Mar 2020 21:29:50 GMT"
      }
    ],
    "update_date": "2020-04-01",
    "authors_parsed": [
      [
        "Zhang",
        "Shifa",
        ""
      ],
      [
        "Kim",
        "Anne",
        ""
      ],
      [
        "Liu",
        "Dianbo",
        ""
      ],
      [
        "Nuckchady",
        "Sandeep C.",
        ""
      ],
      [
        "Huang",
        "Lauren",
        ""
      ],
      [
        "Masurkar",
        "Aditya",
        ""
      ],
      [
        "Zhang",
        "Jingwei",
        ""
      ],
      [
        "Tseng",
        "Lawrence",
        ""
      ],
      [
        "Karnati",
        "Pratheek",
        ""
      ],
      [
        "Martinez",
        "Laura",
        ""
      ],
      [
        "Hardjono",
        "Thomas",
        ""
      ],
      [
        "Kellis",
        "Manolis",
        ""
      ],
      [
        "Zhang",
        "Zhizhuo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1811.01431",
    "publish_date": "2018-11-04"
  },
  {
    "id": "1811.03571",
    "submitter": "Luca Bortolussi",
    "authors": "Luca Bortolussi and Guido Sanguinetti",
    "title": "Intrinsic Geometric Vulnerability of High-Dimensional Artificial\n  Intelligence",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The success of modern Artificial Intelligence (AI) technologies depends\ncritically on the ability to learn non-linear functional dependencies from\nlarge, high dimensional data sets. Despite recent high-profile successes,\nempirical evidence indicates that the high predictive performance is often\npaired with low robustness, making AI systems potentially vulnerable to\nadversarial attacks. In this report, we provide a simple intuitive argument\nsuggesting that high performance and vulnerability are intrinsically coupled,\nand largely dependent on the geometry of typical, high-dimensional data sets.\nOur work highlights a major potential pitfall of modern AI systems, and\nsuggests practical research directions to ameliorate the problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Nov 2018 17:51:27 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 24 Jan 2019 14:13:58 GMT"
      }
    ],
    "update_date": "2019-01-25",
    "authors_parsed": [
      [
        "Bortolussi",
        "Luca",
        ""
      ],
      [
        "Sanguinetti",
        "Guido",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1811.03571",
    "publish_date": "2018-11-08"
  },
  {
    "id": "1811.07767",
    "submitter": "Anton Becker",
    "authors": "Anton S. Becker, Lukas Jendele, Ondrej Skopek, Nicole Berger, Soleen\n  Ghafoor, Magda Marcon, Ender Konukoglu",
    "title": "Injecting and removing malignant features in mammography with CycleGAN:\n  Investigation of an automated adversarial attack using neural networks",
    "comments": "To be presented at RSNA 2018",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  $\\textbf{Purpose}$ To train a cycle-consistent generative adversarial network\n(CycleGAN) on mammographic data to inject or remove features of malignancy, and\nto determine whether these AI-mediated attacks can be detected by radiologists.\n$\\textbf{Material and Methods}$ From the two publicly available datasets, BCDR\nand INbreast, we selected images from cancer patients and healthy controls. An\ninternal dataset served as test data, withheld during training. We ran two\nexperiments training CycleGAN on low and higher resolution images ($256 \\times\n256$ px and $512 \\times 408$ px). Three radiologists read the images and rated\nthe likelihood of malignancy on a scale from 1-5 and the likelihood of the\nimage being manipulated. The readout was evaluated by ROC analysis (Area under\nthe ROC curve = AUC). $\\textbf{Results}$ At the lower resolution, only one\nradiologist exhibited markedly lower detection of cancer (AUC=0.85 vs 0.63,\np=0.06), while the other two were unaffected (0.67 vs. 0.69 and 0.75 vs. 0.77,\np=0.55). Only one radiologist could discriminate between original and modified\nimages slightly better than guessing/chance (0.66, p=0.008). At the higher\nresolution, all radiologists showed significantly lower detection rate of\ncancer in the modified images (0.77-0.84 vs. 0.59-0.69, p=0.008), however, they\nwere now able to reliably detect modified images due to better visibility of\nartifacts (0.92, 0.92 and 0.97). $\\textbf{Conclusion}$ A CycleGAN can\nimplicitly learn malignant features and inject or remove them so that a\nsubstantial proportion of small mammographic images would consequently be\nmisdiagnosed. At higher resolutions, however, the method is currently limited\nand has a clear trade-off between manipulation of images and introduction of\nartifacts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 19 Nov 2018 16:08:11 GMT"
      }
    ],
    "update_date": "2018-11-20",
    "authors_parsed": [
      [
        "Becker",
        "Anton S.",
        ""
      ],
      [
        "Jendele",
        "Lukas",
        ""
      ],
      [
        "Skopek",
        "Ondrej",
        ""
      ],
      [
        "Berger",
        "Nicole",
        ""
      ],
      [
        "Ghafoor",
        "Soleen",
        ""
      ],
      [
        "Marcon",
        "Magda",
        ""
      ],
      [
        "Konukoglu",
        "Ender",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1811.07767",
    "publish_date": "2018-11-19"
  },
  {
    "id": "1812.00622",
    "submitter": "Jean-Philippe Fauvelle",
    "authors": "Jean-Philippe Fauvelle, Alexandre Dey, Sylvain Navers",
    "title": "Protection of an information system by artificial intelligence: a\n  three-phase approach based on behaviour analysis to detect a hostile scenario",
    "comments": "in French. European Cyber Week - C\\&ESAR Conference - Artificial\n  Intelligence and Cybersecurity, Nov 2018, Rennes, France. 2018",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.LG cs.NE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The analysis of the behaviour of individuals and entities (UEBA) is an area\nof artificial intelligence that detects hostile actions (e.g. attacks, fraud,\ninfluence, poisoning) due to the unusual nature of observed events, by affixing\nto a signature-based operation. A UEBA process usually involves two phases,\nlearning and inference. Intrusion detection systems (IDS) available still\nsuffer from bias, including over-simplification of problems, underexploitation\nof the AI potential, insufficient consideration of the temporality of events,\nand perfectible management of the memory cycle of behaviours. In addition,\nwhile an alert generated by a signature-based IDS can refer to the signature on\nwhich the detection is based, the IDS in the UEBA domain produce results, often\nassociated with a score, whose explainable character is less obvious. Our\nunsupervised approach is to enrich this process by adding a third phase to\ncorrelate events (incongruities, weak signals) that are presumed to be linked\ntogether, with the benefit of a reduction of false positives and negatives. We\nalso seek to avoid a so-called \"boiled frog\" bias inherent in continuous\nlearning. Our first results are interesting and have an explainable character,\nboth on synthetic and real data.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Dec 2018 09:29:03 GMT"
      }
    ],
    "update_date": "2018-12-04",
    "authors_parsed": [
      [
        "Fauvelle",
        "Jean-Philippe",
        ""
      ],
      [
        "Dey",
        "Alexandre",
        ""
      ],
      [
        "Navers",
        "Sylvain",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1812.00622",
    "publish_date": "2018-12-03"
  },
  {
    "id": "1812.02575",
    "submitter": "Andrey Malinin",
    "authors": "Andrey Malinin and Mark Gales",
    "title": "Prior Networks for Detection of Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "stat.ML cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial examples are considered a serious issue for safety critical\napplications of AI, such as finance, autonomous vehicle control and medicinal\napplications. Though significant work has resulted in increased robustness of\nsystems to these attacks, systems are still vulnerable to well-crafted attacks.\nTo address this problem, several adversarial attack detection methods have been\nproposed. However, a system can still be vulnerable to adversarial samples that\nare designed to specifically evade these detection methods. One recent\ndetection scheme that has shown good performance is based on uncertainty\nestimates derived from Monte-Carlo dropout ensembles. Prior Networks, a new\nmethod of estimating predictive uncertainty, has been shown to outperform\nMonte-Carlo dropout on a range of tasks. One of the advantages of this approach\nis that the behaviour of a Prior Network can be explicitly tuned to, for\nexample, predict high uncertainty in regions where there are no training data\nsamples. In this work, Prior Networks are applied to adversarial attack\ndetection using measures of uncertainty in a similar fashion to Monte-Carlo\nDropout. Detection based on measures of uncertainty derived from DNNs and\nMonte-Carlo dropout ensembles are used as a baseline. Prior Networks are shown\nto significantly out-perform these baseline approaches over a range of\nadversarial attacks in both detection of whitebox and blackbox configurations.\nEven when the adversarial attacks are constructed with full knowledge of the\ndetection mechanism, it is shown to be highly challenging to successfully\ngenerate an adversarial sample.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 6 Dec 2018 14:59:29 GMT"
      }
    ],
    "update_date": "2018-12-08",
    "authors_parsed": [
      [
        "Malinin",
        "Andrey",
        ""
      ],
      [
        "Gales",
        "Mark",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1812.02575",
    "publish_date": "2018-12-06"
  },
  {
    "id": "1812.06815",
    "submitter": "Fran\\c{c}ois Menet",
    "authors": "Fran\\c{c}ois Menet, Paul Berthier, Jos\\'e M. Fernandez, Michel Gagnon",
    "title": "Spartan Networks: Self-Feature-Squeezing Neural Networks for increased\n  robustness in adversarial settings",
    "comments": "Poster previously accepted at ACM CCS 2018 Toronto, Submitted to\n  Computers & Security",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning models are vulnerable to adversarial examples which are input\nsamples modified in order to maximize the error on the system. We introduce\nSpartan Networks, resistant deep neural networks that do not require input\npreprocessing nor adversarial training. These networks have an adversarial\nlayer designed to discard some information of the network, thus forcing the\nsystem to focus on relevant input. This is done using a new activation function\nto discard data. The added layer trains the neural network to filter-out\nusually-irrelevant parts of its input. Our performance evaluation shows that\nSpartan Networks have a slightly lower precision but report a higher robustness\nunder attack when compared to unprotected models. Results of this study of\nAdversarial AI as a new attack vector are based on tests conducted on the MNIST\ndataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 17 Dec 2018 14:55:41 GMT"
      }
    ],
    "update_date": "2018-12-18",
    "authors_parsed": [
      [
        "Menet",
        "Fran\u00e7ois",
        ""
      ],
      [
        "Berthier",
        "Paul",
        ""
      ],
      [
        "Fernandez",
        "Jos\u00e9 M.",
        ""
      ],
      [
        "Gagnon",
        "Michel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1812.06815",
    "publish_date": "2018-12-17"
  },
  {
    "id": "1901.06796",
    "submitter": "Wei Emma Zhang",
    "authors": "Wei Emma Zhang, Quan Z. Sheng, Ahoud Alhazmi, and Chenliang Li",
    "title": "Adversarial Attacks on Deep Learning Models in Natural Language\n  Processing: A Survey",
    "comments": "40",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the development of high computational devices, deep neural networks\n(DNNs), in recent years, have gained significant popularity in many Artificial\nIntelligence (AI) applications. However, previous efforts have shown that DNNs\nwere vulnerable to strategically modified samples, named adversarial examples.\nThese samples are generated with some imperceptible perturbations but can fool\nthe DNNs to give false predictions. Inspired by the popularity of generating\nadversarial examples for image DNNs, research efforts on attacking DNNs for\ntextual applications emerges in recent years. However, existing perturbation\nmethods for images cannotbe directly applied to texts as text data is discrete.\nIn this article, we review research works that address this difference and\ngeneratetextual adversarial examples on DNNs. We collect, select, summarize,\ndiscuss and analyze these works in a comprehensive way andcover all the related\ninformation to make the article self-contained. Finally, drawing on the\nreviewed literature, we provide further discussions and suggestions on this\ntopic.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Jan 2019 05:55:42 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 27 Jan 2019 02:02:58 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 11 Apr 2019 00:04:22 GMT"
      }
    ],
    "update_date": "2019-04-12",
    "authors_parsed": [
      [
        "Zhang",
        "Wei Emma",
        ""
      ],
      [
        "Sheng",
        "Quan Z.",
        ""
      ],
      [
        "Alhazmi",
        "Ahoud",
        ""
      ],
      [
        "Li",
        "Chenliang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1901.06796",
    "publish_date": "2019-04-11"
  },
  {
    "id": "1904.00859",
    "submitter": "Nicholas Kolokotronis",
    "authors": "Irina Baptista, Stavros Shiaeles, Nicholas Kolokotronis",
    "title": "A Novel Malware Detection System Based On Machine Learning and Binary\n  Visualization",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The continued evolution and diversity of malware constitutes a major threat\nin modern systems. It is well proven that security defenses currently available\nare ineffective to mitigate the skills and imagination of cyber-criminals\nnecessitating the development of novel solutions. Deep learning algorithms and\nartificial intelligence (AI) are rapidly evolving with remarkable results in\nmany application areas. Following the advances of AI and recognizing the need\nfor efficient malware detection methods, this paper presents a new approach for\nmalware detection based on binary visualization and self-organizing incremental\nneural networks. The proposed method's performance in detecting malicious\npayloads in various file types was investigated and the experimental results\nshowed that a detection accuracy of 91.7% and 94.1% was achieved for ransomware\nin .pdf and .doc files respectively. With respect to other formats of malicious\ncode and other file types, including binaries, the proposed method behaved well\nwith an incremental detection rate that allows efficiently detecting unknown\nmalware at real-time.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 Apr 2019 13:59:48 GMT"
      }
    ],
    "update_date": "2019-04-02",
    "authors_parsed": [
      [
        "Baptista",
        "Irina",
        ""
      ],
      [
        "Shiaeles",
        "Stavros",
        ""
      ],
      [
        "Kolokotronis",
        "Nicholas",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1904.00859",
    "publish_date": "2019-04-01"
  },
  {
    "id": "1904.01508",
    "submitter": "Stavros Shiaeles Dr",
    "authors": "Michael Siracusano, Stavros Shiaeles, Bogdan Ghita",
    "title": "Detection of LDDoS Attacks Based on TCP Connection Parameters",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1109/GIIS.2018.8635701",
    "report-no": null,
    "categories": "cs.NI cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Low-rate application layer distributed denial of service (LDDoS) attacks are\nboth powerful and stealthy. They force vulnerable webservers to open all\navailable connections to the adversary, denying resources to real users.\nMitigation advice focuses on solutions that potentially degrade quality of\nservice for legitimate connections. Furthermore, without accurate detection\nmechanisms, distributed attacks can bypass these defences. A methodology for\ndetection of LDDoS attacks, based on characteristics of malicious TCP flows, is\nproposed within this paper. Research will be conducted using combinations of\ntwo datasets: one generated from a simulated network, the other from the\npublically available CIC DoS dataset. Both contain the attacks slowread,\nslowheaders and slowbody, alongside legitimate web browsing. TCP flow features\nare extracted from all connections. Experimentation was carried out using six\nsupervised AI algorithms to categorise attack from legitimate flows. Decision\ntrees and k-NN accurately classified up to 99.99% of flows, with exceptionally\nlow false positive and false negative rates, demonstrating the potential of AI\nin LDDoS detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Mar 2019 16:56:17 GMT"
      }
    ],
    "update_date": "2019-04-03",
    "authors_parsed": [
      [
        "Siracusano",
        "Michael",
        ""
      ],
      [
        "Shiaeles",
        "Stavros",
        ""
      ],
      [
        "Ghita",
        "Bogdan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1904.01508",
    "publish_date": "2019-03-12"
  },
  {
    "id": "1905.05137",
    "submitter": "Olakunle Ibitoye",
    "authors": "Olakunle Ibitoye, Omair Shafiq and Ashraf Matrawy",
    "title": "Analyzing Adversarial Attacks Against Deep Learning for Intrusion\n  Detection in IoT Networks",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks have been widely studied in the field of computer vision\nbut their impact on network security applications remains an area of open\nresearch. As IoT, 5G and AI continue to converge to realize the promise of the\nfourth industrial revolution (Industry 4.0), security incidents and events on\nIoT networks have increased. Deep learning techniques are being applied to\ndetect and mitigate many of such security threats against IoT networks.\nFeedforward Neural Networks (FNN) have been widely used for classifying\nintrusion attacks in IoT networks. In this paper, we consider a variant of the\nFNN known as the Self-normalizing Neural Network (SNN) and compare its\nperformance with the FNN for classifying intrusion attacks in an IoT network.\nOur analysis is performed using the BoT-IoT dataset from the Cyber Range Lab of\nthe center of UNSW Canberra Cyber. In our experimental results, the FNN\noutperforms the SNN for intrusion detection in IoT networks based on multiple\nperformance metrics such as accuracy, precision, and recall as well as\nmulti-classification metrics such as Cohen's Kappa score. However, when tested\nfor adversarial robustness, the SNN demonstrates better resilience against the\nadversarial samples from the IoT dataset, presenting a promising future in the\nquest for safer and more secure deep learning in IoT networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 May 2019 16:43:14 GMT"
      }
    ],
    "update_date": "2019-05-14",
    "authors_parsed": [
      [
        "Ibitoye",
        "Olakunle",
        ""
      ],
      [
        "Shafiq",
        "Omair",
        ""
      ],
      [
        "Matrawy",
        "Ashraf",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1905.05137",
    "publish_date": "2019-05-13"
  },
  {
    "id": "1905.11381",
    "submitter": "Jirong Yi",
    "authors": "Jirong Yi, Hui Xie, Leixin Zhou, Xiaodong Wu, Weiyu Xu, Raghuraman\n  Mudumbai",
    "title": "Trust but Verify: An Information-Theoretic Explanation for the\n  Adversarial Fragility of Machine Learning Systems, and a General Defense\n  against Adversarial Attacks",
    "comments": "44 Pages, 2 Theorems, 35 Figures, 29 Tables. arXiv admin note:\n  substantial text overlap with arXiv:1901.09413",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep-learning based classification algorithms have been shown to be\nsusceptible to adversarial attacks: minor changes to the input of classifiers\ncan dramatically change their outputs, while being imperceptible to humans. In\nthis paper, we present a simple hypothesis about a feature compression property\nof artificial intelligence (AI) classifiers and present theoretical arguments\nto show that this hypothesis successfully accounts for the observed fragility\nof AI classifiers to small adversarial perturbations. Drawing on ideas from\ninformation and coding theory, we propose a general class of defenses for\ndetecting classifier errors caused by abnormally small input perturbations. We\nfurther show theoretical guarantees for the performance of this detection\nmethod. We present experimental results with (a) a voice recognition system,\nand (b) a digit recognition system using the MNIST database, to demonstrate the\neffectiveness of the proposed defense methods. The ideas in this paper are\nmotivated by a simple analogy between AI classifiers and the standard Shannon\nmodel of a communication system.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 25 May 2019 21:57:51 GMT"
      }
    ],
    "update_date": "2019-05-29",
    "authors_parsed": [
      [
        "Yi",
        "Jirong",
        ""
      ],
      [
        "Xie",
        "Hui",
        ""
      ],
      [
        "Zhou",
        "Leixin",
        ""
      ],
      [
        "Wu",
        "Xiaodong",
        ""
      ],
      [
        "Xu",
        "Weiyu",
        ""
      ],
      [
        "Mudumbai",
        "Raghuraman",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1905.11381",
    "publish_date": "2019-05-25"
  },
  {
    "id": "1905.13652",
    "submitter": "S. Asim Ahmed",
    "authors": "S. Asim Ahmed",
    "title": "L0 Regularization Based Neural Network Design and Compression",
    "comments": "4 pages 11 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  We consider complexity of Deep Neural Networks (DNNs) and their associated\nmassive over-parameterization. Such over-parametrization may entail\nsusceptibility to adversarial attacks, loss of interpretability and adverse\nSize, Weight and Power - Cost (SWaP-C) considerations. We ask if there are\nmethodical ways (regularization) to reduce complexity and how can we interpret\ntrade-off between desired metric and complexity of DNN. Reducing complexity is\ndirectly applicable to scaling of AI applications to real world problems\n(especially for off-the-cloud applications). We show that presence and\nevaluation of the knee of the tradeoff curve. We apply a form of L0\nregularization to MNIST data and signal modulation classifications. We show\nthat such regularization captures saliency in the input space as well.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 31 May 2019 14:53:30 GMT"
      }
    ],
    "update_date": "2019-06-03",
    "authors_parsed": [
      [
        "Ahmed",
        "S. Asim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1905.13652",
    "publish_date": "2019-05-31"
  },
  {
    "id": "1906.01478",
    "submitter": "Vegard Antun",
    "authors": "Laura Thesing, Vegard Antun and Anders C. Hansen",
    "title": "What do AI algorithms actually learn? - On false structures in deep\n  learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "stat.ML cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  There are two big unsolved mathematical questions in artificial intelligence\n(AI): (1) Why is deep learning so successful in classification problems and (2)\nwhy are neural nets based on deep learning at the same time universally\nunstable, where the instabilities make the networks vulnerable to adversarial\nattacks. We present a solution to these questions that can be summed up in two\nwords; false structures. Indeed, deep learning does not learn the original\nstructures that humans use when recognising images (cats have whiskers, paws,\nfur, pointy ears, etc), but rather different false structures that correlate\nwith the original structure and hence yield the success. However, the false\nstructure, unlike the original structure, is unstable. The false structure is\nsimpler than the original structure, hence easier to learn with less data and\nthe numerical algorithm used in the training will more easily converge to the\nneural network that captures the false structure. We formally define the\nconcept of false structures and formulate the solution as a conjecture. Given\nthat trained neural networks always are computed with approximations, this\nconjecture can only be established through a combination of theoretical and\ncomputational results similar to how one establishes a postulate in theoretical\nphysics (e.g. the speed of light is constant). Establishing the conjecture\nfully will require a vast research program characterising the false structures.\nWe provide the foundations for such a program establishing the existence of the\nfalse structures in practice. Finally, we discuss the far reaching consequences\nthe existence of the false structures has on state-of-the-art AI and Smale's\n18th problem.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Jun 2019 14:35:32 GMT"
      }
    ],
    "update_date": "2019-06-05",
    "authors_parsed": [
      [
        "Thesing",
        "Laura",
        ""
      ],
      [
        "Antun",
        "Vegard",
        ""
      ],
      [
        "Hansen",
        "Anders C.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1906.01478",
    "publish_date": "2019-06-04"
  },
  {
    "id": "1906.03466",
    "submitter": "Rajagopal A",
    "authors": "Rajagopal. A, Nirmala. V",
    "title": "Strategies to architect AI Safety: Defense to guard AI from Adversaries",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The impact of designing for security of AI is critical for humanity in the AI\nera. With humans increasingly becoming dependent upon AI, there is a need for\nneural networks that work reliably, inspite of Adversarial attacks. The vision\nfor Safe and secure AI for popular use is achievable. To achieve safety of AI,\nthis paper explores strategies and a novel deep learning architecture. To guard\nAI from adversaries, paper explores combination of 3 strategies:\n  1. Introduce randomness at inference time to hide the representation learning\nfrom adversaries.\n  2. Detect presence of adversaries by analyzing the sequence of inferences.\n  3. Exploit visual similarity.\n  To realize these strategies, this paper designs a novel architecture, Dynamic\nNeural Defense, DND. This defense has 3 deep learning architectural features:\n  1. By hiding the way a neural network learns from exploratory attacks using a\nrandom computation graph, DND evades attack.\n  2. By analyzing input sequence to cloud AI inference engine with LSTM, DND\ndetects attack sequence.\n  3. By inferring with visual similar inputs generated by VAE, any AI defended\nby DND approach does not succumb to hackers.\n  Thus, a roadmap to develop reliable, safe and secure AI is presented.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 8 Jun 2019 14:34:47 GMT"
      }
    ],
    "update_date": "2019-06-11",
    "authors_parsed": [
      [
        "A",
        "Rajagopal.",
        ""
      ],
      [
        "V",
        "Nirmala.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1906.03466",
    "publish_date": "2019-06-08"
  },
  {
    "id": "1907.01297",
    "submitter": "Ekaterina Komendantskaya Dr",
    "authors": "Ekaterina Komendantskaya and Rob Stewart and Kirsy Duncan and Daniel\n  Kienitz and Pierre Le Hen and Pascal Bacchus",
    "title": "Neural Network Verification for the Masses (of AI graduates)",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.PL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Rapid development of AI applications has stimulated demand for, and has given\nrise to, the rapidly growing number and diversity of AI MSc degrees. AI and\nRobotics research communities, industries and students are becoming\nincreasingly aware of the problems caused by unsafe or insecure AI\napplications. Among them, perhaps the most famous example is vulnerability of\ndeep neural networks to ``adversarial attacks''. Owing to wide-spread use of\nneural networks in all areas of AI, this problem is seen as particularly acute\nand pervasive.\n  Despite of the growing number of research papers about safety and security\nvulnerabilities of AI applications, there is a noticeable shortage of\naccessible tools, methods and teaching materials for incorporating verification\ninto AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened\nresearch lab at Heriot-Watt university that engages AI and Robotics MSc\nstudents in verification projects, as part of their MSc dissertation work. In\nthis paper, we will report on successes and unexpected difficulties LAIV faces,\nmany of which arise from limitations of existing programming languages used for\nverification. We will discuss future directions for incorporating verification\ninto AI degrees.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 2 Jul 2019 11:09:04 GMT"
      }
    ],
    "update_date": "2019-07-03",
    "authors_parsed": [
      [
        "Komendantskaya",
        "Ekaterina",
        ""
      ],
      [
        "Stewart",
        "Rob",
        ""
      ],
      [
        "Duncan",
        "Kirsy",
        ""
      ],
      [
        "Kienitz",
        "Daniel",
        ""
      ],
      [
        "Hen",
        "Pierre Le",
        ""
      ],
      [
        "Bacchus",
        "Pascal",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.01297",
    "publish_date": "2019-07-02"
  },
  {
    "id": "1907.04774",
    "submitter": "Rohan Reddy Mekala",
    "authors": "Rohan Reddy Mekala, Gudjon Einar Magnusson, Adam Porter, Mikael\n  Lindvall, Madeline Diep",
    "title": "Metamorphic Detection of Adversarial Examples in Deep Learning Models\n  With Affine Transformations",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1109/MET.2019.00016",
    "report-no": null,
    "categories": "cs.CV cs.LG eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks are small, carefully crafted perturbations, imperceptible\nto the naked eye; that when added to an image cause deep learning models to\nmisclassify the image with potentially detrimental outcomes. With the rise of\nartificial intelligence models in consumer safety and security intensive\nindustries such as self-driving cars, camera surveillance and face recognition,\nthere is a growing need for guarding against adversarial attacks. In this\npaper, we present an approach that uses metamorphic testing principles to\nautomatically detect such adversarial attacks. The approach can detect image\nmanipulations that are so small, that they are impossible to detect by a human\nthrough visual inspection. By applying metamorphic relations based on distance\nratio preserving affine image transformations which compare the behavior of the\noriginal and transformed image; we show that our proposed approach can\ndetermine whether or not the input image is adversarial with a high degree of\naccuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 10 Jul 2019 15:04:18 GMT"
      }
    ],
    "update_date": "2019-07-11",
    "authors_parsed": [
      [
        "Mekala",
        "Rohan Reddy",
        ""
      ],
      [
        "Magnusson",
        "Gudjon Einar",
        ""
      ],
      [
        "Porter",
        "Adam",
        ""
      ],
      [
        "Lindvall",
        "Mikael",
        ""
      ],
      [
        "Diep",
        "Madeline",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.04774",
    "publish_date": "2019-07-10"
  },
  {
    "id": "1907.07296",
    "submitter": "Ross Maciejewski",
    "authors": "Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski",
    "title": "Explaining Vulnerabilities to Adversarial Machine Learning through\n  Visual Analytics",
    "comments": "IEEE VAST (Transactions on Visualization and Computer Graphics), 2019",
    "journal-ref": null,
    "doi": "10.1109/TVCG.2019.2934631",
    "report-no": null,
    "categories": "cs.HC cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Machine learning models are currently being deployed in a variety of\nreal-world applications where model predictions are used to make decisions\nabout healthcare, bank loans, and numerous other critical tasks. As the\ndeployment of artificial intelligence technologies becomes ubiquitous, it is\nunsurprising that adversaries have begun developing methods to manipulate\nmachine learning models to their advantage. While the visual analytics\ncommunity has developed methods for opening the black box of machine learning\nmodels, little work has focused on helping the user understand their model\nvulnerabilities in the context of adversarial attacks. In this paper, we\npresent a visual analytics framework for explaining and exploring model\nvulnerabilities to adversarial attacks. Our framework employs a multi-faceted\nvisualization scheme designed to support the analysis of data poisoning attacks\nfrom the perspective of models, data instances, features, and local structures.\nWe demonstrate our framework through two case studies on binary classifiers and\nillustrate model vulnerabilities with respect to varying attack strategies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Jul 2019 00:50:37 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 20 Jul 2019 01:12:30 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 1 Oct 2019 20:34:56 GMT"
      },
      {
        "version": "v4",
        "created": "Thu, 3 Oct 2019 19:38:48 GMT"
      }
    ],
    "update_date": "2019-10-07",
    "authors_parsed": [
      [
        "Ma",
        "Yuxin",
        ""
      ],
      [
        "Xie",
        "Tiankai",
        ""
      ],
      [
        "Li",
        "Jundong",
        ""
      ],
      [
        "Maciejewski",
        "Ross",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.07296",
    "publish_date": "2019-07-17"
  },
  {
    "id": "1907.07640",
    "submitter": "Emin Orhan",
    "authors": "A. Emin Orhan",
    "title": "Robustness properties of Facebook's ResNeXt WSL models",
    "comments": "10 pages, 4 figures, 4 tables; v5 corrects the ImageNet-A results and\n  revises the discussion accordingly",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG cs.NE stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We investigate the robustness properties of ResNeXt class image recognition\nmodels trained with billion scale weakly supervised data (ResNeXt WSL models).\nThese models, recently made public by Facebook AI, were trained with ~1B images\nfrom Instagram and fine-tuned on ImageNet. We show that these models display an\nunprecedented degree of robustness against common image corruptions and\nperturbations, as measured by the ImageNet-C and ImageNet-P benchmarks. They\nalso achieve substantially improved accuracies on the recently introduced\n\"natural adversarial examples\" benchmark (ImageNet-A). The largest of the\nreleased models, in particular, achieves state-of-the-art results on\nImageNet-C, ImageNet-P, and ImageNet-A by a large margin. The gains on\nImageNet-C, ImageNet-P, and ImageNet-A far outpace the gains on ImageNet\nvalidation accuracy, suggesting the former as more useful benchmarks to measure\nfurther progress in image recognition. Remarkably, the ResNeXt WSL models even\nachieve a limited degree of adversarial robustness against state-of-the-art\nwhite-box attacks (10-step PGD attacks). However, in contrast to adversarially\ntrained models, the robustness of the ResNeXt WSL models rapidly declines with\nthe number of PGD steps, suggesting that these models do not achieve genuine\nadversarial robustness. Visualization of the learned features also confirms\nthis conclusion. Finally, we show that although the ResNeXt WSL models are more\nshape-biased than comparable ImageNet-trained models in a shape-texture cue\nconflict experiment, they still remain much more texture-biased than humans,\nsuggesting that they share some of the underlying characteristics of\nImageNet-trained models that make this benchmark challenging.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Jul 2019 17:03:52 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 24 Jul 2019 17:59:19 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 25 Jul 2019 15:52:53 GMT"
      },
      {
        "version": "v4",
        "created": "Fri, 2 Aug 2019 16:30:13 GMT"
      },
      {
        "version": "v5",
        "created": "Mon, 9 Dec 2019 16:28:47 GMT"
      }
    ],
    "update_date": "2019-12-10",
    "authors_parsed": [
      [
        "Orhan",
        "A. Emin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.07640",
    "publish_date": "2019-07-24"
  },
  {
    "id": "1907.10406",
    "submitter": "Qi Xuan",
    "authors": "Yun Xiang, Zhuangzhi Chen, Zuohui Chen, Zebin Fang, Haiyang Hao,\n  Jinyin Chen, Yi Liu, Zhefu Wu, Qi Xuan and Xiaoniu Yang",
    "title": "Open DNN Box by Power Side-Channel Attack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks are becoming popular and important assets of many AI\ncompanies. However, recent studies indicate that they are also vulnerable to\nadversarial attacks. Adversarial attacks can be either white-box or black-box.\nThe white-box attacks assume full knowledge of the models while the black-box\nones assume none. In general, revealing more internal information can enable\nmuch more powerful and efficient attacks. However, in most real-world\napplications, the internal information of embedded AI devices is unavailable,\ni.e., they are black-box. Therefore, in this work, we propose a side-channel\ninformation based technique to reveal the internal information of black-box\nmodels. Specifically, we have made the following contributions: (1) we are the\nfirst to use side-channel information to reveal internal network architecture\nin embedded devices; (2) we are the first to construct models for internal\nparameter estimation; and (3) we validate our methods on real-world devices and\napplications. The experimental results show that our method can achieve 96.50\\%\naccuracy on average. Such results suggest that we should pay strong attention\nto the security problem of many AI applications, and further propose\ncorresponding defensive strategies in the future.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 21 Jul 2019 11:52:36 GMT"
      }
    ],
    "update_date": "2019-07-25",
    "authors_parsed": [
      [
        "Xiang",
        "Yun",
        ""
      ],
      [
        "Chen",
        "Zhuangzhi",
        ""
      ],
      [
        "Chen",
        "Zuohui",
        ""
      ],
      [
        "Fang",
        "Zebin",
        ""
      ],
      [
        "Hao",
        "Haiyang",
        ""
      ],
      [
        "Chen",
        "Jinyin",
        ""
      ],
      [
        "Liu",
        "Yi",
        ""
      ],
      [
        "Wu",
        "Zhefu",
        ""
      ],
      [
        "Xuan",
        "Qi",
        ""
      ],
      [
        "Yang",
        "Xiaoniu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.10406",
    "publish_date": "2019-07-21"
  },
  {
    "id": "1907.11684",
    "submitter": "Pu Zhao",
    "authors": "Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, Bhavya\n  Kailkhura, Xue Lin",
    "title": "On the Design of Black-box Adversarial Examples by Leveraging\n  Gradient-free Optimization and Operator Splitting Method",
    "comments": "accepted by ICCV 2019",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Robust machine learning is currently one of the most prominent topics which\ncould potentially help shaping a future of advanced AI platforms that not only\nperform well in average cases but also in worst cases or adverse situations.\nDespite the long-term vision, however, existing studies on black-box\nadversarial attacks are still restricted to very specific settings of threat\nmodels (e.g., single distortion metric and restrictive assumption on target\nmodel's feedback to queries) and/or suffer from prohibitively high query\ncomplexity. To push for further advances in this field, we introduce a general\nframework based on an operator splitting method, the alternating direction\nmethod of multipliers (ADMM) to devise efficient, robust black-box attacks that\nwork with various distortion metrics and feedback settings without incurring\nhigh query complexity. Due to the black-box nature of the threat model, the\nproposed ADMM solution framework is integrated with zeroth-order (ZO)\noptimization and Bayesian optimization (BO), and thus is applicable to the\ngradient-free regime. This results in two new black-box adversarial attack\ngeneration methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image\nclassification datasets show that our proposed approaches have much lower\nfunction query complexities compared to state-of-the-art attack methods, but\nachieve very competitive attack success rates.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 26 Jul 2019 17:29:52 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 23 Oct 2019 19:34:54 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 28 Oct 2019 16:35:58 GMT"
      },
      {
        "version": "v4",
        "created": "Wed, 4 Dec 2019 21:05:19 GMT"
      }
    ],
    "update_date": "2019-12-06",
    "authors_parsed": [
      [
        "Zhao",
        "Pu",
        ""
      ],
      [
        "Liu",
        "Sijia",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Hoang",
        "Nghia",
        ""
      ],
      [
        "Xu",
        "Kaidi",
        ""
      ],
      [
        "Kailkhura",
        "Bhavya",
        ""
      ],
      [
        "Lin",
        "Xue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1907.11684",
    "publish_date": "2019-12-04"
  },
  {
    "id": "1908.01763",
    "submitter": "Lun Wang",
    "authors": "Wenbo Guo, Lun Wang, Xinyu Xing, Min Du, Dawn Song",
    "title": "TABOR: A Highly Accurate Approach to Inspecting and Restoring Trojan\n  Backdoors in AI Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  A trojan backdoor is a hidden pattern typically implanted in a deep neural\nnetwork. It could be activated and thus forces that infected model behaving\nabnormally only when an input data sample with a particular trigger present is\nfed to that model. As such, given a deep neural network model and clean input\nsamples, it is very challenging to inspect and determine the existence of a\ntrojan backdoor. Recently, researchers design and develop several pioneering\nsolutions to address this acute problem. They demonstrate the proposed\ntechniques have a great potential in trojan detection. However, we show that\nnone of these existing techniques completely address the problem. On the one\nhand, they mostly work under an unrealistic assumption (e.g. assuming\navailability of the contaminated training database). On the other hand, the\nproposed techniques cannot accurately detect the existence of trojan backdoors,\nnor restore high-fidelity trojan backdoor images, especially when the triggers\npertaining to the trojan vary in size, shape and position. In this work, we\npropose TABOR, a new trojan detection technique. Conceptually, it formalizes a\ntrojan detection task as a non-convex optimization problem, and the detection\nof a trojan backdoor as the task of resolving the optimization through an\nobjective function. Different from the existing technique also modeling trojan\ndetection as an optimization problem, TABOR designs a new objective\nfunction--under the guidance of explainable AI techniques as well as\nheuristics--that could guide optimization to identify a trojan backdoor in a\nmore effective fashion. In addition, TABOR defines a new metric to measure the\nquality of a trojan backdoor identified. Using an anomaly detection method, we\nshow the new metric could better facilitate TABOR to identify intentionally\ninjected triggers in an infected model and filter out false alarms......\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 2 Aug 2019 22:46:03 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 8 Aug 2019 22:59:45 GMT"
      }
    ],
    "update_date": "2019-08-12",
    "authors_parsed": [
      [
        "Guo",
        "Wenbo",
        ""
      ],
      [
        "Wang",
        "Lun",
        ""
      ],
      [
        "Xing",
        "Xinyu",
        ""
      ],
      [
        "Du",
        "Min",
        ""
      ],
      [
        "Song",
        "Dawn",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1908.01763",
    "publish_date": "2019-08-08"
  },
  {
    "id": "1908.04954",
    "submitter": "Farhad Farokhi",
    "authors": "Farhad Farokhi",
    "title": "Taking a Lesson from Quantum Particles for Statistical Data Privacy",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.IT math.IT quant-ph",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Privacy is under threat from artificial intelligence revolution fueled by\nunprecedented abundance of data. Differential privacy, an established candidate\nfor privacy protection, is susceptible to adversarial attacks, acts\nconservatively, and leads to miss-implementations because of lacking systematic\nmethods for setting its parameters (known as the privacy budget). An\nalternative is information-theoretic privacy using entropy with the drawback of\nrequiring prior distribution of the private data. Here, by using the Fisher\ninformation, information-theoretic privacy framework is extended to avoid\nunnecessary assumptions on the private data. The optimal privacy-preserving\nadditive noise, extracted by minimizing the Fisher information, must follow the\ntime-independent Schrodinger's equation. A fundamental trade-off between\nprivacy and utility is also proved, reminiscent of the Heisenberg uncertainty\nprinciple.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 14 Aug 2019 04:50:38 GMT"
      }
    ],
    "update_date": "2019-08-15",
    "authors_parsed": [
      [
        "Farokhi",
        "Farhad",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1908.04954",
    "publish_date": "2019-08-14"
  },
  {
    "id": "1908.05195",
    "submitter": "Seugnju Cho",
    "authors": "Seungju Cho, Tae Joon Jun, Byungsoo Oh, Daeyoung Kim",
    "title": "DAPAS : Denoising Autoencoder to Prevent Adversarial attack in Semantic\n  Segmentation",
    "comments": "Accepted to be published in: 2020 International Joint Conference on\n  Neural Networks (IJCNN), Glasgow, July 19--24, 2020",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Nowadays, Deep learning techniques show dramatic performance on computer\nvision area, and they even outperform human. But it is also vulnerable to some\nsmall perturbation called an adversarial attack. This is a problem combined\nwith the safety of artificial intelligence, which has recently been studied a\nlot. These attacks have shown that they can fool models of image\nclassification, semantic segmentation, and object detection. We point out this\nattack can be protected by denoise autoencoder, which is used for denoising the\nperturbation and restoring the original images. We experiment with various\nnoise distributions and verify the effect of denoise autoencoder against\nadversarial attack in semantic segmentation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 14 Aug 2019 16:13:00 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 18 Aug 2019 12:15:53 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 6 Apr 2020 08:06:32 GMT"
      },
      {
        "version": "v4",
        "created": "Tue, 7 Apr 2020 07:01:28 GMT"
      }
    ],
    "update_date": "2020-04-08",
    "authors_parsed": [
      [
        "Cho",
        "Seungju",
        ""
      ],
      [
        "Jun",
        "Tae Joon",
        ""
      ],
      [
        "Oh",
        "Byungsoo",
        ""
      ],
      [
        "Kim",
        "Daeyoung",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1908.05195",
    "publish_date": "2019-08-14"
  },
  {
    "id": "1909.06122",
    "submitter": "Run Wang",
    "authors": "Run Wang, Felix Juefei-Xu, Lei Ma, Xiaofei Xie, Yihao Huang, Jian\n  Wang, Yang Liu",
    "title": "FakeSpotter: A Simple yet Robust Baseline for Spotting AI-Synthesized\n  Fake Faces",
    "comments": "Accepted to IJCAI 2020; SOLE copyright holder is IJCAI (international\n  Joint Conferences on Artificial Intelligence), all rights reserved.\n  https://www.ijcai.org/Proceedings/2020/333",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In recent years, generative adversarial networks (GANs) and its variants have\nachieved unprecedented success in image synthesis. They are widely adopted in\nsynthesizing facial images which brings potential security concerns to humans\nas the fakes spread and fuel the misinformation. However, robust detectors of\nthese AI-synthesized fake faces are still in their infancy and are not ready to\nfully tackle this emerging challenge. In this work, we propose a novel\napproach, named FakeSpotter, based on monitoring neuron behaviors to spot\nAI-synthesized fake faces. The studies on neuron coverage and interactions have\nsuccessfully shown that they can be served as testing criteria for deep\nlearning systems, especially under the settings of being exposed to adversarial\nattacks. Here, we conjecture that monitoring neuron behavior can also serve as\nan asset in detecting fake faces since layer-by-layer neuron activation\npatterns may capture more subtle features that are important for the fake\ndetector. Experimental results on detecting four types of fake faces\nsynthesized with the state-of-the-art GANs and evading four perturbation\nattacks show the effectiveness and robustness of our approach.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 13 Sep 2019 10:08:44 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 24 Jan 2020 06:02:29 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 16 Jul 2020 06:44:53 GMT"
      }
    ],
    "update_date": "2020-07-17",
    "authors_parsed": [
      [
        "Wang",
        "Run",
        ""
      ],
      [
        "Juefei-Xu",
        "Felix",
        ""
      ],
      [
        "Ma",
        "Lei",
        ""
      ],
      [
        "Xie",
        "Xiaofei",
        ""
      ],
      [
        "Huang",
        "Yihao",
        ""
      ],
      [
        "Wang",
        "Jian",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1909.06122",
    "publish_date": "2020-07-16"
  },
  {
    "id": "1909.06539",
    "submitter": "Nicole Bussola",
    "authors": "Nicole Bussola, Alessia Marcolini, Valerio Maggio, Giuseppe Jurman,\n  Cesare Furlanello",
    "title": "AI slipping on tiles: data leakage in digital pathology",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "q-bio.QM eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Reproducibility of AI models on biomedical data still stays as a major\nconcern for their acceptance into the clinical practice. Initiatives for\nreproducibility in the development of predictive biomarkers as the MAQC\nConsortium already underlined the importance of appropriate Data Analysis Plans\n(DAPs) to control for different types of bias, including data leakage from the\ntraining to the test set. In the context of digital pathology, the leakage\ntypically lurks in weakly designed experiments not accounting for the subjects\nin their data partitioning schemes. This issue is then exacerbated when\nfractions or subregions of slides (i.e. \"tiles\") are considered. Despite this\naspect is largely recognized by the community, we argue that it is often\noverlooked. In this study, we assess the impact of data leakage on the\nperformance of machine learning models trained and validated on multiple\nhistology data collection. We prove that, even with a properly designed DAP\n(10x5 repeated cross-validation), predictive scores can be inflated up to 41%\nwhen tiles from the same subject are used both in training and validation sets\nby deep learning models. We replicate the experiments for $4$ classification\ntasks on 3 histopathological datasets, for a total of 374 subjects, 556 slides\nand more than 27,000 tiles. Also, we discuss the effects of data leakage on\ntransfer learning strategies with models pre-trained on general-purpose\ndatasets or off-task digital pathology collections. Finally, we propose a\nsolution that automates the creation of leakage-free deep learning pipelines\nfor digital pathology based on histolab, a novel Python package for histology\ndata preprocessing. We validate the solution on two public datasets (TCGA and\nGTEx).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 14 Sep 2019 05:57:04 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 29 Sep 2019 18:08:58 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 17 Nov 2020 15:21:17 GMT"
      }
    ],
    "update_date": "2020-11-18",
    "authors_parsed": [
      [
        "Bussola",
        "Nicole",
        ""
      ],
      [
        "Marcolini",
        "Alessia",
        ""
      ],
      [
        "Maggio",
        "Valerio",
        ""
      ],
      [
        "Jurman",
        "Giuseppe",
        ""
      ],
      [
        "Furlanello",
        "Cesare",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1909.06539",
    "publish_date": "2020-11-17"
  },
  {
    "id": "1909.09263",
    "submitter": "Kyungyul Kim",
    "authors": "Jihyeun Yoon, Kyungyul Kim, Jongseong Jang",
    "title": "Propagated Perturbation of Adversarial Attack for well-known CNNs:\n  Empirical Study and its Explanation",
    "comments": null,
    "journal-ref": "ICCV 2019 Workshop on Interpreting and Explaining Visual\n  Artificial Intelligence Models",
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep Neural Network based classifiers are known to be vulnerable to\nperturbations of inputs constructed by an adversarial attack to force\nmisclassification. Most studies have focused on how to make vulnerable noise by\ngradient based attack methods or to defense model from adversarial attack. The\nuse of the denoiser model is one of a well-known solution to reduce the\nadversarial noise although classification performance had not significantly\nimproved. In this study, we aim to analyze the propagation of adversarial\nattack as an explainable AI(XAI) point of view. Specifically, we examine the\ntrend of adversarial perturbations through the CNN architectures. To analyze\nthe propagated perturbation, we measured normalized Euclidean Distance and\ncosine distance in each CNN layer between the feature map of the perturbed\nimage passed through denoiser and the non-perturbed original image. We used\nfive well-known CNN based classifiers and three gradient-based adversarial\nattacks. From the experimental results, we observed that in most cases,\nEuclidean Distance explosively increases in the final fully connected layer\nwhile cosine distance fluctuated and disappeared at the last layer. This means\nthat the use of denoiser can decrease the amount of noise. However, it failed\nto defense accuracy degradation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Sep 2019 23:51:07 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 23 Sep 2019 07:18:24 GMT"
      }
    ],
    "update_date": "2019-09-24",
    "authors_parsed": [
      [
        "Yoon",
        "Jihyeun",
        ""
      ],
      [
        "Kim",
        "Kyungyul",
        ""
      ],
      [
        "Jang",
        "Jongseong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1909.09263",
    "publish_date": "2019-09-19"
  },
  {
    "id": "1909.12161",
    "submitter": "Muhammad Usama",
    "authors": "Salah-ud-din Farooq, Muhammad Usama, Junaid Qadir, Muhammad Ali Imran",
    "title": "Adversarial ML Attack on Self Organizing Cellular Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.NI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep Neural Networks (DNN) have been widely adopted in self-organizing\nnetworks (SON) for automating different networking tasks. Recently, it has been\nshown that DNN lack robustness against adversarial examples where an adversary\ncan fool the DNN model into incorrect classification by introducing a small\nimperceptible perturbation to the original example. SON is expected to use DNN\nfor multiple fundamental cellular tasks and many DNN-based solutions for\nperforming SON tasks have been proposed in the literature have not been tested\nagainst adversarial examples. In this paper, we have tested and explained the\nrobustness of SON against adversarial example and investigated the performance\nof an important SON use case in the face of adversarial attacks. We have also\ngenerated explanations of incorrect classifications by utilizing an explainable\nartificial intelligence (AI) technique.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 26 Sep 2019 14:44:43 GMT"
      }
    ],
    "update_date": "2019-09-27",
    "authors_parsed": [
      [
        "Farooq",
        "Salah-ud-din",
        ""
      ],
      [
        "Usama",
        "Muhammad",
        ""
      ],
      [
        "Qadir",
        "Junaid",
        ""
      ],
      [
        "Imran",
        "Muhammad Ali",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1909.12161",
    "publish_date": "2019-09-26"
  },
  {
    "id": "1909.12962",
    "submitter": "Yuezun Li",
    "authors": "Yuezun Li, Xin Yang, Pu Sun, Honggang Qi and Siwei Lyu",
    "title": "Celeb-DF: A Large-scale Challenging Dataset for DeepFake Forensics",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI-synthesized face-swapping videos, commonly known as DeepFakes, is an\nemerging problem threatening the trustworthiness of online information. The\nneed to develop and evaluate DeepFake detection algorithms calls for\nlarge-scale datasets. However, current DeepFake datasets suffer from low visual\nquality and do not resemble DeepFake videos circulated on the Internet. We\npresent a new large-scale challenging DeepFake video dataset, Celeb-DF, which\ncontains 5,639 high-quality DeepFake videos of celebrities generated using\nimproved synthesis process. We conduct a comprehensive evaluation of DeepFake\ndetection methods and datasets to demonstrate the escalated level of challenges\nposed by Celeb-DF.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 27 Sep 2019 21:26:34 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 2 Oct 2019 00:23:11 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 27 Nov 2019 03:46:36 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 16 Mar 2020 16:20:16 GMT"
      }
    ],
    "update_date": "2020-03-17",
    "authors_parsed": [
      [
        "Li",
        "Yuezun",
        ""
      ],
      [
        "Yang",
        "Xin",
        ""
      ],
      [
        "Sun",
        "Pu",
        ""
      ],
      [
        "Qi",
        "Honggang",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1909.12962",
    "publish_date": "2019-11-27"
  },
  {
    "id": "1910.03065",
    "submitter": "Oana-Maria Camburu",
    "authors": "Oana-Maria Camburu, Brendan Shillingford, Pasquale Minervini, Thomas\n  Lukasiewicz, Phil Blunsom",
    "title": "Make Up Your Mind! Adversarial Generation of Inconsistent Natural\n  Language Explanations",
    "comments": null,
    "journal-ref": "Short Paper at ACL, 2020",
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  To increase trust in artificial intelligence systems, a promising research\ndirection consists of designing neural models capable of generating natural\nlanguage explanations for their predictions. In this work, we show that such\nmodels are nonetheless prone to generating mutually inconsistent explanations,\nsuch as \"Because there is a dog in the image\" and \"Because there is no dog in\nthe [same] image\", exposing flaws in either the decision-making process of the\nmodel or in the generation of the explanations. We introduce a simple yet\neffective adversarial framework for sanity checking models against the\ngeneration of inconsistent natural language explanations. Moreover, as part of\nthe framework, we address the problem of adversarial attacks with full target\nsequences, a scenario that was not previously addressed in sequence-to-sequence\nattacks. Finally, we apply our framework on a state-of-the-art neural natural\nlanguage inference model that provides natural language explanations for its\npredictions. Our framework shows that this model is capable of generating a\nsignificant number of inconsistent explanations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 Oct 2019 20:14:23 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 9 Oct 2019 14:56:58 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 2 May 2020 10:37:06 GMT"
      }
    ],
    "update_date": "2020-05-05",
    "authors_parsed": [
      [
        "Camburu",
        "Oana-Maria",
        ""
      ],
      [
        "Shillingford",
        "Brendan",
        ""
      ],
      [
        "Minervini",
        "Pasquale",
        ""
      ],
      [
        "Lukasiewicz",
        "Thomas",
        ""
      ],
      [
        "Blunsom",
        "Phil",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.03065",
    "publish_date": "2019-10-09"
  },
  {
    "id": "1910.03137",
    "submitter": "Xiaojun Xu",
    "authors": "Xiaojun Xu, Qi Wang, Huichen Li, Nikita Borisov, Carl A. Gunter, Bo Li",
    "title": "Detecting AI Trojans Using Meta Neural Analysis",
    "comments": "Accepted by IEEE S&P 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In machine learning Trojan attacks, an adversary trains a corrupted model\nthat obtains good performance on normal data but behaves maliciously on data\nsamples with certain trigger patterns. Several approaches have been proposed to\ndetect such attacks, but they make undesirable assumptions about the attack\nstrategies or require direct access to the trained models, which restricts\ntheir utility in practice.\n  This paper addresses these challenges by introducing a Meta Neural Trojan\nDetection (MNTD) pipeline that does not make assumptions on the attack\nstrategies and only needs black-box access to models. The strategy is to train\na meta-classifier that predicts whether a given target model is Trojaned. To\ntrain the meta-model without knowledge of the attack strategy, we introduce a\ntechnique called jumbo learning that samples a set of Trojaned models following\na general distribution. We then dynamically optimize a query set together with\nthe meta-classifier to distinguish between Trojaned and benign models.\n  We evaluate MNTD with experiments on vision, speech, tabular data and natural\nlanguage text datasets, and against different Trojan attacks such as data\npoisoning attack, model manipulation attack, and latent attack. We show that\nMNTD achieves 97% detection AUC score and significantly outperforms existing\ndetection approaches. In addition, MNTD generalizes well and achieves high\ndetection performance against unforeseen attacks. We also propose a robust MNTD\npipeline which achieves 90% detection AUC even when the attacker aims to evade\nthe detection with full knowledge of the system.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 8 Oct 2019 00:00:23 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 9 Oct 2019 15:35:55 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 25 Aug 2020 00:24:01 GMT"
      },
      {
        "version": "v4",
        "created": "Thu, 1 Oct 2020 17:06:03 GMT"
      }
    ],
    "update_date": "2020-10-02",
    "authors_parsed": [
      [
        "Xu",
        "Xiaojun",
        ""
      ],
      [
        "Wang",
        "Qi",
        ""
      ],
      [
        "Li",
        "Huichen",
        ""
      ],
      [
        "Borisov",
        "Nikita",
        ""
      ],
      [
        "Gunter",
        "Carl A.",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.03137",
    "publish_date": "2019-10-09"
  },
  {
    "id": "1910.04819",
    "submitter": "Theodoros Tsiligkaridis",
    "authors": "Theodoros Tsiligkaridis",
    "title": "Information Aware Max-Norm Dirichlet Networks for Predictive Uncertainty\n  Estimation",
    "comments": "To appear in Neural Networks.\n  https://doi.org/10.1016/j.neunet.2020.12.011",
    "journal-ref": "Neural Networks, Volume 135, March 2021, Pages 105-114",
    "doi": "10.1016/j.neunet.2020.12.011",
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Precise estimation of uncertainty in predictions for AI systems is a critical\nfactor in ensuring trust and safety. Deep neural networks trained with a\nconventional method are prone to over-confident predictions. In contrast to\nBayesian neural networks that learn approximate distributions on weights to\ninfer prediction confidence, we propose a novel method, Information Aware\nDirichlet networks, that learn an explicit Dirichlet prior distribution on\npredictive distributions by minimizing a bound on the expected max norm of the\nprediction error and penalizing information associated with incorrect outcomes.\nProperties of the new cost function are derived to indicate how improved\nuncertainty estimation is achieved. Experiments using real datasets show that\nour technique outperforms, by a large margin, state-of-the-art neural networks\nfor estimating within-distribution and out-of-distribution uncertainty, and\ndetecting adversarial examples.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 10 Oct 2019 19:10:42 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 13 Feb 2020 18:44:49 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 1 Jul 2020 00:32:23 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 4 Jan 2021 16:13:24 GMT"
      }
    ],
    "update_date": "2021-01-05",
    "authors_parsed": [
      [
        "Tsiligkaridis",
        "Theodoros",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.04819",
    "publish_date": "2020-07-01"
  },
  {
    "id": "1910.06907",
    "submitter": "Utku Kose",
    "authors": "Utku Kose",
    "title": "Techniques for Adversarial Examples Threatening the Safety of Artificial\n  Intelligence Based Systems",
    "comments": "International Science and Innovation Congress 2019, pp. 643-655, 13\n  pages, 10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI math.OC",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Artificial intelligence is known as the most effective technological field\nfor rapid developments shaping the future of the world. Even today, it is\npossible to see intense use of intelligence systems in all fields of the life.\nAlthough advantages of the Artificial Intelligence are widely observed, there\nis also a dark side employing efforts to design hacking oriented techniques\nagainst Artificial Intelligence. Thanks to such techniques, it is possible to\ntrick intelligent systems causing directed results for unsuccessful outputs.\nThat is critical for also cyber wars of the future as it is predicted that the\nwars will be done unmanned, autonomous intelligent systems. Moving from the\nexplanations, objective of this study is to provide information regarding\nadversarial examples threatening the Artificial Intelligence and focus on\ndetails of some techniques, which are used for creating adversarial examples.\nAdversarial examples are known as training data, which can trick a Machine\nLearning technique to learn incorrectly about the target problem and cause an\nunsuccessful or maliciously directed intelligent system at the end. The study\nenables the readers to learn enough about details of recent techniques for\ncreating adversarial examples.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 29 Sep 2019 21:56:59 GMT"
      }
    ],
    "update_date": "2019-10-16",
    "authors_parsed": [
      [
        "Kose",
        "Utku",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.06907",
    "publish_date": "2019-09-29"
  },
  {
    "id": "1910.12467",
    "submitter": "Hong Huy Nguyen",
    "authors": "Huy H. Nguyen, Junichi Yamagishi, Isao Echizen",
    "title": "Use of a Capsule Network to Detect Fake Images and Videos",
    "comments": "Fixing Table 2's scale",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The revolution in computer hardware, especially in graphics processing units\nand tensor processing units, has enabled significant advances in computer\ngraphics and artificial intelligence algorithms. In addition to their many\nbeneficial applications in daily life and business,\ncomputer-generated/manipulated images and videos can be used for malicious\npurposes that violate security systems, privacy, and social trust. The deepfake\nphenomenon and its variations enable a normal user to use his or her personal\ncomputer to easily create fake videos of anybody from a short real online\nvideo. Several countermeasures have been introduced to deal with attacks using\nsuch videos. However, most of them are targeted at certain domains and are\nineffective when applied to other domains or new attacks. In this paper, we\nintroduce a capsule network that can detect various kinds of attacks, from\npresentation attacks using printed images and replayed videos to attacks using\nfake videos created using deep learning. It uses many fewer parameters than\ntraditional convolutional neural networks with similar performance. Moreover,\nwe explain, for the first time ever in the literature, the theory behind the\napplication of capsule networks to the forensics problem through detailed\nanalysis and visualization.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 28 Oct 2019 07:01:49 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 29 Oct 2019 14:30:58 GMT"
      }
    ],
    "update_date": "2019-10-30",
    "authors_parsed": [
      [
        "Nguyen",
        "Huy H.",
        ""
      ],
      [
        "Yamagishi",
        "Junichi",
        ""
      ],
      [
        "Echizen",
        "Isao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.12467",
    "publish_date": "2019-10-29"
  },
  {
    "id": "1910.13875",
    "submitter": "Vasisht Duddu",
    "authors": "Vasisht Duddu, N. Rajesh Pillai, D. Vijay Rao, Valentina E. Balas",
    "title": "Fault Tolerance of Neural Networks in Adversarial Settings",
    "comments": null,
    "journal-ref": "Journal of Intelligent and Fuzzy Systems (JIFS) 2020",
    "doi": "10.3233/JIFS-179677",
    "report-no": null,
    "categories": "cs.CR cs.DC cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Artificial Intelligence systems require a through assessment of different\npillars of trust, namely, fairness, interpretability, data and model privacy,\nreliability (safety) and robustness against against adversarial attacks. While\nthese research problems have been extensively studied in isolation, an\nunderstanding of the trade-off between different pillars of trust is lacking.\nTo this extent, the trade-off between fault tolerance, privacy and adversarial\nrobustness is evaluated for the specific case of Deep Neural Networks, by\nconsidering two adversarial settings under a security and a privacy threat\nmodel. Specifically, this work studies the impact of the fault tolerance of the\nNeural Network on training the model by adding noise to the input (Adversarial\nRobustness) and noise to the gradients (Differential Privacy). While training\nmodels with noise to inputs, gradients or weights enhances fault tolerance, it\nis observed that adversarial robustness and fault tolerance are at odds with\neach other. On the other hand, ($\\epsilon,\\delta$)-Differentially Private\nmodels enhance the fault tolerance, measured using generalisation error,\ntheoretically has an upper bound of $e^{\\epsilon} - 1 + \\delta$. This novel\nstudy of the trade-off between different elements of trust is pivotal for\ntraining a model which satisfies the requirements for different pillars of\ntrust simultaneously.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 30 Oct 2019 14:22:22 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 7 Mar 2020 16:21:15 GMT"
      }
    ],
    "update_date": "2020-03-10",
    "authors_parsed": [
      [
        "Duddu",
        "Vasisht",
        ""
      ],
      [
        "Pillai",
        "N. Rajesh",
        ""
      ],
      [
        "Rao",
        "D. Vijay",
        ""
      ],
      [
        "Balas",
        "Valentina E.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1910.13875",
    "publish_date": "2019-10-30"
  },
  {
    "id": "1911.01419",
    "submitter": "Loren Anderson",
    "authors": "Loren Anderson and Sahitya Senapathy",
    "title": "On Solving the 2-Dimensional Greedy Shooter Problem for UAVs",
    "comments": "7 pages, 13 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.RO cs.LG stat.ML",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Unmanned Aerial Vehicles (UAVs), autonomously-guided aircraft, are widely\nused for tasks involving surveillance and reconnaissance. A version of the\npursuit-evasion problems centered around UAVs and its variants has been\nextensively studied in recent years due to numerous breakthroughs in AI. We\npresent an approach to UAV pursuit-evasion in a 2D aerial-engagement\nenvironment using reinforcement learning (RL), a machine learning paradigm\nconcerned with goal-oriented algorithms. In this work, a UAV wielding the\ngreedy shooter strategy engages with a UAV trained using deep Q-learning\ntechniques. Simulated results show that the latter UAV wins every engagement in\nwhich the UAVs are suffciently separated during initialization. This approach\nhighlights an exhaustive and robust application of reinforcement learning to\npursuit-evasion that provides insight into effective strategies for UAV flight\nand interaction.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 2 Nov 2019 21:55:19 GMT"
      }
    ],
    "update_date": "2019-11-06",
    "authors_parsed": [
      [
        "Anderson",
        "Loren",
        ""
      ],
      [
        "Senapathy",
        "Sahitya",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1911.01419",
    "publish_date": "2019-11-02"
  },
  {
    "id": "1911.07421",
    "submitter": "Xiaofeng Liu",
    "authors": "Tong Che, Xiaofeng Liu, Site Li, Yubin Ge, Ruixiang Zhang, Caiming\n  Xiong, Yoshua Bengio",
    "title": "Deep Verifier Networks: Verification of Deep Discriminative Models with\n  Deep Generative Models",
    "comments": "Accepted to AAAI 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG cs.MM",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI Safety is a major concern in many deep learning applications such as\nautonomous driving. Given a trained deep learning model, an important natural\nproblem is how to reliably verify the model's prediction. In this paper, we\npropose a novel framework -- deep verifier networks (DVN) to verify the inputs\nand outputs of deep discriminative models with deep generative models. Our\nproposed model is based on conditional variational auto-encoders with\ndisentanglement constraints. We give both intuitive and theoretical\njustifications of the model. Our verifier network is trained independently with\nthe prediction model, which eliminates the need of retraining the verifier\nnetwork for a new model. We test the verifier network on out-of-distribution\ndetection and adversarial example detection problems, as well as anomaly\ndetection problems in structured prediction tasks such as image caption\ngeneration. We achieve state-of-the-art results in all of these problems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 18 Nov 2019 04:23:12 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 8 Feb 2020 03:10:15 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 1 Jan 2021 21:08:11 GMT"
      }
    ],
    "update_date": "2021-01-05",
    "authors_parsed": [
      [
        "Che",
        "Tong",
        ""
      ],
      [
        "Liu",
        "Xiaofeng",
        ""
      ],
      [
        "Li",
        "Site",
        ""
      ],
      [
        "Ge",
        "Yubin",
        ""
      ],
      [
        "Zhang",
        "Ruixiang",
        ""
      ],
      [
        "Xiong",
        "Caiming",
        ""
      ],
      [
        "Bengio",
        "Yoshua",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1911.07421",
    "publish_date": "2020-02-08"
  },
  {
    "id": "1911.10008",
    "submitter": "Sambuddha Saha",
    "authors": "Sambuddha Saha, Aashish Kumar, Pratyush Sahay, George Jose, Srinivas\n  Kruthiventi, Harikrishna Muralidhara",
    "title": "Attack Agnostic Statistical Method for Adversarial Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep Learning based AI systems have shown great promise in various domains\nsuch as vision, audio, autonomous systems (vehicles, drones), etc. Recent\nresearch on neural networks has shown the susceptibility of deep networks to\nadversarial attacks - a technique of adding small perturbations to the inputs\nwhich can fool a deep network into misclassifying them. Developing defenses\nagainst such adversarial attacks is an active research area, with some\napproaches proposing robust models that are immune to such adversaries, while\nother techniques attempt to detect such adversarial inputs. In this paper, we\npresent a novel statistical approach for adversarial detection in image\nclassification. Our approach is based on constructing a per-class feature\ndistribution and detecting adversaries based on comparison of features of a\ntest image with the feature distribution of its class. For this purpose, we\nmake use of various statistical distances such as ED (Energy Distance), MMD\n(Maximum Mean Discrepancy) for adversarial detection, and analyze the\nperformance of each metric. We experimentally show that our approach achieves\ngood adversarial detection performance on MNIST and CIFAR-10 datasets\nirrespective of the attack method, sample size and the degree of adversarial\nperturbation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 22 Nov 2019 12:52:11 GMT"
      }
    ],
    "update_date": "2019-11-25",
    "authors_parsed": [
      [
        "Saha",
        "Sambuddha",
        ""
      ],
      [
        "Kumar",
        "Aashish",
        ""
      ],
      [
        "Sahay",
        "Pratyush",
        ""
      ],
      [
        "Jose",
        "George",
        ""
      ],
      [
        "Kruthiventi",
        "Srinivas",
        ""
      ],
      [
        "Muralidhara",
        "Harikrishna",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1911.10008",
    "publish_date": "2019-11-22"
  },
  {
    "id": "1912.07742",
    "submitter": "Huy Phan",
    "authors": "Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan",
    "title": "CAG: A Real-time Low-cost Enhanced-robustness High-transferability\n  Content-aware Adversarial Attack Generator",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial attack despite\ntheir tremendous success in many AI fields. Adversarial attack is a method that\ncauses the intended misclassfication by adding imperceptible perturbations to\nlegitimate inputs. Researchers have developed numerous types of adversarial\nattack methods. However, from the perspective of practical deployment, these\nmethods suffer from several drawbacks such as long attack generating time, high\nmemory cost, insufficient robustness and low transferability. We propose a\nContent-aware Adversarial Attack Generator (CAG) to achieve real-time,\nlow-cost, enhanced-robustness and high-transferability adversarial attack.\nFirst, as a type of generative model-based attack, CAG shows significant\nspeedup (at least 500 times) in generating adversarial examples compared to the\nstate-of-the-art attacks such as PGD and C\\&W. CAG only needs a single\ngenerative model to perform targeted attack to any targeted class. Because CAG\nencodes the label information into a trainable embedding layer, it differs from\nprior generative model-based adversarial attacks that use $n$ different copies\nof generative models for $n$ different targeted classes. As a result, CAG\nsignificantly reduces the required memory cost for generating adversarial\nexamples. CAG can generate adversarial perturbations that focus on the critical\nareas of input by integrating the class activation maps information in the\ntraining process, and hence improve the robustness of CAG attack against the\nstate-of-art adversarial defenses. In addition, CAG exhibits high\ntransferability across different DNN classifier models in black-box attack\nscenario by introducing random dropout in the process of generating\nperturbations. Extensive experiments on different datasets and DNN models have\nverified the real-time, low-cost, enhanced-robustness, and high-transferability\nbenefits of CAG.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 16 Dec 2019 22:48:38 GMT"
      }
    ],
    "update_date": "2019-12-18",
    "authors_parsed": [
      [
        "Phan",
        "Huy",
        ""
      ],
      [
        "Xie",
        "Yi",
        ""
      ],
      [
        "Liao",
        "Siyu",
        ""
      ],
      [
        "Chen",
        "Jie",
        ""
      ],
      [
        "Yuan",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1912.07742",
    "publish_date": "2019-12-16"
  },
  {
    "id": "1912.10375",
    "submitter": "Boxin Wang",
    "authors": "Boxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen, Shuohang Wang, Bo Li",
    "title": "T3: Tree-Autoencoder Constrained Adversarial Text Generation for\n  Targeted Attack",
    "comments": "Accepted to EMNLP 2020 as a long paper. 17 pages, 4 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks against natural language processing systems, which\nperform seemingly innocuous modifications to inputs, can induce arbitrary\nmistakes to the target models. Though raised great concerns, such adversarial\nattacks can be leveraged to estimate the robustness of NLP models. Compared\nwith the adversarial example generation in continuous data domain (e.g.,\nimage), generating adversarial text that preserves the original meaning is\nchallenging since the text space is discrete and non-differentiable. To handle\nthese challenges, we propose a target-controllable adversarial attack framework\nT3, which is applicable to a range of NLP tasks. In particular, we propose a\ntree-based autoencoder to embed the discrete text data into a continuous\nrepresentation space, upon which we optimize the adversarial perturbation. A\nnovel tree-based decoder is then applied to regularize the syntactic\ncorrectness of the generated text and manipulate it on either sentence\n(T3(Sent)) or word (T3(Word)) level. We consider two most representative NLP\ntasks: sentiment analysis and question answering (QA). Extensive experimental\nresults and human studies show that T3 generated adversarial texts can\nsuccessfully manipulate the NLP models to output the targeted incorrect answer\nwithout misleading the human. Moreover, we show that the generated adversarial\ntexts have high transferability which enables the black-box attacks in\npractice. Our work sheds light on an effective and general way to examine the\nrobustness of NLP models. Our code is publicly available at\nhttps://github.com/AI-secure/T3/.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 22 Dec 2019 03:02:42 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 6 Oct 2020 02:29:20 GMT"
      }
    ],
    "update_date": "2020-10-07",
    "authors_parsed": [
      [
        "Wang",
        "Boxin",
        ""
      ],
      [
        "Pei",
        "Hengzhi",
        ""
      ],
      [
        "Pan",
        "Boyuan",
        ""
      ],
      [
        "Chen",
        "Qian",
        ""
      ],
      [
        "Wang",
        "Shuohang",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1912.10375",
    "publish_date": "2020-10-06"
  },
  {
    "id": "1912.10833",
    "submitter": "Ziwen He",
    "authors": "Ziwen He, Wei Wang, Xinsheng Xuan, Jing Dong, Tieniu Tan",
    "title": "A New Ensemble Method for Concessively Targeted Multi-model Attack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  It is well known that deep learning models are vulnerable to adversarial\nexamples crafted by maliciously adding perturbations to original inputs. There\nare two types of attacks: targeted attack and non-targeted attack, and most\nresearchers often pay more attention to the targeted adversarial examples.\nHowever, targeted attack has a low success rate, especially when aiming at a\nrobust model or under a black-box attack protocol. In this case, non-targeted\nattack is the last chance to disable AI systems. Thus, in this paper, we\npropose a new attack mechanism which performs the non-targeted attack when the\ntargeted attack fails. Besides, we aim to generate a single adversarial sample\nfor different deployed models of the same task, e.g. image classification\nmodels. Hence, for this practical application, we focus on attacking ensemble\nmodels by dividing them into two groups: easy-to-attack and robust models. We\nalternately attack these two groups of models in the non-targeted or targeted\nmanner. We name it a bagging and stacking ensemble (BAST) attack. The BAST\nattack can generate an adversarial sample that fails multiple models\nsimultaneously. Some of the models classify the adversarial sample as a target\nlabel, and other models which are not attacked successfully may give wrong\nlabels at least. The experimental results show that the proposed BAST attack\noutperforms the state-of-the-art attack methods on the new defined criterion\nthat considers both targeted and non-targeted attack performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Dec 2019 10:56:36 GMT"
      }
    ],
    "update_date": "2019-12-24",
    "authors_parsed": [
      [
        "He",
        "Ziwen",
        ""
      ],
      [
        "Wang",
        "Wei",
        ""
      ],
      [
        "Xuan",
        "Xinsheng",
        ""
      ],
      [
        "Dong",
        "Jing",
        ""
      ],
      [
        "Tan",
        "Tieniu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1912.10833",
    "publish_date": "2019-12-19"
  },
  {
    "id": "1912.12184",
    "submitter": "Yenwu Ti",
    "authors": "Chia-Mu Yu, Ching-Tang Chang, Yen-Wu Ti",
    "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural\n  Network and Image Segmentation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in AI technology have made the forgery of digital images and\nvideos easier, and it has become significantly more difficult to identify such\nforgeries. These forgeries, if disseminated with malicious intent, can\nnegatively impact social and political stability, and pose significant ethical\nand legal challenges as well. Deepfake is a variant of auto-encoders that use\ndeep learning techniques to identify and exchange images of a person's face in\na picture or film. Deepfake can result in an erosion of public trust in digital\nimages and videos, which has far-reaching effects on political and social\nstability. This study therefore proposes a solution for facial forgery\ndetection to determine if a picture or film has ever been processed by\nDeepfake. The proposed solution reaches detection efficiency by using the\nrecently proposed separable convolutional neural network (CNN) and image\nsegmentation. In addition, this study also examined how different image\nsegmentation methods affect detection results. Finally, the ensemble model is\nused to improve detection capabilities. Experiment results demonstrated the\nexcellent performance of the proposed solution.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 21 Dec 2019 08:32:27 GMT"
      }
    ],
    "update_date": "2019-12-30",
    "authors_parsed": [
      [
        "Yu",
        "Chia-Mu",
        ""
      ],
      [
        "Chang",
        "Ching-Tang",
        ""
      ],
      [
        "Ti",
        "Yen-Wu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/1912.12184",
    "publish_date": "2019-12-21"
  },
  {
    "id": "2001.00078",
    "submitter": "Gillian Hadfield",
    "authors": "Jack Clark and Gillian K. Hadfield",
    "title": "Regulatory Markets for AI Safety",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY econ.GN q-fin.EC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We propose a new model for regulation to achieve AI safety: global regulatory\nmarkets. We first sketch the model in general terms and provide an overview of\nthe costs and benefits of this approach. We then demonstrate how the model\nmight work in practice: responding to the risk of adversarial attacks on AI\nmodels employed in commercial drones.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Dec 2019 19:21:54 GMT"
      }
    ],
    "update_date": "2020-01-03",
    "authors_parsed": [
      [
        "Clark",
        "Jack",
        ""
      ],
      [
        "Hadfield",
        "Gillian K.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2001.00078",
    "publish_date": "2019-12-11"
  },
  {
    "id": "2001.01172",
    "submitter": "Yaoshiang Ho",
    "authors": "Yaoshiang Ho, Samuel Wookey",
    "title": "The Human Visual System and Adversarial AI",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper applies theories about the Human Visual System to make Adversarial\nAI more effective. To date, Adversarial AI has modeled perceptual distances\nbetween clean and adversarial examples of images using Lp norms. These norms\nhave the benefit of simple mathematical description and reasonable\neffectiveness in approximating perceptual distance. However, in prior decades,\nother areas of image processing have moved beyond simpler models like Mean\nSquared Error (MSE) towards more complex models that better approximate the\nHuman Visual System (HVS). We demonstrate a proof of concept of incorporating\nHVS models into Adversarial AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 5 Jan 2020 05:47:48 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 7 Jan 2020 21:15:45 GMT"
      }
    ],
    "update_date": "2020-01-09",
    "authors_parsed": [
      [
        "Ho",
        "Yaoshiang",
        ""
      ],
      [
        "Wookey",
        "Samuel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2001.01172",
    "publish_date": "2020-01-07"
  },
  {
    "id": "2001.09388",
    "submitter": "Ning Yu",
    "authors": "Ning Yu, Zachary Tuttle, Carl Jake Thurnau, Emmanuel Mireku",
    "title": "AI-Powered GUI Attack and Its Defensive Methods",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.HC cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Since the first Graphical User Interface (GUI) prototype was invented in the\n1970s, GUI systems have been deployed into various personal computer systems\nand server platforms. Recently, with the development of artificial intelligence\n(AI) technology, malicious malware powered by AI is emerging as a potential\nthreat to GUI systems. This type of AI-based cybersecurity attack, targeting at\nGUI systems, is explored in this paper. It is twofold: (1) A malware is\ndesigned to attack the existing GUI system by using AI-based object recognition\ntechniques. (2) Its defensive methods are discovered by generating adversarial\nexamples and other methods to alleviate the threats from the intelligent GUI\nattack. The results have shown that a generic GUI attack can be implemented and\nperformed in a simple way based on current AI techniques and its\ncountermeasures are temporary but effective to mitigate the threats of GUI\nattack so far.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 26 Jan 2020 02:33:52 GMT"
      }
    ],
    "update_date": "2020-01-28",
    "authors_parsed": [
      [
        "Yu",
        "Ning",
        ""
      ],
      [
        "Tuttle",
        "Zachary",
        ""
      ],
      [
        "Thurnau",
        "Carl Jake",
        ""
      ],
      [
        "Mireku",
        "Emmanuel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2001.09388",
    "publish_date": "2020-01-26"
  },
  {
    "id": "2001.09610",
    "submitter": "Ibrahim Yilmaz",
    "authors": "Ibrahim Yilmaz",
    "title": "Practical Fast Gradient Sign Attack against Mammographic Image\n  Classifier",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG eess.IV stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial intelligence (AI) has been a topic of major research for many\nyears. Especially, with the emergence of deep neural network (DNN), these\nstudies have been tremendously successful. Today machines are capable of making\nfaster, more accurate decision than human. Thanks to the great development of\nmachine learning (ML) techniques, ML have been used many different fields such\nas education, medicine, malware detection, autonomous car etc. In spite of\nhaving this degree of interest and much successful research, ML models are\nstill vulnerable to adversarial attacks. Attackers can manipulate clean data in\norder to fool the ML classifiers to achieve their desire target. For instance;\na benign sample can be modified as a malicious sample or a malicious one can be\naltered as benign while this modification can not be recognized by human\nobserver. This can lead to many financial losses, or serious injuries, even\ndeaths. The motivation behind this paper is that we emphasize this issue and\nwant to raise awareness. Therefore, the security gap of mammographic image\nclassifier against adversarial attack is demonstrated. We use mamographic\nimages to train our model then evaluate our model performance in terms of\naccuracy. Later on, we poison original dataset and generate adversarial samples\nthat missclassified by the model. We then using structural similarity index\n(SSIM) analyze similarity between clean images and adversarial images. Finally,\nwe show how successful we are to misuse by using different poisoning factors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Jan 2020 07:37:07 GMT"
      }
    ],
    "update_date": "2020-01-28",
    "authors_parsed": [
      [
        "Yilmaz",
        "Ibrahim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2001.09610",
    "publish_date": "2020-01-27"
  },
  {
    "id": "2001.11821",
    "submitter": "Jean-Philippe Fauvelle",
    "authors": "Alexandre Dey, Marc Velay, Jean-Philippe Fauvelle, Sylvain Navers",
    "title": "Adversarial vs behavioural-based defensive AI with joint, continual and\n  active learning: automated evaluation of robustness to deception, poisoning\n  and concept drift",
    "comments": "in French. European Cyber Week - CESAR/IAD Conference - Artificial\n  Intelligence and Defence, French Ministry of Defence, Nov 2019, Rennes,\n  France",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.LG cs.NE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advancements in Artificial Intelligence (AI) have brought new\ncapabilities to behavioural analysis (UEBA) for cyber-security consisting in\nthe detection of hostile action based on the unusual nature of events observed\non the Information System.In our previous work (presented at C\\&ESAR 2018 and\nFIC 2019), we have associated deep neural networks auto-encoders for anomaly\ndetection and graph-based events correlation to address major limitations in\nUEBA systems. This resulted in reduced false positive and false negative rates,\nimproved alert explainability, while maintaining real-time performances and\nscalability. However, we did not address the natural evolution of behaviours\nthrough time, also known as concept drift. To maintain effective detection\ncapabilities, an anomaly-based detection system must be continually trained,\nwhich opens a door to an adversary that can conduct the so-called\n\"frog-boiling\" attack by progressively distilling unnoticed attack traces\ninside the behavioural models until the complete attack is considered normal.\nIn this paper, we present a solution to effectively mitigate this attack by\nimproving the detection process and efficiently leveraging human expertise. We\nalso present preliminary work on adversarial AI conducting deception attack,\nwhich, in term, will be used to help assess and improve the defense system.\nThese defensive and offensive AI implement joint, continual and active\nlearning, in a step that is necessary in assessing, validating and certifying\nAI-based defensive solutions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Jan 2020 13:54:36 GMT"
      }
    ],
    "update_date": "2020-02-07",
    "authors_parsed": [
      [
        "Dey",
        "Alexandre",
        ""
      ],
      [
        "Velay",
        "Marc",
        ""
      ],
      [
        "Fauvelle",
        "Jean-Philippe",
        ""
      ],
      [
        "Navers",
        "Sylvain",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2001.11821",
    "publish_date": "2020-01-13"
  },
  {
    "id": "2002.01249",
    "submitter": "Jinhuan Wang",
    "authors": "Qi Xuan and Yalu Shan and Jinhuan Wang and Zhongyuan Ruan and Guanrong\n  Chen",
    "title": "Adversarial Attacks to Scale-Free Networks: Testing the Robustness of\n  Physical Criteria",
    "comments": "10pages, 6figures,",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI physics.soc-ph",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks have been alerting the artificial intelligence community\nrecently, since many machine learning algorithms were found vulnerable to\nmalicious attacks. This paper studies adversarial attacks to scale-free\nnetworks to test their robustness in terms of statistical measures. In addition\nto the well-known random link rewiring (RLR) attack, two heuristic attacks are\nformulated and simulated: degree-addition-based link rewiring (DALR) and\ndegree-interval-based link rewiring (DILR). These three strategies are applied\nto attack a number of strong scale-free networks of various sizes generated\nfrom the Barab\\'asi-Albert model. It is found that both DALR and DILR are more\neffective than RLR, in the sense that rewiring a smaller number of links can\nsucceed in the same attack. However, DILR is as concealed as RLR in the sense\nthat they both are constructed by introducing a relatively small number of\nchanges on several typical structural properties such as average shortest\npath-length, average clustering coefficient, and average diagonal distance. The\nresults of this paper suggest that to classify a network to be scale-free has\nto be very careful from the viewpoint of adversarial attack effects.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Feb 2020 12:16:50 GMT"
      }
    ],
    "update_date": "2020-02-05",
    "authors_parsed": [
      [
        "Xuan",
        "Qi",
        ""
      ],
      [
        "Shan",
        "Yalu",
        ""
      ],
      [
        "Wang",
        "Jinhuan",
        ""
      ],
      [
        "Ruan",
        "Zhongyuan",
        ""
      ],
      [
        "Chen",
        "Guanrong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.01249",
    "publish_date": "2020-02-04"
  },
  {
    "id": "2002.02196",
    "submitter": "Tao Bai",
    "authors": "Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex\n  Kot",
    "title": "AI-GAN: Attack-Inspired Generation of Adversarial Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) are vulnerable to adversarial examples, which are\ncrafted by adding imperceptible perturbations to inputs. Recently different\nattacks and strategies have been proposed, but how to generate adversarial\nexamples perceptually realistic and more efficiently remains unsolved. This\npaper proposes a novel framework called Attack-Inspired GAN (AI-GAN), where a\ngenerator, a discriminator, and an attacker are trained jointly. Once trained,\nit can generate adversarial perturbations efficiently given input images and\ntarget classes. Through extensive experiments on several popular datasets \\eg\nMNIST and CIFAR-10, AI-GAN achieves high attack success rates and reduces\ngeneration time significantly in various settings. Moreover, for the first\ntime, AI-GAN successfully scales to complicated datasets \\eg CIFAR-100 with\naround $90\\%$ success rates among all classes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 6 Feb 2020 10:57:41 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 12 Jan 2021 06:22:17 GMT"
      }
    ],
    "update_date": "2021-01-13",
    "authors_parsed": [
      [
        "Bai",
        "Tao",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ],
      [
        "Zhu",
        "Jinlin",
        ""
      ],
      [
        "Han",
        "Shoudong",
        ""
      ],
      [
        "Chen",
        "Jiefeng",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ],
      [
        "Kot",
        "Alex",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.02196",
    "publish_date": "2021-01-12"
  },
  {
    "id": "2002.08527",
    "submitter": "Aritran Piplai",
    "authors": "Aritran Piplai, Sai Sree Laya Chukkapalli, Anupam Joshi",
    "title": "NAttack! Adversarial Attacks to bypass a GAN based classifier trained to\n  detect Network intrusion",
    "comments": "6 pages, 2 figures. 6th IEEE International Conference on Big Data\n  Security on Cloud (BigDataSecurity 2020)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the recent developments in artificial intelligence and machine learning,\nanomalies in network traffic can be detected using machine learning approaches.\nBefore the rise of machine learning, network anomalies which could imply an\nattack, were detected using well-crafted rules. An attacker who has knowledge\nin the field of cyber-defence could make educated guesses to sometimes\naccurately predict which particular features of network traffic data the\ncyber-defence mechanism is looking at. With this information, the attacker can\ncircumvent a rule-based cyber-defense system. However, after the advancements\nof machine learning for network anomaly, it is not easy for a human to\nunderstand how to bypass a cyber-defence system. Recently, adversarial attacks\nhave become increasingly common to defeat machine learning algorithms. In this\npaper, we show that even if we build a classifier and train it with adversarial\nexamples for network data, we can use adversarial attacks and successfully\nbreak the system. We propose a Generative Adversarial Network(GAN)based\nalgorithm to generate data to train an efficient neural network based\nclassifier, and we subsequently break the system using adversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 20 Feb 2020 01:54:45 GMT"
      }
    ],
    "update_date": "2020-04-10",
    "authors_parsed": [
      [
        "Piplai",
        "Aritran",
        ""
      ],
      [
        "Chukkapalli",
        "Sai Sree Laya",
        ""
      ],
      [
        "Joshi",
        "Anupam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.08527",
    "publish_date": "2020-02-20"
  },
  {
    "id": "2002.10234",
    "submitter": "Yuji Roh",
    "authors": "Yuji Roh, Kangwook Lee, Steven Euijong Whang, Changho Suh",
    "title": "FR-Train: A Mutual Information-Based Approach to Fair and Robust\n  Training",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Trustworthy AI is a critical issue in machine learning where, in addition to\ntraining a model that is accurate, one must consider both fair and robust\ntraining in the presence of data bias and poisoning. However, the existing\nmodel fairness techniques mistakenly view poisoned data as an additional bias\nto be fixed, resulting in severe performance degradation. To address this\nproblem, we propose FR-Train, which holistically performs fair and robust model\ntraining. We provide a mutual information-based interpretation of an existing\nadversarial training-based fairness-only method, and apply this idea to\narchitect an additional discriminator that can identify poisoned data using a\nclean validation set and reduce its influence. In our experiments, FR-Train\nshows almost no decrease in fairness and accuracy in the presence of data\npoisoning by both mitigating the bias and defending against poisoning. We also\ndemonstrate how to construct clean validation sets using crowdsourcing, and\nrelease new benchmark datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 24 Feb 2020 13:37:29 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 3 Jul 2020 07:46:37 GMT"
      }
    ],
    "update_date": "2020-07-06",
    "authors_parsed": [
      [
        "Roh",
        "Yuji",
        ""
      ],
      [
        "Lee",
        "Kangwook",
        ""
      ],
      [
        "Whang",
        "Steven Euijong",
        ""
      ],
      [
        "Suh",
        "Changho",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.10234",
    "publish_date": "2020-02-24"
  },
  {
    "id": "2002.11319",
    "submitter": "Milo M. Lin",
    "authors": "Paul J. Blazek, Milo M. Lin",
    "title": "A neural network model of perception and reasoning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.NE q-bio.NC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  How perception and reasoning arise from neuronal network activity is poorly\nunderstood. This is reflected in the fundamental limitations of connectionist\nartificial intelligence, typified by deep neural networks trained via\ngradient-based optimization. Despite success on many tasks, such networks\nremain unexplainable black boxes incapable of symbolic reasoning and concept\ngeneralization. Here we show that a simple set of biologically consistent\norganizing principles confer these capabilities to neuronal networks. To\ndemonstrate, we implement these principles in a novel machine learning\nalgorithm, based on concept construction instead of optimization, to design\ndeep neural networks that reason with explainable neuron activity. On a range\nof tasks including NP-hard problems, their reasoning capabilities grant\nadditional cognitive functions, like deliberating through self-analysis,\ntolerating adversarial attacks, and learning transferable rules from simple\nexamples to solve problems of unencountered complexity. The networks also\nnaturally display properties of biological nervous systems inherently absent in\ncurrent deep neural networks, including sparsity, modularity, and both\ndistributed and localized firing patterns. Because they do not sacrifice\nperformance, compactness, or training time on standard learning tasks, these\nnetworks provide a new black-box-free approach to artificial intelligence. They\nlikewise serve as a quantitative framework to understand the emergence of\ncognition from neuronal networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 26 Feb 2020 06:26:04 GMT"
      }
    ],
    "update_date": "2020-02-27",
    "authors_parsed": [
      [
        "Blazek",
        "Paul J.",
        ""
      ],
      [
        "Lin",
        "Milo M.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.11319",
    "publish_date": "2020-02-26"
  },
  {
    "id": "2002.12749",
    "submitter": "Paarth Neekhara",
    "authors": "Shehzeen Hussain, Paarth Neekhara, Malhar Jere, Farinaz Koushanfar and\n  Julian McAuley",
    "title": "Adversarial Deepfakes: Evaluating Vulnerability of Deepfake Detectors to\n  Adversarial Examples",
    "comments": "Published as a conference paper at WACV 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in video manipulation techniques have made the generation of\nfake videos more accessible than ever before. Manipulated videos can fuel\ndisinformation and reduce trust in media. Therefore detection of fake videos\nhas garnered immense interest in academia and industry. Recently developed\nDeepfake detection methods rely on deep neural networks (DNNs) to distinguish\nAI-generated fake videos from real videos. In this work, we demonstrate that it\nis possible to bypass such detectors by adversarially modifying fake videos\nsynthesized using existing Deepfake generation methods. We further demonstrate\nthat our adversarial perturbations are robust to image and video compression\ncodecs, making them a real-world threat. We present pipelines in both white-box\nand black-box attack scenarios that can fool DNN based Deepfake detectors into\nclassifying fake videos as real.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 9 Feb 2020 07:10:58 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 14 Mar 2020 01:17:05 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 7 Nov 2020 22:09:38 GMT"
      }
    ],
    "update_date": "2020-11-10",
    "authors_parsed": [
      [
        "Hussain",
        "Shehzeen",
        ""
      ],
      [
        "Neekhara",
        "Paarth",
        ""
      ],
      [
        "Jere",
        "Malhar",
        ""
      ],
      [
        "Koushanfar",
        "Farinaz",
        ""
      ],
      [
        "McAuley",
        "Julian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2002.12749",
    "publish_date": "2020-02-09"
  },
  {
    "id": "2003.02133",
    "submitter": "Lingjuan Lyu",
    "authors": "Lingjuan Lyu, Han Yu, Qiang Yang",
    "title": "Threats to Federated Learning: A Survey",
    "comments": "7 pages, 4 figures, 2 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the emergence of data silos and popular privacy awareness, the\ntraditional centralized approach of training artificial intelligence (AI)\nmodels is facing strong challenges. Federated learning (FL) has recently\nemerged as a promising solution under this new reality. Existing FL protocol\ndesign has been shown to exhibit vulnerabilities which can be exploited by\nadversaries both within and without the system to compromise data privacy. It\nis thus of paramount importance to make FL system designers to be aware of the\nimplications of future FL algorithm design on privacy-preservation. Currently,\nthere is no survey on this topic. In this paper, we bridge this important gap\nin FL literature. By providing a concise introduction to the concept of FL, and\na unique taxonomy covering threat models and two major attacks on FL: 1)\npoisoning attacks and 2) inference attacks, this paper provides an accessible\nreview of this important topic. We highlight the intuitions, key techniques as\nwell as fundamental assumptions adopted by various attacks, and discuss\npromising future research directions towards more robust privacy preservation\nin FL.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Mar 2020 15:30:10 GMT"
      }
    ],
    "update_date": "2020-03-05",
    "authors_parsed": [
      [
        "Lyu",
        "Lingjuan",
        ""
      ],
      [
        "Yu",
        "Han",
        ""
      ],
      [
        "Yang",
        "Qiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2003.02133",
    "publish_date": "2020-03-04"
  },
  {
    "id": "2003.03663",
    "submitter": "Polina Zilberman",
    "authors": "Rami Puzis and Polina Zilberman and Yuval Elovici",
    "title": "ATHAFI: Agile Threat Hunting And Forensic Investigation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Attackers rapidly change their attacks to evade detection. Even the most\nsophisticated Intrusion Detection Systems that are based on artificial\nintelligence and advanced data analytic cannot keep pace with the rapid\ndevelopment of new attacks. When standard detection mechanisms fail or do not\nprovide sufficient forensic information to investigate and mitigate attacks,\ntargeted threat hunting performed by competent personnel is used.\nUnfortunately, many organization do not have enough security analysts to\nperform threat hunting tasks and today the level of automation of threat\nhunting is low.\n  In this paper we describe a framework for agile threat hunting and forensic\ninvestigation (ATHAFI), which automates the threat hunting process at multiple\nlevels. Adaptive targeted data collection, attack hypotheses generation,\nhypotheses testing, and continuous threat intelligence feeds allow to perform\nsimple investigations in a fully automated manner. The increased level of\nautomation will significantly boost the analyst's productivity during\ninvestigation of the harshest cases.\n  Special Workflow Generation module adapts the threat hunting procedures\neither to the latest Threat Intelligence obtained from external sources (e.g.\nNational CERT) or to the likeliest attack hypotheses generated by the Attack\nHypotheses Generation module. The combination of Attack Hypotheses Generation\nand Workflows Generation enables intelligent adjustment of workflows, which\nreact to emerging threats effectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 7 Mar 2020 20:55:57 GMT"
      }
    ],
    "update_date": "2020-03-10",
    "authors_parsed": [
      [
        "Puzis",
        "Rami",
        ""
      ],
      [
        "Zilberman",
        "Polina",
        ""
      ],
      [
        "Elovici",
        "Yuval",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2003.03663",
    "publish_date": "2020-03-07"
  },
  {
    "id": "2003.08837",
    "submitter": "Christian Berghoff",
    "authors": "Christian Berghoff and Matthias Neu and Arndt von Twickel",
    "title": "Vulnerabilities of Connectionist AI Applications: Evaluation and Defence",
    "comments": "20 pages, 8 figures, 1 table",
    "journal-ref": null,
    "doi": "10.3389/fdata.2020.00023",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG cs.SE stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This article deals with the IT security of connectionist artificial\nintelligence (AI) applications, focusing on threats to integrity, one of the\nthree IT security goals. Such threats are for instance most relevant in\nprominent AI computer vision applications. In order to present a holistic view\non the IT security goal integrity, many additional aspects such as\ninterpretability, robustness and documentation are taken into account. A\ncomprehensive list of threats and possible mitigations is presented by\nreviewing the state-of-the-art literature. AI-specific vulnerabilities such as\nadversarial attacks and poisoning attacks as well as their AI-specific root\ncauses are discussed in detail. Additionally and in contrast to former reviews,\nthe whole AI supply chain is analysed with respect to vulnerabilities,\nincluding the planning, data acquisition, training, evaluation and operation\nphases. The discussion of mitigations is likewise not restricted to the level\nof the AI system itself but rather advocates viewing AI systems in the context\nof their supply chains and their embeddings in larger IT infrastructures and\nhardware devices. Based on this and the observation that adaptive attackers may\ncircumvent any single published AI-specific defence to date, the article\nconcludes that single protective measures are not sufficient but rather\nmultiple measures on different levels have to be combined to achieve a minimum\nlevel of IT security for AI applications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 18 Mar 2020 12:33:59 GMT"
      }
    ],
    "update_date": "2020-07-30",
    "authors_parsed": [
      [
        "Berghoff",
        "Christian",
        ""
      ],
      [
        "Neu",
        "Matthias",
        ""
      ],
      [
        "von Twickel",
        "Arndt",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2003.08837",
    "publish_date": "2020-03-18"
  },
  {
    "id": "2003.09234",
    "submitter": "Siwei Lyu",
    "authors": "Siwei Lyu",
    "title": "DeepFake Detection: Current Challenges and Next Steps",
    "comments": "arXiv admin note: text overlap with arXiv:1909.12962",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  High quality fake videos and audios generated by AI-algorithms (the deep\nfakes) have started to challenge the status of videos and audios as definitive\nevidence of events. In this paper, we highlight a few of these challenges and\ndiscuss the research opportunities in this direction.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Mar 2020 13:20:42 GMT"
      }
    ],
    "update_date": "2020-03-23",
    "authors_parsed": [
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2003.09234",
    "publish_date": "2020-03-11"
  },
  {
    "id": "2003.11917",
    "submitter": "Cameron Buckner",
    "authors": "Cameron Buckner",
    "title": "Adversarial Examples and the Deeper Riddle of Induction: The Need for a\n  Theory of Artifacts in Deep Learning",
    "comments": "24 pages, 9 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning is currently the most widespread and successful technology in\nartificial intelligence. It promises to push the frontier of scientific\ndiscovery beyond current limits. However, skeptics have worried that deep\nneural networks are black boxes, and have called into question whether these\nadvances can really be deemed scientific progress if humans cannot understand\nthem. Relatedly, these systems also possess bewildering new vulnerabilities:\nmost notably a susceptibility to \"adversarial examples\". In this paper, I argue\nthat adversarial examples will become a flashpoint of debate in philosophy and\ndiverse sciences. Specifically, new findings concerning adversarial examples\nhave challenged the consensus view that the networks' verdicts on these cases\nare caused by overfitting idiosyncratic noise in the training set, and may\ninstead be the result of detecting predictively useful \"intrinsic features of\nthe data geometry\" that humans cannot perceive (Ilyas et al., 2019). These\nresults should cause us to re-examine responses to one of the deepest puzzles\nat the intersection of philosophy and science: Nelson Goodman's \"new riddle\" of\ninduction. Specifically, they raise the possibility that progress in a number\nof sciences will depend upon the detection and manipulation of useful features\nthat humans find inscrutable. Before we can evaluate this possibility, however,\nwe must decide which (if any) of these inscrutable features are real but\navailable only to \"alien\" perception and cognition, and which are distinctive\nartifacts of deep learning-for artifacts like lens flares or Gibbs phenomena\ncan be similarly useful for prediction, but are usually seen as obstacles to\nscientific theorizing. Thus, machine learning researchers urgently need to\ndevelop a theory of artifacts for deep neural networks, and I conclude by\nsketching some initial directions for this area of research.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 20 Mar 2020 16:24:25 GMT"
      }
    ],
    "update_date": "2020-03-27",
    "authors_parsed": [
      [
        "Buckner",
        "Cameron",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2003.11917",
    "publish_date": "2020-03-20"
  },
  {
    "id": "2004.04479",
    "submitter": "Ivan Y. Tyukin",
    "authors": "Ivan Y. Tyukin, Desmond J. Higham, and Alexander N. Gorban",
    "title": "On Adversarial Examples and Stealth Attacks in Artificial Intelligence\n  Systems",
    "comments": null,
    "journal-ref": "2020 International Joint Conference on Neural Networks (IJCNN),\n  Glasgow, United Kingdom, 2020",
    "doi": "10.1109/IJCNN48605.2020.9207472",
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In this work we present a formal theoretical framework for assessing and\nanalyzing two classes of malevolent action towards generic Artificial\nIntelligence (AI) systems. Our results apply to general multi-class classifiers\nthat map from an input space into a decision space, including artificial neural\nnetworks used in deep learning applications. Two classes of attacks are\nconsidered. The first class involves adversarial examples and concerns the\nintroduction of small perturbations of the input data that cause\nmisclassification. The second class, introduced here for the first time and\nnamed stealth attacks, involves small perturbations to the AI system itself.\nHere the perturbed system produces whatever output is desired by the attacker\non a specific small data set, perhaps even a single input, but performs as\nnormal on a validation set (which is unknown to the attacker). We show that in\nboth cases, i.e., in the case of an attack based on adversarial examples and in\nthe case of a stealth attack, the dimensionality of the AI's decision-making\nspace is a major contributor to the AI's susceptibility. For attacks based on\nadversarial examples, a second crucial parameter is the absence of local\nconcentrations in the data probability distribution, a property known as\nSmeared Absolute Continuity. According to our findings, robustness to\nadversarial examples requires either (a) the data distributions in the AI's\nfeature space to have concentrated probability density functions or (b) the\ndimensionality of the AI's decision variables to be sufficiently small. We also\nshow how to construct stealth attacks on high-dimensional AI systems that are\nhard to spot unless the validation set is made exponentially large.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 9 Apr 2020 10:56:53 GMT"
      }
    ],
    "update_date": "2021-01-01",
    "authors_parsed": [
      [
        "Tyukin",
        "Ivan Y.",
        ""
      ],
      [
        "Higham",
        "Desmond J.",
        ""
      ],
      [
        "Gorban",
        "Alexander N.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2004.04479",
    "publish_date": "2020-04-09"
  },
  {
    "id": "2004.06383",
    "submitter": "Jon Vadillo Jueguen",
    "authors": "Jon Vadillo, Roberto Santana and Jose A. Lozano",
    "title": "Extending Adversarial Attacks to Produce Adversarial Class Probability\n  Distributions",
    "comments": "Final version as accepted in JMLR. Attribution requirements are\n  provided at http://jmlr.org/papers/v24/21-0326.html",
    "journal-ref": "Journal of Machine Learning Research, 24(15):1-42, 2023",
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Despite the remarkable performance and generalization levels of deep learning\nmodels in a wide range of artificial intelligence tasks, it has been\ndemonstrated that these models can be easily fooled by the addition of\nimperceptible yet malicious perturbations to natural inputs. These altered\ninputs are known in the literature as adversarial examples. In this paper, we\npropose a novel probabilistic framework to generalize and extend adversarial\nattacks in order to produce a desired probability distribution for the classes\nwhen we apply the attack method to a large number of inputs. This novel attack\nparadigm provides the adversary with greater control over the target model,\nthereby exposing, in a wide range of scenarios, threats against deep learning\nmodels that cannot be conducted by the conventional paradigms. We introduce\nfour different strategies to efficiently generate such attacks, and illustrate\nour approach by extending multiple adversarial attack algorithms. We also\nexperimentally validate our approach for the spoken command classification task\nand the Tweet emotion classification task, two exemplary machine learning\nproblems in the audio and text domain, respectively. Our results demonstrate\nthat we can closely approximate any probability distribution for the classes\nwhile maintaining a high fooling rate and even prevent the attacks from being\ndetected by label-shift detection methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Apr 2020 09:39:02 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 21 Sep 2021 23:25:22 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 25 Jan 2023 19:19:58 GMT"
      }
    ],
    "update_date": "2023-01-27",
    "authors_parsed": [
      [
        "Vadillo",
        "Jon",
        ""
      ],
      [
        "Santana",
        "Roberto",
        ""
      ],
      [
        "Lozano",
        "Jose A.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2004.06383",
    "publish_date": "2023-01-25"
  },
  {
    "id": "2004.07919",
    "submitter": "Deqiang Li",
    "authors": "Deqiang Li, Qianmu Li, Yanfang Ye, and Shouhuai Xu",
    "title": "A Framework for Enhancing Deep Neural Networks Against Adversarial\n  Malware",
    "comments": "A fully-fledge version for the preliminary paper arXiv:1812.08108 |\n  D. Li, Q. Li, Y. Ye, and S. Xu, \"A Framework for Enhancing Deep Neural\n  Networks Against Adversarial Malware\", in IEEE Transactions on Network\n  Science and Engineering",
    "journal-ref": null,
    "doi": "10.1109/TNSE.2021.3051354",
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Machine learning-based malware detection is known to be vulnerable to\nadversarial evasion attacks. The state-of-the-art is that there are no\neffective defenses against these attacks. As a response to the adversarial\nmalware classification challenge organized by the MIT Lincoln Lab and\nassociated with the AAAI-19 Workshop on Artificial Intelligence for Cyber\nSecurity (AICS'2019), we propose six guiding principles to enhance the\nrobustness of deep neural networks. Some of these principles have been\nscattered in the literature, but the others are introduced in this paper for\nthe first time. Under the guidance of these six principles, we propose a\ndefense framework to enhance the robustness of deep neural networks against\nadversarial malware evasion attacks. By conducting experiments with the Drebin\nAndroid malware dataset, we show that the framework can achieve a 98.49\\%\naccuracy (on average) against grey-box attacks, where the attacker knows some\ninformation about the defense and the defender knows some information about the\nattack, and an 89.14% accuracy (on average) against the more capable white-box\nattacks, where the attacker knows everything about the defense and the defender\nknows some information about the attack. The framework wins the AICS'2019\nchallenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the\nchallenge organizer) knows the framework or defense nor we (the defender) know\nthe attacks. This gap highlights the importance of knowing about the attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Apr 2020 07:00:47 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Jan 2021 06:34:53 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 15 Jan 2021 15:29:02 GMT"
      }
    ],
    "update_date": "2021-01-18",
    "authors_parsed": [
      [
        "Li",
        "Deqiang",
        ""
      ],
      [
        "Li",
        "Qianmu",
        ""
      ],
      [
        "Ye",
        "Yanfang",
        ""
      ],
      [
        "Xu",
        "Shouhuai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2004.07919",
    "publish_date": "2020-04-15"
  },
  {
    "id": "2004.09392",
    "submitter": "WaiChing Sun",
    "authors": "Kun Wang, WaiChing Sun, Qiang Du",
    "title": "A non-cooperative meta-modeling game for automated third-party\n  calibrating, validating, and falsifying constitutive laws with parallelized\n  adversarial attacks",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1016/j.cma.2020.113514",
    "report-no": null,
    "categories": "eess.SP cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The evaluation of constitutive models, especially for high-risk and\nhigh-regret engineering applications, requires efficient and rigorous\nthird-party calibration, validation and falsification. While there are numerous\nefforts to develop paradigms and standard procedures to validate models,\ndifficulties may arise due to the sequential, manual and often biased nature of\nthe commonly adopted calibration and validation processes, thus slowing down\ndata collections, hampering the progress towards discovering new physics,\nincreasing expenses and possibly leading to misinterpretations of the\ncredibility and application ranges of proposed models. This work attempts to\nintroduce concepts from game theory and machine learning techniques to overcome\nmany of these existing difficulties. We introduce an automated meta-modeling\ngame where two competing AI agents systematically generate experimental data to\ncalibrate a given constitutive model and to explore its weakness, in order to\nimprove experiment design and model robustness through competition. The two\nagents automatically search for the Nash equilibrium of the meta-modeling game\nin an adversarial reinforcement learning framework without human intervention.\nBy capturing all possible design options of the laboratory experiments into a\nsingle decision tree, we recast the design of experiments as a game of\ncombinatorial moves that can be resolved through deep reinforcement learning by\nthe two competing players. Our adversarial framework emulates idealized\nscientific collaborations and competitions among researchers to achieve a\nbetter understanding of the application range of the learned material laws and\nprevent misinterpretations caused by conventional AI-based third-party\nvalidation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Apr 2020 18:43:28 GMT"
      }
    ],
    "update_date": "2020-12-02",
    "authors_parsed": [
      [
        "Wang",
        "Kun",
        ""
      ],
      [
        "Sun",
        "WaiChing",
        ""
      ],
      [
        "Du",
        "Qiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2004.09392",
    "publish_date": "2020-04-13"
  },
  {
    "id": "2004.12771",
    "submitter": "Konda Reddy Mopuri",
    "authors": "Konda Reddy Mopuri, Vaisakh Shaj and R. Venkatesh Babu",
    "title": "Adversarial Fooling Beyond \"Flipping the Label\"",
    "comments": "CVPR-AMLCV-2020",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advancements in CNNs have shown remarkable achievements in various\nCV/AI applications. Though CNNs show near human or better than human\nperformance in many critical tasks, they are quite vulnerable to adversarial\nattacks. These attacks are potentially dangerous in real-life deployments.\nThough there have been many adversarial attacks proposed in recent years, there\nis no proper way of quantifying the effectiveness of these attacks. As of\ntoday, mere fooling rate is used for measuring the susceptibility of the\nmodels, or the effectiveness of adversarial attacks. Fooling rate just\nconsiders label flipping and does not consider the cost of such flipping, for\ninstance, in some deployments, flipping between two species of dogs may not be\nas severe as confusing a dog category with that of a vehicle. Therefore, the\nmetric to quantify the vulnerability of the models should capture the severity\nof the flipping as well. In this work we first bring out the drawbacks of the\nexisting evaluation and propose novel metrics to capture various aspects of the\nfooling. Further, for the first time, we present a comprehensive analysis of\nseveral important adversarial attacks over a set of distinct CNN architectures.\nWe believe that the presented analysis brings valuable insights about the\ncurrent adversarial attacks and the CNN models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Apr 2020 13:21:03 GMT"
      }
    ],
    "update_date": "2020-04-28",
    "authors_parsed": [
      [
        "Mopuri",
        "Konda Reddy",
        ""
      ],
      [
        "Shaj",
        "Vaisakh",
        ""
      ],
      [
        "Babu",
        "R. Venkatesh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2004.12771",
    "publish_date": "2020-04-27"
  },
  {
    "id": "2005.00229",
    "submitter": "Rahul U",
    "authors": "Rahul U, Ragul M, Raja Vignesh K, Tejeswinee K",
    "title": "Deepfake Forensics Using Recurrent Neural Networks",
    "comments": "This submission has been removed by arXiv administrators due to\n  copyright infringement",
    "journal-ref": null,
    "doi": "10.1729/Journal.22894",
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As of late an AI based free programming device has made it simple to make\nauthentic face swaps in recordings that leaves barely any hints of control, in\nwhat are known as \"deepfake\" recordings. Situations where these genuine istic\ncounterfeit recordings are utilized to make political pain, extort somebody or\nphony fear based oppression occasions are effectively imagined. This paper\nproposes a transient mindful pipeline to automat-ically recognize deepfake\nrecordings. Our framework utilizes a convolutional neural system (CNN) to\nremove outline level highlights. These highlights are then used to prepare a\nrepetitive neural net-work (RNN) that figures out how to characterize if a\nvideo has been sub-ject to control or not. We assess our technique against a\nhuge arrangement of deepfake recordings gathered from different video sites. We\nshow how our framework can accomplish aggressive outcomes in this assignment\nwhile utilizing a basic design.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 May 2020 05:27:16 GMT"
      }
    ],
    "update_date": "2020-09-17",
    "authors_parsed": [
      [
        "U",
        "Rahul",
        ""
      ],
      [
        "M",
        "Ragul",
        ""
      ],
      [
        "K",
        "Raja Vignesh",
        ""
      ],
      [
        "K",
        "Tejeswinee",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2005.00229",
    "publish_date": "2020-05-01"
  },
  {
    "id": "2005.00816",
    "submitter": "Swaroop Mishra",
    "authors": "Swaroop Mishra, Anjana Arunkumar, Bhavdeep Sachdeva, Chris Bryan,\n  Chitta Baral",
    "title": "DQI: Measuring Data Quality in NLP",
    "comments": "63 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 2 May 2020 12:34:17 GMT"
      }
    ],
    "update_date": "2020-05-05",
    "authors_parsed": [
      [
        "Mishra",
        "Swaroop",
        ""
      ],
      [
        "Arunkumar",
        "Anjana",
        ""
      ],
      [
        "Sachdeva",
        "Bhavdeep",
        ""
      ],
      [
        "Bryan",
        "Chris",
        ""
      ],
      [
        "Baral",
        "Chitta",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2005.00816",
    "publish_date": "2020-05-02"
  },
  {
    "id": "2005.12227",
    "submitter": "Marius Lombard-Platet",
    "authors": "Yao Cheng, Cheng-Kang Chu, Hsiao-Ying Lin, Marius Lombard-Platet,\n  David Naccache",
    "title": "Keyed Non-Parametric Hypothesis Tests",
    "comments": "Paper published in NSS 2019",
    "journal-ref": null,
    "doi": "10.1007/978-3-030-36938-5_39",
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The recent popularity of machine learning calls for a deeper understanding of\nAI security. Amongst the numerous AI threats published so far, poisoning\nattacks currently attract considerable attention. In a poisoning attack the\nopponent partially tampers the dataset used for learning to mislead the\nclassifier during the testing phase.\n  This paper proposes a new protection strategy against poisoning attacks. The\ntechnique relies on a new primitive called keyed non-parametric hypothesis\ntests allowing to evaluate under adversarial conditions the training input's\nconformance with a previously learned distribution $\\mathfrak{D}$. To do so we\nuse a secret key $\\kappa$ unknown to the opponent.\n  Keyed non-parametric hypothesis tests differs from classical tests in that\nthe secrecy of $\\kappa$ prevents the opponent from misleading the keyed test\ninto concluding that a (significantly) tampered dataset belongs to\n$\\mathfrak{D}$.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 25 May 2020 16:58:09 GMT"
      }
    ],
    "update_date": "2020-05-26",
    "authors_parsed": [
      [
        "Cheng",
        "Yao",
        ""
      ],
      [
        "Chu",
        "Cheng-Kang",
        ""
      ],
      [
        "Lin",
        "Hsiao-Ying",
        ""
      ],
      [
        "Lombard-Platet",
        "Marius",
        ""
      ],
      [
        "Naccache",
        "David",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2005.12227",
    "publish_date": "2020-05-25"
  },
  {
    "id": "2006.02724",
    "submitter": "Saurav Musunuru",
    "authors": "Saurav Musunuru, Jay N. Paranjape, Rahul Kumar Dubey and Vijendran G.\n  Venkoparao",
    "title": "Characterizing the Weight Space for Different Learning Models",
    "comments": "6 pages, 8 figures, CONF CDS 2020",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep Learning has become one of the primary research areas in developing\nintelligent machines. Most of the well-known applications (such as Speech\nRecognition, Image Processing and NLP) of AI are driven by Deep Learning. Deep\nLearning algorithms mimic human brain using artificial neural networks and\nprogressively learn to accurately solve a given problem. But there are\nsignificant challenges in Deep Learning systems. There have been many attempts\nto make deep learning models imitate the biological neural network. However,\nmany deep learning models have performed poorly in the presence of adversarial\nexamples. Poor performance in adversarial examples leads to adversarial attacks\nand in turn leads to safety and security in most of the applications. In this\npaper we make an attempt to characterize the solution space of a deep neural\nnetwork in terms of three different subsets viz. weights belonging to exact\ntrained patterns, weights belonging to generalized pattern set and weights\nbelonging to adversarial pattern sets. We attempt to characterize the solution\nspace with two seemingly different learning paradigms viz. the Deep Neural\nNetworks and the Dense Associative Memory Model, which try to achieve learning\nvia quite different mechanisms. We also show that adversarial attacks are\ngenerally less successful against Associative Memory Models than Deep Neural\nNetworks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 4 Jun 2020 09:30:29 GMT"
      }
    ],
    "update_date": "2020-06-05",
    "authors_parsed": [
      [
        "Musunuru",
        "Saurav",
        ""
      ],
      [
        "Paranjape",
        "Jay N.",
        ""
      ],
      [
        "Dubey",
        "Rahul Kumar",
        ""
      ],
      [
        "Venkoparao",
        "Vijendran G.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2006.02724",
    "publish_date": "2020-06-04"
  },
  {
    "id": "2006.13555",
    "submitter": "Xin Li",
    "authors": "Xin Li, Deng Pan, Dongxiao Zhu",
    "title": "Defending against adversarial attacks on medical imaging AI system,\n  classification or detection?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CV eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Medical imaging AI systems such as disease classification and segmentation\nare increasingly inspired and transformed from computer vision based AI\nsystems. Although an array of adversarial training and/or loss function based\ndefense techniques have been developed and proved to be effective in computer\nvision, defending against adversarial attacks on medical images remains largely\nan uncharted territory due to the following unique challenges: 1) label\nscarcity in medical images significantly limits adversarial generalizability of\nthe AI system; 2) vastly similar and dominant fore- and background in medical\nimages make it hard samples for learning the discriminating features between\ndifferent disease classes; and 3) crafted adversarial noises added to the\nentire medical image as opposed to the focused organ target can make clean and\nadversarial examples more discriminate than that between different disease\nclasses. In this paper, we propose a novel robust medical imaging AI framework\nbased on Semi-Supervised Adversarial Training (SSAT) and Unsupervised\nAdversarial Detection (UAD), followed by designing a new measure for assessing\nsystems adversarial risk. We systematically demonstrate the advantages of our\nrobust medical imaging AI system over the existing adversarial defense\ntechniques under diverse real-world settings of adversarial attacks using a\nbenchmark OCT imaging data set.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 24 Jun 2020 08:26:49 GMT"
      }
    ],
    "update_date": "2020-06-25",
    "authors_parsed": [
      [
        "Li",
        "Xin",
        ""
      ],
      [
        "Pan",
        "Deng",
        ""
      ],
      [
        "Zhu",
        "Dongxiao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2006.13555",
    "publish_date": "2020-06-24"
  },
  {
    "id": "2006.15632",
    "submitter": "Mingsong Chen",
    "authors": "Yunfei Song, Tian Liu, Tongquan Wei, Xiangfeng Wang, Zhe Tao, Mingsong\n  Chen",
    "title": "FDA3 : Federated Defense Against Adversarial Attacks for Cloud-Based\n  IIoT Applications",
    "comments": null,
    "journal-ref": "IEEE Transactions on Industrial Informatics, 2020",
    "doi": "10.1109/TII.2020.3005969",
    "report-no": null,
    "categories": "cs.LG cs.CR stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Along with the proliferation of Artificial Intelligence (AI) and Internet of\nThings (IoT) techniques, various kinds of adversarial attacks are increasingly\nemerging to fool Deep Neural Networks (DNNs) used by Industrial IoT (IIoT)\napplications. Due to biased training data or vulnerable underlying models,\nimperceptible modifications on inputs made by adversarial attacks may result in\ndevastating consequences. Although existing methods are promising in defending\nsuch malicious attacks, most of them can only deal with limited existing attack\ntypes, which makes the deployment of large-scale IIoT devices a great\nchallenge. To address this problem, we present an effective federated defense\napproach named FDA3 that can aggregate defense knowledge against adversarial\nexamples from different sources. Inspired by federated learning, our proposed\ncloud-based architecture enables the sharing of defense capabilities against\ndifferent attacks among IIoT devices. Comprehensive experimental results show\nthat the generated DNNs by our approach can not only resist more malicious\nattacks than existing attack-specific adversarial training methods, but also\ncan prevent IIoT applications from new attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 28 Jun 2020 15:17:15 GMT"
      }
    ],
    "update_date": "2020-06-30",
    "authors_parsed": [
      [
        "Song",
        "Yunfei",
        ""
      ],
      [
        "Liu",
        "Tian",
        ""
      ],
      [
        "Wei",
        "Tongquan",
        ""
      ],
      [
        "Wang",
        "Xiangfeng",
        ""
      ],
      [
        "Tao",
        "Zhe",
        ""
      ],
      [
        "Chen",
        "Mingsong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2006.15632",
    "publish_date": "2020-06-28"
  },
  {
    "id": "2007.00337",
    "submitter": "Kishor Datta Gupta",
    "authors": "Kishor Datta Gupta, Zahid Akhtar, Dipankar Dasgupta",
    "title": "Determining Sequence of Image Processing Technique (IPT) to Detect\n  Adversarial Attacks",
    "comments": null,
    "journal-ref": "SN COMPUT. SCI. 2, 383 (2021)",
    "doi": "10.1007/s42979-021-00773-8",
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Developing secure machine learning models from adversarial examples is\nchallenging as various methods are continually being developed to generate\nadversarial attacks. In this work, we propose an evolutionary approach to\nautomatically determine Image Processing Techniques Sequence (IPTS) for\ndetecting malicious inputs. Accordingly, we first used a diverse set of attack\nmethods including adaptive attack methods (on our defense) to generate\nadversarial samples from the clean dataset. A detection framework based on a\ngenetic algorithm (GA) is developed to find the optimal IPTS, where the\noptimality is estimated by different fitness measures such as Euclidean\ndistance, entropy loss, average histogram, local binary pattern and loss\nfunctions. The \"image difference\" between the original and processed images is\nused to extract the features, which are then fed to a classification scheme in\norder to determine whether the input sample is adversarial or clean. This paper\ndescribed our methodology and performed experiments using multiple data-sets\ntested with several adversarial attacks. For each attack-type and dataset, it\ngenerates unique IPTS. A set of IPTS selected dynamically in testing time which\nworks as a filter for the adversarial attack. Our empirical experiments\nexhibited promising results indicating the approach can efficiently be used as\nprocessing for any AI model.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Jul 2020 08:59:14 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 7 Jul 2020 09:26:57 GMT"
      }
    ],
    "update_date": "2021-08-26",
    "authors_parsed": [
      [
        "Gupta",
        "Kishor Datta",
        ""
      ],
      [
        "Akhtar",
        "Zahid",
        ""
      ],
      [
        "Dasgupta",
        "Dipankar",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.00337",
    "publish_date": "2020-07-01"
  },
  {
    "id": "2007.00339",
    "submitter": "Weizhu Qian",
    "authors": "Weizhu Qian, Bowei Chen, Yichao Zhang, Guanghui Wen and Franck Gechter",
    "title": "Multi-Task Variational Information Bottleneck",
    "comments": "10 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG stat.ML",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Multi-task learning (MTL) is an important subject in machine learning and\nartificial intelligence. Its applications to computer vision, signal\nprocessing, and speech recognition are ubiquitous. Although this subject has\nattracted considerable attention recently, the performance and robustness of\nthe existing models to different tasks have not been well balanced. This\narticle proposes an MTL model based on the architecture of the variational\ninformation bottleneck (VIB), which can provide a more effective latent\nrepresentation of the input features for the downstream tasks. Extensive\nobservations on three public data sets under adversarial attacks show that the\nproposed model is competitive to the state-of-the-art algorithms concerning the\nprediction accuracy. Experimental results suggest that combining the VIB and\nthe task-dependent uncertainties is a very effective way to abstract valid\ninformation from the input features for accomplishing multiple tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Jul 2020 09:06:20 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 10 Sep 2020 13:10:30 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 18 Dec 2020 09:35:12 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 1 Mar 2021 12:12:22 GMT"
      }
    ],
    "update_date": "2021-03-02",
    "authors_parsed": [
      [
        "Qian",
        "Weizhu",
        ""
      ],
      [
        "Chen",
        "Bowei",
        ""
      ],
      [
        "Zhang",
        "Yichao",
        ""
      ],
      [
        "Wen",
        "Guanghui",
        ""
      ],
      [
        "Gechter",
        "Franck",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.00339",
    "publish_date": "2020-07-01"
  },
  {
    "id": "2007.03616",
    "submitter": "Michael Falk",
    "authors": "Michael Falk",
    "title": "Artificial Stupidity",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Public debate about AI is dominated by Frankenstein Syndrome, the fear that\nAI will become superhuman and escape human control. Although superintelligence\nis certainly a possibility, the interest it excites can distract the public\nfrom a more imminent concern: the rise of Artificial Stupidity (AS). This\narticle discusses the roots of Frankenstein Syndrome in Mary Shelley's famous\nnovel of 1818. It then provides a philosophical framework for analysing the\nstupidity of artificial agents, demonstrating that modern intelligent systems\ncan be seen to suffer from 'stupidity of judgement'. Finally it identifies an\nalternative literary tradition that exposes the perils and benefits of AS. In\nthe writings of Edmund Spenser, Jonathan Swift and E.T.A. Hoffmann, ASs\nreplace, oppress or seduce their human users. More optimistically, Joseph\nFurphy and Laurence Sterne imagine ASs that can serve human intellect as maps\nor as pipes. These writers provide a strong counternarrative to the myths that\ncurrently drive the AI debate. They identify ways in which even stupid\nartificial agents can evade human control, for instance by appealing to\nstereotypes or distancing us from reality. And they underscore the continuing\nimportance of the literary imagination in an increasingly automated society.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 2 Jul 2020 00:37:23 GMT"
      }
    ],
    "update_date": "2020-07-08",
    "authors_parsed": [
      [
        "Falk",
        "Michael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.03616",
    "publish_date": "2020-07-02"
  },
  {
    "id": "2007.03838",
    "submitter": "Wei Li",
    "authors": "Junhua Zou, Yexin Duan, Boyu Li, Wu Zhang, Yu Pan, Zhisong Pan",
    "title": "Making Adversarial Examples More Transferable and Indistinguishable",
    "comments": "Accepted to AAAI2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Fast gradient sign attack series are popular methods that are used to\ngenerate adversarial examples. However, most of the approaches based on fast\ngradient sign attack series cannot balance the indistinguishability and\ntransferability due to the limitations of the basic sign structure. To address\nthis problem, we propose a method, called Adam Iterative Fast Gradient Tanh\nMethod (AI-FGTM), to generate indistinguishable adversarial examples with high\ntransferability. Besides, smaller kernels and dynamic step size are also\napplied to generate adversarial examples for further increasing the attack\nsuccess rates. Extensive experiments on an ImageNet-compatible dataset show\nthat our method generates more indistinguishable adversarial examples and\nachieves higher attack success rates without extra running time and resource.\nOur best transfer-based attack NI-TI-DI-AITM can fool six classic defense\nmodels with an average success rate of 89.3% and three advanced defense models\nwith an average success rate of 82.7%, which are higher than the\nstate-of-the-art gradient-based attacks. Additionally, our method can also\nreduce nearly 20% mean perturbation. We expect that our method will serve as a\nnew baseline for generating adversarial examples with better transferability\nand indistinguishability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Jul 2020 01:12:56 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 11 Dec 2021 13:42:00 GMT"
      }
    ],
    "update_date": "2021-12-14",
    "authors_parsed": [
      [
        "Zou",
        "Junhua",
        ""
      ],
      [
        "Duan",
        "Yexin",
        ""
      ],
      [
        "Li",
        "Boyu",
        ""
      ],
      [
        "Zhang",
        "Wu",
        ""
      ],
      [
        "Pan",
        "Yu",
        ""
      ],
      [
        "Pan",
        "Zhisong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.03838",
    "publish_date": "2020-07-08"
  },
  {
    "id": "2007.09327",
    "submitter": "Ibrahim Ahmed",
    "authors": "Ibrahim H. Ahmed, Josiah P. Hanna, Elliot Fosong, and Stefano V.\n  Albrecht",
    "title": "Towards Quantum-Secure Authentication and Key Agreement via Abstract\n  Multi-Agent Interaction",
    "comments": "Published at the 19th International Conference on Practical\n  Applications of Agents and Multi-Agent Systems (PAAMS 2021)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG cs.MA",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Current methods for authentication and key agreement based on public-key\ncryptography are vulnerable to quantum computing. We propose a novel approach\nbased on artificial intelligence research in which communicating parties are\nviewed as autonomous agents which interact repeatedly using their private\ndecision models. Authentication and key agreement are decided based on the\nagents' observed behaviors during the interaction. The security of this\napproach rests upon the difficulty of modeling the decisions of interacting\nagents from limited observations, a problem which we conjecture is also hard\nfor quantum computing. We release PyAMI, a prototype authentication and key\nagreement system based on the proposed method. We empirically validate our\nmethod for authenticating legitimate users while detecting different types of\nadversarial attacks. Finally, we show how reinforcement learning techniques can\nbe used to train server models which effectively probe a client's decisions to\nachieve more sample-efficient authentication.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 18 Jul 2020 04:22:02 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 9 Jul 2021 16:13:41 GMT"
      }
    ],
    "update_date": "2021-07-12",
    "authors_parsed": [
      [
        "Ahmed",
        "Ibrahim H.",
        ""
      ],
      [
        "Hanna",
        "Josiah P.",
        ""
      ],
      [
        "Fosong",
        "Elliot",
        ""
      ],
      [
        "Albrecht",
        "Stefano V.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.09327",
    "publish_date": "2020-07-18"
  },
  {
    "id": "2007.11820",
    "submitter": "Junli Shen",
    "authors": "Junli Shen, Maocai Xia",
    "title": "AI Data poisoning attack: Manipulating game AI of Go",
    "comments": "Fixed some inappropriate information from previous versions",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the extensive use of AI in various fields, the issue of AI security has\nbecome more significant. The AI data poisoning attacks will be the most\nthreatening approach against AI security after the adversarial examples. As the\ncontinuous updating of AI applications online, the data pollution models can be\nuploaded by attackers to achieve a certain malicious purpose. Recently, the\nresearch on AI data poisoning attacks is mostly out of practice and use\nself-built experimental environments so that it cannot be as close to reality\nas adversarial example attacks. This article's first contribution is to provide\na solution and a breakthrough for the aforementioned issue with research\nlimitations, to aim at data poisoning attacks that target real businesses, in\nthis case: data poisoning attacks on real Go AI. We install a Trojan virus into\nthe real Go AI that manipulates the AI's behavior. It is the first time that we\nsucceed in manipulating complicated AI and provide a reliable approach to the\nAI data poisoning attack verification method. The method of building Trojan in\nthis article can be expanded to more practical algorithms for other fields such\nas content recommendation, text translation, and intelligent dialogue.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 23 Jul 2020 06:40:46 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 29 Jul 2020 01:59:16 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 30 Jul 2020 02:17:40 GMT"
      }
    ],
    "update_date": "2020-07-31",
    "authors_parsed": [
      [
        "Shen",
        "Junli",
        ""
      ],
      [
        "Xia",
        "Maocai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.11820",
    "publish_date": "2020-07-29"
  },
  {
    "id": "2007.15290",
    "submitter": "Han Qiu",
    "authors": "Yi Zeng, Han Qiu, Gerard Memmi, Meikang Qiu",
    "title": "A Data Augmentation-based Defense Method Against Adversarial Attacks in\n  Neural Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep Neural Networks (DNNs) in Computer Vision (CV) are well-known to be\nvulnerable to Adversarial Examples (AEs), namely imperceptible perturbations\nadded maliciously to cause wrong classification results. Such variability has\nbeen a potential risk for systems in real-life equipped DNNs as core\ncomponents. Numerous efforts have been put into research on how to protect DNN\nmodels from being tackled by AEs. However, no previous work can efficiently\nreduce the effects caused by novel adversarial attacks and be compatible with\nreal-life constraints at the same time. In this paper, we focus on developing a\nlightweight defense method that can efficiently invalidate full whitebox\nadversarial attacks with the compatibility of real-life constraints. From basic\naffine transformations, we integrate three transformations with randomized\ncoefficients that fine-tuned respecting the amount of change to the defended\nsample. Comparing to 4 state-of-art defense methods published in top-tier AI\nconferences in the past two years, our method demonstrates outstanding\nrobustness and efficiency. It is worth highlighting that, our model can\nwithstand advanced adaptive attack, namely BPDA with 50 rounds, and still helps\nthe target model maintain an accuracy around 80 %, meanwhile constraining the\nattack success rate to almost zero.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 30 Jul 2020 08:06:53 GMT"
      }
    ],
    "update_date": "2020-07-31",
    "authors_parsed": [
      [
        "Zeng",
        "Yi",
        ""
      ],
      [
        "Qiu",
        "Han",
        ""
      ],
      [
        "Memmi",
        "Gerard",
        ""
      ],
      [
        "Qiu",
        "Meikang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2007.15290",
    "publish_date": "2020-07-30"
  },
  {
    "id": "2008.04095",
    "submitter": "Luca Guarnera",
    "authors": "Luca Guarnera (1 and 2), Oliver Giudice (1), Sebastiano Battiato (1\n  and 2) ((1) University of Catania, (2) iCTLab s.r.l. - Spin-off of University\n  of Catania)",
    "title": "Fighting Deepfake by Exposing the Convolutional Traces on Images",
    "comments": "arXiv admin note: text overlap with arXiv:2004.10448",
    "journal-ref": "IEEE Access 2020",
    "doi": "10.1109/ACCESS.2020.3023037",
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Advances in Artificial Intelligence and Image Processing are changing the way\npeople interacts with digital images and video. Widespread mobile apps like\nFACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to\nproduce extreme transformations on human face photos such gender swap, aging,\netc. The results are utterly realistic and extremely easy to be exploited even\nfor non-experienced users. This kind of media object took the name of Deepfake\nand raised a new challenge in the multimedia forensics field: the Deepfake\ndetection challenge. Indeed, discriminating a Deepfake from a real image could\nbe a difficult task even for human eyes but recent works are trying to apply\nthe same technology used for generating images for discriminating them with\npreliminary good results but with many limitations: employed Convolutional\nNeural Networks are not so robust, demonstrate to be specific to the context\nand tend to extract semantics from images. In this paper, a new approach aimed\nto extract a Deepfake fingerprint from images is proposed. The method is based\non the Expectation-Maximization algorithm trained to detect and extract a\nfingerprint that represents the Convolutional Traces (CT) left by GANs during\nimage generation. The CT demonstrates to have high discriminative power\nachieving better results than state-of-the-art in the Deepfake detection task\nalso proving to be robust to different attacks. Achieving an overall\nclassification accuracy of over 98%, considering Deepfakes from 10 different\nGAN architectures not only involved in images of faces, the CT demonstrates to\nbe reliable and without any dependence on image semantic. Finally, tests\ncarried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the\nfake detection task, demonstrated the effectiveness of the proposed technique\non a real-case scenario.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 7 Aug 2020 08:49:23 GMT"
      }
    ],
    "update_date": "2020-09-14",
    "authors_parsed": [
      [
        "Guarnera",
        "Luca",
        "",
        "1 and 2"
      ],
      [
        "Giudice",
        "Oliver",
        "",
        "1\n  and 2"
      ],
      [
        "Battiato",
        "Sebastiano",
        "",
        "1\n  and 2"
      ]
    ],
    "url": "https://arxiv.org/pdf/2008.04095",
    "publish_date": "2020-08-07"
  },
  {
    "id": "2008.10138",
    "submitter": "Sayedmasoud Hashemi Amroabadi",
    "authors": "Masoud Hashemi, Ali Fathi",
    "title": "PermuteAttack: Counterfactual Explanation of Machine Learning Credit\n  Scorecards",
    "comments": "16 pages, 10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "stat.ML cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper is a note on new directions and methodologies for validation and\nexplanation of Machine Learning (ML) models employed for retail credit scoring\nin finance. Our proposed framework draws motivation from the field of\nArtificial Intelligence (AI) security and adversarial ML where the need for\ncertifying the performance of the ML algorithms in the face of their\noverwhelming complexity poses a need for rethinking the traditional notions of\nmodel architecture selection, sensitivity analysis and stress testing. Our\npoint of view is that the phenomenon of adversarial perturbations when detached\nfrom the AI security domain, has purely algorithmic roots and fall within the\nscope of model risk assessment. We propose a model criticism and explanation\nframework based on adversarially generated counterfactual examples for tabular\ndata. A counterfactual example to a given instance in this context is defined\nas a synthetically generated data point sampled from the estimated data\ndistribution which is treated differently by a model. The counterfactual\nexamples can be used to provide a black-box instance-level explanation of the\nmodel behaviour as well as studying the regions in the input space where the\nmodel performance deteriorates. Adversarial example generating algorithms are\nextensively studied in the image and natural language processing (NLP) domains.\nHowever, most financial data come in tabular format and naive application of\nthe existing techniques on this class of datasets generates unrealistic\nsamples. In this paper, we propose a counterfactual example generation method\ncapable of handling tabular data including discrete and categorical variables.\nOur proposed algorithm uses a gradient-free optimization based on genetic\nalgorithms and therefore is applicable to any classification model.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 24 Aug 2020 00:05:13 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 28 Aug 2020 18:06:46 GMT"
      }
    ],
    "update_date": "2020-09-01",
    "authors_parsed": [
      [
        "Hashemi",
        "Masoud",
        ""
      ],
      [
        "Fathi",
        "Ali",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2008.10138",
    "publish_date": "2020-08-24"
  },
  {
    "id": "2009.00097",
    "submitter": "Linjun Zhou",
    "authors": "Linjun Zhou, Peng Cui, Yinan Jiang, Shiqiang Yang",
    "title": "Adversarial Eigen Attack on Black-Box Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Black-box adversarial attack has attracted a lot of research interests for\nits practical use in AI safety. Compared with the white-box attack, a black-box\nsetting is more difficult for less available information related to the\nattacked model and the additional constraint on the query budget. A general way\nto improve the attack efficiency is to draw support from a pre-trained\ntransferable white-box model. In this paper, we propose a novel setting of\ntransferable black-box attack: attackers may use external information from a\npre-trained model with available network parameters, however, different from\nprevious studies, no additional training data is permitted to further change or\ntune the pre-trained model. To this end, we further propose a new algorithm,\nEigenBA to tackle this problem. Our method aims to explore more gradient\ninformation of the black-box model, and promote the attack efficiency, while\nkeeping the perturbation to the original attacked image small, by leveraging\nthe Jacobian matrix of the pre-trained white-box model. We show the optimal\nperturbations are closely related to the right singular vectors of the Jacobian\nmatrix. Further experiments on ImageNet and CIFAR-10 show that even the\nunlearnable pre-trained white-box model could also significantly boost the\nefficiency of the black-box attack and our proposed method could further\nimprove the attack efficiency.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Aug 2020 07:37:43 GMT"
      }
    ],
    "update_date": "2020-09-02",
    "authors_parsed": [
      [
        "Zhou",
        "Linjun",
        ""
      ],
      [
        "Cui",
        "Peng",
        ""
      ],
      [
        "Jiang",
        "Yinan",
        ""
      ],
      [
        "Yang",
        "Shiqiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2009.00097",
    "publish_date": "2020-08-27"
  },
  {
    "id": "2009.01110",
    "submitter": "Danilo Vasconcellos  Vargas",
    "authors": "Danilo Vasconcellos Vargas, Bingli Liao, Takahiro Kanzaki",
    "title": "Perceptual Deep Neural Networks: Adversarial Robustness through Input\n  Recreation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial examples have shown that albeit highly accurate, models learned\nby machines, differently from humans, have many weaknesses. However, humans'\nperception is also fundamentally different from machines, because we do not see\nthe signals which arrive at the retina but a rather complex recreation of them.\nIn this paper, we explore how machines could recreate the input as well as\ninvestigate the benefits of such an augmented perception. In this regard, we\npropose Perceptual Deep Neural Networks ($\\varphi$DNN) which also recreate\ntheir own input before further processing. The concept is formalized\nmathematically and two variations of it are developed (one based on inpainting\nthe whole image and the other based on a noisy resized super resolution\nrecreation). Experiments reveal that $\\varphi$DNNs and their adversarial\ntraining variations can increase the robustness substantially, surpassing both\nstate-of-the-art defenses and pre-processing types of defenses in 100% of the\ntests. $\\varphi$DNNs are shown to scale well to bigger image sizes, keeping a\nsimilar high accuracy throughout; while the state-of-the-art worsen up to 35%.\nMoreover, the recreation process intentionally corrupts the input image.\nInterestingly, we show by ablation tests that corrupting the input is, although\ncounter-intuitive, beneficial. Thus, $\\varphi$DNNs reveal that input recreation\nhas strong benefits for artificial neural networks similar to biological ones,\nshedding light into the importance of purposely corrupting the input as well as\npioneering an area of perception models based on GANs and autoencoders for\nrobust recognition in artificial intelligence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 2 Sep 2020 14:36:36 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 8 Sep 2020 09:39:09 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 6 Oct 2020 08:58:13 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 30 Nov 2020 10:36:03 GMT"
      }
    ],
    "update_date": "2020-12-01",
    "authors_parsed": [
      [
        "Vargas",
        "Danilo Vasconcellos",
        ""
      ],
      [
        "Liao",
        "Bingli",
        ""
      ],
      [
        "Kanzaki",
        "Takahiro",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2009.01110",
    "publish_date": "2020-09-02"
  },
  {
    "id": "2009.10639",
    "submitter": "Yi-Shan Lin",
    "authors": "Yi-Shan Lin, Wen-Chuan Lee, Z. Berkay Celik",
    "title": "What Do You See? Evaluation of Explainable Artificial Intelligence (XAI)\n  Interpretability through Neural Backdoors",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  EXplainable AI (XAI) methods have been proposed to interpret how a deep\nneural network predicts inputs through model saliency explanations that\nhighlight the parts of the inputs deemed important to arrive a decision at a\nspecific target. However, it remains challenging to quantify correctness of\ntheir interpretability as current evaluation approaches either require\nsubjective input from humans or incur high computation cost with automated\nevaluation. In this paper, we propose backdoor trigger patterns--hidden\nmalicious functionalities that cause misclassification--to automate the\nevaluation of saliency explanations. Our key observation is that triggers\nprovide ground truth for inputs to evaluate whether the regions identified by\nan XAI method are truly relevant to its output. Since backdoor triggers are the\nmost important features that cause deliberate misclassification, a robust XAI\nmethod should reveal their presence at inference time. We introduce three\ncomplementary metrics for systematic evaluation of explanations that an XAI\nmethod generates and evaluate seven state-of-the-art model-free and\nmodel-specific posthoc methods through 36 models trojaned with specifically\ncrafted triggers using color, shape, texture, location, and size. We discovered\nsix methods that use local explanation and feature relevance fail to completely\nhighlight trigger regions, and only a model-free approach can uncover the\nentire trigger region.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 22 Sep 2020 15:53:19 GMT"
      }
    ],
    "update_date": "2020-09-23",
    "authors_parsed": [
      [
        "Lin",
        "Yi-Shan",
        ""
      ],
      [
        "Lee",
        "Wen-Chuan",
        ""
      ],
      [
        "Celik",
        "Z. Berkay",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2009.10639",
    "publish_date": "2020-09-22"
  },
  {
    "id": "2010.02329",
    "submitter": "Boxin Wang",
    "authors": "Boxin Wang, Shuohang Wang, Yu Cheng, Zhe Gan, Ruoxi Jia, Bo Li,\n  Jingjing Liu",
    "title": "InfoBERT: Improving Robustness of Language Models from An Information\n  Theoretic Perspective",
    "comments": "Accepted to ICLR 2021. 23 pages, 9 tables, 3 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large-scale language models such as BERT have achieved state-of-the-art\nperformance across a wide range of NLP tasks. Recent studies, however, show\nthat such BERT-based models are vulnerable facing the threats of textual\nadversarial attacks. We aim to address this problem from an\ninformation-theoretic perspective, and propose InfoBERT, a novel learning\nframework for robust fine-tuning of pre-trained language models. InfoBERT\ncontains two mutual-information-based regularizers for model training: (i) an\nInformation Bottleneck regularizer, which suppresses noisy mutual information\nbetween the input and the feature representation; and (ii) a Robust Feature\nregularizer, which increases the mutual information between local robust\nfeatures and global features. We provide a principled way to theoretically\nanalyze and improve the robustness of representation learning for language\nmodels in both standard and adversarial training. Extensive experiments\ndemonstrate that InfoBERT achieves state-of-the-art robust accuracy over\nseveral adversarial datasets on Natural Language Inference (NLI) and Question\nAnswering (QA) tasks. Our code is available at\nhttps://github.com/AI-secure/InfoBERT.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Oct 2020 20:49:26 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 14 Oct 2020 13:24:03 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 3 Feb 2021 03:58:19 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 22 Mar 2021 11:44:30 GMT"
      }
    ],
    "update_date": "2021-03-23",
    "authors_parsed": [
      [
        "Wang",
        "Boxin",
        ""
      ],
      [
        "Wang",
        "Shuohang",
        ""
      ],
      [
        "Cheng",
        "Yu",
        ""
      ],
      [
        "Gan",
        "Zhe",
        ""
      ],
      [
        "Jia",
        "Ruoxi",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ],
      [
        "Liu",
        "Jingjing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2010.02329",
    "publish_date": "2021-02-03"
  },
  {
    "id": "2010.10712",
    "submitter": "Fanhua Shang",
    "authors": "Hongying Liu, Zhenyu Zhou, Fanhua Shang, Xiaoyu Qi, Yuanyuan Liu,\n  Licheng Jiao",
    "title": "Boosting Gradient for White-Box Adversarial Attacks",
    "comments": "9 pages,6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) are playing key roles in various artificial\nintelligence applications such as image classification and object recognition.\nHowever, a growing number of studies have shown that there exist adversarial\nexamples in DNNs, which are almost imperceptibly different from original\nsamples, but can greatly change the network output. Existing white-box attack\nalgorithms can generate powerful adversarial examples. Nevertheless, most of\nthe algorithms concentrate on how to iteratively make the best use of gradients\nto improve adversarial performance. In contrast, in this paper, we focus on the\nproperties of the widely-used ReLU activation function, and discover that there\nexist two phenomena (i.e., wrong blocking and over transmission) misleading the\ncalculation of gradients in ReLU during the backpropagation. Both issues\nenlarge the difference between the predicted changes of the loss function from\ngradient and corresponding actual changes, and mislead the gradients which\nresults in larger perturbations. Therefore, we propose a universal adversarial\nexample generation method, called ADV-ReLU, to enhance the performance of\ngradient based white-box attack algorithms. During the backpropagation of the\nnetwork, our approach calculates the gradient of the loss function versus\nnetwork input, maps the values to scores, and selects a part of them to update\nthe misleading gradients. Comprehensive experimental results on \\emph{ImageNet}\ndemonstrate that our ADV-ReLU can be easily integrated into many\nstate-of-the-art gradient-based white-box attack algorithms, as well as\ntransferred to black-box attack attackers, to further decrease perturbations in\nthe ${\\ell _2}$-norm.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 21 Oct 2020 02:13:26 GMT"
      }
    ],
    "update_date": "2020-10-22",
    "authors_parsed": [
      [
        "Liu",
        "Hongying",
        ""
      ],
      [
        "Zhou",
        "Zhenyu",
        ""
      ],
      [
        "Shang",
        "Fanhua",
        ""
      ],
      [
        "Qi",
        "Xiaoyu",
        ""
      ],
      [
        "Liu",
        "Yuanyuan",
        ""
      ],
      [
        "Jiao",
        "Licheng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2010.10712",
    "publish_date": "2020-10-21"
  },
  {
    "id": "2011.02272",
    "submitter": "Mayank Vatsa",
    "authors": "Richa Singh, Mayank Vatsa, Nalini Ratha",
    "title": "Trustworthy AI",
    "comments": "ACM CODS-COMAD 2021 Tutorial",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Modern AI systems are reaping the advantage of novel learning methods. With\ntheir increasing usage, we are realizing the limitations and shortfalls of\nthese systems. Brittleness to minor adversarial changes in the input data,\nability to explain the decisions, address the bias in their training data, high\nopacity in terms of revealing the lineage of the system, how they were trained\nand tested, and under which parameters and conditions they can reliably\nguarantee a certain level of performance, are some of the most prominent\nlimitations. Ensuring the privacy and security of the data, assigning\nappropriate credits to data sources, and delivering decent outputs are also\nrequired features of an AI system. We propose the tutorial on Trustworthy AI to\naddress six critical issues in enhancing user and public trust in AI systems,\nnamely: (i) bias and fairness, (ii) explainability, (iii) robust mitigation of\nadversarial attacks, (iv) improved privacy and security in model building, (v)\nbeing decent, and (vi) model attribution, including the right level of credit\nassignment to the data sources, model architectures, and transparency in\nlineage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 2 Nov 2020 20:04:18 GMT"
      }
    ],
    "update_date": "2020-11-05",
    "authors_parsed": [
      [
        "Singh",
        "Richa",
        ""
      ],
      [
        "Vatsa",
        "Mayank",
        ""
      ],
      [
        "Ratha",
        "Nalini",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2011.02272",
    "publish_date": "2020-11-02"
  },
  {
    "id": "2011.05411",
    "submitter": "Nguyen Truong",
    "authors": "Nguyen Truong, Kai Sun, Siyao Wang, Florian Guitton, Yike Guo",
    "title": "Privacy Preservation in Federated Learning: An insightful survey from\n  the GDPR Perspective",
    "comments": "21 pages, 8 figures, 2 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Along with the blooming of AI and Machine Learning-based applications and\nservices, data privacy and security have become a critical challenge.\nConventionally, data is collected and aggregated in a data centre on which\nmachine learning models are trained. This centralised approach has induced\nsevere privacy risks to personal data leakage, misuse, and abuse. Furthermore,\nin the era of the Internet of Things and big data in which data is essentially\ndistributed, transferring a vast amount of data to a data centre for processing\nseems to be a cumbersome solution. This is not only because of the difficulties\nin transferring and sharing data across data sources but also the challenges on\ncomplying with rigorous data protection regulations and complicated\nadministrative procedures such as the EU General Data Protection Regulation\n(GDPR). In this respect, Federated learning (FL) emerges as a prospective\nsolution that facilitates distributed collaborative learning without disclosing\noriginal training data whilst naturally complying with the GDPR. Recent\nresearch has demonstrated that retaining data and computation on-device in FL\nis not sufficient enough for privacy-guarantee. This is because ML model\nparameters exchanged between parties in an FL system still conceal sensitive\ninformation, which can be exploited in some privacy attacks. Therefore, FL\nsystems shall be empowered by efficient privacy-preserving techniques to comply\nwith the GDPR. This article is dedicated to surveying on the state-of-the-art\nprivacy-preserving techniques which can be employed in FL in a systematic\nfashion, as well as how these techniques mitigate data security and privacy\nrisks. Furthermore, we provide insights into the challenges along with\nprospective approaches following the GDPR regulatory guidelines that an FL\nsystem shall implement to comply with the GDPR.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Nov 2020 21:41:25 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 13 Nov 2020 18:06:17 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 23 Jan 2021 02:07:36 GMT"
      },
      {
        "version": "v4",
        "created": "Sat, 30 Jan 2021 13:15:01 GMT"
      },
      {
        "version": "v5",
        "created": "Thu, 18 Mar 2021 12:32:28 GMT"
      }
    ],
    "update_date": "2021-03-19",
    "authors_parsed": [
      [
        "Truong",
        "Nguyen",
        ""
      ],
      [
        "Sun",
        "Kai",
        ""
      ],
      [
        "Wang",
        "Siyao",
        ""
      ],
      [
        "Guitton",
        "Florian",
        ""
      ],
      [
        "Guo",
        "Yike",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2011.05411",
    "publish_date": "2020-11-10"
  },
  {
    "id": "2011.14934",
    "submitter": "Sahil Suneja",
    "authors": "Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro\n  Morari",
    "title": "Probing Model Signal-Awareness via Prediction-Preserving Input\n  Minimization",
    "comments": "Authors Sahil Suneja, Yunhui Zheng, and Yufan Zhuang contributed\n  equally to this research. FSE 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This work explores the signal awareness of AI models for source code\nunderstanding. Using a software vulnerability detection use case, we evaluate\nthe models' ability to capture the correct vulnerability signals to produce\ntheir predictions. Our prediction-preserving input minimization (P2IM) approach\nsystematically reduces the original source code to a minimal snippet which a\nmodel needs to maintain its prediction. The model's reliance on incorrect\nsignals is then uncovered when the vulnerability in the original code is\nmissing in the minimal snippet, both of which the model however predicts as\nbeing vulnerable. We measure the signal awareness of models using a new metric\nwe propose- Signal-aware Recall (SAR). We apply P2IM on three different neural\nnetwork architectures across multiple datasets. The results show a sharp drop\nin the model's Recall from the high 90s to sub-60s with the new metric,\nhighlighting that the models are presumably picking up a lot of noise or\ndataset nuances while learning their vulnerability detection logic. Although\nthe drop in model performance may be perceived as an adversarial attack, but\nthis isn't P2IM's objective. The idea is rather to uncover the signal-awareness\nof a black-box model in a data-driven manner via controlled queries. SAR's\npurpose is to measure the impact of task-agnostic model training, and not to\nsuggest a shortcoming in the Recall metric. The expectation, in fact, is for\nSAR to match Recall in the ideal scenario where the model truly captures\ntask-specific signals.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Nov 2020 20:05:23 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 22 Jun 2021 21:44:44 GMT"
      }
    ],
    "update_date": "2021-06-24",
    "authors_parsed": [
      [
        "Suneja",
        "Sahil",
        ""
      ],
      [
        "Zheng",
        "Yunhui",
        ""
      ],
      [
        "Zhuang",
        "Yufan",
        ""
      ],
      [
        "Laredo",
        "Jim",
        ""
      ],
      [
        "Morari",
        "Alessandro",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2011.14934",
    "publish_date": "2020-11-25"
  },
  {
    "id": "2012.01971",
    "submitter": "Faisal Hussain",
    "authors": "Faisal Hussain, Syed Ghazanfar Abbas, Muhammad Husnain, Ubaid Ullah\n  Fayyaz, Farrukh Shahzad, Ghalib A. Shah",
    "title": "IoT DoS and DDoS Attack Detection using ResNet",
    "comments": "Accepted in 2020 IEEE 23rd International Multitopic Conference\n  (INMIC), 7 pages, 6 figures, 1 table",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The network attacks are increasing both in frequency and intensity with the\nrapid growth of internet of things (IoT) devices. Recently, denial of service\n(DoS) and distributed denial of service (DDoS) attacks are reported as the most\nfrequent attacks in IoT networks. The traditional security solutions like\nfirewalls, intrusion detection systems, etc., are unable to detect the complex\nDoS and DDoS attacks since most of them filter the normal and attack traffic\nbased upon the static predefined rules. However, these solutions can become\nreliable and effective when integrated with artificial intelligence (AI) based\ntechniques. During the last few years, deep learning models especially\nconvolutional neural networks achieved high significance due to their\noutstanding performance in the image processing field. The potential of these\nconvolutional neural network (CNN) models can be used to efficiently detect the\ncomplex DoS and DDoS by converting the network traffic dataset into images.\nTherefore, in this work, we proposed a methodology to convert the network\ntraffic data into image form and trained a state-of-the-art CNN model, i.e.,\nResNet over the converted data. The proposed methodology accomplished 99.99\\%\naccuracy for detecting the DoS and DDoS in case of binary classification.\nFurthermore, the proposed methodology achieved 87\\% average precision for\nrecognizing eleven types of DoS and DDoS attack patterns which is 9\\% higher as\ncompared to the state-of-the-art.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 2 Dec 2020 13:33:27 GMT"
      }
    ],
    "update_date": "2020-12-04",
    "authors_parsed": [
      [
        "Hussain",
        "Faisal",
        ""
      ],
      [
        "Abbas",
        "Syed Ghazanfar",
        ""
      ],
      [
        "Husnain",
        "Muhammad",
        ""
      ],
      [
        "Fayyaz",
        "Ubaid Ullah",
        ""
      ],
      [
        "Shahzad",
        "Farrukh",
        ""
      ],
      [
        "Shah",
        "Ghalib A.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.01971",
    "publish_date": "2020-12-02"
  },
  {
    "id": "2012.06058",
    "submitter": "Odest Chadwicke Jenkins",
    "authors": "Odest Chadwicke Jenkins, Daniel Lopresti, and Melanie Mitchell",
    "title": "Next Wave Artificial Intelligence: Robust, Explainable, Adaptable,\n  Ethical, and Accountable",
    "comments": "A Computing Community Consortium (CCC) white paper, 5 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": "ccc2020whitepaper_7",
    "categories": "cs.CY cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The history of AI has included several \"waves\" of ideas. The first wave, from\nthe mid-1950s to the 1980s, focused on logic and symbolic hand-encoded\nrepresentations of knowledge, the foundations of so-called \"expert systems\".\nThe second wave, starting in the 1990s, focused on statistics and machine\nlearning, in which, instead of hand-programming rules for behavior, programmers\nconstructed \"statistical learning algorithms\" that could be trained on large\ndatasets. In the most recent wave research in AI has largely focused on deep\n(i.e., many-layered) neural networks, which are loosely inspired by the brain\nand trained by \"deep learning\" methods. However, while deep neural networks\nhave led to many successes and new capabilities in computer vision, speech\nrecognition, language processing, game-playing, and robotics, their potential\nfor broad application remains limited by several factors.\n  A concerning limitation is that even the most successful of today's AI\nsystems suffer from brittleness-they can fail in unexpected ways when faced\nwith situations that differ sufficiently from ones they have been trained on.\nThis lack of robustness also appears in the vulnerability of AI systems to\nadversarial attacks, in which an adversary can subtly manipulate data in a way\nto guarantee a specific wrong answer or action from an AI system. AI systems\nalso can absorb biases-based on gender, race, or other factors-from their\ntraining data and further magnify these biases in their subsequent\ndecision-making. Taken together, these various limitations have prevented AI\nsystems such as automatic medical diagnosis or autonomous vehicles from being\nsufficiently trustworthy for wide deployment. The massive proliferation of AI\nacross society will require radically new ideas to yield technology that will\nnot sacrifice our productivity, our quality of life, or our values.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 11 Dec 2020 00:50:09 GMT"
      }
    ],
    "update_date": "2020-12-14",
    "authors_parsed": [
      [
        "Jenkins",
        "Odest Chadwicke",
        ""
      ],
      [
        "Lopresti",
        "Daniel",
        ""
      ],
      [
        "Mitchell",
        "Melanie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.06058",
    "publish_date": "2020-12-11"
  },
  {
    "id": "2012.06332",
    "submitter": "Ayush Goel",
    "authors": "Ayush Goel",
    "title": "An Empirical Review of Adversarial Defenses",
    "comments": "19 pages, 8 Figures, Report Reviewed by Vivek Menon",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  From face recognition systems installed in phones to self-driving cars, the\nfield of AI is witnessing rapid transformations and is being integrated into\nour everyday lives at an incredible pace. Any major failure in these system's\npredictions could be devastating, leaking sensitive information or even costing\nlives (as in the case of self-driving cars). However, deep neural networks,\nwhich form the basis of such systems, are highly susceptible to a specific type\nof attack, called adversarial attacks. A hacker can, even with bare minimum\ncomputation, generate adversarial examples (images or data points that belong\nto another class, but consistently fool the model to get misclassified as\ngenuine) and crumble the basis of such algorithms. In this paper, we compile\nand test numerous approaches to defend against such adversarial attacks. Out of\nthe ones explored, we found two effective techniques, namely Dropout and\nDenoising Autoencoders, and show their success in preventing such attacks from\nfooling the model. We demonstrate that these techniques are also resistant to\nboth higher noise levels as well as different kinds of adversarial attacks\n(although not tested against all). We also develop a framework for deciding the\nsuitable defense technique to use against attacks, based on the nature of the\napplication and resource constraints of the Deep Neural Network.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 10 Dec 2020 09:34:41 GMT"
      }
    ],
    "update_date": "2020-12-14",
    "authors_parsed": [
      [
        "Goel",
        "Ayush",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.06332",
    "publish_date": "2020-12-10"
  },
  {
    "id": "2012.06337",
    "submitter": "Han Yu",
    "authors": "Lingjuan Lyu, Han Yu, Xingjun Ma, Chen Chen, Lichao Sun, Jun Zhao,\n  Qiang Yang, Philip S. Yu",
    "title": "Privacy and Robustness in Federated Learning: Attacks and Defenses",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As data are increasingly being stored in different silos and societies\nbecoming more aware of data privacy issues, the traditional centralized\ntraining of artificial intelligence (AI) models is facing efficiency and\nprivacy challenges. Recently, federated learning (FL) has emerged as an\nalternative solution and continue to thrive in this new reality. Existing FL\nprotocol design has been shown to be vulnerable to adversaries within or\noutside of the system, compromising data privacy and system robustness. Besides\ntraining powerful global models, it is of paramount importance to design FL\nsystems that have privacy guarantees and are resistant to different types of\nadversaries. In this paper, we conduct the first comprehensive survey on this\ntopic. Through a concise introduction to the concept of FL, and a unique\ntaxonomy covering: 1) threat models; 2) poisoning attacks and defenses against\nrobustness; 3) inference attacks and defenses against privacy, we provide an\naccessible review of this important topic. We highlight the intuitions, key\ntechniques as well as fundamental assumptions adopted by various attacks and\ndefenses. Finally, we discuss promising future research directions towards\nrobust and privacy-preserving federated learning.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 Dec 2020 12:11:45 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 18 Jan 2022 01:06:46 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 19 Jan 2022 04:17:46 GMT"
      }
    ],
    "update_date": "2022-01-20",
    "authors_parsed": [
      [
        "Lyu",
        "Lingjuan",
        ""
      ],
      [
        "Yu",
        "Han",
        ""
      ],
      [
        "Ma",
        "Xingjun",
        ""
      ],
      [
        "Chen",
        "Chen",
        ""
      ],
      [
        "Sun",
        "Lichao",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ],
      [
        "Yang",
        "Qiang",
        ""
      ],
      [
        "Yu",
        "Philip S.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.06337",
    "publish_date": "2022-01-19"
  },
  {
    "id": "2012.07805",
    "submitter": "Nicholas Carlini",
    "authors": "Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski,\n  Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\n  Erlingsson, Alina Oprea, Colin Raffel",
    "title": "Extracting Training Data from Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  It has become common to publish large (billion parameter) language models\nthat have been trained on private datasets. This paper demonstrates that in\nsuch settings, an adversary can perform a training data extraction attack to\nrecover individual training examples by querying the language model.\n  We demonstrate our attack on GPT-2, a language model trained on scrapes of\nthe public Internet, and are able to extract hundreds of verbatim text\nsequences from the model's training data. These extracted examples include\n(public) personally identifiable information (names, phone numbers, and email\naddresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible\neven though each of the above sequences are included in just one document in\nthe training data.\n  We comprehensively evaluate our extraction attack to understand the factors\nthat contribute to its success. Worryingly, we find that larger models are more\nvulnerable than smaller models. We conclude by drawing lessons and discussing\npossible safeguards for training large language models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 Dec 2020 18:39:09 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 15 Jun 2021 17:45:26 GMT"
      }
    ],
    "update_date": "2021-06-16",
    "authors_parsed": [
      [
        "Carlini",
        "Nicholas",
        ""
      ],
      [
        "Tramer",
        "Florian",
        ""
      ],
      [
        "Wallace",
        "Eric",
        ""
      ],
      [
        "Jagielski",
        "Matthew",
        ""
      ],
      [
        "Herbert-Voss",
        "Ariel",
        ""
      ],
      [
        "Lee",
        "Katherine",
        ""
      ],
      [
        "Roberts",
        "Adam",
        ""
      ],
      [
        "Brown",
        "Tom",
        ""
      ],
      [
        "Song",
        "Dawn",
        ""
      ],
      [
        "Erlingsson",
        "Ulfar",
        ""
      ],
      [
        "Oprea",
        "Alina",
        ""
      ],
      [
        "Raffel",
        "Colin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.07805",
    "publish_date": "2021-06-15"
  },
  {
    "id": "2012.10076",
    "submitter": "Kieran Browne",
    "authors": "Kieran Browne, Ben Swift",
    "title": "Semantics and explanation: why counterfactual explanations produce\n  adversarial examples in deep neural networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CY cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent papers in explainable AI have made a compelling case for\ncounterfactual modes of explanation. While counterfactual explanations appear\nto be extremely effective in some instances, they are formally equivalent to\nadversarial examples. This presents an apparent paradox for explainability\nresearchers: if these two procedures are formally equivalent, what accounts for\nthe explanatory divide apparent between counterfactual explanations and\nadversarial examples? We resolve this paradox by placing emphasis back on the\nsemantics of counterfactual expressions. Producing satisfactory explanations\nfor deep learning systems will require that we find ways to interpret the\nsemantics of hidden layer representations in deep neural networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 18 Dec 2020 07:04:04 GMT"
      }
    ],
    "update_date": "2020-12-21",
    "authors_parsed": [
      [
        "Browne",
        "Kieran",
        ""
      ],
      [
        "Swift",
        "Ben",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2012.10076",
    "publish_date": "2020-12-18"
  },
  {
    "id": "2101.02483",
    "submitter": "Rulin Shao",
    "authors": "Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh",
    "title": "Robust Text CAPTCHAs Using Adversarial Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  CAPTCHA (Completely Automated Public Truing test to tell Computers and Humans\nApart) is a widely used technology to distinguish real users and automated\nusers such as bots. However, the advance of AI technologies weakens many\nCAPTCHA tests and can induce security concerns. In this paper, we propose a\nuser-friendly text-based CAPTCHA generation method named Robust Text CAPTCHA\n(RTC). At the first stage, the foregrounds and backgrounds are constructed with\nrandomly sampled font and background images, which are then synthesized into\nidentifiable pseudo adversarial CAPTCHAs. At the second stage, we design and\napply a highly transferable adversarial attack for text CAPTCHAs to better\nobstruct CAPTCHA solvers. Our experiments cover comprehensive models including\nshallow models such as KNN, SVM and random forest, various deep neural networks\nand OCR models. Experiments show that our CAPTCHAs have a failure rate lower\nthan one millionth in general and high usability. They are also robust against\nvarious defensive techniques that attackers may employ, including adversarial\ntraining, data pre-processing and manual tagging.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Jan 2021 11:03:07 GMT"
      }
    ],
    "update_date": "2021-01-08",
    "authors_parsed": [
      [
        "Shao",
        "Rulin",
        ""
      ],
      [
        "Shi",
        "Zhouxing",
        ""
      ],
      [
        "Yi",
        "Jinfeng",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Hsieh",
        "Cho-Jui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.02483",
    "publish_date": "2021-01-07"
  },
  {
    "id": "2101.03204",
    "submitter": "Xiaonan Jing",
    "authors": "Yifei Hu, Xiaonan Jing, Youlim Ko, Julia Taylor Rayz",
    "title": "Misspelling Correction with Pre-trained Contextual Language Model",
    "comments": "Accepted by 2020 IEEE 19th International Conference on Cognitive\n  Informatics & Cognitive Computing (ICCI* CC). IEEE",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Spelling irregularities, known now as spelling mistakes, have been found for\nseveral centuries. As humans, we are able to understand most of the misspelled\nwords based on their location in the sentence, perceived pronunciation, and\ncontext. Unlike humans, computer systems do not possess the convenient auto\ncomplete functionality of which human brains are capable. While many programs\nprovide spelling correction functionality, many systems do not take context\ninto account. Moreover, Artificial Intelligence systems function in the way\nthey are trained on. With many current Natural Language Processing (NLP)\nsystems trained on grammatically correct text data, many are vulnerable against\nadversarial examples, yet correctly spelled text processing is crucial for\nlearning. In this paper, we investigate how spelling errors can be corrected in\ncontext, with a pre-trained language model BERT. We present two experiments,\nbased on BERT and the edit distance algorithm, for ranking and selecting\ncandidate corrections. The results of our experiments demonstrated that when\ncombined properly, contextual word embeddings of BERT and edit distance are\ncapable of effectively correcting spelling errors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Jan 2021 20:11:01 GMT"
      }
    ],
    "update_date": "2021-01-12",
    "authors_parsed": [
      [
        "Hu",
        "Yifei",
        ""
      ],
      [
        "Jing",
        "Xiaonan",
        ""
      ],
      [
        "Ko",
        "Youlim",
        ""
      ],
      [
        "Rayz",
        "Julia Taylor",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.03204",
    "publish_date": "2021-01-08"
  },
  {
    "id": "2101.05967",
    "submitter": "Steven Whang",
    "authors": "Steven Euijong Whang, Ki Hyun Tae, Yuji Roh, Geon Heo",
    "title": "Responsible AI Challenges in End-to-end Machine Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Responsible AI is becoming critical as AI is widely used in our everyday\nlives. Many companies that deploy AI publicly state that when training a model,\nwe not only need to improve its accuracy, but also need to guarantee that the\nmodel does not discriminate against users (fairness), is resilient to noisy or\npoisoned data (robustness), is explainable, and more. In addition, these\nobjectives are not only relevant to model training, but to all steps of\nend-to-end machine learning, which include data collection, data cleaning and\nvalidation, model training, model evaluation, and model management and serving.\nFinally, responsible AI is conceptually challenging, and supporting all the\nobjectives must be as easy as possible. We thus propose three key research\ndirections towards this vision - depth, breadth, and usability - to measure\nprogress and introduce our ongoing research. First, responsible AI must be\ndeeply supported where multiple objectives like fairness and robust must be\nhandled together. To this end, we propose FR-Train, a holistic framework for\nfair and robust model training in the presence of data bias and poisoning.\nSecond, responsible AI must be broadly supported, preferably in all steps of\nmachine learning. Currently we focus on the data pre-processing steps and\npropose Slice Tuner, a selective data acquisition framework for training fair\nand accurate models, and MLClean, a data cleaning framework that also improves\nfairness and robustness. Finally, responsible AI must be usable where the\ntechniques must be easy to deploy and actionable. We propose FairBatch, a batch\nselection approach for fairness that is effective and simple to use, and Slice\nFinder, a model evaluation tool that automatically finds problematic slices. We\nbelieve we scratched the surface of responsible AI for end-to-end machine\nlearning and suggest research challenges moving forward.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 15 Jan 2021 04:55:03 GMT"
      }
    ],
    "update_date": "2021-01-18",
    "authors_parsed": [
      [
        "Whang",
        "Steven Euijong",
        ""
      ],
      [
        "Tae",
        "Ki Hyun",
        ""
      ],
      [
        "Roh",
        "Yuji",
        ""
      ],
      [
        "Heo",
        "Geon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.05967",
    "publish_date": "2021-01-15"
  },
  {
    "id": "2101.06704",
    "submitter": "Xingjun Ma",
    "authors": "Nodens Koren, Qiuhong Ke, Yisen Wang, James Bailey, Xingjun Ma",
    "title": "Adversarial Interaction Attack: Fooling AI to Misinterpret Human\n  Intentions",
    "comments": "Preprint",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Understanding the actions of both humans and artificial intelligence (AI)\nagents is important before modern AI systems can be fully integrated into our\ndaily life. In this paper, we show that, despite their current huge success,\ndeep learning based AI systems can be easily fooled by subtle adversarial noise\nto misinterpret the intention of an action in interaction scenarios. Based on a\ncase study of skeleton-based human interactions, we propose a novel adversarial\nattack on interactions, and demonstrate how DNN-based interaction models can be\ntricked to predict the participants' reactions in unexpected ways. From a\nbroader perspective, the scope of our proposed attack method is not confined to\nproblems related to skeleton data but can also be extended to any type of\nproblems involving sequential regressions. Our study highlights potential risks\nin the interaction loop with AI and humans, which need to be carefully\naddressed when deploying AI systems in safety-critical applications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Jan 2021 16:23:20 GMT"
      }
    ],
    "update_date": "2021-01-19",
    "authors_parsed": [
      [
        "Koren",
        "Nodens",
        ""
      ],
      [
        "Ke",
        "Qiuhong",
        ""
      ],
      [
        "Wang",
        "Yisen",
        ""
      ],
      [
        "Bailey",
        "James",
        ""
      ],
      [
        "Ma",
        "Xingjun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.06704",
    "publish_date": "2021-01-17"
  },
  {
    "id": "2101.08030",
    "submitter": "Francesco Cartella",
    "authors": "Francesco Cartella, Orlando Anunciacao, Yuki Funabiki, Daisuke\n  Yamaguchi, Toru Akishita, Olivier Elshocht",
    "title": "Adversarial Attacks for Tabular Data: Application to Fraud Detection and\n  Imbalanced Data",
    "comments": "Will be published on Proceedings of the Workshop on Artificial\n  Intelligence Safety (SafeAI 2021) co-located with 35th AAAI Conference on\n  Artificial Intelligence (AAAI 2021)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Guaranteeing the security of transactional systems is a crucial priority of\nall institutions that process transactions, in order to protect their\nbusinesses against cyberattacks and fraudulent attempts. Adversarial attacks\nare novel techniques that, other than being proven to be effective to fool\nimage classification models, can also be applied to tabular data. Adversarial\nattacks aim at producing adversarial examples, in other words, slightly\nmodified inputs that induce the Artificial Intelligence (AI) system to return\nincorrect outputs that are advantageous for the attacker. In this paper we\nillustrate a novel approach to modify and adapt state-of-the-art algorithms to\nimbalanced tabular data, in the context of fraud detection. Experimental\nresults show that the proposed modifications lead to a perfect attack success\nrate, obtaining adversarial examples that are also less perceptible when\nanalyzed by humans. Moreover, when applied to a real-world production system,\nthe proposed techniques shows the possibility of posing a serious threat to the\nrobustness of advanced AI-based fraud detection procedures.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 20 Jan 2021 08:58:29 GMT"
      }
    ],
    "update_date": "2021-01-21",
    "authors_parsed": [
      [
        "Cartella",
        "Francesco",
        ""
      ],
      [
        "Anunciacao",
        "Orlando",
        ""
      ],
      [
        "Funabiki",
        "Yuki",
        ""
      ],
      [
        "Yamaguchi",
        "Daisuke",
        ""
      ],
      [
        "Akishita",
        "Toru",
        ""
      ],
      [
        "Elshocht",
        "Olivier",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.08030",
    "publish_date": "2021-01-20"
  },
  {
    "id": "2101.10710",
    "submitter": "Mohammad Naser Sabet Jahromi",
    "authors": "Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos,\n  Laura M. Fenoy, Thomas B. Moeslund",
    "title": "Visual explanation of black-box model: Similarity Difference and\n  Uniqueness (SIDU) method",
    "comments": null,
    "journal-ref": "Pattern Recognition 127 (2022): 108604",
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.HC cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Explainable Artificial Intelligence (XAI) has in recent years become a\nwell-suited framework to generate human understandable explanations of\n\"black-box\" models. In this paper, a novel XAI visual explanation algorithm\nknown as the Similarity Difference and Uniqueness (SIDU) method that can\neffectively localize entire object regions responsible for prediction is\npresented in full detail. The SIDU algorithm robustness and effectiveness is\nanalyzed through various computational and human subject experiments. In\nparticular, the SIDU algorithm is assessed using three different types of\nevaluations (Application, Human and Functionally-Grounded) to demonstrate its\nsuperior performance. The robustness of SIDU is further studied in the presence\nof adversarial attack on \"black-box\" models to better understand its\nperformance. Our code is available at:\nhttps://github.com/satyamahesh84/SIDU_XAI_CODE.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 26 Jan 2021 11:13:50 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 10 Jul 2022 18:07:56 GMT"
      }
    ],
    "update_date": "2022-07-12",
    "authors_parsed": [
      [
        "Muddamsetty",
        "Satya M.",
        ""
      ],
      [
        "Jahromi",
        "Mohammad N. S.",
        ""
      ],
      [
        "Ciontos",
        "Andreea E.",
        ""
      ],
      [
        "Fenoy",
        "Laura M.",
        ""
      ],
      [
        "Moeslund",
        "Thomas B.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.10710",
    "publish_date": "2021-01-26"
  },
  {
    "id": "2101.11563",
    "submitter": "Alan Smeaton",
    "authors": "Rashmiranjan Das and Gaurav Negi and Alan F. Smeaton",
    "title": "Detecting Deepfake Videos Using Euler Video Magnification",
    "comments": "Presented at Electronic Imaging: Media Watermarking, Security, and\n  Forensics, 27 January 2021, 6 pages, 6 figures",
    "journal-ref": null,
    "doi": "10.2352/ISSN.2470-1173.2021.4.MWSF-272",
    "report-no": null,
    "categories": "cs.CV cs.AI cs.MM",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent advances in artificial intelligence make it progressively hard to\ndistinguish between genuine and counterfeit media, especially images and\nvideos. One recent development is the rise of deepfake videos, based on\nmanipulating videos using advanced machine learning techniques. This involves\nreplacing the face of an individual from a source video with the face of a\nsecond person, in the destination video. This idea is becoming progressively\nrefined as deepfakes are getting progressively seamless and simpler to compute.\nCombined with the outreach and speed of social media, deepfakes could easily\nfool individuals when depicting someone saying things that never happened and\nthus could persuade people in believing fictional scenarios, creating distress,\nand spreading fake news. In this paper, we examine a technique for possible\nidentification of deepfake videos. We use Euler video magnification which\napplies spatial decomposition and temporal filtering on video data to highlight\nand magnify hidden features like skin pulsation and subtle motions. Our\napproach uses features extracted from the Euler technique to train three models\nto classify counterfeit and unaltered videos and compare the results with\nexisting techniques.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 27 Jan 2021 17:37:23 GMT"
      }
    ],
    "update_date": "2021-08-19",
    "authors_parsed": [
      [
        "Das",
        "Rashmiranjan",
        ""
      ],
      [
        "Negi",
        "Gaurav",
        ""
      ],
      [
        "Smeaton",
        "Alan F.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2101.11563",
    "publish_date": "2021-01-27"
  },
  {
    "id": "2102.00798",
    "submitter": "Pu Sun",
    "authors": "Pu Sun, Yuezun Li, Honggang Qi and Siwei Lyu",
    "title": "Landmark Breaker: Obstructing DeepFake By Disturbing Landmark Extraction",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The recent development of Deep Neural Networks (DNN) has significantly\nincreased the realism of AI-synthesized faces, with the most notable examples\nbeing the DeepFakes. The DeepFake technology can synthesize a face of target\nsubject from a face of another subject, while retains the same face attributes.\nWith the rapidly increased social media portals (Facebook, Instagram, etc),\nthese realistic fake faces rapidly spread though the Internet, causing a broad\nnegative impact to the society. In this paper, we describe Landmark Breaker,\nthe first dedicated method to disrupt facial landmark extraction, and apply it\nto the obstruction of the generation of DeepFake videos.Our motivation is that\ndisrupting the facial landmark extraction can affect the alignment of input\nface so as to degrade the DeepFake quality. Our method is achieved using\nadversarial perturbations. Compared to the detection methods that only work\nafter DeepFake generation, Landmark Breaker goes one step ahead to prevent\nDeepFake generation. The experiments are conducted on three state-of-the-art\nfacial landmark extractors using the recent Celeb-DF dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 Feb 2021 12:27:08 GMT"
      }
    ],
    "update_date": "2021-02-02",
    "authors_parsed": [
      [
        "Sun",
        "Pu",
        ""
      ],
      [
        "Li",
        "Yuezun",
        ""
      ],
      [
        "Qi",
        "Honggang",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.00798",
    "publish_date": "2021-02-01"
  },
  {
    "id": "2102.01116",
    "submitter": "Michael Chary",
    "authors": "Michael Chary, Ed W Boyer, Michele M Burns",
    "title": "Diagnosis of Acute Poisoning Using Explainable Artificial Intelligence",
    "comments": "Parts submitted to HICSS 54",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.HC cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Medical toxicology is the clinical specialty that treats the toxic effects of\nsubstances, be it an overdose, a medication error, or a scorpion sting. The\nvolume of toxicological knowledge and research has, as with other medical\nspecialties, outstripped the ability of the individual clinician to entirely\nmaster and stay current with it. The application of machine learning techniques\nto medical toxicology is challenging because initial treatment decisions are\noften based on a few pieces of textual data and rely heavily on prior\nknowledge. ML techniques often do not represent knowledge in a way that is\ntransparent for the physician, raising barriers to usability. Rule-based\nsystems and decision tree learning are more transparent approaches, but often\ngeneralize poorly and require expert curation to implement and maintain. Here,\nwe construct a probabilistic logic network to represent a portion of the\nknowledge base of a medical toxicologist. Our approach transparently mimics the\nknowledge representation and clinical decision-making of practicing clinicians.\nThe software, dubbed Tak, performs comparably to humans on straightforward\ncases and intermediate difficulty cases, but is outperformed by humans on\nchallenging clinical cases. Tak outperforms a decision tree classifier at all\nlevels of difficulty. Probabilistic logic provides one form of explainable\nartificial intelligence that may be more acceptable for use in healthcare, if\nit can achieve acceptable levels of performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 Feb 2021 19:16:59 GMT"
      }
    ],
    "update_date": "2021-02-03",
    "authors_parsed": [
      [
        "Chary",
        "Michael",
        ""
      ],
      [
        "Boyer",
        "Ed W",
        ""
      ],
      [
        "Burns",
        "Michele M",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.01116",
    "publish_date": "2021-02-01"
  },
  {
    "id": "2102.04351",
    "submitter": "Sudip Mittal",
    "authors": "Priyanka Ranade, Aritran Piplai, Sudip Mittal, Anupam Joshi, Tim Finin",
    "title": "Generating Fake Cyber Threat Intelligence Using Transformer-Based Models",
    "comments": "In Proceedings of International Joint Conference on Neural Networks\n  2021 (IJCNN 2021), July 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Cyber-defense systems are being developed to automatically ingest Cyber\nThreat Intelligence (CTI) that contains semi-structured data and/or text to\npopulate knowledge graphs. A potential risk is that fake CTI can be generated\nand spread through Open-Source Intelligence (OSINT) communities or on the Web\nto effect a data poisoning attack on these systems. Adversaries can use fake\nCTI examples as training input to subvert cyber defense systems, forcing the\nmodel to learn incorrect inputs to serve their malicious needs.\n  In this paper, we automatically generate fake CTI text descriptions using\ntransformers. We show that given an initial prompt sentence, a public language\nmodel like GPT-2 with fine-tuning, can generate plausible CTI text with the\nability of corrupting cyber-defense systems. We utilize the generated fake CTI\ntext to perform a data poisoning attack on a Cybersecurity Knowledge Graph\n(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse\nimpacts such as returning incorrect reasoning outputs, representation\npoisoning, and corruption of other dependent AI-based cyber defense systems. We\nevaluate with traditional approaches and conduct a human evaluation study with\ncybersecurity professionals and threat hunters. Based on the study,\nprofessional threat hunters were equally likely to consider our fake generated\nCTI as true.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 8 Feb 2021 16:54:35 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 10 Apr 2021 14:36:16 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 18 Jun 2021 18:00:10 GMT"
      }
    ],
    "update_date": "2021-06-22",
    "authors_parsed": [
      [
        "Ranade",
        "Priyanka",
        ""
      ],
      [
        "Piplai",
        "Aritran",
        ""
      ],
      [
        "Mittal",
        "Sudip",
        ""
      ],
      [
        "Joshi",
        "Anupam",
        ""
      ],
      [
        "Finin",
        "Tim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.04351",
    "publish_date": "2021-04-10"
  },
  {
    "id": "2102.04527",
    "submitter": "Michael Stuart",
    "authors": "Markus Kneer and Michael T. Stuart",
    "title": "Playing the Blame Game with Robots",
    "comments": "5 pages, 2 figures, 2 tables, HRI'21",
    "journal-ref": null,
    "doi": "10.1145/3434074.3447202",
    "report-no": null,
    "categories": "cs.HC cs.AI cs.CY",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Recent research shows -- somewhat astonishingly -- that people are willing to\nascribe moral blame to AI-driven systems when they cause harm [1]-[4]. In this\npaper, we explore the moral-psychological underpinnings of these findings. Our\nhypothesis was that the reason why people ascribe moral blame to AI systems is\nthat they consider them capable of entertaining inculpating mental states (what\nis called mens rea in the law). To explore this hypothesis, we created a\nscenario in which an AI system runs a risk of poisoning people by using a novel\ntype of fertilizer. Manipulating the computational (or quasi-cognitive)\nabilities of the AI system in a between-subjects design, we tested whether\npeople's willingness to ascribe knowledge of a substantial risk of harm (i.e.,\nrecklessness) and blame to the AI system. Furthermore, we investigated whether\nthe ascription of recklessness and blame to the AI system would influence the\nperceived blameworthiness of the system's user (or owner). In an experiment\nwith 347 participants, we found (i) that people are willing to ascribe blame to\nAI systems in contexts of recklessness, (ii) that blame ascriptions depend\nstrongly on the willingness to attribute recklessness and (iii) that the\nlatter, in turn, depends on the perceived \"cognitive\" capacities of the system.\nFurthermore, our results suggest (iv) that the higher the computational\nsophistication of the AI system, the more blame is shifted from the human user\nto the AI system.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 8 Feb 2021 20:53:42 GMT"
      }
    ],
    "update_date": "2021-02-10",
    "authors_parsed": [
      [
        "Kneer",
        "Markus",
        ""
      ],
      [
        "Stuart",
        "Michael T.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.04527",
    "publish_date": "2021-02-08"
  },
  {
    "id": "2102.04661",
    "submitter": "Ayodeji Oseni",
    "authors": "Ayodeji Oseni, Nour Moustafa, Helge Janicke, Peng Liu, Zahir Tari and\n  Athanasios Vasilakos",
    "title": "Security and Privacy for Artificial Intelligence: Opportunities and\n  Challenges",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The increased adoption of Artificial Intelligence (AI) presents an\nopportunity to solve many socio-economic and environmental challenges; however,\nthis cannot happen without securing AI-enabled technologies. In recent years,\nmost AI models are vulnerable to advanced and sophisticated hacking techniques.\nThis challenge has motivated concerted research efforts into adversarial AI,\nwith the aim of developing robust machine and deep learning models that are\nresilient to different types of adversarial scenarios. In this paper, we\npresent a holistic cyber security review that demonstrates adversarial attacks\nagainst AI applications, including aspects such as adversarial knowledge and\ncapabilities, as well as existing methods for generating adversarial examples\nand existing cyber defence models. We explain mathematical AI models,\nespecially new variants of reinforcement and federated learning, to demonstrate\nhow attack vectors would exploit vulnerabilities of AI models. We also propose\na systematic framework for demonstrating attack techniques against AI\napplications and reviewed several cyber defences that would protect AI\napplications against those attacks. We also highlight the importance of\nunderstanding the adversarial goals and their capabilities, especially the\nrecent attacks against industry applications, to develop adaptive defences that\nassess to secure AI applications. Finally, we describe the main challenges and\nfuture research directions in the domain of security and privacy of AI\ntechnologies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 9 Feb 2021 06:06:13 GMT"
      }
    ],
    "update_date": "2021-02-10",
    "authors_parsed": [
      [
        "Oseni",
        "Ayodeji",
        ""
      ],
      [
        "Moustafa",
        "Nour",
        ""
      ],
      [
        "Janicke",
        "Helge",
        ""
      ],
      [
        "Liu",
        "Peng",
        ""
      ],
      [
        "Tari",
        "Zahir",
        ""
      ],
      [
        "Vasilakos",
        "Athanasios",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.04661",
    "publish_date": "2021-02-09"
  },
  {
    "id": "2102.06109",
    "submitter": "Sean McGregor",
    "authors": "Claire Leibowicz, Sean McGregor, Aviv Ovadya",
    "title": "The Deepfake Detection Dilemma: A Multistakeholder Exploration of\n  Adversarial Dynamics in Synthetic Media",
    "comments": "11 pages, 8 Figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Synthetic media detection technologies label media as either synthetic or\nnon-synthetic and are increasingly used by journalists, web platforms, and the\ngeneral public to identify misinformation and other forms of problematic\ncontent. As both well-resourced organizations and the non-technical general\npublic generate more sophisticated synthetic media, the capacity for purveyors\nof problematic content to adapt induces a \\newterm{detection dilemma}: as\ndetection practices become more accessible, they become more easily\ncircumvented. This paper describes how a multistakeholder cohort from academia,\ntechnology platforms, media entities, and civil society organizations active in\nsynthetic media detection and its socio-technical implications evaluates the\ndetection dilemma. Specifically, we offer an assessment of detection contexts\nand adversary capacities sourced from the broader, global AI and media\nintegrity community concerned with mitigating the spread of harmful synthetic\nmedia. A collection of personas illustrates the intersection between\nunsophisticated and highly-resourced sponsors of misinformation in the context\nof their technical capacities. This work concludes that there is no \"best\"\napproach to navigating the detector dilemma, but derives a set of implications\nfrom multistakeholder input to better inform detection process decisions and\npolicies, in practice.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 11 Feb 2021 16:44:09 GMT"
      }
    ],
    "update_date": "2021-02-12",
    "authors_parsed": [
      [
        "Leibowicz",
        "Claire",
        ""
      ],
      [
        "McGregor",
        "Sean",
        ""
      ],
      [
        "Ovadya",
        "Aviv",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2102.06109",
    "publish_date": "2021-02-11"
  },
  {
    "id": "2103.04263",
    "submitter": "Jiameng Pu",
    "authors": "Jiameng Pu, Neal Mangaokar, Lauren Kelly, Parantapa Bhattacharya,\n  Kavya Sundaram, Mobin Javed, Bolun Wang, Bimal Viswanath",
    "title": "Deepfake Videos in the Wild: Analysis and Detection",
    "comments": "Accepted to The Web Conference 2021; First two authors contributed\n  equally to this work; 12 pages, 6 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI-manipulated videos, commonly known as deepfakes, are an emerging problem.\nRecently, researchers in academia and industry have contributed several\n(self-created) benchmark deepfake datasets, and deepfake detection algorithms.\nHowever, little effort has gone towards understanding deepfake videos in the\nwild, leading to a limited understanding of the real-world applicability of\nresearch contributions in this space. Even if detection schemes are shown to\nperform well on existing datasets, it is unclear how well the methods\ngeneralize to real-world deepfakes. To bridge this gap in knowledge, we make\nthe following contributions: First, we collect and present the largest dataset\nof deepfake videos in the wild, containing 1,869 videos from YouTube and\nBilibili, and extract over 4.8M frames of content. Second, we present a\ncomprehensive analysis of the growth patterns, popularity, creators,\nmanipulation strategies, and production methods of deepfake content in the\nreal-world. Third, we systematically evaluate existing defenses using our new\ndataset, and observe that they are not ready for deployment in the real-world.\nFourth, we explore the potential for transfer learning schemes and\ncompetition-winning techniques to improve defenses.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 7 Mar 2021 04:40:15 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Mar 2021 01:08:38 GMT"
      }
    ],
    "update_date": "2021-03-12",
    "authors_parsed": [
      [
        "Pu",
        "Jiameng",
        ""
      ],
      [
        "Mangaokar",
        "Neal",
        ""
      ],
      [
        "Kelly",
        "Lauren",
        ""
      ],
      [
        "Bhattacharya",
        "Parantapa",
        ""
      ],
      [
        "Sundaram",
        "Kavya",
        ""
      ],
      [
        "Javed",
        "Mobin",
        ""
      ],
      [
        "Wang",
        "Bolun",
        ""
      ],
      [
        "Viswanath",
        "Bimal",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2103.04263",
    "publish_date": "2021-03-11"
  },
  {
    "id": "2103.07268",
    "submitter": "Ferhat Ozgur Catak",
    "authors": "Evren Catak, Ferhat Ozgur Catak, Arild Moldsvor",
    "title": "Adversarial Machine Learning Security Problems for 6G: mmWave Beam\n  Prediction Use-Case",
    "comments": "7 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  6G is the next generation for the communication systems. In recent years,\nmachine learning algorithms have been applied widely in various fields such as\nhealth, transportation, and the autonomous car. The predictive algorithms will\nbe used in 6G problems. With the rapid developments of deep learning\ntechniques, it is critical to take the security concern into account to apply\nthe algorithms. While machine learning offers significant advantages for 6G, AI\nmodels' security is ignored. Since it has many applications in the real world,\nsecurity is a vital part of the algorithms. This paper has proposed a\nmitigation method for adversarial attacks against proposed 6G machine learning\nmodels for the millimeter-wave (mmWave) beam prediction with adversarial\nlearning. The main idea behind adversarial attacks against machine learning\nmodels is to produce faulty results by manipulating trained deep learning\nmodels for 6G applications for mmWave beam prediction use case. We have also\npresented the adversarial learning mitigation method's performance for 6G\nsecurity in millimeter-wave beam prediction application with fast gradient sign\nmethod attack. The mean square errors of the defended model and undefended\nmodel are very close.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Mar 2021 13:42:25 GMT"
      }
    ],
    "update_date": "2021-03-15",
    "authors_parsed": [
      [
        "Catak",
        "Evren",
        ""
      ],
      [
        "Catak",
        "Ferhat Ozgur",
        ""
      ],
      [
        "Moldsvor",
        "Arild",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2103.07268",
    "publish_date": "2021-03-12"
  },
  {
    "id": "2103.10480",
    "submitter": "David Noever",
    "authors": "David A. Noever, Samantha E. Miller Noever",
    "title": "Reading Isn't Believing: Adversarial Attacks On Multi-Modal Neurons",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CV",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  With Open AI's publishing of their CLIP model (Contrastive Language-Image\nPre-training), multi-modal neural networks now provide accessible models that\ncombine reading with visual recognition. Their network offers novel ways to\nprobe its dual abilities to read text while classifying visual objects. This\npaper demonstrates several new categories of adversarial attacks, spanning\nbasic typographical, conceptual, and iconographic inputs generated to fool the\nmodel into making false or absurd classifications. We demonstrate that\ncontradictory text and image signals can confuse the model into choosing false\n(visual) options. Like previous authors, we show by example that the CLIP model\ntends to read first, look later, a phenomenon we describe as reading isn't\nbelieving.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 Mar 2021 18:56:51 GMT"
      }
    ],
    "update_date": "2021-03-22",
    "authors_parsed": [
      [
        "Noever",
        "David A.",
        ""
      ],
      [
        "Noever",
        "Samantha E. Miller",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2103.10480",
    "publish_date": "2021-03-18"
  },
  {
    "id": "2103.12399",
    "submitter": "Antonio Emanuele Cin\\`a",
    "authors": "Antonio Emanuele Cin\\`a, Sebastiano Vascon, Ambra Demontis, Battista\n  Biggio, Fabio Roli, Marcello Pelillo",
    "title": "The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison\n  Linear Classifiers?",
    "comments": "8 pages, 7 figures, Submitted to IJCNN 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  One of the most concerning threats for modern AI systems is data poisoning,\nwhere the attacker injects maliciously crafted training data to corrupt the\nsystem's behavior at test time. Availability poisoning is a particularly\nworrisome subset of poisoning attacks where the attacker aims to cause a\nDenial-of-Service (DoS) attack. However, the state-of-the-art algorithms are\ncomputationally expensive because they try to solve a complex bi-level\noptimization problem (the \"hammer\"). We observed that in particular conditions,\nnamely, where the target model is linear (the \"nut\"), the usage of\ncomputationally costly procedures can be avoided. We propose a\ncounter-intuitive but efficient heuristic that allows contaminating the\ntraining set such that the target system's performance is highly compromised.\nWe further suggest a re-parameterization trick to decrease the number of\nvariables to be optimized. Finally, we demonstrate that, under the considered\nsettings, our framework achieves comparable, or even better, performances in\nterms of the attacker's objective while being significantly more\ncomputationally efficient.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 Mar 2021 09:08:10 GMT"
      }
    ],
    "update_date": "2021-03-24",
    "authors_parsed": [
      [
        "Cin\u00e0",
        "Antonio Emanuele",
        ""
      ],
      [
        "Vascon",
        "Sebastiano",
        ""
      ],
      [
        "Demontis",
        "Ambra",
        ""
      ],
      [
        "Biggio",
        "Battista",
        ""
      ],
      [
        "Roli",
        "Fabio",
        ""
      ],
      [
        "Pelillo",
        "Marcello",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2103.12399",
    "publish_date": "2021-03-23"
  },
  {
    "id": "2103.13567",
    "submitter": "Yiwen Guo",
    "authors": "Zhi Wang, Yiwen Guo, Wangmeng Zuo",
    "title": "Deepfake Forensics via An Adversarial Game",
    "comments": "Accepted by IEEE Transactions on Image Processing; 13 pages, 4\n  figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG cs.NE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the progress in AI-based facial forgery (i.e., deepfake), people are\nincreasingly concerned about its abuse. Albeit effort has been made for\ntraining classification (also known as deepfake detection) models to recognize\nsuch forgeries, existing models suffer from poor generalization to unseen\nforgery technologies and high sensitivity to changes in image/video quality. In\nthis paper, we advocate adversarial training for improving the generalization\nability to both unseen facial forgeries and unseen image/video qualities. We\nbelieve training with samples that are adversarially crafted to attack the\nclassification models improves the generalization ability considerably.\nConsidering that AI-based face manipulation often leads to high-frequency\nartifacts that can be easily spotted by models yet difficult to generalize, we\nfurther propose a new adversarial training method that attempts to blur out\nthese specific artifacts, by introducing pixel-wise Gaussian blurring models.\nWith adversarial training, the classification models are forced to learn more\ndiscriminative and generalizable features, and the effectiveness of our method\ncan be verified by plenty of empirical evidence. Our code will be made publicly\navailable.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 25 Mar 2021 02:20:08 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 28 Apr 2022 12:42:17 GMT"
      }
    ],
    "update_date": "2022-04-29",
    "authors_parsed": [
      [
        "Wang",
        "Zhi",
        ""
      ],
      [
        "Guo",
        "Yiwen",
        ""
      ],
      [
        "Zuo",
        "Wangmeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2103.13567",
    "publish_date": "2022-04-28"
  },
  {
    "id": "2104.01789",
    "submitter": "Guannan Lou",
    "authors": "Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, Qing-Long\n  Han",
    "title": "Deep Learning-Based Autonomous Driving Systems: A Survey of Attacks and\n  Defenses",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV cs.DC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The rapid development of artificial intelligence, especially deep learning\ntechnology, has advanced autonomous driving systems (ADSs) by providing precise\ncontrol decisions to counterpart almost any driving event, spanning from\nanti-fatigue safe driving to intelligent route planning. However, ADSs are\nstill plagued by increasing threats from different attacks, which could be\ncategorized into physical attacks, cyberattacks and learning-based adversarial\nattacks. Inevitably, the safety and security of deep learning-based autonomous\ndriving are severely challenged by these attacks, from which the\ncountermeasures should be analyzed and studied comprehensively to mitigate all\npotential risks. This survey provides a thorough analysis of different attacks\nthat may jeopardize ADSs, as well as the corresponding state-of-the-art defense\nmechanisms. The analysis is unrolled by taking an in-depth overview of each\nstep in the ADS workflow, covering adversarial attacks for various deep\nlearning models and attacks in both physical and cyber context. Furthermore,\nsome promising research directions are suggested in order to improve deep\nlearning-based autonomous driving safety, including model robustness training,\nmodel testing and verification, and anomaly detection based on cloud/edge\nservers.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Apr 2021 06:31:47 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 10 Apr 2021 02:28:02 GMT"
      }
    ],
    "update_date": "2021-04-13",
    "authors_parsed": [
      [
        "Deng",
        "Yao",
        ""
      ],
      [
        "Zhang",
        "Tiehua",
        ""
      ],
      [
        "Lou",
        "Guannan",
        ""
      ],
      [
        "Zheng",
        "Xi",
        ""
      ],
      [
        "Jin",
        "Jiong",
        ""
      ],
      [
        "Han",
        "Qing-Long",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2104.01789",
    "publish_date": "2021-04-10"
  },
  {
    "id": "2104.02425",
    "submitter": "Kashif Ahmad",
    "authors": "Senthil Kumar Jagatheesaperumal, Mohamed Rahouti, Kashif Ahmad, Ala\n  Al-Fuqaha, Mohsen Guizani",
    "title": "The Duo of Artificial Intelligence and Big Data for Industry 4.0: Review\n  of Applications, Techniques, Challenges, and Future Research Directions",
    "comments": "33 pages, 10 figures, 7 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The increasing need for economic, safe, and sustainable smart manufacturing\ncombined with novel technological enablers, has paved the way for Artificial\nIntelligence (AI) and Big Data in support of smart manufacturing. This implies\na substantial integration of AI, Industrial Internet of Things (IIoT),\nRobotics, Big data, Blockchain, 5G communications, in support of smart\nmanufacturing and the dynamical processes in modern industries. In this paper,\nwe provide a comprehensive overview of different aspects of AI and Big Data in\nIndustry 4.0 with a particular focus on key applications, techniques, the\nconcepts involved, key enabling technologies, challenges, and research\nperspective towards deployment of Industry 5.0. In detail, we highlight and\nanalyze how the duo of AI and Big Data is helping in different applications of\nIndustry 4.0. We also highlight key challenges in a successful deployment of AI\nand Big Data methods in smart industries with a particular emphasis on\ndata-related issues, such as availability, bias, auditing, management,\ninterpretability, communication, and different adversarial attacks and security\nissues. In a nutshell, we have explored the significance of AI and Big data\ntowards Industry 4.0 applications through panoramic reviews and discussions. We\nbelieve, this work will provide a baseline for future research in the domain.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 6 Apr 2021 11:08:02 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 7 Apr 2021 10:59:47 GMT"
      }
    ],
    "update_date": "2021-04-08",
    "authors_parsed": [
      [
        "Jagatheesaperumal",
        "Senthil Kumar",
        ""
      ],
      [
        "Rahouti",
        "Mohamed",
        ""
      ],
      [
        "Ahmad",
        "Kashif",
        ""
      ],
      [
        "Al-Fuqaha",
        "Ala",
        ""
      ],
      [
        "Guizani",
        "Mohsen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2104.02425",
    "publish_date": "2021-04-07"
  },
  {
    "id": "2104.02821",
    "submitter": "Caner Hazirbas",
    "authors": "Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert\n  Gordo, Cristian Canton Ferrer",
    "title": "Towards Measuring Fairness in AI: the Casual Conversations Dataset",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper introduces a novel dataset to help researchers evaluate their\ncomputer vision and audio models for accuracy across a diverse set of age,\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\n15 videos per person. The videos were recorded in multiple U.S. states with a\ndiverse set of adults in various age, gender and apparent skin tone groups. A\nkey feature is that each subject agreed to participate for their likenesses to\nbe used. Additionally, our age and gender annotations are provided by the\nsubjects themselves. A group of trained annotators labeled the subjects'\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\nfor videos recorded in low ambient lighting are also provided. As an\napplication to measure robustness of predictions across certain attributes, we\nprovide a comprehensive study on the top five winners of the DeepFake Detection\nChallenge (DFDC). Experimental evaluation shows that the winning models are\nless performant on some specific groups of people, such as subjects with darker\nskin tones and thus may not generalize to all people. In addition, we also\nevaluate the state-of-the-art apparent age and gender classification methods.\nOur experiments provides a thorough analysis on these models in terms of fair\ntreatment of people from various backgrounds.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 6 Apr 2021 22:48:22 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 3 Nov 2021 20:49:28 GMT"
      }
    ],
    "update_date": "2021-11-05",
    "authors_parsed": [
      [
        "Hazirbas",
        "Caner",
        ""
      ],
      [
        "Bitton",
        "Joanna",
        ""
      ],
      [
        "Dolhansky",
        "Brian",
        ""
      ],
      [
        "Pan",
        "Jacqueline",
        ""
      ],
      [
        "Gordo",
        "Albert",
        ""
      ],
      [
        "Ferrer",
        "Cristian Canton",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2104.02821",
    "publish_date": "2021-11-03"
  },
  {
    "id": "2104.05921",
    "submitter": "Xinyi Zhang",
    "authors": "Xinyi Zhang, Chengfang Fang, Jie Shi",
    "title": "Thief, Beware of What Get You There: Towards Understanding Model\n  Extraction Attack",
    "comments": "8 pages, 1 figure",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Model extraction increasingly attracts research attentions as keeping\ncommercial AI models private can retain a competitive advantage. In some\nscenarios, AI models are trained proprietarily, where neither pre-trained\nmodels nor sufficient in-distribution data is publicly available. Model\nextraction attacks against these models are typically more devastating.\nTherefore, in this paper, we empirically investigate the behaviors of model\nextraction under such scenarios. We find the effectiveness of existing\ntechniques significantly affected by the absence of pre-trained models. In\naddition, the impacts of the attacker's hyperparameters, e.g. model\narchitecture and optimizer, as well as the utilities of information retrieved\nfrom queries, are counterintuitive. We provide some insights on explaining the\npossible causes of these phenomena. With these observations, we formulate model\nextraction attacks into an adaptive framework that captures these factors with\ndeep reinforcement learning. Experiments show that the proposed framework can\nbe used to improve existing techniques, and show that model extraction is still\npossible in such strict scenarios. Our research can help system designers to\nconstruct better defense strategies based on their scenarios.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 13 Apr 2021 03:46:59 GMT"
      }
    ],
    "update_date": "2021-04-14",
    "authors_parsed": [
      [
        "Zhang",
        "Xinyi",
        ""
      ],
      [
        "Fang",
        "Chengfang",
        ""
      ],
      [
        "Shi",
        "Jie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2104.05921",
    "publish_date": "2021-04-13"
  },
  {
    "id": "2104.08231",
    "submitter": "Xiang Gao",
    "authors": "Xiang Gao, Yizhe Zhang, Michel Galley, Bill Dolan",
    "title": "An Adversarially-Learned Turing Test for Dialog Generation Models",
    "comments": "7 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The design of better automated dialogue evaluation metrics offers the\npotential of accelerate evaluation research on conversational AI. However,\nexisting trainable dialogue evaluation models are generally restricted to\nclassifiers trained in a purely supervised manner, which suffer a significant\nrisk from adversarial attacking (e.g., a nonsensical response that enjoys a\nhigh classification score). To alleviate this risk, we propose an adversarial\ntraining approach to learn a robust model, ATT (Adversarial Turing Test), that\ndiscriminates machine-generated responses from human-written replies. In\ncontrast to previous perturbation-based methods, our discriminator is trained\nby iteratively generating unrestricted and diverse adversarial examples using\nreinforcement learning. The key benefit of this unrestricted adversarial\ntraining approach is allowing the discriminator to improve robustness in an\niterative attack-defense game. Our discriminator shows high accuracy on strong\nattackers including DialoGPT and GPT-3.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 16 Apr 2021 17:13:14 GMT"
      }
    ],
    "update_date": "2021-04-19",
    "authors_parsed": [
      [
        "Gao",
        "Xiang",
        ""
      ],
      [
        "Zhang",
        "Yizhe",
        ""
      ],
      [
        "Galley",
        "Michel",
        ""
      ],
      [
        "Dolan",
        "Bill",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2104.08231",
    "publish_date": "2021-04-16"
  },
  {
    "id": "2105.00227",
    "submitter": "Cory Merkel",
    "authors": "Micah Gorsline, James Smith, Cory Merkel",
    "title": "On the Adversarial Robustness of Quantized Neural Networks",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1145/3453688.3461755",
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Reducing the size of neural network models is a critical step in moving AI\nfrom a cloud-centric to an edge-centric (i.e. on-device) compute paradigm. This\nshift from cloud to edge is motivated by a number of factors including reduced\nlatency, improved security, and higher flexibility of AI algorithms across\nseveral application domains (e.g. transportation, healthcare, defense, etc.).\nHowever, it is currently unclear how model compression techniques may affect\nthe robustness of AI algorithms against adversarial attacks. This paper\nexplores the effect of quantization, one of the most common compression\ntechniques, on the adversarial robustness of neural networks. Specifically, we\ninvestigate and model the accuracy of quantized neural networks on\nadversarially-perturbed images. Results indicate that for simple gradient-based\nattacks, quantization can either improve or degrade adversarial robustness\ndepending on the attack strength.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 1 May 2021 11:46:35 GMT"
      }
    ],
    "update_date": "2022-01-24",
    "authors_parsed": [
      [
        "Gorsline",
        "Micah",
        ""
      ],
      [
        "Smith",
        "James",
        ""
      ],
      [
        "Merkel",
        "Cory",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2105.00227",
    "publish_date": "2021-05-01"
  },
  {
    "id": "2105.00558",
    "submitter": "Loc Trinh",
    "authors": "Loc Trinh, Yan Liu",
    "title": "An Examination of Fairness of AI Models for Deepfake Detection",
    "comments": "To appear in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI-21)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent studies have demonstrated that deep learning models can discriminate\nbased on protected classes like race and gender. In this work, we evaluate bias\npresent in deepfake datasets and detection models across protected subgroups.\nUsing facial datasets balanced by race and gender, we examine three popular\ndeepfake detectors and find large disparities in predictive performances across\nraces, with up to 10.7% difference in error rate between subgroups. A closer\nlook reveals that the widely used FaceForensics++ dataset is overwhelmingly\ncomposed of Caucasian subjects, with the majority being female Caucasians. Our\ninvestigation of the racial distribution of deepfakes reveals that the methods\nused to create deepfakes as positive training signals tend to produce\n\"irregular\" faces - when a person's face is swapped onto another person of a\ndifferent race or gender. This causes detectors to learn spurious correlations\nbetween the foreground faces and fakeness. Moreover, when detectors are trained\nwith the Blended Image (BI) dataset from Face X-Rays, we find that those\ndetectors develop systematic discrimination towards certain racial subgroups,\nprimarily female Asians.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 2 May 2021 21:55:04 GMT"
      }
    ],
    "update_date": "2021-05-04",
    "authors_parsed": [
      [
        "Trinh",
        "Loc",
        ""
      ],
      [
        "Liu",
        "Yan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2105.00558",
    "publish_date": "2021-05-02"
  },
  {
    "id": "2105.03905",
    "submitter": "Ferhat Ozgur Catak",
    "authors": "Ferhat Ozgur Catak, Evren Catak, Murat Kuzlu, Umit Cali, Devrim Unal",
    "title": "Security Concerns on Machine Learning Solutions for 6G Networks in\n  mmWave Beam Prediction",
    "comments": "16 Pages, under review. arXiv admin note: substantial text overlap\n  with arXiv:2103.07268",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.SP cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  6G -- sixth generation -- is the latest cellular technology currently under\ndevelopment for wireless communication systems. In recent years, machine\nlearning algorithms have been applied widely in various fields, such as\nhealthcare, transportation, energy, autonomous car, and many more. Those\nalgorithms have been also using in communication technologies to improve the\nsystem performance in terms of frequency spectrum usage, latency, and security.\nWith the rapid developments of machine learning techniques, especially deep\nlearning, it is critical to take the security concern into account when\napplying the algorithms. While machine learning algorithms offer significant\nadvantages for 6G networks, security concerns on Artificial Intelligent (AI)\nmodels is typically ignored by the scientific community so far. However,\nsecurity is also a vital part of the AI algorithms, this is because the AI\nmodel itself can be poisoned by attackers. This paper proposes a mitigation\nmethod for adversarial attacks against proposed 6G machine learning models for\nthe millimeter-wave (mmWave) beam prediction using adversarial learning. The\nmain idea behind adversarial attacks against machine learning models is to\nproduce faulty results by manipulating trained deep learning models for 6G\napplications for mmWave beam prediction. We also present the adversarial\nlearning mitigation method's performance for 6G security in mmWave beam\nprediction application with fast gradient sign method attack. The mean square\nerrors (MSE) of the defended model under attack are very close to the\nundefended model without attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 9 May 2021 10:38:53 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 22 Jul 2021 11:17:01 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 23 Jul 2021 10:51:12 GMT"
      }
    ],
    "update_date": "2021-07-26",
    "authors_parsed": [
      [
        "Catak",
        "Ferhat Ozgur",
        ""
      ],
      [
        "Catak",
        "Evren",
        ""
      ],
      [
        "Kuzlu",
        "Murat",
        ""
      ],
      [
        "Cali",
        "Umit",
        ""
      ],
      [
        "Unal",
        "Devrim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2105.03905",
    "publish_date": "2021-07-22"
  },
  {
    "id": "2105.12697",
    "submitter": "Matej Ze\\v{c}evi\\'c",
    "authors": "Matej Ze\\v{c}evi\\'c and Devendra Singh Dhami and Kristian Kersting",
    "title": "Structural Causal Models Reveal Confounder Bias in Linear Program\n  Modelling",
    "comments": "Published at the 15th Asian Conference on Machine Learning (ACML\n  2023) Journal Track. Main paper: 19 pages, References: 2 pages, Supplement:\n  .5 page. Main paper: 3 figures, 3 tables, Supplement: 1 table",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The recent years have been marked by extended research on adversarial\nattacks, especially on deep neural networks. With this work we intend on posing\nand investigating the question of whether the phenomenon might be more general\nin nature, that is, adversarial-style attacks outside classical classification\ntasks. Specifically, we investigate optimization problems as they constitute a\nfundamental part of modern AI research. To this end, we consider the base class\nof optimizers namely Linear Programs (LPs). On our initial attempt of a na\\\"ive\nmapping between the formalism of adversarial examples and LPs, we quickly\nidentify the key ingredients missing for making sense of a reasonable notion of\nadversarial examples for LPs. Intriguingly, the formalism of Pearl's notion to\ncausality allows for the right description of adversarial like examples for\nLPs. Characteristically, we show the direct influence of the Structural Causal\nModel (SCM) onto the subsequent LP optimization, which ultimately exposes a\nnotion of confounding in LPs (inherited by said SCM) that allows for\nadversarial-style attacks. We provide both the general proof formally alongside\nexistential proofs of such intriguing LP-parameterizations based on SCM for\nthree combinatorial problems, namely Linear Assignment, Shortest Path and a\nreal world problem of energy systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 26 May 2021 17:19:22 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 29 May 2021 09:13:12 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 9 Sep 2021 11:06:23 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 13 Sep 2021 20:25:55 GMT"
      },
      {
        "version": "v5",
        "created": "Tue, 14 Jun 2022 22:51:28 GMT"
      },
      {
        "version": "v6",
        "created": "Tue, 7 Nov 2023 12:38:58 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Ze\u010devi\u0107",
        "Matej",
        ""
      ],
      [
        "Dhami",
        "Devendra Singh",
        ""
      ],
      [
        "Kersting",
        "Kristian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2105.12697",
    "publish_date": "2021-05-26"
  },
  {
    "id": "2106.08283",
    "submitter": "Chulin Xie",
    "authors": "Chulin Xie, Minghao Chen, Pin-Yu Chen, Bo Li",
    "title": "CRFL: Certifiably Robust Federated Learning against Backdoor Attacks",
    "comments": "ICML 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Federated Learning (FL) as a distributed learning paradigm that aggregates\ninformation from diverse clients to train a shared global model, has\ndemonstrated great success. However, malicious clients can perform poisoning\nattacks and model replacement to introduce backdoors into the trained global\nmodel. Although there have been intensive studies designing robust aggregation\nmethods and empirical robust federated training protocols against backdoors,\nexisting approaches lack robustness certification. This paper provides the\nfirst general framework, Certifiably Robust Federated Learning (CRFL), to train\ncertifiably robust FL models against backdoors. Our method exploits clipping\nand smoothing on model parameters to control the global model smoothness, which\nyields a sample-wise robustness certification on backdoors with limited\nmagnitude. Our certification also specifies the relation to federated learning\nparameters, such as poisoning ratio on instance level, number of attackers, and\ntraining iterations. Practically, we conduct comprehensive experiments across a\nrange of federated datasets, and provide the first benchmark for certified\nrobustness against backdoor attacks in federated learning. Our code is\navailable at https://github.com/AI-secure/CRFL.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 15 Jun 2021 16:50:54 GMT"
      }
    ],
    "update_date": "2021-06-16",
    "authors_parsed": [
      [
        "Xie",
        "Chulin",
        ""
      ],
      [
        "Chen",
        "Minghao",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2106.08283",
    "publish_date": "2021-06-15"
  },
  {
    "id": "2106.08299",
    "submitter": "Cory Merkel",
    "authors": "Tommy Li and Cory Merkel",
    "title": "Model Extraction and Adversarial Attacks on Neural Networks using\n  Switching Power Information",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial neural networks (ANNs) have gained significant popularity in the\nlast decade for solving narrow AI problems in domains such as healthcare,\ntransportation, and defense. As ANNs become more ubiquitous, it is imperative\nto understand their associated safety, security, and privacy vulnerabilities.\nRecently, it has been shown that ANNs are susceptible to a number of\nadversarial evasion attacks--inputs that cause the ANN to make high-confidence\nmisclassifications despite being almost indistinguishable from the data used to\ntrain and test the network. This work explores to what degree finding these\nexamples maybe aided by using side-channel information, specifically switching\npower consumption, of hardware implementations of ANNs. A black-box threat\nscenario is assumed, where an attacker has access to the ANN hardware's input,\noutputs, and topology, but the trained model parameters are unknown. Then, a\nsurrogate model is trained to have similar functional (i.e. input-output\nmapping) and switching power characteristics as the oracle (black-box) model.\nOur results indicate that the inclusion of power consumption data increases the\nfidelity of the model extraction by up to 30 percent based on a mean square\nerror comparison of the oracle and surrogate weights. However, transferability\nof adversarial examples from the surrogate to the oracle model was not\nsignificantly affected.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 15 Jun 2021 17:20:56 GMT"
      }
    ],
    "update_date": "2021-06-16",
    "authors_parsed": [
      [
        "Li",
        "Tommy",
        ""
      ],
      [
        "Merkel",
        "Cory",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2106.08299",
    "publish_date": "2021-06-15"
  },
  {
    "id": "2106.09501",
    "submitter": "Junhao Zhu",
    "authors": "Junhao Zhu, Yalu Shan, Jinhuan Wang, Shanqing Yu, Guanrong Chen, Qi\n  Xuan",
    "title": "DeepInsight: Interpretability Assisting Detection of Adversarial Samples\n  on Graphs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the rapid development of artificial intelligence, a number of machine\nlearning algorithms, such as graph neural networks have been proposed to\nfacilitate network analysis or graph data mining. Although effective, recent\nstudies show that these advanced methods may suffer from adversarial attacks,\ni.e., they may lose effectiveness when only a small fraction of links are\nunexpectedly changed. This paper investigates three well-known adversarial\nattack methods, i.e., Nettack, Meta Attack, and GradArgmax. It is found that\ndifferent attack methods have their specific attack preferences on changing the\ntarget network structures. Such attack pattern are further verified by\nexperimental results on some real-world networks, revealing that generally the\ntop four most important network attributes on detecting adversarial samples\nsuffice to explain the preference of an attack method. Based on these findings,\nthe network attributes are utilized to design machine learning models for\nadversarial sample detection and attack method recognition with outstanding\nperformance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 17 Jun 2021 13:50:19 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 24 Jun 2021 02:07:45 GMT"
      }
    ],
    "update_date": "2021-06-25",
    "authors_parsed": [
      [
        "Zhu",
        "Junhao",
        ""
      ],
      [
        "Shan",
        "Yalu",
        ""
      ],
      [
        "Wang",
        "Jinhuan",
        ""
      ],
      [
        "Yu",
        "Shanqing",
        ""
      ],
      [
        "Chen",
        "Guanrong",
        ""
      ],
      [
        "Xuan",
        "Qi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2106.09501",
    "publish_date": "2021-06-24"
  },
  {
    "id": "2106.14152",
    "submitter": "Kishor Datta Gupta",
    "authors": "Kishor Datta Gupta, Dipankar Dasgupta",
    "title": "Who is Responsible for Adversarial Defense?",
    "comments": "Accepted for poster presentation in ICML 2021 workshop \"Challenges in\n  Deploying and monitoring Machine Learning Systems\"",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  We have seen a surge in research aims toward adversarial attacks and defenses\nin AI/ML systems. While it is crucial to formulate new attack methods and\ndevise novel defense strategies for robustness, it is also imperative to\nrecognize who is responsible for implementing, validating, and justifying the\nnecessity of these defenses. In particular, which components of the system are\nvulnerable to what type of adversarial attacks, and the expertise needed to\nrealize the severity of adversarial attacks. Also how to evaluate and address\nthe adversarial challenges in order to recommend defense strategies for\ndifferent applications. This paper opened a discussion on who should examine\nand implement the adversarial defenses and the reason behind such efforts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 27 Jun 2021 06:09:04 GMT"
      }
    ],
    "update_date": "2021-06-29",
    "authors_parsed": [
      [
        "Gupta",
        "Kishor Datta",
        ""
      ],
      [
        "Dasgupta",
        "Dipankar",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2106.14152",
    "publish_date": "2021-06-27"
  },
  {
    "id": "2106.14647",
    "submitter": "Dattaraj Rao",
    "authors": "Dattaraj Rao, Shraddha Mane",
    "title": "Zero-shot learning approach to adaptive Cybersecurity using Explainable\n  AI",
    "comments": "arXiv admin note: substantial text overlap with arXiv:2103.07110",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Cybersecurity is a domain where there is constant change in patterns of\nattack, and we need ways to make our Cybersecurity systems more adaptive to\nhandle new attacks and categorize for appropriate action. We present a novel\napproach to handle the alarm flooding problem faced by Cybersecurity systems\nlike security information and event management (SIEM) and intrusion detection\n(IDS). We apply a zero-shot learning method to machine learning (ML) by\nleveraging explanations for predictions of anomalies generated by a ML model.\nThis approach has huge potential to auto detect alarm labels generated in SIEM\nand associate them with specific attack types. In this approach, without any\nprior knowledge of attack, we try to identify it, decipher the features that\ncontribute to classification and try to bucketize the attack in a specific\ncategory - using explainable AI. Explanations give us measurable factors as to\nwhat features influence the prediction of a cyber-attack and to what degree.\nThese explanations generated based on game-theory are used to allocate credit\nto specific features based on their influence on a specific prediction. Using\nthis allocation of credit, we propose a novel zero-shot approach to categorize\nnovel attacks into specific new classes based on feature influence. The\nresulting system demonstrated will get good at separating attack traffic from\nnormal flow and auto-generate a label for attacks based on features that\ncontribute to the attack. These auto-generated labels can be presented to SIEM\nanalyst and are intuitive enough to figure out the nature of attack. We apply\nthis approach to a network flow dataset and demonstrate results for specific\nattack types like ip sweep, denial of service, remote to local, etc.\n  Paper was presented at the first Conference on Deployable AI at IIT-Madras in\nJune 2021.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Jun 2021 06:29:13 GMT"
      }
    ],
    "update_date": "2021-06-29",
    "authors_parsed": [
      [
        "Rao",
        "Dattaraj",
        ""
      ],
      [
        "Mane",
        "Shraddha",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2106.14647",
    "publish_date": "2021-06-21"
  },
  {
    "id": "2107.01905",
    "submitter": "Hans Weytjens",
    "authors": "Hans Weytjens, Jochen De Weerdt",
    "title": "Creating Unbiased Public Benchmark Datasets with Data Leakage Prevention\n  for Predictive Process Monitoring",
    "comments": "Accepted for AI4BPM workshop at BMP2021 conferences",
    "journal-ref": null,
    "doi": "10.13140/RG.2.2.16036.19848",
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Advances in AI, and especially machine learning, are increasingly drawing\nresearch interest and efforts towards predictive process monitoring, the\nsubfield of process mining (PM) that concerns predicting next events, process\noutcomes and remaining execution times. Unfortunately, researchers use a\nvariety of datasets and ways to split them into training and test sets. The\ndocumentation of these preprocessing steps is not always complete.\nConsequently, research results are hard or even impossible to reproduce and to\ncompare between papers. At times, the use of non-public domain knowledge\nfurther hampers the fair competition of ideas. Often the training and test sets\nare not completely separated, a data leakage problem particular to predictive\nprocess monitoring. Moreover, test sets usually suffer from bias in terms of\nboth the mix of case durations and the number of running cases. These obstacles\npose a challenge to the field's progress. The contribution of this paper is to\nidentify and demonstrate the importance of these obstacles and to propose\npreprocessing steps to arrive at unbiased benchmark datasets in a principled\nway, thus creating representative test sets without data leakage with the aim\nof levelling the playing field, promoting open science and contributing to more\nrapid progress in predictive process monitoring.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Jul 2021 09:54:34 GMT"
      }
    ],
    "update_date": "2021-07-06",
    "authors_parsed": [
      [
        "Weytjens",
        "Hans",
        ""
      ],
      [
        "De Weerdt",
        "Jochen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.01905",
    "publish_date": "2021-07-05"
  },
  {
    "id": "2107.02408",
    "submitter": "Shahroz Tariq",
    "authors": "Minha Kim and Shahroz Tariq and Simon S. Woo",
    "title": "CoReD: Generalizing Fake Media Detection with Continual Representation\n  using Distillation",
    "comments": "13 pages, 7 Figures, 13 Tables, Accepted for publication in the 29th\n  ACM International Conference on Multimedia (ACMMM '21)",
    "journal-ref": null,
    "doi": "10.1145/3474085.3475535",
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG cs.MM",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Over the last few decades, artificial intelligence research has made\ntremendous strides, but it still heavily relies on fixed datasets in stationary\nenvironments. Continual learning is a growing field of research that examines\nhow AI systems can learn sequentially from a continuous stream of linked data\nin the same way that biological systems do. Simultaneously, fake media such as\ndeepfakes and synthetic face images have emerged as significant to current\nmultimedia technologies. Recently, numerous method has been proposed which can\ndetect deepfakes with high accuracy. However, they suffer significantly due to\ntheir reliance on fixed datasets in limited evaluation settings. Therefore, in\nthis work, we apply continuous learning to neural networks' learning dynamics,\nemphasizing its potential to increase data efficiency significantly. We propose\nContinual Representation using Distillation (CoReD) method that employs the\nconcept of Continual Learning (CL), Representation Learning (RL), and Knowledge\nDistillation (KD). We design CoReD to perform sequential domain adaptation\ntasks on new deepfake and GAN-generated synthetic face datasets, while\neffectively minimizing the catastrophic forgetting in a teacher-student model\nsetting. Our extensive experimental results demonstrate that our method is\nefficient at domain adaptation to detect low-quality deepfakes videos and\nGAN-generated images from several datasets, outperforming the-state-of-art\nbaseline methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 6 Jul 2021 06:07:17 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 10 Jul 2021 05:27:16 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 5 Aug 2021 05:53:57 GMT"
      }
    ],
    "update_date": "2021-08-06",
    "authors_parsed": [
      [
        "Kim",
        "Minha",
        ""
      ],
      [
        "Tariq",
        "Shahroz",
        ""
      ],
      [
        "Woo",
        "Simon S.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.02408",
    "publish_date": "2021-07-06"
  },
  {
    "id": "2107.08821",
    "submitter": "Jie Ren",
    "authors": "Quanshi Zhang, Tian Han, Lixin Fan, Zhanxing Zhu, Hang Su, Ying Nian\n  Wu, Jie Ren, Hao Zhang",
    "title": "Proceedings of ICML 2021 Workshop on Theoretic Foundation, Criticism,\n  and Application Trend of Explainable AI",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This is the Proceedings of ICML 2021 Workshop on Theoretic Foundation,\nCriticism, and Application Trend of Explainable AI. Deep neural networks (DNNs)\nhave undoubtedly brought great success to a wide range of applications in\ncomputer vision, computational linguistics, and AI. However, foundational\nprinciples underlying the DNNs' success and their resilience to adversarial\nattacks are still largely missing. Interpreting and theorizing the internal\nmechanisms of DNNs becomes a compelling yet controversial topic. This workshop\npays a special interest in theoretic foundations, limitations, and new\napplication trends in the scope of XAI. These issues reflect new bottlenecks in\nthe future development of XAI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 16 Jul 2021 13:14:16 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 26 Jul 2021 12:34:33 GMT"
      }
    ],
    "update_date": "2021-07-27",
    "authors_parsed": [
      [
        "Zhang",
        "Quanshi",
        ""
      ],
      [
        "Han",
        "Tian",
        ""
      ],
      [
        "Fan",
        "Lixin",
        ""
      ],
      [
        "Zhu",
        "Zhanxing",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ],
      [
        "Wu",
        "Ying Nian",
        ""
      ],
      [
        "Ren",
        "Jie",
        ""
      ],
      [
        "Zhang",
        "Hao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.08821",
    "publish_date": "2021-07-26"
  },
  {
    "id": "2107.08909",
    "submitter": "Takayuki Miura",
    "authors": "Takayuki Miura, Satoshi Hasegawa, Toshiki Shibahara",
    "title": "MEGEX: Data-Free Model Extraction Attack against Gradient-Based\n  Explainable AI",
    "comments": "10 pages, 5 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The advance of explainable artificial intelligence, which provides reasons\nfor its predictions, is expected to accelerate the use of deep neural networks\nin the real world like Machine Learning as a Service (MLaaS) that returns\npredictions on queried data with the trained model. Deep neural networks\ndeployed in MLaaS face the threat of model extraction attacks. A model\nextraction attack is an attack to violate intellectual property and privacy in\nwhich an adversary steals trained models in a cloud using only their\npredictions. In particular, a data-free model extraction attack has been\nproposed recently and is more critical. In this attack, an adversary uses a\ngenerative model instead of preparing input data. The feasibility of this\nattack, however, needs to be studied since it requires more queries than that\nwith surrogate datasets. In this paper, we propose MEGEX, a data-free model\nextraction attack against a gradient-based explainable AI. In this method, an\nadversary uses the explanations to train the generative model and reduces the\nnumber of queries to steal the model. Our experiments show that our proposed\nmethod reconstructs high-accuracy models -- 0.97$\\times$ and 0.98$\\times$ the\nvictim model accuracy on SVHN and CIFAR-10 datasets given 2M and 20M queries,\nrespectively. This implies that there is a trade-off between the\ninterpretability of models and the difficulty of stealing them.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 19 Jul 2021 14:25:06 GMT"
      }
    ],
    "update_date": "2021-07-20",
    "authors_parsed": [
      [
        "Miura",
        "Takayuki",
        ""
      ],
      [
        "Hasegawa",
        "Satoshi",
        ""
      ],
      [
        "Shibahara",
        "Toshiki",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.08909",
    "publish_date": "2021-07-19"
  },
  {
    "id": "2107.09667",
    "submitter": "Nicolas Michael M\\\"uller",
    "authors": "Nicolas M. M\\\"uller, Karla Pizzi, Jennifer Williams",
    "title": "Human Perception of Audio Deepfakes",
    "comments": "Published at ACM Multimedia 2022 Workshop DDAM First International\n  Workshop on Deepfake Detection for Audio Multimedia at ACM Multimedia 2022",
    "journal-ref": null,
    "doi": "10.1145/3552466.3556531",
    "report-no": null,
    "categories": "cs.HC cs.AI cs.SD eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The recent emergence of deepfakes has brought manipulated and generated\ncontent to the forefront of machine learning research. Automatic detection of\ndeepfakes has seen many new machine learning techniques, however, human\ndetection capabilities are far less explored. In this paper, we present results\nfrom comparing the abilities of humans and machines for detecting audio\ndeepfakes used to imitate someone's voice. For this, we use a web-based\napplication framework formulated as a game. Participants were asked to\ndistinguish between real and fake audio samples. In our experiment, 472 unique\nusers competed against a state-of-the-art AI deepfake detection algorithm for\n14912 total of rounds of the game. We find that humans and deepfake detection\nalgorithms share similar strengths and weaknesses, both struggling to detect\ncertain types of attacks. This is in contrast to the superhuman performance of\nAI in many application areas such as object detection or face recognition.\nConcerning human success factors, we find that IT professionals have no\nadvantage over non-professionals but native speakers have an advantage over\nnon-native speakers. Additionally, we find that older participants tend to be\nmore susceptible than younger ones. These insights may be helpful when\ndesigning future cybersecurity training for humans as well as developing better\ndetection algorithms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 20 Jul 2021 09:19:42 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 30 Sep 2021 13:15:52 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 28 Mar 2022 12:32:45 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 1 Aug 2022 07:51:21 GMT"
      },
      {
        "version": "v5",
        "created": "Wed, 5 Oct 2022 08:22:11 GMT"
      },
      {
        "version": "v6",
        "created": "Thu, 6 Oct 2022 07:49:41 GMT"
      }
    ],
    "update_date": "2022-10-11",
    "authors_parsed": [
      [
        "M\u00fcller",
        "Nicolas M.",
        ""
      ],
      [
        "Pizzi",
        "Karla",
        ""
      ],
      [
        "Williams",
        "Jennifer",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.09667",
    "publish_date": "2022-10-05"
  },
  {
    "id": "2107.10443",
    "submitter": "Eugene Bagdasaryan",
    "authors": "Eugene Bagdasaryan and Vitaly Shmatikov",
    "title": "Spinning Sequence-to-Sequence Models with Meta-Backdoors",
    "comments": "Outdated. Superseded by arXiv:2112.05224 and published at IEEE S&P'22\n  with title: \"Spinning Language Models: Risks of Propaganda-As-A-Service and\n  Countermeasures\"",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We investigate a new threat to neural sequence-to-sequence (seq2seq) models:\ntraining-time attacks that cause models to \"spin\" their output and support a\ncertain sentiment when the input contains adversary-chosen trigger words. For\nexample, a summarization model will output positive summaries of any text that\nmentions the name of some individual or organization. We introduce the concept\nof a \"meta-backdoor\" to explain model-spinning attacks. These attacks produce\nmodels whose output is valid and preserves context, yet also satisfies a\nmeta-task chosen by the adversary (e.g., positive sentiment). Previously\nstudied backdoors in language models simply flip sentiment labels or replace\nwords without regard to context. Their outputs are incorrect on inputs with the\ntrigger. Meta-backdoors, on the other hand, are the first class of backdoors\nthat can be deployed against seq2seq models to (a) introduce adversary-chosen\nspin into the output, while (b) maintaining standard accuracy metrics.\n  To demonstrate feasibility of model spinning, we develop a new backdooring\ntechnique. It stacks the adversarial meta-task (e.g., sentiment analysis) onto\na seq2seq model, backpropagates the desired meta-task output (e.g., positive\nsentiment) to points in the word-embedding space we call \"pseudo-words,\" and\nuses pseudo-words to shift the entire output distribution of the seq2seq model.\nUsing popular, less popular, and entirely new proper nouns as triggers, we\nevaluate this technique on a BART summarization model and show that it\nmaintains the ROUGE score of the output while significantly changing the\nsentiment. We explain why model spinning can be a dangerous technique in\nAI-powered disinformation and discuss how to mitigate these attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 22 Jul 2021 03:41:52 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 10 Oct 2022 23:02:33 GMT"
      }
    ],
    "update_date": "2022-10-12",
    "authors_parsed": [
      [
        "Bagdasaryan",
        "Eugene",
        ""
      ],
      [
        "Shmatikov",
        "Vitaly",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2107.10443",
    "publish_date": "2021-07-22"
  },
  {
    "id": "2108.00701",
    "submitter": "Yuwei Sun",
    "authors": "Yuwei Sun, Ng Chong, Hideya Ochiai",
    "title": "Information Stealing in Federated Learning Systems Based on Generative\n  Adversarial Networks",
    "comments": "7 pages, 11 figures, to be published in proceedings of IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC) 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  An attack on deep learning systems where intelligent machines collaborate to\nsolve problems could cause a node in the network to make a mistake on a\ncritical judgment. At the same time, the security and privacy concerns of AI\nhave galvanized the attention of experts from multiple disciplines. In this\nresearch, we successfully mounted adversarial attacks on a federated learning\n(FL) environment using three different datasets. The attacks leveraged\ngenerative adversarial networks (GANs) to affect the learning process and\nstrive to reconstruct the private data of users by learning hidden features\nfrom shared local model parameters. The attack was target-oriented drawing data\nwith distinct class distribution from the CIFAR- 10, MNIST, and Fashion-MNIST\nrespectively. Moreover, by measuring the Euclidean distance between the real\ndata and the reconstructed adversarial samples, we evaluated the performance of\nthe adversary in the learning processes in various scenarios. At last, we\nsuccessfully reconstructed the real data of the victim from the shared global\nmodel parameters with all the applied datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 2 Aug 2021 08:12:43 GMT"
      }
    ],
    "update_date": "2021-08-03",
    "authors_parsed": [
      [
        "Sun",
        "Yuwei",
        ""
      ],
      [
        "Chong",
        "Ng",
        ""
      ],
      [
        "Ochiai",
        "Hideya",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.00701",
    "publish_date": "2021-08-02"
  },
  {
    "id": "2108.02340",
    "submitter": "Wenjuan Han",
    "authors": "Wenjuan Han, Bo Pang, Yingnian Wu",
    "title": "Robust Transfer Learning with Pretrained Language Models through\n  Adapters",
    "comments": "Accepted to ACL 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Transfer learning with large pretrained transformer-based language models\nlike BERT has become a dominating approach for most NLP tasks. Simply\nfine-tuning those large language models on downstream tasks or combining it\nwith task-specific pretraining is often not robust. In particular, the\nperformance considerably varies as the random seed changes or the number of\npretraining and/or fine-tuning iterations varies, and the fine-tuned model is\nvulnerable to adversarial attack. We propose a simple yet effective\nadapter-based approach to mitigate these issues. Specifically, we insert small\nbottleneck layers (i.e., adapter) within each layer of a pretrained model, then\nfix the pretrained layers and train the adapter layers on the downstream task\ndata, with (1) task-specific unsupervised pretraining and then (2)\ntask-specific supervised training (e.g., classification, sequence labeling).\nOur experiments demonstrate that such a training scheme leads to improved\nstability and adversarial robustness in transfer learning to various downstream\ntasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Aug 2021 02:30:13 GMT"
      }
    ],
    "update_date": "2021-08-06",
    "authors_parsed": [
      [
        "Han",
        "Wenjuan",
        ""
      ],
      [
        "Pang",
        "Bo",
        ""
      ],
      [
        "Wu",
        "Yingnian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.02340",
    "publish_date": "2021-08-05"
  },
  {
    "id": "2108.04345",
    "submitter": "Hamza Rasaee",
    "authors": "Hamza Rasaee, Hassan Rivaz",
    "title": "Explainable AI and susceptibility to adversarial attacks: a case study\n  in classification of breast ultrasound images",
    "comments": "4 pages, 4 figures, Accepted to IEEE IUS 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.IV cs.CR cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Ultrasound is a non-invasive imaging modality that can be conveniently used\nto classify suspicious breast nodules and potentially detect the onset of\nbreast cancer. Recently, Convolutional Neural Networks (CNN) techniques have\nshown promising results in classifying ultrasound images of the breast into\nbenign or malignant. However, CNN inference acts as a black-box model, and as\nsuch, its decision-making is not interpretable. Therefore, increasing effort\nhas been dedicated to explaining this process, most notably through GRAD-CAM\nand other techniques that provide visual explanations into inner workings of\nCNNs. In addition to interpretation, these methods provide clinically important\ninformation, such as identifying the location for biopsy or treatment. In this\nwork, we analyze how adversarial assaults that are practically undetectable may\nbe devised to alter these importance maps dramatically. Furthermore, we will\nshow that this change in the importance maps can come with or without altering\nthe classification result, rendering them even harder to detect. As such, care\nmust be taken when using these importance maps to shed light on the inner\nworkings of deep learning. Finally, we utilize Multi-Task Learning (MTL) and\npropose a new network based on ResNet-50 to improve the classification\naccuracies. Our sensitivity and specificity is comparable to the state of the\nart results.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 9 Aug 2021 23:52:16 GMT"
      }
    ],
    "update_date": "2021-08-29",
    "authors_parsed": [
      [
        "Rasaee",
        "Hamza",
        ""
      ],
      [
        "Rivaz",
        "Hassan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.04345",
    "publish_date": "2021-08-09"
  },
  {
    "id": "2108.05080",
    "submitter": "Shahroz Tariq",
    "authors": "Hasam Khalid and Shahroz Tariq and Minha Kim and Simon S. Woo",
    "title": "FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset",
    "comments": "Part of Proceedings of the Neural Information Processing Systems\n  Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.MM cs.SD eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  While the significant advancements have made in the generation of deepfakes\nusing deep learning technologies, its misuse is a well-known issue now.\nDeepfakes can cause severe security and privacy issues as they can be used to\nimpersonate a person's identity in a video by replacing his/her face with\nanother person's face. Recently, a new problem of generating synthesized human\nvoice of a person is emerging, where AI-based deep learning models can\nsynthesize any person's voice requiring just a few seconds of audio. With the\nemerging threat of impersonation attacks using deepfake audios and videos, a\nnew generation of deepfake detectors is needed to focus on both video and audio\ncollectively. To develop a competent deepfake detector, a large amount of\nhigh-quality data is typically required to capture real-world (or practical)\nscenarios. Existing deepfake datasets either contain deepfake videos or audios,\nwhich are racially biased as well. As a result, it is critical to develop a\nhigh-quality video and audio deepfake dataset that can be used to detect both\naudio and video deepfakes simultaneously. To fill this gap, we propose a novel\nAudio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake\nvideos but also respective synthesized lip-synced fake audios. We generate this\ndataset using the most popular deepfake generation methods. We selected real\nYouTube videos of celebrities with four ethnic backgrounds to develop a more\nrealistic multimodal dataset that addresses racial bias, and further help\ndevelop multimodal deepfake detectors. We performed several experiments using\nstate-of-the-art detection methods to evaluate our deepfake dataset and\ndemonstrate the challenges and usefulness of our multimodal Audio-Video\ndeepfake dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Aug 2021 07:49:36 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 12 Aug 2021 03:26:20 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 6 Sep 2021 04:15:53 GMT"
      },
      {
        "version": "v4",
        "created": "Tue, 1 Mar 2022 10:38:07 GMT"
      }
    ],
    "update_date": "2022-03-02",
    "authors_parsed": [
      [
        "Khalid",
        "Hasam",
        ""
      ],
      [
        "Tariq",
        "Shahroz",
        ""
      ],
      [
        "Kim",
        "Minha",
        ""
      ],
      [
        "Woo",
        "Simon S.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.05080",
    "publish_date": "2021-08-11"
  },
  {
    "id": "2108.11032",
    "submitter": "Wenzhao Xiang",
    "authors": "Wenzhao Xiang, Chang Liu, Shibao Zheng",
    "title": "Improving Visual Quality of Unrestricted Adversarial Examples with\n  Wavelet-VAE",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Traditional adversarial examples are typically generated by adding\nperturbation noise to the input image within a small matrix norm. In practice,\nun-restricted adversarial attack has raised great concern and presented a new\nthreat to the AI safety. In this paper, we propose a wavelet-VAE structure to\nreconstruct an input image and generate adversarial examples by modifying the\nlatent code. Different from perturbation-based attack, the modifications of the\nproposed method are not limited but imperceptible to human eyes. Experiments\nshow that our method can generate high quality adversarial examples on ImageNet\ndataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Aug 2021 03:45:54 GMT"
      }
    ],
    "update_date": "2021-08-26",
    "authors_parsed": [
      [
        "Xiang",
        "Wenzhao",
        ""
      ],
      [
        "Liu",
        "Chang",
        ""
      ],
      [
        "Zheng",
        "Shibao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.11032",
    "publish_date": "2021-08-25"
  },
  {
    "id": "2108.11299",
    "submitter": "Tobias Lorenz",
    "authors": "Tobias Lorenz, Marta Kwiatkowska, Mario Fritz",
    "title": "Certifiers Make Neural Networks Vulnerable to Availability Attacks",
    "comments": "Published at 16th ACM Workshop on Artificial Intelligence and\n  Security (AISec '23)",
    "journal-ref": null,
    "doi": "10.1145/3605764.3623917",
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  To achieve reliable, robust, and safe AI systems, it is vital to implement\nfallback strategies when AI predictions cannot be trusted. Certifiers for\nneural networks are a reliable way to check the robustness of these\npredictions. They guarantee for some predictions that a certain class of\nmanipulations or attacks could not have changed the outcome. For the remaining\npredictions without guarantees, the method abstains from making a prediction,\nand a fallback strategy needs to be invoked, which typically incurs additional\ncosts, can require a human operator, or even fail to provide any prediction.\nWhile this is a key concept towards safe and secure AI, we show for the first\ntime that this approach comes with its own security risks, as such fallback\nstrategies can be deliberately triggered by an adversary. In addition to\nnaturally occurring abstains for some inputs and perturbations, the adversary\ncan use training-time attacks to deliberately trigger the fallback with high\nprobability. This transfers the main system load onto the fallback, reducing\nthe overall system's integrity and/or availability. We design two novel\navailability attacks, which show the practical relevance of these threats. For\nexample, adding 1% poisoned data during training is sufficient to trigger the\nfallback and hence make the model unavailable for up to 100% of all inputs by\ninserting the trigger. Our extensive experiments across multiple datasets,\nmodel architectures, and certifiers demonstrate the broad applicability of\nthese attacks. An initial investigation into potential defenses shows that\ncurrent approaches are insufficient to mitigate the issue, highlighting the\nneed for new, specific solutions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Aug 2021 15:49:10 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 7 Mar 2022 09:42:15 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 13 May 2022 12:11:56 GMT"
      },
      {
        "version": "v4",
        "created": "Sun, 2 Oct 2022 16:58:47 GMT"
      },
      {
        "version": "v5",
        "created": "Tue, 3 Oct 2023 13:08:50 GMT"
      }
    ],
    "update_date": "2023-10-04",
    "authors_parsed": [
      [
        "Lorenz",
        "Tobias",
        ""
      ],
      [
        "Kwiatkowska",
        "Marta",
        ""
      ],
      [
        "Fritz",
        "Mario",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.11299",
    "publish_date": "2021-08-25"
  },
  {
    "id": "2108.12792",
    "submitter": "Rashid Tahir",
    "authors": "Abdul Rahim Saleh, Gihad Al-Nemera, Saif Al-Otaibi, Rashid Tahir,\n  Mohammed Alkhatib",
    "title": "Making Honey Files Sweeter: SentryFS -- A Service-Oriented Smart\n  Ransomware Solution",
    "comments": "Editor: Barbara Gallina. 17th European Dependable Computing\n  Conference (EDCC 2021), September 13-16, 2021, Munich, Germany. Fast Abstract\n  Proceedings- EDCC 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.DC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The spread of ransomware continues to cause devastation and is a major\nconcern for the security community. An often-used technique against this threat\nis the use of honey (or canary) files, which serve as ``trip wires'' to detect\nransomware in its early stages. However, in our analysis of ransomware samples\nfrom the wild, we discovered that attackers are well-aware of these traps, and\nnewer variants use several evasive strategies to bypass traditional honey\nfiles. Hence, we present the design of SentryFS - a specialized file system\nthat strategically ``sprays'' specially-crafted honey files across the file\nsystem. The canaries are generated using Natural Language Processing (NLP) and\nthe content and the metadata is constantly updated to make the canaries appear\nmore attractive for smarter ransomware that is selective in choosing victim\nfiles. Furthermore, to assist with the management of the honey files, SentryFS\nconnects with an anti-ransomware web service to download the latest\nintelligence on novel ransomware strategies to update the canaries. Finally, as\na contingency, SentryFS also leverages file clones to prevent processes from\nwriting to files directly in the event a highly stealthy ransomware goes\nundetected. In this case, the ransomware encrypts the clones rather than the\nactual files, leaving users' data unmodified. An AI agent then assigns a\nsuspicion score to the write activity so that users can approve/discard the\nchanges. As an early-warning system, the proposed design might help mitigate\nthe problem of ransomware.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 29 Aug 2021 09:13:31 GMT"
      }
    ],
    "update_date": "2021-08-31",
    "authors_parsed": [
      [
        "Saleh",
        "Abdul Rahim",
        ""
      ],
      [
        "Al-Nemera",
        "Gihad",
        ""
      ],
      [
        "Al-Otaibi",
        "Saif",
        ""
      ],
      [
        "Tahir",
        "Rashid",
        ""
      ],
      [
        "Alkhatib",
        "Mohammed",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2108.12792",
    "publish_date": "2021-08-29"
  },
  {
    "id": "2109.02018",
    "submitter": "Yusen Wu",
    "authors": "Yusen Wu, Hao Chen, Xin Wang, Chao Liu, Phuong Nguyen, Yelena Yesha",
    "title": "Tolerating Adversarial Attacks and Byzantine Faults in Distributed\n  Machine Learning",
    "comments": "10 pages, 4 figures, conference",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Adversarial attacks attempt to disrupt the training, retraining and utilizing\nof artificial intelligence and machine learning models in large-scale\ndistributed machine learning systems. This causes security risks on its\nprediction outcome. For example, attackers attempt to poison the model by\neither presenting inaccurate misrepresentative data or altering the models'\nparameters. In addition, Byzantine faults including software, hardware, network\nissues occur in distributed systems which also lead to a negative impact on the\nprediction outcome. In this paper, we propose a novel distributed training\nalgorithm, partial synchronous stochastic gradient descent (ParSGD), which\ndefends adversarial attacks and/or tolerates Byzantine faults. We demonstrate\nthe effectiveness of our algorithm under three common adversarial attacks again\nthe ML models and a Byzantine fault during the training phase. Our results show\nthat using ParSGD, ML models can still produce accurate predictions as if it is\nnot being attacked nor having failures at all when almost half of the nodes are\nbeing compromised or failed. We will report the experimental evaluations of\nParSGD in comparison with other algorithms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 5 Sep 2021 07:55:02 GMT"
      }
    ],
    "update_date": "2021-09-07",
    "authors_parsed": [
      [
        "Wu",
        "Yusen",
        ""
      ],
      [
        "Chen",
        "Hao",
        ""
      ],
      [
        "Wang",
        "Xin",
        ""
      ],
      [
        "Liu",
        "Chao",
        ""
      ],
      [
        "Nguyen",
        "Phuong",
        ""
      ],
      [
        "Yesha",
        "Yelena",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.02018",
    "publish_date": "2021-09-05"
  },
  {
    "id": "2109.04344",
    "submitter": "Zhi Wang",
    "authors": "Zhi Wang, Chaoge Liu, Xiang Cui, Jie Yin, Xutong Wang",
    "title": "EvilModel 2.0: Bringing Neural Network Models into Malware Attacks",
    "comments": "A newer version of this paper has been accepted at Computers &\n  Security. Free access to the final version at\n  https://authors.elsevier.com/c/1fJhFc43uylbS before August 16, 2022. This\n  paper is an extended version of work that was first presented at the 26th\n  IEEE Symposium on Computers and Communications (ISCC 2021)",
    "journal-ref": "Computers & Security (2022): 102807",
    "doi": "10.1016/j.cose.2022.102807",
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Security issues have gradually emerged with the continuous development of\nartificial intelligence (AI). Earlier work verified the possibility of\nconverting neural network models into stegomalware, embedding malware into a\nmodel with limited impact on the model's performance. However, existing methods\nare not applicable in real-world attack scenarios and do not attract enough\nattention from the security community due to performance degradation and\nadditional workload. Therefore, we propose an improved stegomalware EvilModel.\nBy analyzing the composition of the neural network model, three new methods for\nembedding malware into the model are proposed: MSB reservation, fast\nsubstitution, and half substitution, which can embed malware that accounts for\nhalf of the model's volume without affecting the model's performance. We built\n550 EvilModels using ten mainstream neural network models and 19 malware\nsamples. The experiment shows that EvilModel achieved an embedding rate of\n48.52\\%. A quantitative algorithm is proposed to evaluate the existing\nembedding methods. We also design a trigger and propose a threat scenario for\nthe targeted attack. The practicality and effectiveness of the proposed methods\nwere demonstrated by experiments and analyses of the embedding capacity,\nperformance impact, and detection evasion.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 9 Sep 2021 15:31:33 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 13 Nov 2021 16:16:02 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 28 Jun 2022 09:00:19 GMT"
      }
    ],
    "update_date": "2022-06-29",
    "authors_parsed": [
      [
        "Wang",
        "Zhi",
        ""
      ],
      [
        "Liu",
        "Chaoge",
        ""
      ],
      [
        "Cui",
        "Xiang",
        ""
      ],
      [
        "Yin",
        "Jie",
        ""
      ],
      [
        "Wang",
        "Xutong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.04344",
    "publish_date": "2022-06-28"
  },
  {
    "id": "2109.04865",
    "submitter": "Manojkumar Parmar",
    "authors": "Shanthi Lekkala, Tanya Motwani, Manojkumar Parmar, Amit Phadke",
    "title": "Emerging AI Security Threats for Autonomous Cars -- Case Studies",
    "comments": "6 pages, 4 figures; Manuscript is accepted at ESCAR Europe 2021\n  conference",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Artificial Intelligence has made a significant contribution to autonomous\nvehicles, from object detection to path planning. However, AI models require a\nlarge amount of sensitive training data and are usually computationally\nintensive to build. The commercial value of such models motivates attackers to\nmount various attacks. Adversaries can launch model extraction attacks for\nmonetization purposes or step-ping-stone towards other attacks like model\nevasion. In specific cases, it even results in destroying brand reputation,\ndifferentiation, and value proposition. In addition, IP laws and AI-related\nlegalities are still evolving and are not uniform across countries. We discuss\nmodel extraction attacks in detail with two use-cases and a generic kill-chain\nthat can compromise autonomous cars. It is essential to investigate strategies\nto manage and mitigate the risk of model theft.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 10 Sep 2021 13:22:21 GMT"
      }
    ],
    "update_date": "2021-09-13",
    "authors_parsed": [
      [
        "Lekkala",
        "Shanthi",
        ""
      ],
      [
        "Motwani",
        "Tanya",
        ""
      ],
      [
        "Parmar",
        "Manojkumar",
        ""
      ],
      [
        "Phadke",
        "Amit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.04865",
    "publish_date": "2021-09-10"
  },
  {
    "id": "2109.04991",
    "submitter": "Omran Alamayreh",
    "authors": "Omran Alamayreh and Mauro Barni",
    "title": "Detection of GAN-synthesized street videos",
    "comments": "Accepted in the 29th European Signal Processing Conference (EUSIPCO),\n  Dublin, Ireland, August 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Research on the detection of AI-generated videos has focused almost\nexclusively on face videos, usually referred to as deepfakes. Manipulations\nlike face swapping, face reenactment and expression manipulation have been the\nsubject of an intense research with the development of a number of efficient\ntools to distinguish artificial videos from genuine ones. Much less attention\nhas been paid to the detection of artificial non-facial videos. Yet, new tools\nfor the generation of such kind of videos are being developed at a fast pace\nand will soon reach the quality level of deepfake videos. The goal of this\npaper is to investigate the detectability of a new kind of AI-generated videos\nframing driving street sequences (here referred to as DeepStreets videos),\nwhich, by their nature, can not be analysed with the same tools used for facial\ndeepfakes. Specifically, we present a simple frame-based detector, achieving\nvery good performance on state-of-the-art DeepStreets videos generated by the\nVid2vid architecture. Noticeably, the detector retains very good performance on\ncompressed videos, even when the compression level used during training does\nnot match that used for the test videos.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 10 Sep 2021 16:59:15 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 17 Sep 2021 10:13:49 GMT"
      }
    ],
    "update_date": "2021-09-20",
    "authors_parsed": [
      [
        "Alamayreh",
        "Omran",
        ""
      ],
      [
        "Barni",
        "Mauro",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.04991",
    "publish_date": "2021-09-17"
  },
  {
    "id": "2109.05820",
    "submitter": "Wenzhao Xiang",
    "authors": "Wenzhao Xiang, Hang Su, Chang Liu, Yandong Guo, Shibao Zheng",
    "title": "Improving the Robustness of Adversarial Attacks Using an\n  Affine-Invariant Gradient Estimator",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As designers of artificial intelligence try to outwit hackers, both sides\ncontinue to hone in on AI's inherent vulnerabilities. Designed and trained from\ncertain statistical distributions of data, AI's deep neural networks (DNNs)\nremain vulnerable to deceptive inputs that violate a DNN's statistical,\npredictive assumptions. Before being fed into a neural network, however, most\nexisting adversarial examples cannot maintain malicious functionality when\napplied to an affine transformation. For practical purposes, maintaining that\nmalicious functionality serves as an important measure of the robustness of\nadversarial attacks. To help DNNs learn to defend themselves more thoroughly\nagainst attacks, we propose an affine-invariant adversarial attack, which can\nconsistently produce more robust adversarial examples over affine\ntransformations. For efficiency, we propose to disentangle current\naffine-transformation strategies from the Euclidean geometry coordinate plane\nwith its geometric translations, rotations and dilations; we reformulate the\nlatter two in polar coordinates. Afterwards, we construct an affine-invariant\ngradient estimator by convolving the gradient at the original image with\nderived kernels, which can be integrated with any gradient-based attack\nmethods. Extensive experiments on ImageNet, including some experiments under\nphysical condition, demonstrate that our method can significantly improve the\naffine invariance of adversarial examples and, as a byproduct, improve the\ntransferability of adversarial examples, compared with alternative\nstate-of-the-art methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Sep 2021 09:43:17 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 22 Apr 2022 07:06:17 GMT"
      }
    ],
    "update_date": "2022-04-25",
    "authors_parsed": [
      [
        "Xiang",
        "Wenzhao",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ],
      [
        "Liu",
        "Chang",
        ""
      ],
      [
        "Guo",
        "Yandong",
        ""
      ],
      [
        "Zheng",
        "Shibao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.05820",
    "publish_date": "2021-09-13"
  },
  {
    "id": "2109.06098",
    "submitter": "Alexander Bastounis",
    "authors": "Alexander Bastounis, Anders C Hansen, Verner Vla\\v{c}i\\'c",
    "title": "The mathematics of adversarial attacks in AI -- Why deep learning is\n  unstable despite the existence of stable neural networks",
    "comments": "29 pages, 1 figure",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CV cs.NA math.NA stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The unprecedented success of deep learning (DL) makes it unchallenged when it\ncomes to classification problems. However, it is well established that the\ncurrent DL methodology produces universally unstable neural networks (NNs). The\ninstability problem has caused an enormous research effort -- with a vast\nliterature on so-called adversarial attacks -- yet there has been no solution\nto the problem. Our paper addresses why there has been no solution to the\nproblem, as we prove the following mathematical paradox: any training procedure\nbased on training neural networks for classification problems with a fixed\narchitecture will yield neural networks that are either inaccurate or unstable\n(if accurate) -- despite the provable existence of both accurate and stable\nneural networks for the same classification problems. The key is that the\nstable and accurate neural networks must have variable dimensions depending on\nthe input, in particular, variable dimensions is a necessary condition for\nstability.\n  Our result points towards the paradox that accurate and stable neural\nnetworks exist, however, modern algorithms do not compute them. This yields the\nquestion: if the existence of neural networks with desirable properties can be\nproven, can one also find algorithms that compute them? There are cases in\nmathematics where provable existence implies computability, but will this be\nthe case for neural networks? The contrary is true, as we demonstrate how\nneural networks can provably exist as approximate minimisers to standard\noptimisation problems with standard cost functions, however, no randomised\nalgorithm can compute them with probability better than 1/2.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Sep 2021 16:19:25 GMT"
      }
    ],
    "update_date": "2021-09-14",
    "authors_parsed": [
      [
        "Bastounis",
        "Alexander",
        ""
      ],
      [
        "Hansen",
        "Anders C",
        ""
      ],
      [
        "Vla\u010di\u0107",
        "Verner",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.06098",
    "publish_date": "2021-09-13"
  },
  {
    "id": "2109.06634",
    "submitter": "Praveen Fernando",
    "authors": "Praveen Fernando, Jin Wei-Kocsis",
    "title": "A Novel Data Encryption Method Inspired by Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Due to the advances of sensing and storage technologies, a tremendous amount\nof data becomes available and, it supports the phenomenal growth of artificial\nintelligence (AI) techniques especially, deep learning (DL), in various\napplication domains. While the data sources become valuable assets for enabling\nthe success of autonomous decision-making, they also lead to critical\nvulnerabilities in privacy and security. For example, data leakage can be\nexploited via querying and eavesdropping in the exploratory phase for black-box\nattacks against DL-based autonomous decision-making systems. To address this\nissue, in this work, we propose a novel data encryption method, called\nAdvEncryption, by exploiting the principle of adversarial attacks. Different\nfrom existing encryption technologies, the AdvEncryption method is not\ndeveloped to prevent attackers from exploiting the dataset. Instead, our\nproposed method aims to trap the attackers in a misleading feature distillation\nof the data. To achieve this goal, our AdvEncryption method consists of two\nessential components: 1) an adversarial attack-inspired encryption mechanism to\nencrypt the data with stealthy adversarial perturbation, and 2) a decryption\nmechanism that minimizes the impact of the perturbations on the effectiveness\nof autonomous decision making. In the performance evaluation section, we\nevaluate the performance of our proposed AdvEncryption method through case\nstudies considering different scenarios.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Sep 2021 12:36:16 GMT"
      }
    ],
    "update_date": "2021-09-15",
    "authors_parsed": [
      [
        "Fernando",
        "Praveen",
        ""
      ],
      [
        "Wei-Kocsis",
        "Jin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.06634",
    "publish_date": "2021-09-14"
  },
  {
    "id": "2109.11542",
    "submitter": "Mohit Sewak",
    "authors": "Mohit Sewak, Sanjay K. Sahay, Hemant Rathore",
    "title": "ADVERSARIALuscator: An Adversarial-DRL Based Obfuscator and Metamorphic\n  Malware SwarmGenerator",
    "comments": null,
    "journal-ref": "2021 International Joint Conference on Neural Networks (IJCNN),\n  2021, pp. 1-9",
    "doi": "10.1109/IJCNN52387.2021.9534016",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Advanced metamorphic malware and ransomware, by using obfuscation, could\nalter their internal structure with every attack. If such malware could intrude\neven into any of the IoT networks, then even if the original malware instance\ngets detected, by that time it can still infect the entire network. It is\nchallenging to obtain training data for such evasive malware. Therefore, in\nthis paper, we present ADVERSARIALuscator, a novel system that uses specialized\nAdversarial-DRL to obfuscate malware at the opcode level and create multiple\nmetamorphic instances of the same. To the best of our knowledge,\nADVERSARIALuscator is the first-ever system that adopts the Markov Decision\nProcess-based approach to convert and find a solution to the problem of\ncreating individual obfuscations at the opcode level. This is important as the\nmachine language level is the least at which functionality could be preserved\nso as to mimic an actual attack effectively. ADVERSARIALuscator is also the\nfirst-ever system to use efficient continuous action control capable of deep\nreinforcement learning agents like the Proximal Policy Optimization in the area\nof cyber security. Experimental results indicate that ADVERSARIALuscator could\nraise the metamorphic probability of a corpus of malware by >0.45.\nAdditionally, more than 33% of metamorphic instances generated by\nADVERSARIALuscator were able to evade the most potent IDS. If such malware\ncould intrude even into any of the IoT networks, then even if the original\nmalware instance gets detected, by that time it can still infect the entire\nnetwork. Hence ADVERSARIALuscator could be used to generate data representative\nof a swarm of very potent and coordinated AI-based metamorphic malware attacks.\nThe so generated data and simulations could be used to bolster the defenses of\nan IDS against an actual AI-based metamorphic attack from advanced malware and\nransomware.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 23 Sep 2021 10:50:41 GMT"
      }
    ],
    "update_date": "2021-09-27",
    "authors_parsed": [
      [
        "Sewak",
        "Mohit",
        ""
      ],
      [
        "Sahay",
        "Sanjay K.",
        ""
      ],
      [
        "Rathore",
        "Hemant",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2109.11542",
    "publish_date": "2021-09-23"
  },
  {
    "id": "2110.03309",
    "submitter": "Wanying Ge",
    "authors": "Wanying Ge, Jose Patino, Massimiliano Todisco and Nicholas Evans",
    "title": "Explaining deep learning models for spoofing and deepfake detection with\n  SHapley Additive exPlanations",
    "comments": "Submitted to ICASSP 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Substantial progress in spoofing and deepfake detection has been made in\nrecent years. Nonetheless, the community has yet to make notable inroads in\nproviding an explanation for how a classifier produces its output. The\ndominance of black box spoofing detection solutions is at further odds with the\ndrive toward trustworthy, explainable artificial intelligence. This paper\ndescribes our use of SHapley Additive exPlanations (SHAP) to gain new insights\nin spoofing detection. We demonstrate use of the tool in revealing unexpected\nclassifier behaviour, the artefacts that contribute most to classifier outputs\nand differences in the behaviour of competing spoofing detection models. The\ntool is both efficient and flexible, being readily applicable to a host of\ndifferent architecture models in addition to related, different applications.\nAll results reported in the paper are reproducible using open-source software.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Oct 2021 10:00:04 GMT"
      }
    ],
    "update_date": "2021-10-08",
    "authors_parsed": [
      [
        "Ge",
        "Wanying",
        ""
      ],
      [
        "Patino",
        "Jose",
        ""
      ],
      [
        "Todisco",
        "Massimiliano",
        ""
      ],
      [
        "Evans",
        "Nicholas",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.03309",
    "publish_date": "2021-10-07"
  },
  {
    "id": "2110.08042",
    "submitter": "Yinpeng Dong",
    "authors": "Yinpeng Dong, Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang\n  Su, Jun Zhu, Jiayu Tang, Yuefeng Chen, XiaoFeng Mao, Yuan He, Hui Xue, Chao\n  Li, Ye Liu, Qilong Zhang, Lianli Gao, Yunrui Yu, Xitong Gao, Zhe Zhao, Daquan\n  Lin, Jiadong Lin, Chuanbiao Song, Zihao Wang, Zhennan Wu, Yang Guo, Jiequan\n  Cui, Xiaogang Xu, Pengguang Chen",
    "title": "Adversarial Attacks on ML Defense Models Competition",
    "comments": "Competition Report",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Due to the vulnerability of deep neural networks (DNNs) to adversarial\nexamples, a large number of defense techniques have been proposed to alleviate\nthis problem in recent years. However, the progress of building more robust\nmodels is usually hampered by the incomplete or incorrect robustness\nevaluation. To accelerate the research on reliable evaluation of adversarial\nrobustness of the current defense models in image classification, the TSAIL\ngroup at Tsinghua University and the Alibaba Security group organized this\ncompetition along with a CVPR 2021 workshop on adversarial machine learning\n(https://aisecure-workshop.github.io/amlcvpr2021/). The purpose of this\ncompetition is to motivate novel attack algorithms to evaluate adversarial\nrobustness more effectively and reliably. The participants were encouraged to\ndevelop stronger white-box attack algorithms to find the worst-case robustness\nof different defenses. This competition was conducted on an adversarial\nrobustness evaluation platform -- ARES (https://github.com/thu-ml/ares), and is\nheld on the TianChi platform\n(https://tianchi.aliyun.com/competition/entrance/531847/introduction) as one of\nthe series of AI Security Challengers Program. After the competition, we\nsummarized the results and established a new adversarial robustness benchmark\nat https://ml.cs.tsinghua.edu.cn/ares-bench/, which allows users to upload\nadversarial attack algorithms and defense models for evaluation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 15 Oct 2021 12:12:41 GMT"
      }
    ],
    "update_date": "2021-10-18",
    "authors_parsed": [
      [
        "Dong",
        "Yinpeng",
        ""
      ],
      [
        "Fu",
        "Qi-An",
        ""
      ],
      [
        "Yang",
        "Xiao",
        ""
      ],
      [
        "Xiang",
        "Wenzhao",
        ""
      ],
      [
        "Pang",
        "Tianyu",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ],
      [
        "Zhu",
        "Jun",
        ""
      ],
      [
        "Tang",
        "Jiayu",
        ""
      ],
      [
        "Chen",
        "Yuefeng",
        ""
      ],
      [
        "Mao",
        "XiaoFeng",
        ""
      ],
      [
        "He",
        "Yuan",
        ""
      ],
      [
        "Xue",
        "Hui",
        ""
      ],
      [
        "Li",
        "Chao",
        ""
      ],
      [
        "Liu",
        "Ye",
        ""
      ],
      [
        "Zhang",
        "Qilong",
        ""
      ],
      [
        "Gao",
        "Lianli",
        ""
      ],
      [
        "Yu",
        "Yunrui",
        ""
      ],
      [
        "Gao",
        "Xitong",
        ""
      ],
      [
        "Zhao",
        "Zhe",
        ""
      ],
      [
        "Lin",
        "Daquan",
        ""
      ],
      [
        "Lin",
        "Jiadong",
        ""
      ],
      [
        "Song",
        "Chuanbiao",
        ""
      ],
      [
        "Wang",
        "Zihao",
        ""
      ],
      [
        "Wu",
        "Zhennan",
        ""
      ],
      [
        "Guo",
        "Yang",
        ""
      ],
      [
        "Cui",
        "Jiequan",
        ""
      ],
      [
        "Xu",
        "Xiaogang",
        ""
      ],
      [
        "Chen",
        "Pengguang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.08042",
    "publish_date": "2021-10-15"
  },
  {
    "id": "2110.08322",
    "submitter": "Vishal Rajput",
    "authors": "Vishal Rajput",
    "title": "Robustness of different loss functions and their impact on networks\n  learning capability",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Recent developments in AI have made it ubiquitous, every industry is trying\nto adopt some form of intelligent processing of their data. Despite so many\nadvances in the field, AIs full capability is yet to be exploited by the\nindustry. Industries that involve some risk factors still remain cautious about\nthe usage of AI due to the lack of trust in such autonomous systems.\nPresent-day AI might be very good in a lot of things but it is very bad in\nreasoning and this behavior of AI can lead to catastrophic results. Autonomous\ncars crashing into a person or a drone getting stuck in a tree are a few\nexamples where AI decisions lead to catastrophic results. To develop insight\nand generate an explanation about the learning capability of AI, we will try to\nanalyze the working of loss functions. For our case, we will use two sets of\nloss functions, generalized loss functions like Binary cross-entropy or BCE and\nspecialized loss functions like Dice loss or focal loss. Through a series of\nexperiments, we will establish whether combining different loss functions is\nbetter than using a single loss function and if yes, then what is the reason\nbehind it. In order to establish the difference between generalized loss and\nspecialized losses, we will train several models using the above-mentioned\nlosses and then compare their robustness on adversarial examples. In\nparticular, we will look at how fast the accuracy of different models decreases\nwhen we change the pixels corresponding to the most salient gradients.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 15 Oct 2021 19:12:42 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 9 Nov 2021 15:00:23 GMT"
      }
    ],
    "update_date": "2021-11-10",
    "authors_parsed": [
      [
        "Rajput",
        "Vishal",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.08322",
    "publish_date": "2021-11-09"
  },
  {
    "id": "2110.09903",
    "submitter": "YueFeng Chen",
    "authors": "Yuefeng Chen, Xiaofeng Mao, Yuan He, Hui Xue, Chao Li, Yinpeng Dong,\n  Qi-An Fu, Xiao Yang, Wenzhao Xiang, Tianyu Pang, Hang Su, Jun Zhu, Fangcheng\n  Liu, Chao Zhang, Hongyang Zhang, Yichi Zhang, Shilong Liu, Chang Liu, Wenzhao\n  Xiang, Yajie Wang, Huipeng Zhou, Haoran Lyu, Yidan Xu, Zixuan Xu, Taoyu Zhu,\n  Wenjun Li, Xianfeng Gao, Guoqiu Wang, Huanqian Yan, Ying Guo, Chaoning Zhang,\n  Zheng Fang, Yang Wang, Bingyang Fu, Yunfei Zheng, Yekui Wang, Haorong Luo and\n  Zhen Yang",
    "title": "Unrestricted Adversarial Attacks on ImageNet Competition",
    "comments": "CVPR-2021 AIC Phase VI Track2: Unrestricted Adversarial Attacks on\n  ImageNet",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Many works have investigated the adversarial attacks or defenses under the\nsettings where a bounded and imperceptible perturbation can be added to the\ninput. However in the real-world, the attacker does not need to comply with\nthis restriction. In fact, more threats to the deep model come from\nunrestricted adversarial examples, that is, the attacker makes large and\nvisible modifications on the image, which causes the model classifying\nmistakenly, but does not affect the normal observation in human perspective.\nUnrestricted adversarial attack is a popular and practical direction but has\nnot been studied thoroughly. We organize this competition with the purpose of\nexploring more effective unrestricted adversarial attack algorithm, so as to\naccelerate the academical research on the model robustness under stronger\nunbounded attacks. The competition is held on the TianChi platform\n(\\url{https://tianchi.aliyun.com/competition/entrance/531853/introduction}) as\none of the series of AI Security Challengers Program.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Oct 2021 04:27:15 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 25 Oct 2021 11:35:09 GMT"
      }
    ],
    "update_date": "2021-10-27",
    "authors_parsed": [
      [
        "Chen",
        "Yuefeng",
        ""
      ],
      [
        "Mao",
        "Xiaofeng",
        ""
      ],
      [
        "He",
        "Yuan",
        ""
      ],
      [
        "Xue",
        "Hui",
        ""
      ],
      [
        "Li",
        "Chao",
        ""
      ],
      [
        "Dong",
        "Yinpeng",
        ""
      ],
      [
        "Fu",
        "Qi-An",
        ""
      ],
      [
        "Yang",
        "Xiao",
        ""
      ],
      [
        "Xiang",
        "Wenzhao",
        ""
      ],
      [
        "Pang",
        "Tianyu",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ],
      [
        "Zhu",
        "Jun",
        ""
      ],
      [
        "Liu",
        "Fangcheng",
        ""
      ],
      [
        "Zhang",
        "Chao",
        ""
      ],
      [
        "Zhang",
        "Hongyang",
        ""
      ],
      [
        "Zhang",
        "Yichi",
        ""
      ],
      [
        "Liu",
        "Shilong",
        ""
      ],
      [
        "Liu",
        "Chang",
        ""
      ],
      [
        "Xiang",
        "Wenzhao",
        ""
      ],
      [
        "Wang",
        "Yajie",
        ""
      ],
      [
        "Zhou",
        "Huipeng",
        ""
      ],
      [
        "Lyu",
        "Haoran",
        ""
      ],
      [
        "Xu",
        "Yidan",
        ""
      ],
      [
        "Xu",
        "Zixuan",
        ""
      ],
      [
        "Zhu",
        "Taoyu",
        ""
      ],
      [
        "Li",
        "Wenjun",
        ""
      ],
      [
        "Gao",
        "Xianfeng",
        ""
      ],
      [
        "Wang",
        "Guoqiu",
        ""
      ],
      [
        "Yan",
        "Huanqian",
        ""
      ],
      [
        "Guo",
        "Ying",
        ""
      ],
      [
        "Zhang",
        "Chaoning",
        ""
      ],
      [
        "Fang",
        "Zheng",
        ""
      ],
      [
        "Wang",
        "Yang",
        ""
      ],
      [
        "Fu",
        "Bingyang",
        ""
      ],
      [
        "Zheng",
        "Yunfei",
        ""
      ],
      [
        "Wang",
        "Yekui",
        ""
      ],
      [
        "Luo",
        "Haorong",
        ""
      ],
      [
        "Yang",
        "Zhen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.09903",
    "publish_date": "2021-10-17"
  },
  {
    "id": "2110.10655",
    "submitter": "Thai Le",
    "authors": "Thai Le, Long Tran-Thanh, Dongwon Lee",
    "title": "Socialbots on Fire: Modeling Adversarial Behaviors of Socialbots via\n  Multi-Agent Hierarchical Reinforcement Learning",
    "comments": "Accepted to The ACM Web Conference 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI cs.AI cs.CR cs.LG cs.MA",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Socialbots are software-driven user accounts on social platforms, acting\nautonomously (mimicking human behavior), with the aims to influence the\nopinions of other users or spread targeted misinformation for particular goals.\nAs socialbots undermine the ecosystem of social platforms, they are often\nconsidered harmful. As such, there have been several computational efforts to\nauto-detect the socialbots. However, to our best knowledge, the adversarial\nnature of these socialbots has not yet been studied. This begs a question \"can\nadversaries, controlling socialbots, exploit AI techniques to their advantage?\"\nTo this question, we successfully demonstrate that indeed it is possible for\nadversaries to exploit computational learning mechanism such as reinforcement\nlearning (RL) to maximize the influence of socialbots while avoiding being\ndetected. We first formulate the adversarial socialbot learning as a\ncooperative game between two functional hierarchical RL agents. While one agent\ncurates a sequence of activities that can avoid the detection, the other agent\naims to maximize network influence by selectively connecting with right users.\nOur proposed policy networks train with a vast amount of synthetic graphs and\ngeneralize better than baselines on unseen real-life graphs both in terms of\nmaximizing network influence (up to +18%) and sustainable stealthiness (up to\n+40% undetectability) under a strong bot detector (with 90% detection\naccuracy). During inference, the complexity of our approach scales linearly,\nindependent of a network's structure and the virality of news. This makes our\napproach a practical adversarial attack when deployed in a real-life setting.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 20 Oct 2021 16:49:26 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 26 Feb 2022 16:55:44 GMT"
      }
    ],
    "update_date": "2022-03-01",
    "authors_parsed": [
      [
        "Le",
        "Thai",
        ""
      ],
      [
        "Tran-Thanh",
        "Long",
        ""
      ],
      [
        "Lee",
        "Dongwon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.10655",
    "publish_date": "2021-10-20"
  },
  {
    "id": "2110.11411",
    "submitter": "Yevgeniy Vorobeychik",
    "authors": "Mingyang Xie, Manav Kulshrestha, Shaojie Wang, Jinghan Yang, Ayan\n  Chakrabarti, Ning Zhang, and Yevgeniy Vorobeychik",
    "title": "PROVES: Establishing Image Provenance using Semantic Signatures",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Modern AI tools, such as generative adversarial networks, have transformed\nour ability to create and modify visual data with photorealistic results.\nHowever, one of the deleterious side-effects of these advances is the emergence\nof nefarious uses in manipulating information in visual data, such as through\nthe use of deep fakes. We propose a novel architecture for preserving the\nprovenance of semantic information in images to make them less susceptible to\ndeep fake attacks. Our architecture includes semantic signing and verification\nsteps. We apply this architecture to verifying two types of semantic\ninformation: individual identities (faces) and whether the photo was taken\nindoors or outdoors. Verification accounts for a collection of common image\ntransformation, such as translation, scaling, cropping, and small rotations,\nand rejects adversarial transformations, such as adversarially perturbed or, in\nthe case of face verification, swapped faces. Experiments demonstrate that in\nthe case of provenance of faces in an image, our approach is robust to\nblack-box adversarial transformations (which are rejected) as well as benign\ntransformations (which are accepted), with few false negatives and false\npositives. Background verification, on the other hand, is susceptible to\nblack-box adversarial examples, but becomes significantly more robust after\nadversarial training.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Oct 2021 18:30:09 GMT"
      }
    ],
    "update_date": "2021-10-25",
    "authors_parsed": [
      [
        "Xie",
        "Mingyang",
        ""
      ],
      [
        "Kulshrestha",
        "Manav",
        ""
      ],
      [
        "Wang",
        "Shaojie",
        ""
      ],
      [
        "Yang",
        "Jinghan",
        ""
      ],
      [
        "Chakrabarti",
        "Ayan",
        ""
      ],
      [
        "Zhang",
        "Ning",
        ""
      ],
      [
        "Vorobeychik",
        "Yevgeniy",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.11411",
    "publish_date": "2021-10-21"
  },
  {
    "id": "2110.12925",
    "submitter": "Zhensu Sun",
    "authors": "Zhensu Sun, Xiaoning Du, Fu Song, Mingze Ni, Li Li",
    "title": "CoProtector: Protect Open-Source Code against Unauthorized Training\n  Usage with Data Poisoning",
    "comments": "8 pages, accepted to WWW 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Github Copilot, trained on billions of lines of public code, has recently\nbecome the buzzword in the computer science research and practice community.\nAlthough it is designed to help developers implement safe and effective code\nwith powerful intelligence, practitioners and researchers raise concerns about\nits ethical and security problems, e.g., should the copyleft licensed code be\nfreely leveraged or insecure code be considered for training in the first\nplace? These problems pose a significant impact on Copilot and other similar\nproducts that aim to learn knowledge from large-scale open-source code through\ndeep learning models, which are inevitably on the rise with the fast\ndevelopment of artificial intelligence. To mitigate such impacts, we argue that\nthere is a need to invent effective mechanisms for protecting open-source code\nfrom being exploited by deep learning models. Here, we design and implement a\nprototype, CoProtector, which utilizes data poisoning techniques to arm source\ncode repositories for defending against such exploits. Our large-scale\nexperiments empirically show that CoProtector is effective in achieving its\npurpose, significantly reducing the performance of Copilot-like deep learning\nmodels while being able to stably reveal the secretly embedded watermark\nbackdoors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 25 Oct 2021 13:01:41 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 14 Feb 2022 07:49:14 GMT"
      }
    ],
    "update_date": "2022-02-15",
    "authors_parsed": [
      [
        "Sun",
        "Zhensu",
        ""
      ],
      [
        "Du",
        "Xiaoning",
        ""
      ],
      [
        "Song",
        "Fu",
        ""
      ],
      [
        "Ni",
        "Mingze",
        ""
      ],
      [
        "Li",
        "Li",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.12925",
    "publish_date": "2021-10-25"
  },
  {
    "id": "2110.13541",
    "submitter": "Sanghyun Hong",
    "authors": "Sanghyun Hong, Michael-Andrei Panaitescu-Liess, Yi\\u{g}itcan Kaya,\n  Tudor Dumitra\\c{s}",
    "title": "Qu-ANTI-zation: Exploiting Quantization Artifacts for Achieving\n  Adversarial Outcomes",
    "comments": "Accepted to NeurIPS 2021 [Poster]",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Quantization is a popular technique that $transforms$ the parameter\nrepresentation of a neural network from floating-point numbers into\nlower-precision ones ($e.g.$, 8-bit integers). It reduces the memory footprint\nand the computational cost at inference, facilitating the deployment of\nresource-hungry models. However, the parameter perturbations caused by this\ntransformation result in $behavioral$ $disparities$ between the model before\nand after quantization. For example, a quantized model can misclassify some\ntest-time samples that are otherwise classified correctly. It is not known\nwhether such differences lead to a new security vulnerability. We hypothesize\nthat an adversary may control this disparity to introduce specific behaviors\nthat activate upon quantization. To study this hypothesis, we weaponize\nquantization-aware training and propose a new training framework to implement\nadversarial quantization outcomes. Following this framework, we present three\nattacks we carry out with quantization: (i) an indiscriminate attack for\nsignificant accuracy loss; (ii) a targeted attack against specific samples; and\n(iii) a backdoor attack for controlling the model with an input trigger. We\nfurther show that a single compromised model defeats multiple quantization\nschemes, including robust quantization techniques. Moreover, in a federated\nlearning scenario, we demonstrate that a set of malicious participants who\nconspire can inject our quantization-activated backdoor. Lastly, we discuss\npotential counter-measures and show that only re-training consistently removes\nthe attack artifacts. Our code is available at\nhttps://github.com/Secure-AI-Systems-Group/Qu-ANTI-zation\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 26 Oct 2021 10:09:49 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Nov 2021 08:58:23 GMT"
      }
    ],
    "update_date": "2021-11-12",
    "authors_parsed": [
      [
        "Hong",
        "Sanghyun",
        ""
      ],
      [
        "Panaitescu-Liess",
        "Michael-Andrei",
        ""
      ],
      [
        "Kaya",
        "Yi\u011fitcan",
        ""
      ],
      [
        "Dumitra\u015f",
        "Tudor",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.13541",
    "publish_date": "2021-10-26"
  },
  {
    "id": "2110.13650",
    "submitter": "Venkatesh Subramaniyan",
    "authors": "Venkatesh Subramaniyan, Vignesh Sivakumar, A. K. Vagheesan, S.\n  Sakthivelan, K. J. Jegadish Kumar, K. K. Nagarajan",
    "title": "GANash -- A GAN approach to steganography",
    "comments": "Presented at the 6 th National Conference on Information and\n  Communication Technologies (NCICT 2020), June 12, 2020",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Data security is of the utmost concern of a communication system. Since the\nearly days, many developments have been made to improve the performance of the\nsystem. PSNR of the received signal, secure transmission channel, quality of\nencoding used, etc. are some of the key attributes of a good system. To ensure\nsecurity, the most commonly used technique is cryptography in which the message\nis altered with respect to a key and using the same, the encoded message is\ndecoded at the receiver side. A complementary technique that is popularly used\nto insure security is steganography. The advancements in Artificial\nIntelligence(AI) have paved way for performing steganography in an intelligent,\ntamper-proof manner. The recent discovery by researchers in the field of Deep\nLearning(DL), an unsupervised learning network known as the Generative\nAdversarial Networks(GAN) has improved the performance of this technique\nexponentially. It has been demonstrated that deep neural networks are highly\nsensitive to tiny perturbations of input data, giving rise to adversarial\nexamples. Though this property is usually considered a weakness of learned\nmodels, it could be beneficial if used appropriately. The work that has been\naccomplished by MIT for this purpose, a deep-neural model by the name of\nSteganoGAN, has shown obligation for using this technique for steganography. In\nthis work, we have proposed a novel approach to improve the performance of the\nexisting system using latent space compression on the encoded data. This\ntheoretically would improve the performance exponentially. Thus, the algorithms\nused to improve the system's performance and the results obtained have been\nenunciated in this work. The results indicate the level of dominance this\nsystem could achieve to be able to diminish the difficulties in solving\nreal-time problems in terms of security, deployment and database management.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 25 Oct 2021 15:09:10 GMT"
      }
    ],
    "update_date": "2021-10-27",
    "authors_parsed": [
      [
        "Subramaniyan",
        "Venkatesh",
        ""
      ],
      [
        "Sivakumar",
        "Vignesh",
        ""
      ],
      [
        "Vagheesan",
        "A. K.",
        ""
      ],
      [
        "Sakthivelan",
        "S.",
        ""
      ],
      [
        "Kumar",
        "K. J. Jegadish",
        ""
      ],
      [
        "Nagarajan",
        "K. K.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.13650",
    "publish_date": "2021-10-25"
  },
  {
    "id": "2110.14693",
    "submitter": "Zhaohan Xi",
    "authors": "Zhaohan Xi, Ren Pang, Changjiang Li, Shouling Ji, Xiapu Luo, Xusheng\n  Xiao, Ting Wang",
    "title": "Towards Robust Reasoning over Knowledge Graphs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Answering complex logical queries over large-scale knowledge graphs (KGs)\nrepresents an important artificial intelligence task, entailing a range of\napplications. Recently, knowledge representation learning (KRL) has emerged as\nthe state-of-the-art approach, wherein KG entities and the query are embedded\ninto a latent space such that entities that answer the query are embedded close\nto the query. Yet, despite its surging popularity, the potential security risks\nof KRL are largely unexplored, which is concerning, given the increasing use of\nsuch capabilities in security-critical domains (e.g., cyber-security and\nhealthcare).\n  This work represents a solid initial step towards bridging this gap. We\nsystematize the potential security threats to KRL according to the underlying\nattack vectors (e.g., knowledge poisoning and query perturbation) and the\nadversary's background knowledge. More importantly, we present ROAR(Reasoning\nOver Adversarial Representations), a new class of attacks that instantiate a\nvariety of such threats. We demonstrate the practicality of ROAR in two\nrepresentative use cases (i.e., cyber-threat hunting and drug repurposing). For\ninstance, ROAR attains over 99% attack success rate in misleading the threat\nintelligence engine to give pre-defined answers for target queries, yet without\nany impact on non-target ones. Further, we discuss potential countermeasures\nagainst ROAR, including filtering of poisoning facts and robust training with\nadversarial queries, which leads to several promising research directions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 27 Oct 2021 18:19:24 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 1 Nov 2021 03:28:54 GMT"
      }
    ],
    "update_date": "2021-11-02",
    "authors_parsed": [
      [
        "Xi",
        "Zhaohan",
        ""
      ],
      [
        "Pang",
        "Ren",
        ""
      ],
      [
        "Li",
        "Changjiang",
        ""
      ],
      [
        "Ji",
        "Shouling",
        ""
      ],
      [
        "Luo",
        "Xiapu",
        ""
      ],
      [
        "Xiao",
        "Xusheng",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2110.14693",
    "publish_date": "2021-10-27"
  },
  {
    "id": "2111.02845",
    "submitter": "Ao Qu",
    "authors": "Ao Qu, Yihong Tang, Wei Ma",
    "title": "Attacking Deep Reinforcement Learning-Based Traffic Signal Control\n  Systems with Colluding Vehicles",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The rapid advancements of Internet of Things (IoT) and artificial\nintelligence (AI) have catalyzed the development of adaptive traffic signal\ncontrol systems (ATCS) for smart cities. In particular, deep reinforcement\nlearning (DRL) methods produce the state-of-the-art performance and have great\npotentials for practical applications. In the existing DRL-based ATCS, the\ncontrolled signals collect traffic state information from nearby vehicles, and\nthen optimal actions (e.g., switching phases) can be determined based on the\ncollected information. The DRL models fully \"trust\" that vehicles are sending\nthe true information to the signals, making the ATCS vulnerable to adversarial\nattacks with falsified information. In view of this, this paper first time\nformulates a novel task in which a group of vehicles can cooperatively send\nfalsified information to \"cheat\" DRL-based ATCS in order to save their total\ntravel time. To solve the proposed task, we develop CollusionVeh, a generic and\neffective vehicle-colluding framework composed of a road situation encoder, a\nvehicle interpreter, and a communication mechanism. We employ our method to\nattack established DRL-based ATCS and demonstrate that the total travel time\nfor the colluding vehicles can be significantly reduced with a reasonable\nnumber of learning episodes, and the colluding effect will decrease if the\nnumber of colluding vehicles increases. Additionally, insights and suggestions\nfor the real-world deployment of DRL-based ATCS are provided. The research\noutcomes could help improve the reliability and robustness of the ATCS and\nbetter protect the smart mobility systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 4 Nov 2021 13:10:33 GMT"
      }
    ],
    "update_date": "2021-11-05",
    "authors_parsed": [
      [
        "Qu",
        "Ao",
        ""
      ],
      [
        "Tang",
        "Yihong",
        ""
      ],
      [
        "Ma",
        "Wei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.02845",
    "publish_date": "2021-11-04"
  },
  {
    "id": "2111.04801",
    "submitter": "Marian Gusatu",
    "authors": "Marian Gusatu and Ruxandra F. Olimid",
    "title": "Improved security solutions for DDoS mitigation in 5G Multi-access Edge\n  Computing",
    "comments": "10 pages, 5 figures, in review at SECITC: http://secitc.eu/",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.NI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Multi-access Edge Computing (MEC) is a 5G-enabling solution that aims to\nbring cloud-computing capabilities closer to the end-users. This paper focuses\non mitigation techniques against Distributed Denial-of-Service (DDoS) attacks\nin the context of 5G MEC, providing solutions that involve the virtualized\nenvironment and the management entities from the MEC architecture. The proposed\nsolutions aim to reduce the risk of affecting legitimate traffic in the context\nof DDoS attacks. Our work supports the idea of using a network flow collector\nthat sends the data to an anomaly detection system based on artificial\nintelligence techniques and, as an improvement over the previous work, it\ncontributes to redirecting detected anomalies for isolation to a separate\nvirtual machine. This virtual machine uses deep packet inspection tools to\nanalyze the traffic and provides services until the final verdict. We decrease\nthe risk of compromising the virtual machine that provides services to\nlegitimate users by isolating the suspicious traffic. The management entities\nof the MEC architecture allow to re-instantiate or reconfigure the virtual\nmachines. Hence, if the machine inspecting the isolated traffic crashes because\nof an attack, the damaged machine can be restored while the services provided\nto legitimate users are not affected.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 8 Nov 2021 20:14:55 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 10 Nov 2021 09:32:29 GMT"
      }
    ],
    "update_date": "2021-11-11",
    "authors_parsed": [
      [
        "Gusatu",
        "Marian",
        ""
      ],
      [
        "Olimid",
        "Ruxandra F.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.04801",
    "publish_date": "2021-11-10"
  },
  {
    "id": "2111.05391",
    "submitter": "Yili Hong",
    "authors": "Yili Hong and Jiayi Lian and Li Xu and Jie Min and Yueyao Wang and\n  Laura J. Freeman and Xinwei Deng",
    "title": "Statistical Perspectives on Reliability of Artificial Intelligence\n  Systems",
    "comments": "40 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI stat.AP",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial intelligence (AI) systems have become increasingly popular in many\nareas. Nevertheless, AI technologies are still in their developing stages, and\nmany issues need to be addressed. Among those, the reliability of AI systems\nneeds to be demonstrated so that the AI systems can be used with confidence by\nthe general public. In this paper, we provide statistical perspectives on the\nreliability of AI systems. Different from other considerations, the reliability\nof AI systems focuses on the time dimension. That is, the system can perform\nits designed functionality for the intended period. We introduce a so-called\nSMART statistical framework for AI reliability research, which includes five\ncomponents: Structure of the system, Metrics of reliability, Analysis of\nfailure causes, Reliability assessment, and Test planning. We review\ntraditional methods in reliability data analysis and software reliability, and\ndiscuss how those existing methods can be transformed for reliability modeling\nand assessment of AI systems. We also describe recent developments in modeling\nand analysis of AI reliability and outline statistical research challenges in\nthis area, including out-of-distribution detection, the effect of the training\nset, adversarial attacks, model accuracy, and uncertainty quantification, and\ndiscuss how those topics can be related to AI reliability, with illustrative\nexamples. Finally, we discuss data collection and test planning for AI\nreliability assessment and how to improve system designs for higher AI\nreliability. The paper closes with some concluding remarks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 9 Nov 2021 20:00:14 GMT"
      }
    ],
    "update_date": "2021-11-11",
    "authors_parsed": [
      [
        "Hong",
        "Yili",
        ""
      ],
      [
        "Lian",
        "Jiayi",
        ""
      ],
      [
        "Xu",
        "Li",
        ""
      ],
      [
        "Min",
        "Jie",
        ""
      ],
      [
        "Wang",
        "Yueyao",
        ""
      ],
      [
        "Freeman",
        "Laura J.",
        ""
      ],
      [
        "Deng",
        "Xinwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.05391",
    "publish_date": "2021-11-09"
  },
  {
    "id": "2111.07601",
    "submitter": "YuYang Sun",
    "authors": "Yuyang Sun, Zhiyong Zhang, Changzhen Qiu, Liang Wang and Zekai Wang",
    "title": "FakeTransformer: Exposing Face Forgery From Spatial-Temporal\n  Representation Modeled By Facial Pixel Variations",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the rapid development of generation model, AI-based face manipulation\ntechnology, which called DeepFakes, has become more and more realistic. This\nmeans of face forgery can attack any target, which poses a new threat to\npersonal privacy and property security. Moreover, the misuse of synthetic video\nshows potential dangers in many areas, such as identity harassment, pornography\nand news rumors. Inspired by the fact that the spatial coherence and temporal\nconsistency of physiological signal are destroyed in the generated content, we\nattempt to find inconsistent patterns that can distinguish between real videos\nand synthetic videos from the variations of facial pixels, which are highly\nrelated to physiological information. Our approach first applies Eulerian Video\nMagnification (EVM) at multiple Gaussian scales to the original video to\nenlarge the physiological variations caused by the change of facial blood\nvolume, and then transform the original video and magnified videos into a\nMulti-Scale Eulerian Magnified Spatial-Temporal map (MEMSTmap), which can\nrepresent time-varying physiological enhancement sequences on different\noctaves. Then, these maps are reshaped into frame patches in column units and\nsent to the vision Transformer to learn the spatio-time descriptors of frame\nlevels. Finally, we sort out the feature embedding and output the probability\nof judging whether the video is real or fake. We validate our method on the\nFaceForensics++ and DeepFake Detection datasets. The results show that our\nmodel achieves excellent performance in forgery detection, and also show\noutstanding generalization capability in cross-data domain.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 15 Nov 2021 08:44:52 GMT"
      }
    ],
    "update_date": "2021-11-16",
    "authors_parsed": [
      [
        "Sun",
        "Yuyang",
        ""
      ],
      [
        "Zhang",
        "Zhiyong",
        ""
      ],
      [
        "Qiu",
        "Changzhen",
        ""
      ],
      [
        "Wang",
        "Liang",
        ""
      ],
      [
        "Wang",
        "Zekai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.07601",
    "publish_date": "2021-11-15"
  },
  {
    "id": "2111.08429",
    "submitter": "Wei Guo",
    "authors": "Wei Guo, Benedetta Tondi, Mauro Barni",
    "title": "An Overview of Backdoor Attacks Against Deep Neural Networks and\n  Possible Defences",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Together with impressive advances touching every aspect of our society, AI\ntechnology based on Deep Neural Networks (DNN) is bringing increasing security\nconcerns. While attacks operating at test time have monopolised the initial\nattention of researchers, backdoor attacks, exploiting the possibility of\ncorrupting DNN models by interfering with the training process, represents a\nfurther serious threat undermining the dependability of AI techniques. In a\nbackdoor attack, the attacker corrupts the training data so to induce an\nerroneous behaviour at test time. Test time errors, however, are activated only\nin the presence of a triggering event corresponding to a properly crafted input\nsample. In this way, the corrupted network continues to work as expected for\nregular inputs, and the malicious behaviour occurs only when the attacker\ndecides to activate the backdoor hidden within the network. In the last few\nyears, backdoor attacks have been the subject of an intense research activity\nfocusing on both the development of new classes of attacks, and the proposal of\npossible countermeasures. The goal of this overview paper is to review the\nworks published until now, classifying the different types of attacks and\ndefences proposed so far. The classification guiding the analysis is based on\nthe amount of control that the attacker has on the training process, and the\ncapability of the defender to verify the integrity of the data used for\ntraining, and to monitor the operations of the DNN at training and test time.\nAs such, the proposed analysis is particularly suited to highlight the\nstrengths and weaknesses of both attacks and defences with reference to the\napplication scenarios they are operating in.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 16 Nov 2021 13:06:31 GMT"
      }
    ],
    "update_date": "2021-11-17",
    "authors_parsed": [
      [
        "Guo",
        "Wei",
        ""
      ],
      [
        "Tondi",
        "Benedetta",
        ""
      ],
      [
        "Barni",
        "Mauro",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.08429",
    "publish_date": "2021-11-16"
  },
  {
    "id": "2111.08973",
    "submitter": "Xuelong Dai",
    "authors": "Xuelong Dai, Yanjie Li, Hua Dai, Bin Xiao",
    "title": "Generating Unrestricted 3D Adversarial Point Clouds",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Utilizing 3D point cloud data has become an urgent need for the deployment of\nartificial intelligence in many areas like facial recognition and self-driving.\nHowever, deep learning for 3D point clouds is still vulnerable to adversarial\nattacks, e.g., iterative attacks, point transformation attacks, and generative\nattacks. These attacks need to restrict perturbations of adversarial examples\nwithin a strict bound, leading to the unrealistic adversarial 3D point clouds.\nIn this paper, we propose an Adversarial Graph-Convolutional Generative\nAdversarial Network (AdvGCGAN) to generate visually realistic adversarial 3D\npoint clouds from scratch. Specifically, we use a graph convolutional generator\nand a discriminator with an auxiliary classifier to generate realistic point\nclouds, which learn the latent distribution from the real 3D data. The\nunrestricted adversarial attack loss is incorporated in the special adversarial\ntraining of GAN, which enables the generator to generate the adversarial\nexamples to spoof the target network. Compared with the existing state-of-art\nattack methods, the experiment results demonstrate the effectiveness of our\nunrestricted adversarial attack methods with a higher attack success rate and\nvisual quality. Additionally, the proposed AdvGCGAN can achieve better\nperformance against defense models and better transferability than existing\nattack methods with strong camouflage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Nov 2021 08:30:18 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 19 Nov 2021 04:24:42 GMT"
      }
    ],
    "update_date": "2021-11-22",
    "authors_parsed": [
      [
        "Dai",
        "Xuelong",
        ""
      ],
      [
        "Li",
        "Yanjie",
        ""
      ],
      [
        "Dai",
        "Hua",
        ""
      ],
      [
        "Xiao",
        "Bin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.08973",
    "publish_date": "2021-11-17"
  },
  {
    "id": "2111.10055",
    "submitter": "Tao Bai",
    "authors": "Tao Bai, Jun Zhao, Jinlin Zhu, Shoudong Han, Jiefeng Chen, Bo Li, Alex\n  Kot",
    "title": "Towards Efficiently Evaluating the Robustness of Deep Neural Networks in\n  IoT Systems: A GAN-based Method",
    "comments": "arXiv admin note: text overlap with arXiv:2002.02196",
    "journal-ref": null,
    "doi": "10.1109/JIOT.2021.3091683",
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Intelligent Internet of Things (IoT) systems based on deep neural networks\n(DNNs) have been widely deployed in the real world. However, DNNs are found to\nbe vulnerable to adversarial examples, which raises people's concerns about\nintelligent IoT systems' reliability and security. Testing and evaluating the\nrobustness of IoT systems becomes necessary and essential. Recently various\nattacks and strategies have been proposed, but the efficiency problem remains\nunsolved properly. Existing methods are either computationally extensive or\ntime-consuming, which is not applicable in practice. In this paper, we propose\na novel framework called Attack-Inspired GAN (AI-GAN) to generate adversarial\nexamples conditionally. Once trained, it can generate adversarial perturbations\nefficiently given input images and target classes. We apply AI-GAN on different\ndatasets in white-box settings, black-box settings and targeted models\nprotected by state-of-the-art defenses. Through extensive experiments, AI-GAN\nachieves high attack success rates, outperforming existing methods, and reduces\ngeneration time significantly. Moreover, for the first time, AI-GAN\nsuccessfully scales to complex datasets e.g. CIFAR-100 and ImageNet, with about\n$90\\%$ success rates among all classes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 19 Nov 2021 05:54:14 GMT"
      }
    ],
    "update_date": "2021-11-22",
    "authors_parsed": [
      [
        "Bai",
        "Tao",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ],
      [
        "Zhu",
        "Jinlin",
        ""
      ],
      [
        "Han",
        "Shoudong",
        ""
      ],
      [
        "Chen",
        "Jiefeng",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ],
      [
        "Kot",
        "Alex",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.10055",
    "publish_date": "2021-11-19"
  },
  {
    "id": "2111.12197",
    "submitter": "Christian Schroeder de Witt",
    "authors": "Christian Schroeder de Witt, Yongchao Huang, Philip H.S. Torr, Martin\n  Strohmeier",
    "title": "Fixed Points in Cyber Space: Rethinking Optimal Evasion Attacks in the\n  Age of AI-NIDS",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Cyber attacks are increasing in volume, frequency, and complexity. In\nresponse, the security community is looking toward fully automating cyber\ndefense systems using machine learning. However, so far the resultant effects\non the coevolutionary dynamics of attackers and defenders have not been\nexamined. In this whitepaper, we hypothesise that increased automation on both\nsides will accelerate the coevolutionary cycle, thus begging the question of\nwhether there are any resultant fixed points, and how they are characterised.\nWorking within the threat model of Locked Shields, Europe's largest\ncyberdefense exercise, we study blackbox adversarial attacks on network\nclassifiers. Given already existing attack capabilities, we question the\nutility of optimal evasion attack frameworks based on minimal evasion\ndistances. Instead, we suggest a novel reinforcement learning setting that can\nbe used to efficiently generate arbitrary adversarial perturbations. We then\nargue that attacker-defender fixed points are themselves general-sum games with\ncomplex phase transitions, and introduce a temporally extended multi-agent\nreinforcement learning framework in which the resultant dynamics can be\nstudied. We hypothesise that one plausible fixed point of AI-NIDS may be a\nscenario where the defense strategy relies heavily on whitelisted feature flow\nsubspaces. Finally, we demonstrate that a continual learning approach is\nrequired to study attacker-defender dynamics in temporally extended general-sum\ngames.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 Nov 2021 23:42:16 GMT"
      }
    ],
    "update_date": "2021-11-25",
    "authors_parsed": [
      [
        "de Witt",
        "Christian Schroeder",
        ""
      ],
      [
        "Huang",
        "Yongchao",
        ""
      ],
      [
        "Torr",
        "Philip H. S.",
        ""
      ],
      [
        "Strohmeier",
        "Martin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.12197",
    "publish_date": "2021-11-23"
  },
  {
    "id": "2111.12965",
    "submitter": "Xiangyu Qi",
    "authors": "Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, Kai Bu",
    "title": "Towards Practical Deployment-Stage Backdoor Attack on Deep Neural\n  Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  One major goal of the AI security community is to securely and reliably\nproduce and deploy deep learning models for real-world applications. To this\nend, data poisoning based backdoor attacks on deep neural networks (DNNs) in\nthe production stage (or training stage) and corresponding defenses are\nextensively explored in recent years. Ironically, backdoor attacks in the\ndeployment stage, which can often happen in unprofessional users' devices and\nare thus arguably far more threatening in real-world scenarios, draw much less\nattention of the community. We attribute this imbalance of vigilance to the\nweak practicality of existing deployment-stage backdoor attack algorithms and\nthe insufficiency of real-world attack demonstrations. To fill the blank, in\nthis work, we study the realistic threat of deployment-stage backdoor attacks\non DNNs. We base our study on a commonly used deployment-stage attack paradigm\n-- adversarial weight attack, where adversaries selectively modify model\nweights to embed backdoor into deployed DNNs. To approach realistic\npracticality, we propose the first gray-box and physically realizable weights\nattack algorithm for backdoor injection, namely subnet replacement attack\n(SRA), which only requires architecture information of the victim model and can\nsupport physical triggers in the real world. Extensive experimental simulations\nand system-level real-world attack demonstrations are conducted. Our results\nnot only suggest the effectiveness and practicality of the proposed attack\nalgorithm, but also reveal the practical risk of a novel type of computer virus\nthat may widely spread and stealthily inject backdoor into DNN models in user\ndevices. By our study, we call for more attention to the vulnerability of DNNs\nin the deployment stage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 25 Nov 2021 08:25:27 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 26 May 2022 20:52:27 GMT"
      }
    ],
    "update_date": "2022-05-30",
    "authors_parsed": [
      [
        "Qi",
        "Xiangyu",
        ""
      ],
      [
        "Xie",
        "Tinghao",
        ""
      ],
      [
        "Pan",
        "Ruizhe",
        ""
      ],
      [
        "Zhu",
        "Jifeng",
        ""
      ],
      [
        "Yang",
        "Yong",
        ""
      ],
      [
        "Bu",
        "Kai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.12965",
    "publish_date": "2022-05-26"
  },
  {
    "id": "2111.14062",
    "submitter": "Mahdi S. Hosseini Dr.",
    "authors": "Andre Fu and Elisa Ding and Mahdi S. Hosseini and Konstantinos N.\n  Plataniotis",
    "title": "P4AI: Approaching AI Ethics through Principlism",
    "comments": "Human-Centered AI workshop at NeurIPS 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The field of computer vision is rapidly evolving, particularly in the context\nof new methods of neural architecture design. These models contribute to (1)\nthe Climate Crisis - increased CO2 emissions and (2) the Privacy Crisis - data\nleakage concerns. To address the often overlooked impact the Computer Vision\n(CV) community has on these crises, we outline a novel ethical framework,\n\\textit{P4AI}: Principlism for AI, an augmented principlistic view of ethical\ndilemmas within AI. We then suggest using P4AI to make concrete recommendations\nto the community to mitigate the climate and privacy crises.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 28 Nov 2021 06:25:49 GMT"
      }
    ],
    "update_date": "2021-11-30",
    "authors_parsed": [
      [
        "Fu",
        "Andre",
        ""
      ],
      [
        "Ding",
        "Elisa",
        ""
      ],
      [
        "Hosseini",
        "Mahdi S.",
        ""
      ],
      [
        "Plataniotis",
        "Konstantinos N.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.14062",
    "publish_date": "2021-11-28"
  },
  {
    "id": "2111.14203",
    "submitter": "Vandana Janeja",
    "authors": "Zahra Khanjani, Gabrielle Watson, and Vandana P. Janeja",
    "title": "How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey",
    "comments": "Abbreviated version of a longer survey under review",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.AI eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deepfake is content or material that is synthetically generated or\nmanipulated using artificial intelligence (AI) methods, to be passed off as\nreal and can include audio, video, image, and text synthesis. This survey has\nbeen conducted with a different perspective compared to existing survey papers,\nthat mostly focus on just video and image deepfakes. This survey not only\nevaluates generation and detection methods in the different deepfake\ncategories, but mainly focuses on audio deepfakes that are overlooked in most\nof the existing surveys. This paper critically analyzes and provides a unique\nsource of audio deepfake research, mostly ranging from 2016 to 2020. To the\nbest of our knowledge, this is the first survey focusing on audio deepfakes in\nEnglish. This survey provides readers with a summary of 1) different deepfake\ncategories 2) how they could be created and detected 3) the most recent trends\nin this domain and shortcomings in detection methods 4) audio deepfakes, how\nthey are created and detected in more detail which is the main focus of this\npaper. We found that Generative Adversarial Networks(GAN), Convolutional Neural\nNetworks (CNN), and Deep Neural Networks (DNN) are common ways of creating and\ndetecting deepfakes. In our evaluation of over 140 methods we found that the\nmajority of the focus is on video deepfakes and in particular in the generation\nof video deepfakes. We found that for text deepfakes there are more generation\nmethods but very few robust methods for detection, including fake news\ndetection, which has become a controversial area of research because of the\npotential of heavy overlaps with human generation of fake content. This paper\nis an abbreviated version of the full survey and reveals a clear need to\nresearch audio deepfakes and particularly detection of audio deepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 28 Nov 2021 18:28:30 GMT"
      }
    ],
    "update_date": "2021-11-30",
    "authors_parsed": [
      [
        "Khanjani",
        "Zahra",
        ""
      ],
      [
        "Watson",
        "Gabrielle",
        ""
      ],
      [
        "Janeja",
        "Vandana P.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.14203",
    "publish_date": "2021-11-28"
  },
  {
    "id": "2111.14833",
    "submitter": "Ted Fujimoto",
    "authors": "Ted Fujimoto and Arthur Paul Pedersen",
    "title": "Adversarial Attacks in Cooperative AI",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.MA",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Single-agent reinforcement learning algorithms in a multi-agent environment\nare inadequate for fostering cooperation. If intelligent agents are to interact\nand work together to solve complex problems, methods that counter\nnon-cooperative behavior are needed to facilitate the training of multiple\nagents. This is the goal of cooperative AI. Recent research in adversarial\nmachine learning, however, shows that models (e.g., image classifiers) can be\neasily deceived into making inferior decisions. Meanwhile, an important line of\nresearch in cooperative AI has focused on introducing algorithmic improvements\nthat accelerate learning of optimally cooperative behavior. We argue that\nprominent methods of cooperative AI are exposed to weaknesses analogous to\nthose studied in prior machine learning research. More specifically, we show\nthat three algorithms inspired by human-like social intelligence are, in\nprinciple, vulnerable to attacks that exploit weaknesses introduced by\ncooperative AI's algorithmic improvements and report experimental findings that\nillustrate how these vulnerabilities can be exploited in practice.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 29 Nov 2021 07:34:12 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 6 Dec 2021 20:27:17 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 8 Mar 2022 01:24:01 GMT"
      }
    ],
    "update_date": "2022-03-09",
    "authors_parsed": [
      [
        "Fujimoto",
        "Ted",
        ""
      ],
      [
        "Pedersen",
        "Arthur Paul",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2111.14833",
    "publish_date": "2022-03-08"
  },
  {
    "id": "2112.01148",
    "submitter": "Benteng Ma",
    "authors": "Yu Feng, Benteng Ma, Jing Zhang, Shanshan Zhao, Yong Xia, Dacheng Tao",
    "title": "FIBA: Frequency-Injection based Backdoor Attack in Medical Image\n  Analysis",
    "comments": "Accepted by CVPR 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In recent years, the security of AI systems has drawn increasing research\nattention, especially in the medical imaging realm. To develop a secure medical\nimage analysis (MIA) system, it is a must to study possible backdoor attacks\n(BAs), which can embed hidden malicious behaviors into the system. However,\ndesigning a unified BA method that can be applied to various MIA systems is\nchallenging due to the diversity of imaging modalities (e.g., X-Ray, CT, and\nMRI) and analysis tasks (e.g., classification, detection, and segmentation).\nMost existing BA methods are designed to attack natural image classification\nmodels, which apply spatial triggers to training images and inevitably corrupt\nthe semantics of poisoned pixels, leading to the failures of attacking dense\nprediction models. To address this issue, we propose a novel\nFrequency-Injection based Backdoor Attack method (FIBA) that is capable of\ndelivering attacks in various MIA tasks. Specifically, FIBA leverages a trigger\nfunction in the frequency domain that can inject the low-frequency information\nof a trigger image into the poisoned image by linearly combining the spectral\namplitude of both images. Since it preserves the semantics of the poisoned\nimage pixels, FIBA can perform attacks on both classification and dense\nprediction models. Experiments on three benchmarks in MIA (i.e., ISIC-2019 for\nskin lesion classification, KiTS-19 for kidney tumor segmentation, and EAD-2019\nfor endoscopic artifact detection), validate the effectiveness of FIBA and its\nsuperiority over state-of-the-art methods in attacking MIA models as well as\nbypassing backdoor defense. Source code will be available at\nhttps://github.com/HazardFY/FIBA.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 2 Dec 2021 11:52:17 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 14 Apr 2022 12:14:41 GMT"
      }
    ],
    "update_date": "2022-04-15",
    "authors_parsed": [
      [
        "Feng",
        "Yu",
        ""
      ],
      [
        "Ma",
        "Benteng",
        ""
      ],
      [
        "Zhang",
        "Jing",
        ""
      ],
      [
        "Zhao",
        "Shanshan",
        ""
      ],
      [
        "Xia",
        "Yong",
        ""
      ],
      [
        "Tao",
        "Dacheng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2112.01148",
    "publish_date": "2021-12-02"
  },
  {
    "id": "2112.02223",
    "submitter": "Julian Jang-Jaccard Dr.",
    "authors": "Hooman Alavizadeh and Julian Jang-Jaccard and Tansu Alpcan and Seyit\n  A. Camtepe",
    "title": "A Game-Theoretic Approach for AI-based Botnet Attack Defence",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The new generation of botnets leverages Artificial Intelligent (AI)\ntechniques to conceal the identity of botmasters and the attack intention to\navoid detection. Unfortunately, there has not been an existing assessment tool\ncapable of evaluating the effectiveness of existing defense strategies against\nthis kind of AI-based botnet attack. In this paper, we propose a sequential\ngame theory model that is capable to analyse the details of the potential\nstrategies botnet attackers and defenders could use to reach Nash Equilibrium\n(NE). The utility function is computed under the assumption when the attacker\nlaunches the maximum number of DDoS attacks with the minimum attack cost while\nthe defender utilises the maximum number of defense strategies with the minimum\ndefense cost. We conduct a numerical analysis based on a various number of\ndefense strategies involved on different (simulated) cloud-band sizes in\nrelation to different attack success rate values. Our experimental results\nconfirm that the success of defense highly depends on the number of defense\nstrategies used according to careful evaluation of attack rates.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 4 Dec 2021 02:53:40 GMT"
      }
    ],
    "update_date": "2021-12-07",
    "authors_parsed": [
      [
        "Alavizadeh",
        "Hooman",
        ""
      ],
      [
        "Jang-Jaccard",
        "Julian",
        ""
      ],
      [
        "Alpcan",
        "Tansu",
        ""
      ],
      [
        "Camtepe",
        "Seyit A.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2112.02223",
    "publish_date": "2021-12-04"
  },
  {
    "id": "2112.03351",
    "submitter": "Vandana Janeja",
    "authors": "Gabrielle Watson, Zahra Khanjani, Vandana P. Janeja",
    "title": "Audio Deepfake Perceptions in College Going Populations",
    "comments": "Summary of study findings",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.AI eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deepfake is content or material that is generated or manipulated using AI\nmethods, to pass off as real. There are four different deepfake types: audio,\nvideo, image and text. In this research we focus on audio deepfakes and how\npeople perceive it. There are several audio deepfake generation frameworks, but\nwe chose MelGAN which is a non-autoregressive and fast audio deepfake\ngenerating framework, requiring fewer parameters. This study tries to assess\naudio deepfake perceptions among college students from different majors. This\nstudy also answers the question of how their background and major can affect\ntheir perception towards AI generated deepfakes. We also analyzed the results\nbased on different aspects of: grade level, complexity of the grammar used in\nthe audio clips, length of the audio clips, those who knew the term deepfakes\nand those who did not, as well as the political angle. It is interesting that\nthe results show when an audio clip has a political connotation, it can affect\nwhat people think about whether it is real or fake, even if the content is\nfairly similar. This study also explores the question of how background and\nmajor can affect perception towards deepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Dec 2021 20:53:41 GMT"
      }
    ],
    "update_date": "2021-12-08",
    "authors_parsed": [
      [
        "Watson",
        "Gabrielle",
        ""
      ],
      [
        "Khanjani",
        "Zahra",
        ""
      ],
      [
        "Janeja",
        "Vandana P.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2112.03351",
    "publish_date": "2021-12-06"
  },
  {
    "id": "2112.06409",
    "submitter": "Steven Whang",
    "authors": "Steven Euijong Whang, Yuji Roh, Hwanjun Song, Jae-Gil Lee",
    "title": "Data Collection and Quality Challenges in Deep Learning: A Data-Centric\n  AI Perspective",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Data-centric AI is at the center of a fundamental shift in software\nengineering where machine learning becomes the new software, powered by big\ndata and computing infrastructure. Here software engineering needs to be\nre-thought where data becomes a first-class citizen on par with code. One\nstriking observation is that a significant portion of the machine learning\nprocess is spent on data preparation. Without good data, even the best machine\nlearning algorithms cannot perform well. As a result, data-centric AI practices\nare now becoming mainstream. Unfortunately, many datasets in the real world are\nsmall, dirty, biased, and even poisoned. In this survey, we study the research\nlandscape for data collection and data quality primarily for deep learning\napplications. Data collection is important because there is lesser need for\nfeature engineering for recent deep learning approaches, but instead more need\nfor large amounts of data. For data quality, we study data validation,\ncleaning, and integration techniques. Even if the data cannot be fully cleaned,\nwe can still cope with imperfect data during model training using robust model\ntraining techniques. In addition, while bias and fairness have been less\nstudied in traditional data management research, these issues become essential\ntopics in modern machine learning applications. We thus study fairness measures\nand unfairness mitigation techniques that can be applied before, during, or\nafter model training. We believe that the data management community is well\npoised to solve these problems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Dec 2021 03:57:36 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 4 Aug 2022 13:56:57 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 26 Dec 2022 13:27:51 GMT"
      }
    ],
    "update_date": "2022-12-27",
    "authors_parsed": [
      [
        "Whang",
        "Steven Euijong",
        ""
      ],
      [
        "Roh",
        "Yuji",
        ""
      ],
      [
        "Song",
        "Hwanjun",
        ""
      ],
      [
        "Lee",
        "Jae-Gil",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2112.06409",
    "publish_date": "2022-08-04"
  },
  {
    "id": "2201.00912",
    "submitter": "Lorenzo Jaime Flores",
    "authors": "Lorenzo Jaime Yu Flores, Yiding Hao",
    "title": "An Adversarial Benchmark for Fake News Detection Models",
    "comments": "6 pages, 2 figures, Presented at AAAI 2022, Workshop on Adversarial\n  Machine Learning and Beyond",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With the proliferation of online misinformation, fake news detection has\ngained importance in the artificial intelligence community. In this paper, we\npropose an adversarial benchmark that tests the ability of fake news detectors\nto reason about real-world facts. We formulate adversarial attacks that target\nthree aspects of \"understanding\": compositional semantics, lexical relations,\nand sensitivity to modifiers. We test our benchmark using BERT classifiers\nfine-tuned on the LIAR arXiv:arch-ive/1705648 and Kaggle Fake-News datasets,\nand show that both models fail to respond to changes in compositional and\nlexical meaning. Our results strengthen the need for such models to be used in\nconjunction with other fact checking methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Jan 2022 23:51:55 GMT"
      }
    ],
    "update_date": "2022-01-05",
    "authors_parsed": [
      [
        "Flores",
        "Lorenzo Jaime Yu",
        ""
      ],
      [
        "Hao",
        "Yiding",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.00912",
    "publish_date": "2022-01-03"
  },
  {
    "id": "2201.01080",
    "submitter": "Hui Liu",
    "authors": "Hui Liu, Bo Zhao, Yuefeng Peng, Weidong Li, Peng Liu",
    "title": "Towards Understanding and Harnessing the Effect of Image Transformation\n  in Adversarial Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep neural networks (DNNs) are threatened by adversarial examples.\nAdversarial detection, which distinguishes adversarial images from benign\nimages, is fundamental for robust DNN-based services. Image transformation is\none of the most effective approaches to detect adversarial examples. During the\nlast few years, a variety of image transformations have been studied and\ndiscussed to design reliable adversarial detectors. In this paper, we\nsystematically synthesize the recent progress on adversarial detection via\nimage transformations with a novel classification method. Then, we conduct\nextensive experiments to test the detection performance of image\ntransformations against state-of-the-art adversarial attacks. Furthermore, we\nreveal that each individual transformation is not capable of detecting\nadversarial examples in a robust way, and propose a DNN-based approach referred\nto as \\emph{AdvJudge}, which combines scores of 9 image transformations.\nWithout knowing which individual scores are misleading or not misleading,\nAdvJudge can make the right judgment, and achieve a significant improvement in\ndetection rate. Finally, we utilize an explainable AI tool to show the\ncontribution of each image transformation to adversarial detection.\nExperimental results show that the contribution of image transformations to\nadversarial detection is significantly different, the combination of them can\nsignificantly improve the generic detection ability against state-of-the-art\nadversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Jan 2022 10:58:59 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 7 Jan 2022 05:45:05 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 26 May 2022 15:21:05 GMT"
      }
    ],
    "update_date": "2022-05-27",
    "authors_parsed": [
      [
        "Liu",
        "Hui",
        ""
      ],
      [
        "Zhao",
        "Bo",
        ""
      ],
      [
        "Peng",
        "Yuefeng",
        ""
      ],
      [
        "Li",
        "Weidong",
        ""
      ],
      [
        "Liu",
        "Peng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.01080",
    "publish_date": "2022-01-04"
  },
  {
    "id": "2201.01102",
    "submitter": "Fangcheng Liu",
    "authors": "Fangcheng Liu, Chao Zhang, Hongyang Zhang",
    "title": "Towards Transferable Unrestricted Adversarial Examples with Minimum\n  Changes",
    "comments": "Accepted at SaTML 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Transfer-based adversarial example is one of the most important classes of\nblack-box attacks. However, there is a trade-off between transferability and\nimperceptibility of the adversarial perturbation. Prior work in this direction\noften requires a fixed but large $\\ell_p$-norm perturbation budget to reach a\ngood transfer success rate, leading to perceptible adversarial perturbations.\nOn the other hand, most of the current unrestricted adversarial attacks that\naim to generate semantic-preserving perturbations suffer from weaker\ntransferability to the target model. In this work, we propose a geometry-aware\nframework to generate transferable adversarial examples with minimum changes.\nAnalogous to model selection in statistical machine learning, we leverage a\nvalidation model to select the best perturbation budget for each image under\nboth the $\\ell_{\\infty}$-norm and unrestricted threat models. We propose a\nprincipled method for the partition of training and validation models by\nencouraging intra-group diversity while penalizing extra-group similarity.\nExtensive experiments verify the effectiveness of our framework on balancing\nimperceptibility and transferability of the crafted adversarial examples. The\nmethodology is the foundation of our entry to the CVPR'21 Security AI\nChallenger: Unrestricted Adversarial Attacks on ImageNet, in which we ranked\n1st place out of 1,559 teams and surpassed the runner-up submissions by 4.59%\nand 23.91% in terms of final score and average image quality level,\nrespectively. Code is available at https://github.com/Equationliu/GA-Attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Jan 2022 12:03:20 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 27 Dec 2022 12:47:14 GMT"
      }
    ],
    "update_date": "2022-12-29",
    "authors_parsed": [
      [
        "Liu",
        "Fangcheng",
        ""
      ],
      [
        "Zhang",
        "Chao",
        ""
      ],
      [
        "Zhang",
        "Hongyang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.01102",
    "publish_date": "2022-01-04"
  },
  {
    "id": "2201.04569",
    "submitter": "Sunder Ali Khowaja",
    "authors": "Sunder Ali Khowaja, Ik Hyun Lee, Kapal Dev, Muhammad Aslam Jarwar,\n  Nawab Muhammad Faseeh Qureshi",
    "title": "Get your Foes Fooled: Proximal Gradient Split Learning for Defense\n  against Model Inversion Attacks on IoMT data",
    "comments": "10 pages, 5 figures, 2 tables",
    "journal-ref": "IEEE Transactions on Network Science and Engineering, 2022",
    "doi": "10.1109/TNSE.2022.3188575",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  The past decade has seen a rapid adoption of Artificial Intelligence (AI),\nspecifically the deep learning networks, in Internet of Medical Things (IoMT)\necosystem. However, it has been shown recently that the deep learning networks\ncan be exploited by adversarial attacks that not only make IoMT vulnerable to\nthe data theft but also to the manipulation of medical diagnosis. The existing\nstudies consider adding noise to the raw IoMT data or model parameters which\nnot only reduces the overall performance concerning medical inferences but also\nis ineffective to the likes of deep leakage from gradients method. In this\nwork, we propose proximal gradient split learning (PSGL) method for defense\nagainst the model inversion attacks. The proposed method intentionally attacks\nthe IoMT data when undergoing the deep neural network training process at\nclient side. We propose the use of proximal gradient method to recover gradient\nmaps and a decision-level fusion strategy to improve the recognition\nperformance. Extensive analysis show that the PGSL not only provides effective\ndefense mechanism against the model inversion attacks but also helps in\nimproving the recognition performance on publicly available datasets. We report\n14.0$\\%$, 17.9$\\%$, and 36.9$\\%$ gains in accuracy over reconstructed and\nadversarial attacked images, respectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 12 Jan 2022 17:01:19 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 20 Jan 2022 01:02:30 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 9 Aug 2022 09:19:40 GMT"
      }
    ],
    "update_date": "2022-08-10",
    "authors_parsed": [
      [
        "Khowaja",
        "Sunder Ali",
        ""
      ],
      [
        "Lee",
        "Ik Hyun",
        ""
      ],
      [
        "Dev",
        "Kapal",
        ""
      ],
      [
        "Jarwar",
        "Muhammad Aslam",
        ""
      ],
      [
        "Qureshi",
        "Nawab Muhammad Faseeh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.04569",
    "publish_date": "2022-01-12"
  },
  {
    "id": "2201.05320",
    "submitter": "Alon Talmor",
    "authors": "Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav\n  Goldberg, Yejin Choi, Jonathan Berant",
    "title": "CommonsenseQA 2.0: Exposing the Limits of AI through Gamification",
    "comments": "Presented as Oral at NeurIPS 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Constructing benchmarks that test the abilities of modern natural language\nunderstanding models is difficult - pre-trained language models exploit\nartifacts in benchmarks to achieve human parity, but still fail on adversarial\nexamples and make errors that demonstrate a lack of common sense. In this work,\nwe propose gamification as a framework for data construction. The goal of\nplayers in the game is to compose questions that mislead a rival AI while using\nspecific phrases for extra points. The game environment leads to enhanced user\nengagement and simultaneously gives the game designer control over the\ncollected data, allowing us to collect high-quality data at scale. Using our\nmethod we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and\ndemonstrate its difficulty for models that are orders-of-magnitude larger than\nthe AI used in the game itself. Our best baseline, the T5-based Unicorn with\n11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3\n(52.9%) in a few-shot inference setup. Both score well below human performance\nwhich is at 94.1%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 14 Jan 2022 06:49:15 GMT"
      }
    ],
    "update_date": "2022-01-17",
    "authors_parsed": [
      [
        "Talmor",
        "Alon",
        ""
      ],
      [
        "Yoran",
        "Ori",
        ""
      ],
      [
        "Bras",
        "Ronan Le",
        ""
      ],
      [
        "Bhagavatula",
        "Chandra",
        ""
      ],
      [
        "Goldberg",
        "Yoav",
        ""
      ],
      [
        "Choi",
        "Yejin",
        ""
      ],
      [
        "Berant",
        "Jonathan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.05320",
    "publish_date": "2022-01-14"
  },
  {
    "id": "2201.08135",
    "submitter": "Nuria Rodr\\'iguez-Barroso",
    "authors": "Nuria Rodr\\'iguez-Barroso, Daniel Jim\\'enez L\\'opez, M. Victoria\n  Luz\\'on, Francisco Herrera and Eugenio Mart\\'inez-C\\'amara",
    "title": "Survey on Federated Learning Threats: concepts, taxonomy on attacks and\n  defences, experimental study and challenges",
    "comments": null,
    "journal-ref": "Information Fusion (2022)",
    "doi": "10.1016/j.inffus.2022.09.011",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Federated learning is a machine learning paradigm that emerges as a solution\nto the privacy-preservation demands in artificial intelligence. As machine\nlearning, federated learning is threatened by adversarial attacks against the\nintegrity of the learning model and the privacy of data via a distributed\napproach to tackle local and global learning. This weak point is exacerbated by\nthe inaccessibility of data in federated learning, which makes harder the\nprotection against adversarial attacks and evidences the need to furtherance\nthe research on defence methods to make federated learning a real solution for\nsafeguarding data privacy. In this paper, we present an extensive review of the\nthreats of federated learning, as well as as their corresponding\ncountermeasures, attacks versus defences. This survey provides a taxonomy of\nadversarial attacks and a taxonomy of defence methods that depict a general\npicture of this vulnerability of federated learning and how to overcome it.\nLikewise, we expound guidelines for selecting the most adequate defence method\naccording to the category of the adversarial attack. Besides, we carry out an\nextensive experimental study from which we draw further conclusions about the\nbehaviour of attacks and defences and the guidelines for selecting the most\nadequate defence method according to the category of the adversarial attack.\nThis study is finished leading to meditated learned lessons and challenges.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 20 Jan 2022 12:23:03 GMT"
      }
    ],
    "update_date": "2022-09-20",
    "authors_parsed": [
      [
        "Rodr\u00edguez-Barroso",
        "Nuria",
        ""
      ],
      [
        "L\u00f3pez",
        "Daniel Jim\u00e9nez",
        ""
      ],
      [
        "Luz\u00f3n",
        "M. Victoria",
        ""
      ],
      [
        "Herrera",
        "Francisco",
        ""
      ],
      [
        "Mart\u00ednez-C\u00e1mara",
        "Eugenio",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.08135",
    "publish_date": "2022-01-20"
  },
  {
    "id": "2201.09754",
    "submitter": "Ding Chen",
    "authors": "Ding Chen, Peixi Peng, Tiejun Huang, Yonghong Tian",
    "title": "Deep Reinforcement Learning with Spiking Q-learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NE cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With the help of special neuromorphic hardware, spiking neural networks\n(SNNs) are expected to realize artificial intelligence with less energy\nconsumption. It provides a promising energy-efficient way for realistic control\ntasks by combing SNNs and deep reinforcement learning (RL). There are only a\nfew existing SNN-based RL methods at present. Most of them either lack\ngeneralization ability or employ Artificial Neural Networks (ANNs) to estimate\nvalue function in training. The former needs to tune numerous hyper-parameters\nfor each scenario, and the latter limits the application of different types of\nRL algorithm and ignores the large energy consumption in training. To develop a\nrobust spike-based RL method, we draw inspiration from non-spiking interneurons\nfound in insects and propose the deep spiking Q-network (DSQN), using the\nmembrane voltage of non-spiking neurons as the representation of Q-value, which\ncan directly learn robust policies from high-dimensional sensory inputs using\nend-to-end RL. Experiments conducted on 17 Atari games demonstrate the\neffectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in\nmost games. Moreover, the experimental results show superior learning stability\nand robustness to adversarial attacks of DSQN.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 21 Jan 2022 16:42:11 GMT"
      }
    ],
    "update_date": "2022-01-25",
    "authors_parsed": [
      [
        "Chen",
        "Ding",
        ""
      ],
      [
        "Peng",
        "Peixi",
        ""
      ],
      [
        "Huang",
        "Tiejun",
        ""
      ],
      [
        "Tian",
        "Yonghong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.09754",
    "publish_date": "2022-01-21"
  },
  {
    "id": "2201.12107",
    "submitter": "Raoul Schoenhof",
    "authors": "Raoul Sch\\\"onhof and Artem Werner and Jannes Elstner and Boldizsar\n  Zopcsak and Ramez Awad and Marco Huber",
    "title": "Feature Visualization within an Automated Design Assessment leveraging\n  Explainable Artificial Intelligence Methods",
    "comments": "CIRP Design 2021, 10.1016/j.procir.2021.05.075",
    "journal-ref": "2021, Procedia CIRP 100(7):331-336",
    "doi": "10.1016/j.procir.2021.05.075",
    "report-no": null,
    "categories": "cs.AI cs.CV cs.LG cs.RO",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Not only automation of manufacturing processes but also automation of\nautomation procedures itself become increasingly relevant to automation\nresearch. In this context, automated capability assessment, mainly leveraged by\ndeep learning systems driven from 3D CAD data, have been presented. Current\nassessment systems may be able to assess CAD data with regards to abstract\nfeatures, e.g. the ability to automatically separate components from bulk\ngoods, or the presence of gripping surfaces. Nevertheless, they suffer from the\nfactor of black box systems, where an assessment can be learned and generated\neasily, but without any geometrical indicator about the reasons of the system's\ndecision. By utilizing explainable AI (xAI) methods, we attempt to open up the\nblack box. Explainable AI methods have been used in order to assess whether a\nneural network has successfully learned a given task or to analyze which\nfeatures of an input might lead to an adversarial attack. These methods aim to\nderive additional insights into a neural network, by analyzing patterns from a\ngiven input and its impact to the network output. Within the NeuroCAD Project,\nxAI methods are used to identify geometrical features which are associated with\na certain abstract feature. Within this work, a sensitivity analysis (SA), the\nlayer-wise relevance propagation (LRP), the Gradient-weighted Class Activation\nMapping (Grad-CAM) method as well as the Local Interpretable Model-Agnostic\nExplanations (LIME) have been implemented in the NeuroCAD environment, allowing\nnot only to assess CAD models but also to identify features which have been\nrelevant for the network decision. In the medium run, this might enable to\nidentify regions of interest supporting product designers to optimize their\nmodels with regards to assembly processes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 28 Jan 2022 13:31:42 GMT"
      }
    ],
    "update_date": "2022-01-31",
    "authors_parsed": [
      [
        "Sch\u00f6nhof",
        "Raoul",
        ""
      ],
      [
        "Werner",
        "Artem",
        ""
      ],
      [
        "Elstner",
        "Jannes",
        ""
      ],
      [
        "Zopcsak",
        "Boldizsar",
        ""
      ],
      [
        "Awad",
        "Ramez",
        ""
      ],
      [
        "Huber",
        "Marco",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2201.12107",
    "publish_date": "2022-01-28"
  },
  {
    "id": "2202.02510",
    "submitter": "Wenjun Qiu",
    "authors": "Wenjun Qiu",
    "title": "A Survey on Poisoning Attacks Against Supervised Machine Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With the rise of artificial intelligence and machine learning in modern\ncomputing, one of the major concerns regarding such techniques is to provide\nprivacy and security against adversaries. We present this survey paper to cover\nthe most representative papers in poisoning attacks against supervised machine\nlearning models. We first provide a taxonomy to categorize existing studies and\nthen present detailed summaries for selected papers. We summarize and compare\nthe methodology and limitations of existing literature. We conclude this paper\nwith potential improvements and future directions to further exploit and\nprevent poisoning attacks on supervised models. We propose several unanswered\nresearch questions to encourage and inspire researchers for future work.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 5 Feb 2022 08:02:22 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 8 Feb 2022 02:06:14 GMT"
      }
    ],
    "update_date": "2022-02-09",
    "authors_parsed": [
      [
        "Qiu",
        "Wenjun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.02510",
    "publish_date": "2022-02-08"
  },
  {
    "id": "2202.02626",
    "submitter": "Mohammad Khalooei",
    "authors": "Mohammad Khalooei, Mohammad Mehdi Homayounpour, Maryam Amirmazlaghani",
    "title": "Layer-wise Regularized Adversarial Training using Layers Sustainability\n  Analysis (LSA) framework",
    "comments": "Layers Sustainability Analysis (LSA) framework",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Deep neural network models are used today in various applications of\nartificial intelligence, the strengthening of which, in the face of adversarial\nattacks is of particular importance. An appropriate solution to adversarial\nattacks is adversarial training, which reaches a trade-off between robustness\nand generalization. This paper introduces a novel framework (Layer\nSustainability Analysis (LSA)) for the analysis of layer vulnerability in an\narbitrary neural network in the scenario of adversarial attacks. LSA can be a\nhelpful toolkit to assess deep neural networks and to extend the adversarial\ntraining approaches towards improving the sustainability of model layers via\nlayer monitoring and analysis. The LSA framework identifies a list of Most\nVulnerable Layers (MVL list) of the given network. The relative error, as a\ncomparison measure, is used to evaluate representation sustainability of each\nlayer against adversarial inputs. The proposed approach for obtaining robust\nneural networks to fend off adversarial attacks is based on a layer-wise\nregularization (LR) over LSA proposal(s) for adversarial training (AT); i.e.\nthe AT-LR procedure. AT-LR could be used with any benchmark adversarial attack\nto reduce the vulnerability of network layers and to improve conventional\nadversarial training approaches. The proposed idea performs well theoretically\nand experimentally for state-of-the-art multilayer perceptron and convolutional\nneural network architectures. Compared with the AT-LR and its corresponding\nbase adversarial training, the classification accuracy of more significant\nperturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and\nCIFAR-10 benchmark datasets, respectively. The LSA framework is available and\npublished at https://github.com/khalooei/LSA.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 5 Feb 2022 20:05:53 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 9 Feb 2022 20:50:58 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 15 Feb 2022 08:44:54 GMT"
      }
    ],
    "update_date": "2022-02-16",
    "authors_parsed": [
      [
        "Khalooei",
        "Mohammad",
        ""
      ],
      [
        "Homayounpour",
        "Mohammad Mehdi",
        ""
      ],
      [
        "Amirmazlaghani",
        "Maryam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.02626",
    "publish_date": "2022-02-09"
  },
  {
    "id": "2202.06924",
    "submitter": "Holger R. Roth",
    "authors": "Ali Hatamizadeh, Hongxu Yin, Pavlo Molchanov, Andriy Myronenko, Wenqi\n  Li, Prerna Dogra, Andrew Feng, Mona G. Flores, Jan Kautz, Daguang Xu, Holger\n  R. Roth",
    "title": "Do Gradient Inversion Attacks Make Federated Learning Unsafe?",
    "comments": "Revised version; Accepted to IEEE Transactions on Medical Imaging;\n  Improved and reformatted version of\n  https://www.researchsquare.com/article/rs-1147182/v2; Added NVFlare reference",
    "journal-ref": null,
    "doi": "10.1109/TMI.2023.3239391",
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV cs.DC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Federated learning (FL) allows the collaborative training of AI models\nwithout needing to share raw data. This capability makes it especially\ninteresting for healthcare applications where patient and data privacy is of\nutmost concern. However, recent works on the inversion of deep neural networks\nfrom model gradients raised concerns about the security of FL in preventing the\nleakage of training data. In this work, we show that these attacks presented in\nthe literature are impractical in FL use-cases where the clients' training\ninvolves updating the Batch Normalization (BN) statistics and provide a new\nbaseline attack that works for such scenarios. Furthermore, we present new ways\nto measure and visualize potential data leakage in FL. Our work is a step\ntowards establishing reproducible methods of measuring data leakage in FL and\ncould help determine the optimal tradeoffs between privacy-preserving\ntechniques, such as differential privacy, and model accuracy based on\nquantifiable metrics.\n  Code is available at\nhttps://nvidia.github.io/NVFlare/research/quantifying-data-leakage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 Feb 2022 18:33:12 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 23 Jan 2023 16:03:10 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 30 Jan 2023 23:11:08 GMT"
      }
    ],
    "update_date": "2023-02-01",
    "authors_parsed": [
      [
        "Hatamizadeh",
        "Ali",
        ""
      ],
      [
        "Yin",
        "Hongxu",
        ""
      ],
      [
        "Molchanov",
        "Pavlo",
        ""
      ],
      [
        "Myronenko",
        "Andriy",
        ""
      ],
      [
        "Li",
        "Wenqi",
        ""
      ],
      [
        "Dogra",
        "Prerna",
        ""
      ],
      [
        "Feng",
        "Andrew",
        ""
      ],
      [
        "Flores",
        "Mona G.",
        ""
      ],
      [
        "Kautz",
        "Jan",
        ""
      ],
      [
        "Xu",
        "Daguang",
        ""
      ],
      [
        "Roth",
        "Holger R.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.06924",
    "publish_date": "2023-01-30"
  },
  {
    "id": "2202.07050",
    "submitter": "Jema David Ndibwile Prof",
    "authors": "J.D. Ndibwile",
    "title": "Artificial Intelligence-Based Smart Grid Vulnerabilities and Potential\n  Solutions for Fake-Normal Attacks: A Short Review",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Smart grid systems are critical to the power industry, however their\nsophisticated architectural design and operations expose them to a number of\ncybersecurity threats, such as data tampering, data eavesdropping, and Denial\nof Service, among others. Artificial Intelligence (AI)-based technologies are\nbecoming increasingly popular for detecting cyber assaults in a variety of\ncomputer settings, and several efforts have been made to secure various\nsystems. The present AI systems are being exposed and vanquished because of the\nrecent emergence of sophisticated adversarial systems such as Generative\nAdversarial Networks (GAN). The purpose of this short review is to outline some\nof the initiatives to protect smart grid systems, their obstacles, and what\nmight be a potential future AI research direction\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 Feb 2022 21:41:36 GMT"
      }
    ],
    "update_date": "2022-02-16",
    "authors_parsed": [
      [
        "Ndibwile",
        "J. D.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.07050",
    "publish_date": "2022-02-14"
  },
  {
    "id": "2202.08185",
    "submitter": "Ferhat Ozgur Catak",
    "authors": "Murat Kuzlu, Ferhat Ozgur Catak, Umit Cali, Evren Catak, Ozgur Guler",
    "title": "The Adversarial Security Mitigations of mmWave Beamforming Prediction\n  Models using Defensive Distillation and Adversarial Retraining",
    "comments": "26 pages, under review",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG cs.NI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The design of a security scheme for beamforming prediction is critical for\nnext-generation wireless networks (5G, 6G, and beyond). However, there is no\nconsensus about protecting the beamforming prediction using deep learning\nalgorithms in these networks. This paper presents the security vulnerabilities\nin deep learning for beamforming prediction using deep neural networks (DNNs)\nin 6G wireless networks, which treats the beamforming prediction as a\nmulti-output regression problem. It is indicated that the initial DNN model is\nvulnerable against adversarial attacks, such as Fast Gradient Sign Method\n(FGSM), Basic Iterative Method (BIM), Projected Gradient Descent (PGD), and\nMomentum Iterative Method (MIM), because the initial DNN model is sensitive to\nthe perturbations of the adversarial samples of the training data. This study\nalso offers two mitigation methods, such as adversarial training and defensive\ndistillation, for adversarial attacks against artificial intelligence\n(AI)-based models used in the millimeter-wave (mmWave) beamforming prediction.\nFurthermore, the proposed scheme can be used in situations where the data are\ncorrupted due to the adversarial examples in the training data. Experimental\nresults show that the proposed methods effectively defend the DNN models\nagainst adversarial attacks in next-generation wireless networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 16 Feb 2022 16:47:17 GMT"
      }
    ],
    "update_date": "2022-02-17",
    "authors_parsed": [
      [
        "Kuzlu",
        "Murat",
        ""
      ],
      [
        "Catak",
        "Ferhat Ozgur",
        ""
      ],
      [
        "Cali",
        "Umit",
        ""
      ],
      [
        "Catak",
        "Evren",
        ""
      ],
      [
        "Guler",
        "Ozgur",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.08185",
    "publish_date": "2022-02-16"
  },
  {
    "id": "2202.10276",
    "submitter": "Song-Kyoo Amang Kim Ph.D.",
    "authors": "Miguel A. Ramirez, Song-Kyoo Kim, Hussam Al Hamadi, Ernesto Damiani,\n  Young-Ji Byon, Tae-Yeon Kim, Chung-Suk Cho and Chan Yeob Yeun",
    "title": "Poisoning Attacks and Defenses on Artificial Intelligence: A Survey",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Machine learning models have been widely adopted in several fields. However,\nmost recent studies have shown several vulnerabilities from attacks with a\npotential to jeopardize the integrity of the model, presenting a new window of\nresearch opportunity in terms of cyber-security. This survey is conducted with\na main intention of highlighting the most relevant information related to\nsecurity vulnerabilities in the context of machine learning (ML) classifiers;\nmore specifically, directed towards training procedures against data poisoning\nattacks, representing a type of attack that consists of tampering the data\nsamples fed to the model during the training phase, leading to a degradation in\nthe models accuracy during the inference phase. This work compiles the most\nrelevant insights and findings found in the latest existing literatures\naddressing this type of attacks. Moreover, this paper also covers several\ndefense techniques that promise feasible detection and mitigation mechanisms,\ncapable of conferring a certain level of robustness to a target model against\nan attacker. A thorough assessment is performed on the reviewed works,\ncomparing the effects of data poisoning on a wide range of ML models in\nreal-world conditions, performing quantitative and qualitative analyses. This\npaper analyzes the main characteristics for each approach including performance\nsuccess metrics, required hyperparameters, and deployment complexity. Moreover,\nthis paper emphasizes the underlying assumptions and limitations considered by\nboth attackers and defenders along with their intrinsic properties such as:\navailability, reliability, privacy, accountability, interpretability, etc.\nFinally, this paper concludes by making references of some of main existing\nresearch trends that provide pathways towards future research directions in the\nfield of cyber-security.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Feb 2022 14:43:38 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 22 Feb 2022 07:20:43 GMT"
      }
    ],
    "update_date": "2022-02-23",
    "authors_parsed": [
      [
        "Ramirez",
        "Miguel A.",
        ""
      ],
      [
        "Kim",
        "Song-Kyoo",
        ""
      ],
      [
        "Hamadi",
        "Hussam Al",
        ""
      ],
      [
        "Damiani",
        "Ernesto",
        ""
      ],
      [
        "Byon",
        "Young-Ji",
        ""
      ],
      [
        "Kim",
        "Tae-Yeon",
        ""
      ],
      [
        "Cho",
        "Chung-Suk",
        ""
      ],
      [
        "Yeun",
        "Chan Yeob",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.10276",
    "publish_date": "2022-02-22"
  },
  {
    "id": "2202.10377",
    "submitter": "Basel Halak",
    "authors": "Cato Pauling, Michael Gimson, Muhammed Qaid, Ahmad Kida and Basel\n  Halak",
    "title": "A Tutorial on Adversarial Learning Attacks and Countermeasures",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Machine learning algorithms are used to construct a mathematical model for a\nsystem based on training data. Such a model is capable of making highly\naccurate predictions without being explicitly programmed to do so. These\ntechniques have a great many applications in all areas of the modern digital\neconomy and artificial intelligence. More importantly, these methods are\nessential for a rapidly increasing number of safety-critical applications such\nas autonomous vehicles and intelligent defense systems. However, emerging\nadversarial learning attacks pose a serious security threat that greatly\nundermines further such systems. The latter are classified into four types,\nevasion (manipulating data to avoid detection), poisoning (injection malicious\ntraining samples to disrupt retraining), model stealing (extraction), and\ninference (leveraging over-generalization on training data). Understanding this\ntype of attacks is a crucial first step for the development of effective\ncountermeasures. The paper provides a detailed tutorial on the principles of\nadversarial machining learning, explains the different attack scenarios, and\ngives an in-depth insight into the state-of-art defense mechanisms against this\nrising threat .\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Feb 2022 17:14:45 GMT"
      }
    ],
    "update_date": "2022-02-22",
    "authors_parsed": [
      [
        "Pauling",
        "Cato",
        ""
      ],
      [
        "Gimson",
        "Michael",
        ""
      ],
      [
        "Qaid",
        "Muhammed",
        ""
      ],
      [
        "Kida",
        "Ahmad",
        ""
      ],
      [
        "Halak",
        "Basel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.10377",
    "publish_date": "2022-02-21"
  },
  {
    "id": "2202.10582",
    "submitter": "Shangxi Wu",
    "authors": "Shangxi Wu and Qiuyang He and Yi Zhang and Jitao Sang",
    "title": "Debiasing Backdoor Attack: A Benign Application of Backdoor Attack in\n  Eliminating Data Bias",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Backdoor attack is a new AI security risk that has emerged in recent years.\nDrawing on the previous research of adversarial attack, we argue that the\nbackdoor attack has the potential to tap into the model learning process and\nimprove model performance. Based on Clean Accuracy Drop (CAD) in backdoor\nattack, we found that CAD came out of the effect of pseudo-deletion of data. We\nprovided a preliminary explanation of this phenomenon from the perspective of\nmodel classification boundaries and observed that this pseudo-deletion had\nadvantages over direct deletion in the data debiasing problem. Based on the\nabove findings, we proposed Debiasing Backdoor Attack (DBA). It achieves SOTA\nin the debiasing task and has a broader application scenario than\nundersampling.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 18 Feb 2022 05:00:08 GMT"
      }
    ],
    "update_date": "2022-02-23",
    "authors_parsed": [
      [
        "Wu",
        "Shangxi",
        ""
      ],
      [
        "He",
        "Qiuyang",
        ""
      ],
      [
        "Zhang",
        "Yi",
        ""
      ],
      [
        "Sang",
        "Jitao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.10582",
    "publish_date": "2022-02-18"
  },
  {
    "id": "2202.10594",
    "submitter": "Mohamed Reda Bouadjenek",
    "authors": "Ngoc Dung Huynh, Mohamed Reda Bouadjenek, Imran Razzak, Kevin Lee,\n  Chetan Arora, Ali Hassani, Arkady Zaslavsky",
    "title": "Adversarial Attacks on Speech Recognition Systems for Mission-Critical\n  Applications: A Survey",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.CR cs.LG eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  A Machine-Critical Application is a system that is fundamentally necessary to\nthe success of specific and sensitive operations such as search and recovery,\nrescue, military, and emergency management actions. Recent advances in Machine\nLearning, Natural Language Processing, voice recognition, and speech processing\ntechnologies have naturally allowed the development and deployment of\nspeech-based conversational interfaces to interact with various\nmachine-critical applications. While these conversational interfaces have\nallowed users to give voice commands to carry out strategic and critical\nactivities, their robustness to adversarial attacks remains uncertain and\nunclear. Indeed, Adversarial Artificial Intelligence (AI) which refers to a set\nof techniques that attempt to fool machine learning models with deceptive data,\nis a growing threat in the AI and machine learning research community, in\nparticular for machine-critical applications. The most common reason of\nadversarial attacks is to cause a malfunction in a machine learning model. An\nadversarial attack might entail presenting a model with inaccurate or\nfabricated samples as it's training data, or introducing maliciously designed\ndata to deceive an already trained model. While focusing on speech recognition\nfor machine-critical applications, in this paper, we first review existing\nspeech recognition techniques, then, we investigate the effectiveness of\nadversarial attacks and defenses against these systems, before outlining\nresearch challenges, defense recommendations, and future work. This paper is\nexpected to serve researchers and practitioners as a reference to help them in\nunderstanding the challenges, position themselves and, ultimately, help them to\nimprove existing models of speech recognition for mission-critical\napplications. Keywords: Mission-Critical Applications, Adversarial AI, Speech\nRecognition Systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 22 Feb 2022 00:29:40 GMT"
      }
    ],
    "update_date": "2022-02-23",
    "authors_parsed": [
      [
        "Huynh",
        "Ngoc Dung",
        ""
      ],
      [
        "Bouadjenek",
        "Mohamed Reda",
        ""
      ],
      [
        "Razzak",
        "Imran",
        ""
      ],
      [
        "Lee",
        "Kevin",
        ""
      ],
      [
        "Arora",
        "Chetan",
        ""
      ],
      [
        "Hassani",
        "Ali",
        ""
      ],
      [
        "Zaslavsky",
        "Arkady",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.10594",
    "publish_date": "2022-02-22"
  },
  {
    "id": "2202.12951",
    "submitter": "Shan Jia",
    "authors": "Shan Jia, Xin Li, Siwei Lyu",
    "title": "Model Attribution of Face-swap Deepfake Videos",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.MM",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI-created face-swap videos, commonly known as Deepfakes, have attracted wide\nattention as powerful impersonation attacks. Existing research on Deepfakes\nmostly focuses on binary detection to distinguish between real and fake videos.\nHowever, it is also important to determine the specific generation model for a\nfake video, which can help attribute it to the source for forensic\ninvestigation. In this paper, we fill this gap by studying the model\nattribution problem of Deepfake videos. We first introduce a new dataset with\nDeepFakes from Different Models (DFDM) based on several Autoencoder models.\nSpecifically, five generation models with variations in encoder, decoder,\nintermediate layer, input resolution, and compression ratio have been used to\ngenerate a total of 6,450 Deepfake videos based on the same input. Then we take\nDeepfakes model attribution as a multiclass classification task and propose a\nspatial and temporal attention based method to explore the differences among\nDeepfakes in the new dataset. Experimental evaluation shows that most existing\nDeepfakes detection methods failed in Deepfakes model attribution, while the\nproposed method achieved over 70% accuracy on the high-quality DFDM dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 25 Feb 2022 20:05:18 GMT"
      }
    ],
    "update_date": "2022-03-01",
    "authors_parsed": [
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Li",
        "Xin",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.12951",
    "publish_date": "2022-02-25"
  },
  {
    "id": "2202.14010",
    "submitter": "Arunesh Sinha",
    "authors": "James Holt, Edward Raff, Ahmad Ridley, Dennis Ross, Arunesh Sinha,\n  Diane Staheli, William Streilen, Milind Tambe, Yevgeniy Vorobeychik, Allan\n  Wollaber",
    "title": "Proceedings of the Artificial Intelligence for Cyber Security (AICS)\n  Workshop at AAAI 2022",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.GT cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The workshop will focus on the application of AI to problems in cyber\nsecurity. Cyber systems generate large volumes of data, utilizing this\neffectively is beyond human capabilities. Additionally, adversaries continue to\ndevelop new attacks. Hence, AI methods are required to understand and protect\nthe cyber domain. These challenges are widely studied in enterprise networks,\nbut there are many gaps in research and practice as well as novel problems in\nother domains.\n  In general, AI techniques are still not widely adopted in the real world.\nReasons include: (1) a lack of certification of AI for security, (2) a lack of\nformal study of the implications of practical constraints (e.g., power, memory,\nstorage) for AI systems in the cyber domain, (3) known vulnerabilities such as\nevasion, poisoning attacks, (4) lack of meaningful explanations for security\nanalysts, and (5) lack of analyst trust in AI solutions. There is a need for\nthe research community to develop novel solutions for these practical issues.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 28 Feb 2022 18:27:41 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 1 Mar 2022 14:57:13 GMT"
      }
    ],
    "update_date": "2022-03-02",
    "authors_parsed": [
      [
        "Holt",
        "James",
        ""
      ],
      [
        "Raff",
        "Edward",
        ""
      ],
      [
        "Ridley",
        "Ahmad",
        ""
      ],
      [
        "Ross",
        "Dennis",
        ""
      ],
      [
        "Sinha",
        "Arunesh",
        ""
      ],
      [
        "Staheli",
        "Diane",
        ""
      ],
      [
        "Streilen",
        "William",
        ""
      ],
      [
        "Tambe",
        "Milind",
        ""
      ],
      [
        "Vorobeychik",
        "Yevgeniy",
        ""
      ],
      [
        "Wollaber",
        "Allan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2202.14010",
    "publish_date": "2022-03-01"
  },
  {
    "id": "2203.01895",
    "submitter": "Pervaiz Khan",
    "authors": "Pervaiz Iqbal Khan, Shoaib Ahmed Siddiqui, Imran Razzak, Andreas\n  Dengel, and Sheraz Ahmed",
    "title": "Improving Health Mentioning Classification of Tweets using Contrastive\n  Adversarial Training",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Health mentioning classification (HMC) classifies an input text as health\nmention or not. Figurative and non-health mention of disease words makes the\nclassification task challenging. Learning the context of the input text is the\nkey to this problem. The idea is to learn word representation by its\nsurrounding words and utilize emojis in the text to help improve the\nclassification results. In this paper, we improve the word representation of\nthe input text using adversarial training that acts as a regularizer during\nfine-tuning of the model. We generate adversarial examples by perturbing the\nembeddings of the model and then train the model on a pair of clean and\nadversarial examples. Additionally, we utilize contrastive loss that pushes a\npair of clean and perturbed examples close to each other and other examples\naway in the representation space. We train and evaluate the method on an\nextended version of the publicly available PHM2017 dataset. Experiments show an\nimprovement of 1.0% over BERT-Large baseline and 0.6% over RoBERTa-Large\nbaseline, whereas 5.8% over the state-of-the-art in terms of F1 score.\nFurthermore, we provide a brief analysis of the results by utilizing the power\nof explainable AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 3 Mar 2022 18:20:51 GMT"
      }
    ],
    "update_date": "2022-03-04",
    "authors_parsed": [
      [
        "Khan",
        "Pervaiz Iqbal",
        ""
      ],
      [
        "Siddiqui",
        "Shoaib Ahmed",
        ""
      ],
      [
        "Razzak",
        "Imran",
        ""
      ],
      [
        "Dengel",
        "Andreas",
        ""
      ],
      [
        "Ahmed",
        "Sheraz",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.01895",
    "publish_date": "2022-03-03"
  },
  {
    "id": "2203.03373",
    "submitter": "Zhanhao Hu",
    "authors": "Zhanhao Hu, Siyuan Huang, Xiaopei Zhu, Fuchun Sun, Bo Zhang, Xiaolin\n  Hu",
    "title": "Adversarial Texture for Fooling Person Detectors in the Physical World",
    "comments": "Accepted by CVPR 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Nowadays, cameras equipped with AI systems can capture and analyze images to\ndetect people automatically. However, the AI system can make mistakes when\nreceiving deliberately designed patterns in the real world, i.e., physical\nadversarial examples. Prior works have shown that it is possible to print\nadversarial patches on clothes to evade DNN-based person detectors. However,\nthese adversarial examples could have catastrophic drops in the attack success\nrate when the viewing angle (i.e., the camera's angle towards the object)\nchanges. To perform a multi-angle attack, we propose Adversarial Texture\n(AdvTexture). AdvTexture can cover clothes with arbitrary shapes so that people\nwearing such clothes can hide from person detectors from different viewing\nangles. We propose a generative method, named Toroidal-Cropping-based\nExpandable Generative Attack (TC-EGA), to craft AdvTexture with repetitive\nstructures. We printed several pieces of cloth with AdvTexure and then made\nT-shirts, skirts, and dresses in the physical world. Experiments showed that\nthese clothes could fool person detectors in the physical world.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 Mar 2022 13:22:25 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 8 Mar 2022 14:29:07 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 18 Mar 2022 06:47:05 GMT"
      },
      {
        "version": "v4",
        "created": "Sat, 13 Aug 2022 17:21:34 GMT"
      }
    ],
    "update_date": "2022-08-16",
    "authors_parsed": [
      [
        "Hu",
        "Zhanhao",
        ""
      ],
      [
        "Huang",
        "Siyuan",
        ""
      ],
      [
        "Zhu",
        "Xiaopei",
        ""
      ],
      [
        "Sun",
        "Fuchun",
        ""
      ],
      [
        "Zhang",
        "Bo",
        ""
      ],
      [
        "Hu",
        "Xiaolin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.03373",
    "publish_date": "2022-03-08"
  },
  {
    "id": "2203.05314",
    "submitter": "Junjie Shen",
    "authors": "Junjie Shen, Ningfei Wang, Ziwen Wan, Yunpeng Luo, Takami Sato,\n  Zhisheng Hu, Xinyang Zhang, Shengjian Guo, Zhenyu Zhong, Kang Li, Ziming\n  Zhao, Chunming Qiao, Qi Alfred Chen",
    "title": "SoK: On the Semantic AI Security in Autonomous Driving",
    "comments": "Project website: https://sites.google.com/view/cav-sec/pass",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.RO",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Autonomous Driving (AD) systems rely on AI components to make safety and\ncorrect driving decisions. Unfortunately, today's AI algorithms are known to be\ngenerally vulnerable to adversarial attacks. However, for such AI\ncomponent-level vulnerabilities to be semantically impactful at the system\nlevel, it needs to address non-trivial semantic gaps both (1) from the\nsystem-level attack input spaces to those at AI component level, and (2) from\nAI component-level attack impacts to those at the system level. In this paper,\nwe define such research space as semantic AI security as opposed to generic AI\nsecurity. Over the past 5 years, increasingly more research works are performed\nto tackle such semantic AI security challenges in AD context, which has started\nto show an exponential growth trend.\n  In this paper, we perform the first systematization of knowledge of such\ngrowing semantic AD AI security research space. In total, we collect and\nanalyze 53 such papers, and systematically taxonomize them based on research\naspects critical for the security field. We summarize 6 most substantial\nscientific gaps observed based on quantitative comparisons both vertically\namong existing AD AI security works and horizontally with security works from\nclosely-related domains. With these, we are able to provide insights and\npotential future directions not only at the design level, but also at the\nresearch goal, methodology, and community levels. To address the most critical\nscientific methodology-level gap, we take the initiative to develop an\nopen-source, uniform, and extensible system-driven evaluation platform, named\nPASS, for the semantic AD AI security research community. We also use our\nimplemented platform prototype to showcase the capabilities and benefits of\nsuch a platform using representative semantic AD AI attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 10 Mar 2022 12:00:34 GMT"
      }
    ],
    "update_date": "2022-03-11",
    "authors_parsed": [
      [
        "Shen",
        "Junjie",
        ""
      ],
      [
        "Wang",
        "Ningfei",
        ""
      ],
      [
        "Wan",
        "Ziwen",
        ""
      ],
      [
        "Luo",
        "Yunpeng",
        ""
      ],
      [
        "Sato",
        "Takami",
        ""
      ],
      [
        "Hu",
        "Zhisheng",
        ""
      ],
      [
        "Zhang",
        "Xinyang",
        ""
      ],
      [
        "Guo",
        "Shengjian",
        ""
      ],
      [
        "Zhong",
        "Zhenyu",
        ""
      ],
      [
        "Li",
        "Kang",
        ""
      ],
      [
        "Zhao",
        "Ziming",
        ""
      ],
      [
        "Qiao",
        "Chunming",
        ""
      ],
      [
        "Chen",
        "Qi Alfred",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.05314",
    "publish_date": "2022-03-10"
  },
  {
    "id": "2203.05323",
    "submitter": "Yiqi Zhong",
    "authors": "Yiqi Zhong, Lei Wu, Xianming Liu, Junjun Jiang",
    "title": "Exploiting the Potential of Datasets: A Data-Centric Approach for Model\n  Robustness",
    "comments": "Accepted by the AAAI2022 Workshop on Adversarial Machine Learning and\n  Beyond as a competition paper",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Robustness of deep neural networks (DNNs) to malicious perturbations is a hot\ntopic in trustworthy AI. Existing techniques obtain robust models given fixed\ndatasets, either by modifying model structures, or by optimizing the process of\ninference or training. While significant improvements have been made, the\npossibility of constructing a high-quality dataset for model robustness remain\nunexplored. Follow the campaign of data-centric AI launched by Andrew Ng, we\npropose a novel algorithm for dataset enhancement that works well for many\nexisting DNN models to improve robustness. Transferable adversarial examples\nand 14 kinds of common corruptions are included in our optimized dataset. In\nthe data-centric robust learning competition hosted by Alibaba Group and\nTsinghua University, our algorithm came third out of more than 3000 competitors\nin the first stage while we ranked fourth in the second stage. Our code is\navailable at \\url{https://github.com/hncszyq/tianchi_challenge}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 10 Mar 2022 12:16:32 GMT"
      }
    ],
    "update_date": "2022-03-11",
    "authors_parsed": [
      [
        "Zhong",
        "Yiqi",
        ""
      ],
      [
        "Wu",
        "Lei",
        ""
      ],
      [
        "Liu",
        "Xianming",
        ""
      ],
      [
        "Jiang",
        "Junjun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.05323",
    "publish_date": "2022-03-10"
  },
  {
    "id": "2203.06825",
    "submitter": "Chun Yong Chong",
    "authors": "Muxin Pu, Meng Yi Kuan, Nyee Thoang Lim, Chun Yong Chong, Mei Kuan Lim",
    "title": "Fairness Evaluation in Deepfake Detection Models using Metamorphic\n  Testing",
    "comments": "8 pages, accepted at 7th International Workshop on Metamorphic\n  Testing (MET22)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Fairness of deepfake detectors in the presence of anomalies are not well\ninvestigated, especially if those anomalies are more prominent in either male\nor female subjects. The primary motivation for this work is to evaluate how\ndeepfake detection model behaves under such anomalies. However, due to the\nblack-box nature of deep learning (DL) and artificial intelligence (AI)\nsystems, it is hard to predict the performance of a model when the input data\nis modified. Crucially, if this defect is not addressed properly, it will\nadversely affect the fairness of the model and result in discrimination of\ncertain sub-population unintentionally. Therefore, the objective of this work\nis to adopt metamorphic testing to examine the reliability of the selected\ndeepfake detection model, and how the transformation of input variation places\ninfluence on the output. We have chosen MesoInception-4, a state-of-the-art\ndeepfake detection model, as the target model and makeup as the anomalies.\nMakeups are applied through utilizing the Dlib library to obtain the 68 facial\nlandmarks prior to filling in the RGB values. Metamorphic relations are derived\nbased on the notion that realistic perturbations of the input images, such as\nmakeup, involving eyeliners, eyeshadows, blushes, and lipsticks (which are\ncommon cosmetic appearance) applied to male and female images, should not alter\nthe output of the model by a huge margin. Furthermore, we narrow down the scope\nto focus on revealing potential gender biases in DL and AI systems.\nSpecifically, we are interested to examine whether MesoInception-4 model\nproduces unfair decisions, which should be considered as a consequence of\nrobustness issues. The findings from our work have the potential to pave the\nway for new research directions in the quality assurance and fairness in DL and\nAI systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 Mar 2022 02:44:56 GMT"
      }
    ],
    "update_date": "2022-03-15",
    "authors_parsed": [
      [
        "Pu",
        "Muxin",
        ""
      ],
      [
        "Kuan",
        "Meng Yi",
        ""
      ],
      [
        "Lim",
        "Nyee Thoang",
        ""
      ],
      [
        "Chong",
        "Chun Yong",
        ""
      ],
      [
        "Lim",
        "Mei Kuan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.06825",
    "publish_date": "2022-03-14"
  },
  {
    "id": "2203.10930",
    "submitter": "Anirudh Yadav",
    "authors": "Anirudh Yadav, Ashutosh Upadhyay, S.Sharanya",
    "title": "An integrated Auto Encoder-Block Switching defense approach to prevent\n  adversarial attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  According to recent studies, the vulnerability of state-of-the-art Neural\nNetworks to adversarial input samples has increased drastically. A neural\nnetwork is an intermediate path or technique by which a computer learns to\nperform tasks using Machine learning algorithms. Machine Learning and\nArtificial Intelligence model has become a fundamental aspect of life, such as\nself-driving cars [1], smart home devices, so any vulnerability is a\nsignificant concern. The smallest input deviations can fool these extremely\nliteral systems and deceive their users as well as administrator into\nprecarious situations. This article proposes a defense algorithm that utilizes\nthe combination of an auto-encoder [3] and block-switching architecture.\nAuto-coder is intended to remove any perturbations found in input images\nwhereas the block switching method is used to make it more robust against\nWhite-box attacks. The attack is planned using FGSM [9] model, and the\nsubsequent counter-attack by the proposed architecture will take place thereby\ndemonstrating the feasibility and security delivered by the algorithm.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 11 Mar 2022 10:58:24 GMT"
      }
    ],
    "update_date": "2022-03-22",
    "authors_parsed": [
      [
        "Yadav",
        "Anirudh",
        ""
      ],
      [
        "Upadhyay",
        "Ashutosh",
        ""
      ],
      [
        "Sharanya",
        "S.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.10930",
    "publish_date": "2022-03-11"
  },
  {
    "id": "2203.15044",
    "submitter": "Dilrukshi Gamage",
    "authors": "Dilrukshi Gamage, Piyush Ghasiya, Vamshi Krishna Bonagiri, Mark E\n  Whiting, Kazutoshi Sasahara",
    "title": "Are Deepfakes Concerning? Analyzing Conversations of Deepfakes on Reddit\n  and Exploring Societal Implications",
    "comments": "19pgs, CHI22: CHI Conference on Human Factors in Computing Systems\n  April 29-May 5, 2022 New Orleans, LA, USA",
    "journal-ref": null,
    "doi": "10.1145/3491102.3517446",
    "report-no": null,
    "categories": "cs.HC cs.SI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Deepfakes are synthetic content generated using advanced deep learning and AI\ntechnologies. The advancement of technology has created opportunities for\nanyone to create and share deepfakes much easier. This may lead to societal\nconcerns based on how communities engage with it. However, there is limited\nresearch available to understand how communities perceive deepfakes. We\nexamined deepfake conversations on Reddit from 2018 to 2021 -- including major\ntopics and their temporal changes as well as implications of these\nconversations. Using a mixed-method approach -- topic modeling and qualitative\ncoding, we found 6,638 posts and 86,425 comments discussing concerns of the\nbelievable nature of deepfakes and how platforms moderate them. We also found\nReddit conversations to be pro-deepfake and building a community that supports\ncreating and sharing deepfake artifacts and building a marketplace regardless\nof the consequences. Possible implications derived from qualitative codes\nindicate that deepfake conversations raise societal concerns. We propose that\nthere are implications for Human Computer Interaction (HCI) to mitigate the\nharm created from deepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 14 Mar 2022 09:36:17 GMT"
      }
    ],
    "update_date": "2022-03-30",
    "authors_parsed": [
      [
        "Gamage",
        "Dilrukshi",
        ""
      ],
      [
        "Ghasiya",
        "Piyush",
        ""
      ],
      [
        "Bonagiri",
        "Vamshi Krishna",
        ""
      ],
      [
        "Whiting",
        "Mark E",
        ""
      ],
      [
        "Sasahara",
        "Kazutoshi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.15044",
    "publish_date": "2022-03-14"
  },
  {
    "id": "2203.16871",
    "submitter": "Richard Adeyemi Ikuesan Dr.",
    "authors": "Avinash Singh, Richard Adeyemi Ikuesan, and Hein Venter",
    "title": "Ransomware Detection using Process Memory",
    "comments": "11 Pages, 3 Figures, and 11 Tables",
    "journal-ref": "17th International Conference on Cyber Warfare and Security,\n  03/2022",
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Ransomware attacks have increased significantly in recent years, causing\ngreat destruction and damage to critical systems and business operations.\nAttackers are unfailingly finding innovative ways to bypass detection\nmechanisms, whichencouraged the adoption of artificial intelligence. However,\nmost research summarizes the general features of AI and induces many false\npositives, as the behavior of ransomware constantly differs to bypass\ndetection. Focusing on the key indicating features of ransomware becomes vital\nas this guides the investigator to the inner workings and main function of\nransomware itself. By utilizing access privileges in process memory, the main\nfunction of the ransomware can be detected more easily and accurately.\nFurthermore, new signatures and fingerprints of ransomware families can be\nidentified to classify novel ransomware attacks correctly. The current research\nused the process memory access privileges of the different memory regions of\nthe behavior of an executable to quickly determine its intent before serious\nharm can occur. To achieve this aim, several well-known machine learning\nalgorithms were explored with an accuracy range of 81.38 to 96.28 percents. The\nstudy thus confirms the feasibility of utilizing process memory as a detection\nmechanism for ransomware.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 31 Mar 2022 08:03:48 GMT"
      }
    ],
    "update_date": "2022-04-01",
    "authors_parsed": [
      [
        "Singh",
        "Avinash",
        ""
      ],
      [
        "Ikuesan",
        "Richard Adeyemi",
        ""
      ],
      [
        "Venter",
        "Hein",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2203.16871",
    "publish_date": "2022-03-31"
  },
  {
    "id": "2204.01738",
    "submitter": "Weikang Li",
    "authors": "Wenhui Ren, Weikang Li, Shibo Xu, Ke Wang, Wenjie Jiang, Feitong Jin,\n  Xuhao Zhu, Jiachen Chen, Zixuan Song, Pengfei Zhang, Hang Dong, Xu Zhang,\n  Jinfeng Deng, Yu Gao, Chuanyu Zhang, Yaozu Wu, Bing Zhang, Qiujiang Guo,\n  Hekang Li, Zhen Wang, Jacob Biamonte, Chao Song, Dong-Ling Deng, H. Wang",
    "title": "Experimental quantum adversarial learning with programmable\n  superconducting qubits",
    "comments": "26 pages, 17 figures, 8 algorithms",
    "journal-ref": "Nature Computational Science 2, 711 (2022)",
    "doi": "10.1038/s43588-022-00351-9",
    "report-no": null,
    "categories": "quant-ph cond-mat.dis-nn cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Quantum computing promises to enhance machine learning and artificial\nintelligence. Different quantum algorithms have been proposed to improve a wide\nspectrum of machine learning tasks. Yet, recent theoretical works show that,\nsimilar to traditional classifiers based on deep classical neural networks,\nquantum classifiers would suffer from the vulnerability problem: adding tiny\ncarefully-crafted perturbations to the legitimate original data samples would\nfacilitate incorrect predictions at a notably high confidence level. This will\npose serious problems for future quantum machine learning applications in\nsafety and security-critical scenarios. Here, we report the first experimental\ndemonstration of quantum adversarial learning with programmable superconducting\nqubits. We train quantum classifiers, which are built upon variational quantum\ncircuits consisting of ten transmon qubits featuring average lifetimes of 150\n$\\mu$s, and average fidelities of simultaneous single- and two-qubit gates\nabove 99.94% and 99.4% respectively, with both real-life images (e.g., medical\nmagnetic resonance imaging scans) and quantum data. We demonstrate that these\nwell-trained classifiers (with testing accuracy up to 99%) can be practically\ndeceived by small adversarial perturbations, whereas an adversarial training\nprocess would significantly enhance their robustness to such perturbations. Our\nresults reveal experimentally a crucial vulnerability aspect of quantum\nlearning systems under adversarial scenarios and demonstrate an effective\ndefense strategy against adversarial attacks, which provide a valuable guide\nfor quantum artificial intelligence applications with both near-term and future\nquantum devices.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Apr 2022 18:00:00 GMT"
      }
    ],
    "update_date": "2022-11-29",
    "authors_parsed": [
      [
        "Ren",
        "Wenhui",
        ""
      ],
      [
        "Li",
        "Weikang",
        ""
      ],
      [
        "Xu",
        "Shibo",
        ""
      ],
      [
        "Wang",
        "Ke",
        ""
      ],
      [
        "Jiang",
        "Wenjie",
        ""
      ],
      [
        "Jin",
        "Feitong",
        ""
      ],
      [
        "Zhu",
        "Xuhao",
        ""
      ],
      [
        "Chen",
        "Jiachen",
        ""
      ],
      [
        "Song",
        "Zixuan",
        ""
      ],
      [
        "Zhang",
        "Pengfei",
        ""
      ],
      [
        "Dong",
        "Hang",
        ""
      ],
      [
        "Zhang",
        "Xu",
        ""
      ],
      [
        "Deng",
        "Jinfeng",
        ""
      ],
      [
        "Gao",
        "Yu",
        ""
      ],
      [
        "Zhang",
        "Chuanyu",
        ""
      ],
      [
        "Wu",
        "Yaozu",
        ""
      ],
      [
        "Zhang",
        "Bing",
        ""
      ],
      [
        "Guo",
        "Qiujiang",
        ""
      ],
      [
        "Li",
        "Hekang",
        ""
      ],
      [
        "Wang",
        "Zhen",
        ""
      ],
      [
        "Biamonte",
        "Jacob",
        ""
      ],
      [
        "Song",
        "Chao",
        ""
      ],
      [
        "Deng",
        "Dong-Ling",
        ""
      ],
      [
        "Wang",
        "H.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.01738",
    "publish_date": "2022-04-04"
  },
  {
    "id": "2204.02255",
    "submitter": "Qianru Zhou",
    "authors": "Qianru Zhou, Rongzhen Li, Lei Xu, Arumugam Nallanathan, Jian Yang,\n  Anmin Fu",
    "title": "Towards Explainable Meta-Learning for DDoS Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.NI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The Internet is the most complex machine humankind has ever built, and how to\ndefense it from intrusions is even more complex. With the ever increasing of\nnew intrusions, intrusion detection task rely on Artificial Intelligence more\nand more. Interpretability and transparency of the machine learning model is\nthe foundation of trust in AI-driven intrusion detection results. Current\ninterpretation Artificial Intelligence technologies in intrusion detection are\nheuristic, which is neither accurate nor sufficient. This paper proposed a\nrigorous interpretable Artificial Intelligence driven intrusion detection\napproach, based on artificial immune system. Details of rigorous interpretation\ncalculation process for a decision tree model is presented. Prime implicant\nexplanation for benign traffic flow are given in detail as rule for negative\nselection of the cyber immune system. Experiments are carried out in real-life\ntraffic.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 5 Apr 2022 14:46:08 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 26 Apr 2022 00:26:17 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 16 Aug 2022 13:46:28 GMT"
      }
    ],
    "update_date": "2022-08-17",
    "authors_parsed": [
      [
        "Zhou",
        "Qianru",
        ""
      ],
      [
        "Li",
        "Rongzhen",
        ""
      ],
      [
        "Xu",
        "Lei",
        ""
      ],
      [
        "Nallanathan",
        "Arumugam",
        ""
      ],
      [
        "Yang",
        "Jian",
        ""
      ],
      [
        "Fu",
        "Anmin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.02255",
    "publish_date": "2022-04-05"
  },
  {
    "id": "2204.08612",
    "submitter": "Chun Yong Chong",
    "authors": "Nyee Thoang Lim, Meng Yi Kuan, Muxin Pu, Mei Kuan Lim, Chun Yong Chong",
    "title": "Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors",
    "comments": "paper accepted at 26TH International Conference on Pattern\n  Recognition (ICPR2022)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deepfakes utilise Artificial Intelligence (AI) techniques to create synthetic\nmedia where the likeness of one person is replaced with another. There are\ngrowing concerns that deepfakes can be maliciously used to create misleading\nand harmful digital contents. As deepfakes become more common, there is a dire\nneed for deepfake detection technology to help spot deepfake media. Present\ndeepfake detection models are able to achieve outstanding accuracy (>90%).\nHowever, most of them are limited to within-dataset scenario, where the same\ndataset is used for training and testing. Most models do not generalise well\nenough in cross-dataset scenario, where models are tested on unseen datasets\nfrom another source. Furthermore, state-of-the-art deepfake detection models\nrely on neural network-based classification models that are known to be\nvulnerable to adversarial attacks. Motivated by the need for a robust deepfake\ndetection model, this study adapts metamorphic testing (MT) principles to help\nidentify potential factors that could influence the robustness of the examined\nmodel, while overcoming the test oracle problem in this domain. Metamorphic\ntesting is specifically chosen as the testing technique as it fits our demand\nto address learning-based system testing with probabilistic outcomes from\nlargely black-box components, based on potentially large input domains. We\nperformed our evaluations on MesoInception-4 and TwoStreamNet models, which are\nthe state-of-the-art deepfake detection models. This study identified makeup\napplication as an adversarial attack that could fool deepfake detectors. Our\nexperimental results demonstrate that both the MesoInception-4 and TwoStreamNet\nmodels degrade in their performance by up to 30\\% when the input data is\nperturbed with makeup.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 19 Apr 2022 02:24:30 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 1 Jun 2022 01:24:47 GMT"
      }
    ],
    "update_date": "2022-06-02",
    "authors_parsed": [
      [
        "Lim",
        "Nyee Thoang",
        ""
      ],
      [
        "Kuan",
        "Meng Yi",
        ""
      ],
      [
        "Pu",
        "Muxin",
        ""
      ],
      [
        "Lim",
        "Mei Kuan",
        ""
      ],
      [
        "Chong",
        "Chun Yong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.08612",
    "publish_date": "2022-06-01"
  },
  {
    "id": "2204.08624",
    "submitter": "German Magay",
    "authors": "German Magai, Anton Ayzenberg",
    "title": "Topology and geometry of data manifold in deep learning",
    "comments": "12 pages, 15 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CV math.AT",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Despite significant advances in the field of deep learning in applications to\nvarious fields, explaining the inner processes of deep learning models remains\nan important and open question. The purpose of this article is to describe and\nsubstantiate the geometric and topological view of the learning process of\nneural networks. Our attention is focused on the internal representation of\nneural networks and on the dynamics of changes in the topology and geometry of\nthe data manifold on different layers. We also propose a method for assessing\nthe generalizing ability of neural networks based on topological descriptors.\nIn this paper, we use the concepts of topological data analysis and intrinsic\ndimension, and we present a wide range of experiments on different datasets and\ndifferent configurations of convolutional neural network architectures. In\naddition, we consider the issue of the geometry of adversarial attacks in the\nclassification task and spoofing attacks on face recognition systems. Our work\nis a contribution to the development of an important area of explainable and\ninterpretable AI through the example of computer vision.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 19 Apr 2022 02:57:47 GMT"
      }
    ],
    "update_date": "2022-04-20",
    "authors_parsed": [
      [
        "Magai",
        "German",
        ""
      ],
      [
        "Ayzenberg",
        "Anton",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.08624",
    "publish_date": "2022-04-19"
  },
  {
    "id": "2204.09391",
    "submitter": "Richard Plant",
    "authors": "Richard Plant, Valerio Giuffrida, Dimitra Gkatzia",
    "title": "You Are What You Write: Preserving Privacy in the Era of Large Language\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Large scale adoption of large language models has introduced a new era of\nconvenient knowledge transfer for a slew of natural language processing tasks.\nHowever, these models also run the risk of undermining user trust by exposing\nunwanted information about the data subjects, which may be extracted by a\nmalicious party, e.g. through adversarial attacks. We present an empirical\ninvestigation into the extent of the personal information encoded into\npre-trained representations by a range of popular models, and we show a\npositive correlation between the complexity of a model, the amount of data used\nin pre-training, and data leakage. In this paper, we present the first wide\ncoverage evaluation and comparison of some of the most popular\nprivacy-preserving algorithms, on a large, multi-lingual dataset on sentiment\nanalysis annotated with demographic information (location, age and gender). The\nresults show since larger and more complex models are more prone to leaking\nprivate information, use of privacy-preserving methods is highly desirable. We\nalso find that highly privacy-preserving technologies like differential privacy\n(DP) can have serious model utility effects, which can be ameliorated using\nhybrid or metric-DP techniques.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 20 Apr 2022 11:12:53 GMT"
      }
    ],
    "update_date": "2022-04-21",
    "authors_parsed": [
      [
        "Plant",
        "Richard",
        ""
      ],
      [
        "Giuffrida",
        "Valerio",
        ""
      ],
      [
        "Gkatzia",
        "Dimitra",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.09391",
    "publish_date": "2022-04-20"
  },
  {
    "id": "2204.09975",
    "submitter": "Mingsong Chen",
    "authors": "Jun Xia, Ting Wang, Jiepin Ding, Xian Wei, Mingsong Chen",
    "title": "Eliminating Backdoor Triggers for Deep Neural Networks Using Attention\n  Relation Graph Distillation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Due to the prosperity of Artificial Intelligence (AI) techniques, more and\nmore backdoors are designed by adversaries to attack Deep Neural Networks\n(DNNs).Although the state-of-the-art method Neural Attention Distillation (NAD)\ncan effectively erase backdoor triggers from DNNs, it still suffers from\nnon-negligible Attack Success Rate (ASR) together with lowered classification\nACCuracy (ACC), since NAD focuses on backdoor defense using attention features\n(i.e., attention maps) of the same order. In this paper, we introduce a novel\nbackdoor defense framework named Attention Relation Graph Distillation (ARGD),\nwhich fully explores the correlation among attention features with different\norders using our proposed Attention Relation Graphs (ARGs). Based on the\nalignment of ARGs between both teacher and student models during knowledge\ndistillation, ARGD can eradicate more backdoor triggers than NAD. Comprehensive\nexperimental results show that, against six latest backdoor attacks, ARGD\noutperforms NAD by up to 94.85% reduction in ASR, while ACC can be improved by\nup to 3.23%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Apr 2022 09:01:22 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 24 Apr 2022 03:22:38 GMT"
      }
    ],
    "update_date": "2022-04-26",
    "authors_parsed": [
      [
        "Xia",
        "Jun",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ],
      [
        "Ding",
        "Jiepin",
        ""
      ],
      [
        "Wei",
        "Xian",
        ""
      ],
      [
        "Chen",
        "Mingsong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.09975",
    "publish_date": "2022-04-21"
  },
  {
    "id": "2204.12848",
    "submitter": "Lukas Schulth",
    "authors": "Lukas Schulth, Christian Berghoff, Matthias Neu",
    "title": "Detecting Backdoor Poisoning Attacks on Deep Neural Networks by Heatmap\n  Clustering",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Predicitions made by neural networks can be fraudulently altered by so-called\npoisoning attacks. A special case are backdoor poisoning attacks. We study\nsuitable detection methods and introduce a new method called Heatmap\nClustering. There, we apply a $k$-means clustering algorithm on heatmaps\nproduced by the state-of-the-art explainable AI method Layer-wise relevance\npropagation. The goal is to separate poisoned from un-poisoned data in the\ndataset. We compare this method with a similar method, called Activation\nClustering, which also uses $k$-means clustering but applies it on the\nactivation of certain hidden layers of the neural network as input. We test the\nperformance of both approaches for standard backdoor poisoning attacks,\nlabel-consistent poisoning attacks and label-consistent poisoning attacks with\nreduced amplitude stickers. We show that Heatmap Clustering consistently\nperforms better than Activation Clustering. However, when considering\nlabel-consistent poisoning attacks, the latter method also yields good\ndetection performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 27 Apr 2022 11:17:23 GMT"
      }
    ],
    "update_date": "2022-04-28",
    "authors_parsed": [
      [
        "Schulth",
        "Lukas",
        ""
      ],
      [
        "Berghoff",
        "Christian",
        ""
      ],
      [
        "Neu",
        "Matthias",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2204.12848",
    "publish_date": "2022-04-27"
  },
  {
    "id": "2205.00047",
    "submitter": "Alex Gaskell",
    "authors": "Alexander Gaskell, Yishu Miao, Lucia Specia, Francesca Toni",
    "title": "Logically Consistent Adversarial Attacks for Soft Theorem Provers",
    "comments": "IJCAI-ECAI 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent efforts within the AI community have yielded impressive results\ntowards \"soft theorem proving\" over natural language sentences using language\nmodels. We propose a novel, generative adversarial framework for probing and\nimproving these models' reasoning capabilities. Adversarial attacks in this\ndomain suffer from the logical inconsistency problem, whereby perturbations to\nthe input may alter the label. Our Logically consistent AdVersarial Attacker,\nLAVA, addresses this by combining a structured generative process with a\nsymbolic solver, guaranteeing logical consistency. Our framework successfully\ngenerates adversarial attacks and identifies global weaknesses common across\nmultiple target models. Our analyses reveal naive heuristics and\nvulnerabilities in these models' reasoning capabilities, exposing an incomplete\ngrasp of logical deduction under logic programs. Finally, in addition to\neffective probing of these models, we show that training on the generated\nsamples improves the target model's performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Apr 2022 19:10:12 GMT"
      }
    ],
    "update_date": "2022-05-03",
    "authors_parsed": [
      [
        "Gaskell",
        "Alexander",
        ""
      ],
      [
        "Miao",
        "Yishu",
        ""
      ],
      [
        "Specia",
        "Lucia",
        ""
      ],
      [
        "Toni",
        "Francesca",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.00047",
    "publish_date": "2022-04-29"
  },
  {
    "id": "2205.01287",
    "submitter": "Chejian Xu",
    "authors": "Boxin Wang, Chejian Xu, Xiangyu Liu, Yu Cheng, Bo Li",
    "title": "SemAttack: Natural Textual Attacks via Different Semantic Spaces",
    "comments": "Published at Findings of NAACL 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent studies show that pre-trained language models (LMs) are vulnerable to\ntextual adversarial attacks. However, existing attack methods either suffer\nfrom low attack success rates or fail to search efficiently in the\nexponentially large perturbation space. We propose an efficient and effective\nframework SemAttack to generate natural adversarial text by constructing\ndifferent semantic perturbation functions. In particular, SemAttack optimizes\nthe generated perturbations constrained on generic semantic spaces, including\ntypo space, knowledge space (e.g., WordNet), contextualized semantic space\n(e.g., the embedding space of BERT clusterings), or the combination of these\nspaces. Thus, the generated adversarial texts are more semantically close to\nthe original inputs. Extensive experiments reveal that state-of-the-art (SOTA)\nlarge-scale LMs (e.g., DeBERTa-v2) and defense strategies (e.g., FreeLB) are\nstill vulnerable to SemAttack. We further demonstrate that SemAttack is general\nand able to generate natural adversarial texts for different languages (e.g.,\nEnglish and Chinese) with high attack success rates. Human evaluations also\nconfirm that our generated adversarial texts are natural and barely affect\nhuman performance. Our code is publicly available at\nhttps://github.com/AI-secure/SemAttack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 May 2022 03:44:03 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 16 May 2022 13:39:11 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 11 Jun 2022 03:42:27 GMT"
      }
    ],
    "update_date": "2022-06-14",
    "authors_parsed": [
      [
        "Wang",
        "Boxin",
        ""
      ],
      [
        "Xu",
        "Chejian",
        ""
      ],
      [
        "Liu",
        "Xiangyu",
        ""
      ],
      [
        "Cheng",
        "Yu",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.01287",
    "publish_date": "2022-05-03"
  },
  {
    "id": "2205.01663",
    "submitter": "Daniel M. Ziegler",
    "authors": "Daniel M. Ziegler, Seraphina Nix, Lawrence Chan, Tim Bauman, Peter\n  Schmidt-Nielsen, Tao Lin, Adam Scherlis, Noa Nabeshima, Ben Weinstein-Raun,\n  Daniel de Haas, Buck Shlegeris, Nate Thomas",
    "title": "Adversarial Training for High-Stakes Reliability",
    "comments": "30 pages, 7 figures, NeurIPS camera-ready",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In the future, powerful AI systems may be deployed in high-stakes settings,\nwhere a single failure could be catastrophic. One technique for improving AI\nsafety in high-stakes settings is adversarial training, which uses an adversary\nto generate examples to train on in order to achieve better worst-case\nperformance.\n  In this work, we used a safe language generation task (``avoid injuries'') as\na testbed for achieving high reliability through adversarial training. We\ncreated a series of adversarial training techniques -- including a tool that\nassists human adversaries -- to find and eliminate failures in a classifier\nthat filters text completions suggested by a generator. In our task, we\ndetermined that we can set very conservative classifier thresholds without\nsignificantly impacting the quality of the filtered outputs. We found that\nadversarial training increased robustness to the adversarial attacks that we\ntrained on -- doubling the time for our contractors to find adversarial\nexamples both with our tool (from 13 to 26 minutes) and without (from 20 to 44\nminutes) -- without affecting in-distribution performance.\n  We hope to see further work in the high-stakes reliability setting, including\nmore powerful tools for enhancing human adversaries and better ways to measure\nhigh levels of reliability, until we can confidently rule out the possibility\nof catastrophic deployment-time failures of powerful models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 May 2022 17:50:06 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 4 May 2022 17:58:20 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 15 Sep 2022 17:36:48 GMT"
      },
      {
        "version": "v4",
        "created": "Fri, 7 Oct 2022 01:30:53 GMT"
      },
      {
        "version": "v5",
        "created": "Thu, 10 Nov 2022 01:02:29 GMT"
      }
    ],
    "update_date": "2022-11-11",
    "authors_parsed": [
      [
        "Ziegler",
        "Daniel M.",
        ""
      ],
      [
        "Nix",
        "Seraphina",
        ""
      ],
      [
        "Chan",
        "Lawrence",
        ""
      ],
      [
        "Bauman",
        "Tim",
        ""
      ],
      [
        "Schmidt-Nielsen",
        "Peter",
        ""
      ],
      [
        "Lin",
        "Tao",
        ""
      ],
      [
        "Scherlis",
        "Adam",
        ""
      ],
      [
        "Nabeshima",
        "Noa",
        ""
      ],
      [
        "Weinstein-Raun",
        "Ben",
        ""
      ],
      [
        "de Haas",
        "Daniel",
        ""
      ],
      [
        "Shlegeris",
        "Buck",
        ""
      ],
      [
        "Thomas",
        "Nate",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.01663",
    "publish_date": "2022-05-04"
  },
  {
    "id": "2205.04002",
    "submitter": "Zhendong Wang",
    "authors": "Zhendong Wang, Yang Hu",
    "title": "Towards a High-performance and Secure Memory System and Architecture for\n  Emerging Applications",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.DC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this dissertation, we propose a memory and computing coordinated\nmethodology to thoroughly exploit the characteristics and capabilities of the\nGPU-based heterogeneous system to effectively optimize applications'\nperformance and privacy. Specifically, 1) we propose a task-aware and dynamic\nmemory management mechanism to co-optimize applications' latency and memory\nfootprint, especially in multitasking scenarios. 2) We propose a novel\nlatency-aware memory management framework that analyzes the application\ncharacteristics and hardware features to reduce applications' initialization\nlatency and response time. 3) We develop a new model extraction attack that\nexplores the vulnerability of the GPU unified memory system to accurately steal\nprivate DNN models. 4) We propose a CPU/GPU Co-Encryption mechanism that can\ndefend against a timing-correlation attack in an integrated CPU/GPU platform to\nprovide a secure execution environment for the edge applications.\n  This dissertation aims at developing a high-performance and secure memory\nsystem and architecture in GPU heterogeneous platforms to deploy emerging\nAI-enabled applications efficiently and safely.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 9 May 2022 01:59:54 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 20 Jul 2022 11:43:47 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 6 Sep 2022 06:11:20 GMT"
      }
    ],
    "update_date": "2022-09-07",
    "authors_parsed": [
      [
        "Wang",
        "Zhendong",
        ""
      ],
      [
        "Hu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.04002",
    "publish_date": "2022-07-20"
  },
  {
    "id": "2205.04411",
    "submitter": "Mingsong Chen",
    "authors": "Zhihao Yue, Jun Xia, Zhiwei Ling, Ming Hu, Ting Wang, Xian Wei,\n  Mingsong Chen",
    "title": "Model-Contrastive Learning for Backdoor Defense",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Due to the popularity of Artificial Intelligence (AI) techniques, we are\nwitnessing an increasing number of backdoor injection attacks that are designed\nto maliciously threaten Deep Neural Networks (DNNs) causing misclassification.\nAlthough there exist various defense methods that can effectively erase\nbackdoors from DNNs, they greatly suffer from both high Attack Success Rate\n(ASR) and a non-negligible loss in Benign Accuracy (BA). Inspired by the\nobservation that a backdoored DNN tends to form a new cluster in its feature\nspaces for poisoned data, in this paper we propose a novel two-stage backdoor\ndefense method, named MCLDef, based on Model-Contrastive Learning (MCL). In the\nfirst stage, our approach performs trigger inversion based on trigger\nsynthesis, where the resultant trigger can be used to generate poisoned data.\nIn the second stage, under the guidance of MCL and our defined positive and\nnegative pairs, MCLDef can purify the backdoored model by pulling the feature\nrepresentations of poisoned data towards those of their clean data\ncounterparts. Due to the shrunken cluster of poisoned data, the backdoor formed\nby end-to-end supervised learning is eliminated. Comprehensive experimental\nresults show that, with only 5% of clean data, MCLDef significantly outperforms\nstate-of-the-art defense methods by up to 95.79% reduction in ASR, while in\nmost cases the BA degradation can be controlled within less than 2%. Our code\nis available at https://github.com/WeCanShow/MCL.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 9 May 2022 16:36:46 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 17 May 2022 10:11:14 GMT"
      }
    ],
    "update_date": "2022-05-18",
    "authors_parsed": [
      [
        "Yue",
        "Zhihao",
        ""
      ],
      [
        "Xia",
        "Jun",
        ""
      ],
      [
        "Ling",
        "Zhiwei",
        ""
      ],
      [
        "Hu",
        "Ming",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ],
      [
        "Wei",
        "Xian",
        ""
      ],
      [
        "Chen",
        "Mingsong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.04411",
    "publish_date": "2022-05-17"
  },
  {
    "id": "2205.12506",
    "submitter": "Fatemehsadat Mireshghallah",
    "authors": "Fatemehsadat Mireshghallah, Archit Uniyal, Tianhao Wang, David Evans,\n  Taylor Berg-Kirkpatrick",
    "title": "Memorization in NLP Fine-tuning Methods",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models are shown to present privacy risks through memorization\nof training data, and several recent works have studied such risks for the\npre-training phase. Little attention, however, has been given to the\nfine-tuning phase and it is not well understood how different fine-tuning\nmethods (such as fine-tuning the full model, the model head, and adapter)\ncompare in terms of memorization risk. This presents increasing concern as the\n\"pre-train and fine-tune\" paradigm proliferates. In this paper, we empirically\nstudy memorization of fine-tuning methods using membership inference and\nextraction attacks, and show that their susceptibility to attacks is very\ndifferent. We observe that fine-tuning the head of the model has the highest\nsusceptibility to attacks, whereas fine-tuning smaller adapters appears to be\nless vulnerable to known extraction attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 May 2022 05:49:31 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 4 Nov 2022 03:13:41 GMT"
      }
    ],
    "update_date": "2022-11-07",
    "authors_parsed": [
      [
        "Mireshghallah",
        "Fatemehsadat",
        ""
      ],
      [
        "Uniyal",
        "Archit",
        ""
      ],
      [
        "Wang",
        "Tianhao",
        ""
      ],
      [
        "Evans",
        "David",
        ""
      ],
      [
        "Berg-Kirkpatrick",
        "Taylor",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.12506",
    "publish_date": "2022-05-25"
  },
  {
    "id": "2205.13681",
    "submitter": "Ali Shirali",
    "authors": "Ali Shirali",
    "title": "Sequential Nature of Recommender Systems Disrupts the Evaluation Process",
    "comments": "To Appear in Third International Workshop on Algorithmic Bias in\n  Search and Recommendation (Bias 2022)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Datasets are often generated in a sequential manner, where the previous\nsamples and intermediate decisions or interventions affect subsequent samples.\nThis is especially prominent in cases where there are significant human-AI\ninteractions, such as in recommender systems. To characterize the importance of\nthis relationship across samples, we propose to use adversarial attacks on\npopular evaluation processes. We present sequence-aware boosting attacks and\nprovide a lower bound on the amount of extra information that can be exploited\nfrom a confidential test set solely based on the order of the observed data. We\nuse real and synthetic data to test our methods and show that the evaluation\nprocess on the MovieLense-100k dataset can be affected by $\\sim1\\%$ which is\nimportant when considering the close competition. Codes are publicly available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 26 May 2022 23:56:29 GMT"
      }
    ],
    "update_date": "2022-05-30",
    "authors_parsed": [
      [
        "Shirali",
        "Ali",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.13681",
    "publish_date": "2022-05-26"
  },
  {
    "id": "2205.14772",
    "submitter": "Zachariah Carmichael",
    "authors": "Zachariah Carmichael, Walter J Scheirer",
    "title": "Unfooling Perturbation-Based Post Hoc Explainers",
    "comments": "Accepted to AAAI-23. See the companion blog post at\n  https://medium.com/@craymichael/noncompliance-in-algorithmic-audits-and-defending-auditors-5b9fbdab2615.\n  9 pages (not including references and supplemental)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Monumental advancements in artificial intelligence (AI) have lured the\ninterest of doctors, lenders, judges, and other professionals. While these\nhigh-stakes decision-makers are optimistic about the technology, those familiar\nwith AI systems are wary about the lack of transparency of its decision-making\nprocesses. Perturbation-based post hoc explainers offer a model agnostic means\nof interpreting these systems while only requiring query-level access. However,\nrecent work demonstrates that these explainers can be fooled adversarially.\nThis discovery has adverse implications for auditors, regulators, and other\nsentinels. With this in mind, several natural questions arise - how can we\naudit these black box systems? And how can we ascertain that the auditee is\ncomplying with the audit in good faith? In this work, we rigorously formalize\nthis problem and devise a defense against adversarial attacks on\nperturbation-based explainers. We propose algorithms for the detection\n(CAD-Detect) and defense (CAD-Defend) of these attacks, which are aided by our\nnovel conditional anomaly detection approach, KNN-CAD. We demonstrate that our\napproach successfully detects whether a black box system adversarially conceals\nits decision-making process and mitigates the adversarial attack on real-world\ndata for the prevalent explainers, LIME and SHAP.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 29 May 2022 21:28:12 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 9 Dec 2022 06:01:53 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 12 Apr 2023 00:13:41 GMT"
      }
    ],
    "update_date": "2023-04-13",
    "authors_parsed": [
      [
        "Carmichael",
        "Zachariah",
        ""
      ],
      [
        "Scheirer",
        "Walter J",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.14772",
    "publish_date": "2023-04-12"
  },
  {
    "id": "2205.15944",
    "submitter": "Bo Luo",
    "authors": "Zeyan Liu, Fengjun Li, Jingqiang Lin, Zhu Li, Bo Luo",
    "title": "Hide and Seek: on the Stealthiness of Attacks against Deep Learning\n  Systems",
    "comments": "To appear in European Symposium on Research in Computer Security\n  (ESORICS) 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the growing popularity of artificial intelligence and machine learning,\na wide spectrum of attacks against deep learning models have been proposed in\nthe literature. Both the evasion attacks and the poisoning attacks attempt to\nutilize adversarially altered samples to fool the victim model to misclassify\nthe adversarial sample. While such attacks claim to be or are expected to be\nstealthy, i.e., imperceptible to human eyes, such claims are rarely evaluated.\nIn this paper, we present the first large-scale study on the stealthiness of\nadversarial samples used in the attacks against deep learning. We have\nimplemented 20 representative adversarial ML attacks on six popular\nbenchmarking datasets. We evaluate the stealthiness of the attack samples using\ntwo complementary approaches: (1) a numerical study that adopts 24 metrics for\nimage similarity or quality assessment; and (2) a user study of 3 sets of\nquestionnaires that has collected 20,000+ annotations from 1,000+ responses.\nOur results show that the majority of the existing attacks introduce\nnonnegligible perturbations that are not stealthy to human eyes. We further\nanalyze the factors that contribute to attack stealthiness. We further examine\nthe correlation between the numerical analysis and the user studies, and\ndemonstrate that some image quality metrics may provide useful guidance in\nattack designs, while there is still a significant gap between assessed image\nquality and visual stealthiness of attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 31 May 2022 16:43:22 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 12 Aug 2022 22:44:59 GMT"
      }
    ],
    "update_date": "2022-08-16",
    "authors_parsed": [
      [
        "Liu",
        "Zeyan",
        ""
      ],
      [
        "Li",
        "Fengjun",
        ""
      ],
      [
        "Lin",
        "Jingqiang",
        ""
      ],
      [
        "Li",
        "Zhu",
        ""
      ],
      [
        "Luo",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2205.15944",
    "publish_date": "2022-05-31"
  },
  {
    "id": "2206.02419",
    "submitter": "Charl Maree",
    "authors": "Charl Maree and Jan Erik Modal and Christian W. Omlin",
    "title": "Towards Responsible AI for Financial Transactions",
    "comments": null,
    "journal-ref": "IEEE SSCI (2020)",
    "doi": "10.1109/SSCI47803.2020.9308456",
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The application of AI in finance is increasingly dependent on the principles\nof responsible AI. These principles - explainability, fairness, privacy,\naccountability, transparency and soundness form the basis for trust in future\nAI systems. In this study, we address the first principle by providing an\nexplanation for a deep neural network that is trained on a mixture of\nnumerical, categorical and textual inputs for financial transaction\nclassification. The explanation is achieved through (1) a feature importance\nanalysis using Shapley additive explanations (SHAP) and (2) a hybrid approach\nof text clustering and decision tree classifiers. We then test the robustness\nof the model by exposing it to a targeted evasion attack, leveraging the\nknowledge we gained about the model through the extracted explanation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Jun 2022 08:29:47 GMT"
      }
    ],
    "update_date": "2022-06-07",
    "authors_parsed": [
      [
        "Maree",
        "Charl",
        ""
      ],
      [
        "Modal",
        "Jan Erik",
        ""
      ],
      [
        "Omlin",
        "Christian W.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.02419",
    "publish_date": "2022-06-06"
  },
  {
    "id": "2206.02670",
    "submitter": "Thomas Hickling",
    "authors": "Thomas Hickling, Nabil Aouf and Phillippa Spencer",
    "title": "Robust Adversarial Attacks Detection based on Explainable Deep\n  Reinforcement Learning For UAV Guidance and Planning",
    "comments": "13 pages, 16 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR cs.RO",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents\noperating in public are increasing. Adopting AI-based techniques and, more\nspecifically, Deep Learning (DL) approaches to control and guide these UAVs can\nbe beneficial in terms of performance but can add concerns regarding the safety\nof those techniques and their vulnerability against adversarial attacks.\nConfusion in the agent's decision-making process caused by these attacks can\nseriously affect the safety of the UAV. This paper proposes an innovative\napproach based on the explainability of DL methods to build an efficient\ndetector that will protect these DL schemes and the UAVs adopting them from\nattacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for\nguidance and planning. The agent is trained with a Deep Deterministic Policy\nGradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that\nutilises Artificial Potential Field (APF) to improve training times and\nobstacle avoidance performance. A simulated environment for UAV explainable\nDRL-based planning and guidance, including obstacles and adversarial attacks,\nis built. The adversarial attacks are generated by the Basic Iterative Method\n(BIM) algorithm and reduced obstacle course completion rates from 97\\% to 35\\%.\nTwo adversarial attack detectors are proposed to counter this reduction. The\nfirst one is a Convolutional Neural Network Adversarial Detector (CNN-AD),\nwhich achieves accuracy in the detection of 80\\%. The second detector utilises\na Long Short Term Memory (LSTM) network. It achieves an accuracy of 91\\% with\nfaster computing times compared to the CNN-AD, allowing for real-time\nadversarial detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Jun 2022 15:16:10 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 7 Jun 2022 10:18:03 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 17 Mar 2023 22:03:57 GMT"
      },
      {
        "version": "v4",
        "created": "Tue, 20 Jun 2023 16:07:31 GMT"
      }
    ],
    "update_date": "2023-06-21",
    "authors_parsed": [
      [
        "Hickling",
        "Thomas",
        ""
      ],
      [
        "Aouf",
        "Nabil",
        ""
      ],
      [
        "Spencer",
        "Phillippa",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.02670",
    "publish_date": "2022-06-07"
  },
  {
    "id": "2206.04793",
    "submitter": "Shruti Patil",
    "authors": "Rucha Shinde, Shruti Patil, Ketan Kotecha, Vidyasagar Potdar,\n  Ganeshsree Selvachandran, Ajith Abraham",
    "title": "Securing AI-based Healthcare Systems using Blockchain Technology: A\n  State-of-the-Art Systematic Literature Review and Future Research Directions",
    "comments": "44 Pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Healthcare systems are increasingly incorporating Artificial Intelligence\ninto their systems, but it is not a solution for all difficulties. AI's\nextraordinary potential is being held back by challenges such as a lack of\nmedical datasets for training AI models, adversarial attacks, and a lack of\ntrust due to its black box working style. We explored how blockchain technology\ncan improve the reliability and trustworthiness of AI-based healthcare. This\npaper has conducted a Systematic Literature Review to explore the\nstate-of-the-art research studies conducted in healthcare applications\ndeveloped with different AI techniques and Blockchain Technology. This\nsystematic literature review proceeds with three different paths as natural\nlanguage processing-based healthcare systems, computer vision-based healthcare\nsystems and acoustic AI-based healthcare systems. We found that 1) Defence\ntechniques for adversarial attacks on AI are available for specific kind of\nattacks and even adversarial training is AI based technique which in further\nprone to different attacks. 2) Blockchain can address security and privacy\nissues in healthcare fraternity. 3) Medical data verification and user\nprovenance can be enabled with Blockchain. 4) Blockchain can protect\ndistributed learning on heterogeneous medical data. 5) The issues like single\npoint of failure, non-transparency in healthcare systems can be resolved with\nBlockchain. Nevertheless, it has been identified that research is at the\ninitial stage. As a result, we have synthesized a conceptual framework using\nBlockchain Technology for AI-based healthcare applications that considers the\nneeds of each NLP, Computer Vision, and Acoustic AI application. A global\nsolution for all sort of adversarial attacks on AI based healthcare. However,\nthis technique has significant limits and challenges that need to be addressed\nin future studies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 May 2022 14:54:00 GMT"
      }
    ],
    "update_date": "2022-06-13",
    "authors_parsed": [
      [
        "Shinde",
        "Rucha",
        ""
      ],
      [
        "Patil",
        "Shruti",
        ""
      ],
      [
        "Kotecha",
        "Ketan",
        ""
      ],
      [
        "Potdar",
        "Vidyasagar",
        ""
      ],
      [
        "Selvachandran",
        "Ganeshsree",
        ""
      ],
      [
        "Abraham",
        "Ajith",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.04793",
    "publish_date": "2022-05-30"
  },
  {
    "id": "2206.06854",
    "submitter": "Franck MAMALET",
    "authors": "Mathieu Serrurier (IRIT, UT), Franck Mamalet (UT), Thomas Fel (UT),\n  Louis B\\'ethune (UT3, UT, IRIT), Thibaut Boissin (UT)",
    "title": "On the explainable properties of 1-Lipschitz Neural Networks: An Optimal\n  Transport Perspective",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.CV cs.LG stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Input gradients have a pivotal role in a variety of applications, including\nadversarial attack algorithms for evaluating model robustness, explainable AI\ntechniques for generating Saliency Maps, and counterfactual explanations.\nHowever, Saliency Maps generated by traditional neural networks are often noisy\nand provide limited insights. In this paper, we demonstrate that, on the\ncontrary, the Saliency Maps of 1-Lipschitz neural networks, learnt with the\ndual loss of an optimal transportation problem, exhibit desirable XAI\nproperties: They are highly concentrated on the essential parts of the image\nwith low noise, significantly outperforming state-of-the-art explanation\napproaches across various models and metrics. We also prove that these maps\nalign unprecedentedly well with human explanations on ImageNet. To explain the\nparticularly beneficial properties of the Saliency Map for such models, we\nprove this gradient encodes both the direction of the transportation plan and\nthe direction towards the nearest adversarial attack. Following the gradient\ndown to the decision boundary is no longer considered an adversarial attack,\nbut rather a counterfactual explanation that explicitly transports the input\nfrom one class to another. Thus, Learning with such a loss jointly optimizes\nthe classification objective and the alignment of the gradient , i.e. the\nSaliency Map, to the transportation plan direction. These networks were\npreviously known to be certifiably robust by design, and we demonstrate that\nthey scale well for large problems and models, and are tailored for\nexplainability using a fast and straightforward method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Jun 2022 13:49:08 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 22 Jun 2023 12:33:20 GMT"
      }
    ],
    "update_date": "2023-06-23",
    "authors_parsed": [
      [
        "Serrurier",
        "Mathieu",
        "",
        "IRIT, UT"
      ],
      [
        "Mamalet",
        "Franck",
        "",
        "UT"
      ],
      [
        "Fel",
        "Thomas",
        "",
        "UT"
      ],
      [
        "B\u00e9thune",
        "Louis",
        "",
        "UT3, UT, IRIT"
      ],
      [
        "Boissin",
        "Thibaut",
        "",
        "UT"
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.06854",
    "publish_date": "2022-06-14"
  },
  {
    "id": "2206.08304",
    "submitter": "Yijun Bian",
    "authors": "Abhijith Sharma, Yijun Bian, Phil Munz, Apurva Narayan",
    "title": "Adversarial Patch Attacks and Defences in Vision-Based Tasks: A Survey",
    "comments": "A. Sharma and Y. Bian share equal contribution",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks in deep learning models, especially for safety-critical\nsystems, are gaining more and more attention in recent years, due to the lack\nof trust in the security and robustness of AI models. Yet the more primitive\nadversarial attacks might be physically infeasible or require some resources\nthat are hard to access like the training data, which motivated the emergence\nof patch attacks. In this survey, we provide a comprehensive overview to cover\nexisting techniques of adversarial patch attacks, aiming to help interested\nresearchers quickly catch up with the progress in this field. We also discuss\nexisting techniques for developing detection and defences against adversarial\npatches, aiming to help the community better understand this field and its\napplications in the real world.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Jun 2022 17:06:47 GMT"
      }
    ],
    "update_date": "2022-06-17",
    "authors_parsed": [
      [
        "Sharma",
        "Abhijith",
        ""
      ],
      [
        "Bian",
        "Yijun",
        ""
      ],
      [
        "Munz",
        "Phil",
        ""
      ],
      [
        "Narayan",
        "Apurva",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.08304",
    "publish_date": "2022-06-16"
  },
  {
    "id": "2206.12725",
    "submitter": "Gavin Hartnett S",
    "authors": "Gavin S. Hartnett, Li Ang Zhang, Caolionn O'Connell, Andrew J. Lohn,\n  Jair Aguirre",
    "title": "Empirical Evaluation of Physical Adversarial Patch Attacks Against\n  Overhead Object Detection Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial patches are images designed to fool otherwise well-performing\nneural network-based computer vision models. Although these attacks were\ninitially conceived of and studied digitally, in that the raw pixel values of\nthe image were perturbed, recent work has demonstrated that these attacks can\nsuccessfully transfer to the physical world. This can be accomplished by\nprinting out the patch and adding it into scenes of newly captured images or\nvideo footage. In this work we further test the efficacy of adversarial patch\nattacks in the physical world under more challenging conditions. We consider\nobject detection models trained on overhead imagery acquired through aerial or\nsatellite cameras, and we test physical adversarial patches inserted into\nscenes of a desert environment. Our main finding is that it is far more\ndifficult to successfully implement the adversarial patch attacks under these\nconditions than in the previously considered conditions. This has important\nimplications for AI safety as the real-world threat posed by adversarial\nexamples may be overstated.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 25 Jun 2022 20:05:11 GMT"
      }
    ],
    "update_date": "2022-06-28",
    "authors_parsed": [
      [
        "Hartnett",
        "Gavin S.",
        ""
      ],
      [
        "Zhang",
        "Li Ang",
        ""
      ],
      [
        "O'Connell",
        "Caolionn",
        ""
      ],
      [
        "Lohn",
        "Andrew J.",
        ""
      ],
      [
        "Aguirre",
        "Jair",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.12725",
    "publish_date": "2022-06-25"
  },
  {
    "id": "2206.14200",
    "submitter": "Wenhao Zhang",
    "authors": "Minh Cao, Tianqi Zhao, Yanxun Li, Wenhao Zhang, Peyman Benharash,\n  Ramin Ramezani",
    "title": "ECG Heartbeat classification using deep transfer learning with\n  Convolutional Neural Network and STFT technique",
    "comments": "14 pages, 5 figures, 4 tables, submitted to The 4th International\n  Conference on Computing and Data Science (CONF-CDS 2022)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Electrocardiogram (ECG) is a simple non-invasive measure to identify\nheart-related issues such as irregular heartbeats known as arrhythmias. While\nartificial intelligence and machine learning is being utilized in a wide range\nof healthcare related applications and datasets, many arrhythmia classifiers\nusing deep learning methods have been proposed in recent years. However, sizes\nof the available datasets from which to build and assess machine learning\nmodels is often very small and the lack of well-annotated public ECG datasets\nis evident. In this paper, we propose a deep transfer learning framework that\nis aimed to perform classification on a small size training dataset. The\nproposed method is to fine-tune a general-purpose image classifier ResNet-18\nwith MIT-BIH arrhythmia dataset in accordance with the AAMI EC57 standard. This\npaper further investigates many existing deep learning models that have failed\nto avoid data leakage against AAMI recommendations. We compare how different\ndata split methods impact the model performance. This comparison study implies\nthat future work in arrhythmia classification should follow the AAMI EC57\nstandard when using any including MIT-BIH arrhythmia dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 28 Jun 2022 04:57:02 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Jul 2022 02:43:21 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 7 Jul 2022 22:35:18 GMT"
      }
    ],
    "update_date": "2022-07-11",
    "authors_parsed": [
      [
        "Cao",
        "Minh",
        ""
      ],
      [
        "Zhao",
        "Tianqi",
        ""
      ],
      [
        "Li",
        "Yanxun",
        ""
      ],
      [
        "Zhang",
        "Wenhao",
        ""
      ],
      [
        "Benharash",
        "Peyman",
        ""
      ],
      [
        "Ramezani",
        "Ramin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2206.14200",
    "publish_date": "2022-07-05"
  },
  {
    "id": "2207.00091",
    "submitter": "Lionel Nganyewou Tidjon",
    "authors": "Lionel Nganyewou Tidjon and Foutse Khomh",
    "title": "Threat Assessment in Machine Learning based Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Machine learning is a field of artificial intelligence (AI) that is becoming\nessential for several critical systems, making it a good target for threat\nactors. Threat actors exploit different Tactics, Techniques, and Procedures\n(TTPs) against the confidentiality, integrity, and availability of Machine\nLearning (ML) systems. During the ML cycle, they exploit adversarial TTPs to\npoison data and fool ML-based systems. In recent years, multiple security\npractices have been proposed for traditional systems but they are not enough to\ncope with the nature of ML-based systems. In this paper, we conduct an\nempirical study of threats reported against ML-based systems with the aim to\nunderstand and characterize the nature of ML threats and identify common\nmitigation strategies. The study is based on 89 real-world ML attack scenarios\nfrom the MITRE's ATLAS database, the AI Incident Database, and the literature;\n854 ML repositories from the GitHub search and the Python Packaging Advisory\ndatabase, selected based on their reputation. Attacks from the AI Incident\nDatabase and the literature are used to identify vulnerabilities and new types\nof threats that were not documented in ATLAS. Results show that convolutional\nneural networks were one of the most targeted models among the attack\nscenarios. ML repositories with the largest vulnerability prominence include\nTensorFlow, OpenCV, and Notebook. In this paper, we also report the most\nfrequent vulnerabilities in the studied ML repositories, the most targeted ML\nphases and models, the most used TTPs in ML phases and attack scenarios. This\ninformation is particularly important for red/blue teams to better conduct\nattacks/defenses, for practitioners to prevent threats during ML development,\nand for researchers to develop efficient defense mechanisms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 30 Jun 2022 20:19:50 GMT"
      }
    ],
    "update_date": "2022-07-04",
    "authors_parsed": [
      [
        "Tidjon",
        "Lionel Nganyewou",
        ""
      ],
      [
        "Khomh",
        "Foutse",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.00091",
    "publish_date": "2022-06-30"
  },
  {
    "id": "2207.00295",
    "submitter": "Kaspar Rosager Ludvigsen",
    "authors": "Kaspar Rosager Ludvigsen, Shishir Nagaraja, Angela Daly",
    "title": "The Dangers of Computational Law and Cybersecurity; Perspectives from\n  Engineering and the AI Act",
    "comments": "17 pages, 1 figure",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Computational Law has begun taking the role in society which has been\npredicted for some time. Automated decision-making and systems which assist\nusers are now used in various jurisdictions, but with this maturity come\ncertain caveats. Computational Law exists on the platforms which enable it, in\nthis case digital systems, which means that it inherits the same flaws.\nCybersecurity addresses these potential weaknesses. In this paper we go through\nknown issues and discuss them in the various levels, from design to the\nphysical realm. We also look at machine-learning specific adversarial problems.\nAdditionally, we make certain considerations regarding computational law and\nexisting and future legislation. Finally, we present three recommendations\nwhich are necessary for computational law to function globally, and which\nfollow ideas in safety and security engineering. As indicated, we find that\ncomputational law must seriously consider that not only does it face the same\nrisks as other types of software and computer systems, but that failures within\nit may cause financial or physical damage, as well as injustice. Consequences\nof Computational Legal systems failing are greater than if they were merely\nsoftware and hardware. If the system employs machine-learning, it must take\nnote of the very specific dangers which this brings, of which data poisoning is\nthe classic example. Computational law must also be explicitly legislated for,\nwhich we show is not the case currently in the EU, and this is also true for\nthe cybersecurity aspects that will be relevant to it. But there is great hope\nin EU's proposed AI Act, which makes an important attempt at taking the\nspecific problems which Computational Law bring into the legal sphere. Our\nrecommendations for Computational Law and Cybersecurity are: Accommodation of\nthreats, adequate use, and that humans remain in the centre of their\ndeployment.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 Jul 2022 09:42:11 GMT"
      }
    ],
    "update_date": "2022-07-04",
    "authors_parsed": [
      [
        "Ludvigsen",
        "Kaspar Rosager",
        ""
      ],
      [
        "Nagaraja",
        "Shishir",
        ""
      ],
      [
        "Daly",
        "Angela",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.00295",
    "publish_date": "2022-07-01"
  },
  {
    "id": "2207.02108",
    "submitter": "Benjamin Marais",
    "authors": "Benjamin Marais and Tony Quertier and St\\'ephane Morucci",
    "title": "AI-based Malware and Ransomware Detection Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Cybercrime is one of the major digital threats of this century. In\nparticular, ransomware attacks have significantly increased, resulting in\nglobal damage costs of tens of billion dollars. In this paper, we train and\ntest different Machine Learning and Deep Learning models for malware detection,\nmalware classification and ransomware detection. We introduce a novel and\nflexible solution that combines two optimized models for malware and ransomware\ndetection. Our results demonstrate some improvements both in terms of detection\nperformances and flexibility. In particular, our combined models pave the way\nfor easier future enhancements using specialized and thus interchangeable\ndetection modules.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 5 Jul 2022 15:22:13 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 28 Nov 2022 10:17:21 GMT"
      }
    ],
    "update_date": "2022-11-29",
    "authors_parsed": [
      [
        "Marais",
        "Benjamin",
        ""
      ],
      [
        "Quertier",
        "Tony",
        ""
      ],
      [
        "Morucci",
        "St\u00e9phane",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.02108",
    "publish_date": "2022-07-05"
  },
  {
    "id": "2207.03895",
    "submitter": "Haripriya Harikumar",
    "authors": "Haripriya Harikumar, Santu Rana, Kien Do, Sunil Gupta, Wei Zong, Willy\n  Susilo, Svetha Venkastesh",
    "title": "Defense Against Multi-target Trojan Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Adversarial attacks on deep learning-based models pose a significant threat\nto the current AI infrastructure. Among them, Trojan attacks are the hardest to\ndefend against. In this paper, we first introduce a variation of the Badnet\nkind of attacks that introduces Trojan backdoors to multiple target classes and\nallows triggers to be placed anywhere in the image. The former makes it more\npotent and the latter makes it extremely easy to carry out the attack in the\nphysical space. The state-of-the-art Trojan detection methods fail with this\nthreat model. To defend against this attack, we first introduce a trigger\nreverse-engineering mechanism that uses multiple images to recover a variety of\npotential triggers. We then propose a detection mechanism by measuring the\ntransferability of such recovered triggers. A Trojan trigger will have very\nhigh transferability i.e. they make other images also go to the same class. We\nstudy many practical advantages of our attack method and then demonstrate the\ndetection performance using a variety of image datasets. The experimental\nresults show the superior detection performance of our method over the\nstate-of-the-arts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Jul 2022 13:29:13 GMT"
      }
    ],
    "update_date": "2022-07-11",
    "authors_parsed": [
      [
        "Harikumar",
        "Haripriya",
        ""
      ],
      [
        "Rana",
        "Santu",
        ""
      ],
      [
        "Do",
        "Kien",
        ""
      ],
      [
        "Gupta",
        "Sunil",
        ""
      ],
      [
        "Zong",
        "Wei",
        ""
      ],
      [
        "Susilo",
        "Willy",
        ""
      ],
      [
        "Venkastesh",
        "Svetha",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.03895",
    "publish_date": "2022-07-08"
  },
  {
    "id": "2207.07863",
    "submitter": "Jonathan Pan",
    "authors": "Jonathan Pan",
    "title": "Deep Set Classifier for Financial Forensics: An application to detect\n  money laundering",
    "comments": "To be improved. Will be uploading a revised paper with major changes",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Financial forensics has an important role in the field of finance to detect\nand investigate the occurrence of finance related crimes like money laundering.\nHowever, as with other forms of criminal activities, the forensics analysis of\nsuch activities is a complex undertaking with attempts by the adversaries to\nconstantly upgrade their ability to evade detection. Also, the extent of the\nvolume and complexity of financial activities or transactions further\ncomplicates the task of performing financial forensics. Machine Learning or\nArtificial Intelligence algorithms could be used to deal with such\ncomplexities. However, the challenge of limitedly available labeled datasets\nespecially with fraudulent activities limits the means to develop efficient\nalgorithms. Additionally, the complexity of defining precise search patterns of\nevasive fraudulent transactions further complicates this challenge. In this\npaper, we developed a novel deep set classifier algorithm based on meta\nlearning and applied it to deal with the complexity deriving patterns of\ninterest with sample of limitedly labelled transactions to detect fraudulent\ncryptocurrency money laundering transactions. We a unique approach to train our\nmodel with progressive provision of samples and the test result exceeds leading\nresearch algorithms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 16 Jul 2022 07:37:10 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 9 Mar 2023 04:18:11 GMT"
      }
    ],
    "update_date": "2023-03-10",
    "authors_parsed": [
      [
        "Pan",
        "Jonathan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.07863",
    "publish_date": "2023-03-09"
  },
  {
    "id": "2207.09031",
    "submitter": "Christopher Wiedeman",
    "authors": "Christopher Wiedeman and Ge Wang",
    "title": "Decorrelative Network Architecture for Robust Electrocardiogram\n  Classification",
    "comments": "16 pages, 6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Artificial intelligence has made great progress in medical data analysis, but\nthe lack of robustness and trustworthiness has kept these methods from being\nwidely deployed. As it is not possible to train networks that are accurate in\nall situations, models must recognize situations where they cannot operate\nconfidently. Bayesian deep learning methods sample the model parameter space to\nestimate uncertainty, but these parameters are often subject to the same\nvulnerabilities, which can be exploited by adversarial attacks. We propose a\nnovel ensemble approach based on feature decorrelation and Fourier partitioning\nfor teaching networks diverse complementary features, reducing the chance of\nperturbation-based fooling. We test our approach on electrocardiogram\nclassification, demonstrating superior accuracy confidence measurement, on a\nvariety of adversarial attacks. For example, on our ensemble trained with both\ndecorrelation and Fourier partitioning scored a 50.18% inference accuracy and\n48.01% uncertainty accuracy (area under the curve) on {\\epsilon} = 50 projected\ngradient descent attacks, while a conventionally trained ensemble scored 21.1%\nand 30.31% on these metrics respectively. Our approach does not require\nexpensive optimization with adversarial samples and can be scaled to large\nproblems. These methods can easily be applied to other tasks for more robust\nand trustworthy models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 19 Jul 2022 02:36:36 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 8 Dec 2022 20:29:47 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 22 Feb 2023 22:27:40 GMT"
      }
    ],
    "update_date": "2023-02-24",
    "authors_parsed": [
      [
        "Wiedeman",
        "Christopher",
        ""
      ],
      [
        "Wang",
        "Ge",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.09031",
    "publish_date": "2023-02-22"
  },
  {
    "id": "2207.10802",
    "submitter": "Bargav Jayaraman",
    "authors": "Bargav Jayaraman, Esha Ghosh, Melissa Chase, Sambuddha Roy, Wei Dai,\n  David Evans",
    "title": "Combing for Credentials: Active Pattern Extraction from Smart Reply",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Pre-trained large language models, such as GPT\\nobreakdash-2 and BERT, are\noften fine-tuned to achieve state-of-the-art performance on a downstream task.\nOne natural example is the ``Smart Reply'' application where a pre-trained\nmodel is tuned to provide suggested responses for a given query message. Since\nthe tuning data is often sensitive data such as emails or chat transcripts, it\nis important to understand and mitigate the risk that the model leaks its\ntuning data. We investigate potential information leakage vulnerabilities in a\ntypical Smart Reply pipeline. We consider a realistic setting where the\nadversary can only interact with the underlying model through a front-end\ninterface that constrains what types of queries can be sent to the model.\nPrevious attacks do not work in these settings, but require the ability to send\nunconstrained queries directly to the model. Even when there are no constraints\non the queries, previous attacks typically require thousands, or even millions,\nof queries to extract useful information, while our attacks can extract\nsensitive data in just a handful of queries. We introduce a new type of active\nextraction attack that exploits canonical patterns in text containing sensitive\ndata. We show experimentally that it is possible for an adversary to extract\nsensitive user information present in the training data, even in realistic\nsettings where all interactions with the model must go through a front-end that\nlimits the types of queries. We explore potential mitigation strategies and\ndemonstrate empirically how differential privacy appears to be a reasonably\neffective defense mechanism to such pattern extraction attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 14 Jul 2022 05:03:56 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 7 Sep 2022 22:10:50 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 2 Sep 2023 22:33:09 GMT"
      }
    ],
    "update_date": "2023-09-06",
    "authors_parsed": [
      [
        "Jayaraman",
        "Bargav",
        ""
      ],
      [
        "Ghosh",
        "Esha",
        ""
      ],
      [
        "Chase",
        "Melissa",
        ""
      ],
      [
        "Roy",
        "Sambuddha",
        ""
      ],
      [
        "Dai",
        "Wei",
        ""
      ],
      [
        "Evans",
        "David",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.10802",
    "publish_date": "2022-09-07"
  },
  {
    "id": "2207.10809",
    "submitter": "Hans Dermot Doran",
    "authors": "Hans Dermot Doran",
    "title": "Security and Safety Aspects of AI in Industry Applications",
    "comments": "As presented at the Embedded World Conference, Nuremberg, 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this relatively informal discussion-paper we summarise issues in the\ndomains of safety and security in machine learning that will affect industry\nsectors in the next five to ten years. Various products using neural network\nclassification, most often in vision related applications but also in\npredictive maintenance, have been researched and applied in real-world\napplications in recent years. Nevertheless, reports of underlying problems in\nboth safety and security related domains, for instance adversarial attacks have\nunsettled early adopters and are threatening to hinder wider scale adoption of\nthis technology. The problem for real-world applicability lies in being able to\nassess the risk of applying these technologies. In this discussion-paper we\ndescribe the process of arriving at a machine-learnt neural network classifier\npointing out safety and security vulnerabilities in that workflow, citing\nrelevant research where appropriate.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 16 Jul 2022 16:41:00 GMT"
      }
    ],
    "update_date": "2022-07-25",
    "authors_parsed": [
      [
        "Doran",
        "Hans Dermot",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.10809",
    "publish_date": "2022-07-16"
  },
  {
    "id": "2207.13064",
    "submitter": "Trisha Mittal",
    "authors": "Trisha Mittal, Ritwik Sinha, Viswanathan Swaminathan, John Collomosse,\n  Dinesh Manocha",
    "title": "Video Manipulations Beyond Faces: A Dataset with Human-Machine Analysis",
    "comments": "Accepted to WACV2023 - Workshop on Manipulation, Adversarial, and\n  Presentation Attacks in Biometrics",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.MM",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As tools for content editing mature, and artificial intelligence (AI) based\nalgorithms for synthesizing media grow, the presence of manipulated content\nacross online media is increasing. This phenomenon causes the spread of\nmisinformation, creating a greater need to distinguish between ``real'' and\n``manipulated'' content. To this end, we present VideoSham, a dataset\nconsisting of 826 videos (413 real and 413 manipulated). Many of the existing\ndeepfake datasets focus exclusively on two types of facial manipulations --\nswapping with a different subject's face or altering the existing face.\nVideoSham, on the other hand, contains more diverse, context-rich, and\nhuman-centric, high-resolution videos manipulated using a combination of 6\ndifferent spatial and temporal attacks. Our analysis shows that\nstate-of-the-art manipulation detection algorithms only work for a few specific\nattacks and do not scale well on VideoSham. We performed a user study on Amazon\nMechanical Turk with 1200 participants to understand if they can differentiate\nbetween the real and manipulated videos in VideoSham. Finally, we dig deeper\ninto the strengths and weaknesses of performances by humans and SOTA-algorithms\nto identify gaps that need to be filled with better AI algorithms. We present\nthe dataset at https://github.com/adobe-research/VideoSham-dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 26 Jul 2022 17:39:04 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 27 Jul 2022 02:50:45 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 8 Dec 2022 01:47:06 GMT"
      }
    ],
    "update_date": "2022-12-09",
    "authors_parsed": [
      [
        "Mittal",
        "Trisha",
        ""
      ],
      [
        "Sinha",
        "Ritwik",
        ""
      ],
      [
        "Swaminathan",
        "Viswanathan",
        ""
      ],
      [
        "Collomosse",
        "John",
        ""
      ],
      [
        "Manocha",
        "Dinesh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2207.13064",
    "publish_date": "2022-07-27"
  },
  {
    "id": "2208.01919",
    "submitter": "Sicheng Zhang",
    "authors": "Sicheng Zhang (1), Jiarun Yu (1), Zhida Bao (1), Shiwen Mao (2), Yun\n  Lin (1) ((1) College of Information and Communication Engineering, Harbin\n  Engineering University, Harbin, (2) Department of Electrical & Computer\n  Engineering, Auburn University, Auburn)",
    "title": "Spectrum Focused Frequency Adversarial Attacks for Automatic Modulation\n  Classification",
    "comments": "6 pages, 9 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.SI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial intelligence (AI) technology has provided a potential solution for\nautomatic modulation recognition (AMC). Unfortunately, AI-based AMC models are\nvulnerable to adversarial examples, which seriously threatens the efficient,\nsecure and trusted application of AI in AMC. This issue has attracted the\nattention of researchers. Various studies on adversarial attacks and defenses\nevolve in a spiral. However, the existing adversarial attack methods are all\ndesigned in the time domain. They introduce more high-frequency components in\nthe frequency domain, due to abrupt updates in the time domain. For this issue,\nfrom the perspective of frequency domain, we propose a spectrum focused\nfrequency adversarial attacks (SFFAA) for AMC model, and further draw on the\nidea of meta-learning, propose a Meta-SFFAA algorithm to improve the\ntransferability in the black-box attacks. Extensive experiments, qualitative\nand quantitative metrics demonstrate that the proposed algorithm can\nconcentrate the adversarial energy on the spectrum where the signal is located,\nsignificantly improve the adversarial attack performance while maintaining the\nconcealment in the frequency domain.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 3 Aug 2022 08:54:56 GMT"
      }
    ],
    "update_date": "2022-08-04",
    "authors_parsed": [
      [
        "Zhang",
        "Sicheng",
        ""
      ],
      [
        "Yu",
        "Jiarun",
        ""
      ],
      [
        "Bao",
        "Zhida",
        ""
      ],
      [
        "Mao",
        "Shiwen",
        ""
      ],
      [
        "Lin",
        "Yun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.01919",
    "publish_date": "2022-08-03"
  },
  {
    "id": "2208.02710",
    "submitter": "Aras Asaad",
    "authors": "Tahir Hassan, Aras Asaad, Dashti Ali, Sabah Jassim",
    "title": "Artificial Image Tampering Distorts Spatial Distribution of Texture\n  Landmarks and Quality Characteristics",
    "comments": "6 pages, 7 figures, 3 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV math.AT",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Advances in AI based computer vision has led to a significant growth in\nsynthetic image generation and artificial image tampering with serious\nimplications for unethical exploitations that undermine person identification\nand could make render AI predictions less explainable.Morphing, Deepfake and\nother artificial generation of face photographs undermine the reliability of\nface biometrics authentication using different electronic ID documents.Morphed\nface photographs on e-passports can fool automated border control systems and\nhuman guards.This paper extends our previous work on using the persistent\nhomology (PH) of texture landmarks to detect morphing attacks.We demonstrate\nthat artificial image tampering distorts the spatial distribution of texture\nlandmarks (i.e. their PH) as well as that of a set of image quality\ncharacteristics.We shall demonstrate that the tamper caused distortion of these\ntwo slim feature vectors provide significant potentials for building\nexplainable (Handcrafted) tamper detectors with low error rates and suitable\nfor implementation on constrained devices.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 4 Aug 2022 15:13:00 GMT"
      }
    ],
    "update_date": "2022-08-05",
    "authors_parsed": [
      [
        "Hassan",
        "Tahir",
        ""
      ],
      [
        "Asaad",
        "Aras",
        ""
      ],
      [
        "Ali",
        "Dashti",
        ""
      ],
      [
        "Jassim",
        "Sabah",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.02710",
    "publish_date": "2022-08-04"
  },
  {
    "id": "2208.05845",
    "submitter": "Ying Xu",
    "authors": "Ying Xu, Philipp Terh\\\"orst, Kiran Raja, Marius Pedersen",
    "title": "A Comprehensive Analysis of AI Biases in DeepFake Detection With\n  Massively Annotated Databases",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CY cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  In recent years, image and video manipulations with Deepfake have become a\nsevere concern for security and society. Many detection models and datasets\nhave been proposed to detect Deepfake data reliably. However, there is an\nincreased concern that these models and training databases might be biased and,\nthus, cause Deepfake detectors to fail. In this work, we investigate the bias\nissue caused by public Deepfake datasets by (a) providing large-scale\ndemographic and non-demographic attribute annotations of 47 different\nattributes for five popular Deepfake datasets and (b) comprehensively analysing\nAI-bias of three state-of-the-art Deepfake detection backbone models on these\ndatasets. The investigation analyses the influence of a large variety of\ndistinctive attributes (from over 65M labels) on the detection performance,\nincluding demographic (age, gender, ethnicity) and non-demographic (hair, skin,\naccessories, etc.) information. The results indicate that investigated\ndatabases lack diversity and, more importantly, show that the utilised Deepfake\ndetection backbone models are strongly biased towards many investigated\nattributes. The Deepfake detection backbone methods, which are trained with\nbiased datasets, might output incorrect detection results, thereby leading to\ngeneralisability, fairness, and security issues. We hope that the findings of\nthis study and the annotation databases will help to evaluate and mitigate bias\nin future Deepfake detection techniques. The annotation datasets and the\ncorresponding code are publicly available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 11 Aug 2022 14:28:21 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 23 Mar 2023 17:56:52 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 20 Sep 2023 16:42:49 GMT"
      }
    ],
    "update_date": "2023-09-21",
    "authors_parsed": [
      [
        "Xu",
        "Ying",
        ""
      ],
      [
        "Terh\u00f6rst",
        "Philipp",
        ""
      ],
      [
        "Raja",
        "Kiran",
        ""
      ],
      [
        "Pedersen",
        "Marius",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.05845",
    "publish_date": "2023-09-20"
  },
  {
    "id": "2208.07476",
    "submitter": "Sudip Mittal",
    "authors": "Chuyen Nguyen and Caleb Morgan and Sudip Mittal",
    "title": "CTI4AI: Threat Intelligence Generation and Sharing after Red Teaming AI\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As the practicality of Artificial Intelligence (AI) and Machine Learning (ML)\nbased techniques grow, there is an ever increasing threat of adversarial\nattacks. There is a need to red team this ecosystem to identify system\nvulnerabilities, potential threats, characterize properties that will enhance\nsystem robustness, and encourage the creation of effective defenses. A\nsecondary need is to share this AI security threat intelligence between\ndifferent stakeholders like, model developers, users, and AI/ML security\nprofessionals. In this paper, we create and describe a prototype system CTI4AI,\nto overcome the need to methodically identify and share AI/ML specific\nvulnerabilities and threat intelligence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 16 Aug 2022 00:16:58 GMT"
      }
    ],
    "update_date": "2022-08-17",
    "authors_parsed": [
      [
        "Nguyen",
        "Chuyen",
        ""
      ],
      [
        "Morgan",
        "Caleb",
        ""
      ],
      [
        "Mittal",
        "Sudip",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.07476",
    "publish_date": "2022-08-16"
  },
  {
    "id": "2208.08071",
    "submitter": "Jong-Wouk Kim",
    "authors": "Jong-Wouk Kim, Yang-Sae Moon, Mi-Jung Choi",
    "title": "An Efficient Multi-Step Framework for Malware Packing Identification",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Malware developers use combinations of techniques such as compression,\nencryption, and obfuscation to bypass anti-virus software. Malware with\nanti-analysis technologies can bypass AI-based anti-virus software and malware\nanalysis tools. Therefore, classifying pack files is one of the big challenges.\nProblems arise if the malware classifiers learn packers' features, not those of\nmalware. Training the models with unintended erroneous data turn into poisoning\nattacks, adversarial attacks, and evasion attacks. Therefore, researchers\nshould consider packing to build appropriate malware classifier models. In this\npaper, we propose a multi-step framework for classifying and identifying packed\nsamples which consists of pseudo-optimal feature selection, machine\nlearning-based classifiers, and packer identification steps. In the first step,\nwe use the CART algorithm and the permutation importance to preselect important\n20 features. In the second step, each model learns 20 preselected features for\nclassifying the packed files with the highest performance. As a result, the\nXGBoost, which learned the features preselected by XGBoost with the permutation\nimportance, showed the highest performance of any other experiment scenarios\nwith an accuracy of 99.67%, an F1-Score of 99.46%, and an area under the curve\n(AUC) of 99.98%. In the third step, we propose a new approach that can identify\npackers only for samples classified as Well-Known Packed.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Aug 2022 05:03:14 GMT"
      }
    ],
    "update_date": "2022-08-18",
    "authors_parsed": [
      [
        "Kim",
        "Jong-Wouk",
        ""
      ],
      [
        "Moon",
        "Yang-Sae",
        ""
      ],
      [
        "Choi",
        "Mi-Jung",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.08071",
    "publish_date": "2022-08-17"
  },
  {
    "id": "2208.08524",
    "submitter": "Yisroel Mirsky Dr.",
    "authors": "Yisroel Mirsky",
    "title": "DF-Captcha: A Deepfake Captcha for Preventing Fake Calls",
    "comments": "A draft academic paper based on and protected by the provisional\n  patent submitted January 1st 2022 under provisional Number 63/302,086. arXiv\n  admin note: text overlap with arXiv:2004.11138",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Social engineering (SE) is a form of deception that aims to trick people into\ngiving access to data, information, networks and even money. For decades SE has\nbeen a key method for attackers to gain access to an organization, virtually\nskipping all lines of defense. Attackers also regularly use SE to scam innocent\npeople by making threatening phone calls which impersonate an authority or by\nsending infected emails which look like they have been sent from a loved one.\nSE attacks will likely remain a top attack vector for criminals because humans\nare the weakest link in cyber security.\n  Unfortunately, the threat will only get worse now that a new technology\ncalled deepfakes as arrived. A deepfake is believable media (e.g., videos)\ncreated by an AI. Although the technology has mostly been used to swap the\nfaces of celebrities, it can also be used to `puppet' different personas.\nRecently, researchers have shown how this technology can be deployed in\nreal-time to clone someone's voice in a phone call or reenact a face in a video\ncall. Given that any novice user can download this technology to use it, it is\nno surprise that criminals have already begun to monetize it to perpetrate\ntheir SE attacks.\n  In this paper, we propose a lightweight application which can protect\norganizations and individuals from deepfake SE attacks. Through a challenge and\nresponse approach, we leverage the technical and theoretical limitations of\ndeepfake technologies to expose the attacker. Existing defence solutions are\ntoo heavy as an end-point solution and can be evaded by a dynamic attacker. In\ncontrast, our approach is lightweight and breaks the reactive arms race,\nputting the attacker at a disadvantage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Aug 2022 20:40:54 GMT"
      }
    ],
    "update_date": "2022-08-19",
    "authors_parsed": [
      [
        "Mirsky",
        "Yisroel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.08524",
    "publish_date": "2022-08-17"
  },
  {
    "id": "2208.09953",
    "submitter": "Jiayi Lian",
    "authors": "J. Lian, K. Choi, B. Veeramani, A. Hu, L. Freeman, E. Bowen, X. Deng",
    "title": "Do-AIQ: A Design-of-Experiment Approach to Quality Evaluation of AI\n  Mislabel Detection Algorithm",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "stat.ML cs.LG stat.AP stat.ME",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The quality of Artificial Intelligence (AI) algorithms is of significant\nimportance for confidently adopting algorithms in various applications such as\ncybersecurity, healthcare, and autonomous driving. This work presents a\nprincipled framework of using a design-of-experimental approach to\nsystematically evaluate the quality of AI algorithms, named as Do-AIQ.\nSpecifically, we focus on investigating the quality of the AI mislabel data\nalgorithm against data poisoning. The performance of AI algorithms is affected\nby hyperparameters in the algorithm and data quality, particularly, data\nmislabeling, class imbalance, and data types. To evaluate the quality of the AI\nalgorithms and obtain a trustworthy assessment on the quality of the\nalgorithms, we establish a design-of-experiment framework to construct an\nefficient space-filling design in a high-dimensional constraint space and\ndevelop an effective surrogate model using additive Gaussian process to enable\nthe emulation of the quality of AI algorithms. Both theoretical and numerical\nstudies are conducted to justify the merits of the proposed framework. The\nproposed framework can set an exemplar for AI algorithm to enhance the AI\nassurance of robustness, reproducibility, and transparency.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 21 Aug 2022 19:47:41 GMT"
      }
    ],
    "update_date": "2022-08-23",
    "authors_parsed": [
      [
        "Lian",
        "J.",
        ""
      ],
      [
        "Choi",
        "K.",
        ""
      ],
      [
        "Veeramani",
        "B.",
        ""
      ],
      [
        "Hu",
        "A.",
        ""
      ],
      [
        "Freeman",
        "L.",
        ""
      ],
      [
        "Bowen",
        "E.",
        ""
      ],
      [
        "Deng",
        "X.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.09953",
    "publish_date": "2022-08-21"
  },
  {
    "id": "2208.10279",
    "submitter": "Ferhat Ozgur Catak",
    "authors": "Ferhat Ozgur Catak, Murat Kuzlu, Evren Catak, Umit Cali, Ozgur Guler",
    "title": "Defensive Distillation based Adversarial Attacks Mitigation Method for\n  Channel Estimation using Deep Learning Models in Next-Generation Wireless\n  Networks",
    "comments": "13 Pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Future wireless networks (5G and beyond) are the vision of forthcoming\ncellular systems, connecting billions of devices and people together. In the\nlast decades, cellular networks have been dramatically growth with advanced\ntelecommunication technologies for high-speed data transmission, high cell\ncapacity, and low latency. The main goal of those technologies is to support a\nwide range of new applications, such as virtual reality, metaverse, telehealth,\nonline education, autonomous and flying vehicles, smart cities, smart grids,\nadvanced manufacturing, and many more. The key motivation of NextG networks is\nto meet the high demand for those applications by improving and optimizing\nnetwork functions. Artificial Intelligence (AI) has a high potential to achieve\nthese requirements by being integrated in applications throughout all layers of\nthe network. However, the security concerns on network functions of NextG using\nAI-based models, i.e., model poising, have not been investigated deeply.\nTherefore, it needs to design efficient mitigation techniques and secure\nsolutions for NextG networks using AI-based methods. This paper proposes a\ncomprehensive vulnerability analysis of deep learning (DL)-based channel\nestimation models trained with the dataset obtained from MATLAB's 5G toolbox\nfor adversarial attacks and defensive distillation-based mitigation methods.\nThe adversarial attacks produce faulty results by manipulating trained DL-based\nmodels for channel estimation in NextG networks, while making models more\nrobust against any attacks through mitigation methods. This paper also presents\nthe performance of the proposed defensive distillation mitigation method for\neach adversarial attack against the channel estimation model. The results\nindicated that the proposed mitigation method can defend the DL-based channel\nestimation models against adversarial attacks in NextG networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Aug 2022 08:35:36 GMT"
      }
    ],
    "update_date": "2022-08-23",
    "authors_parsed": [
      [
        "Catak",
        "Ferhat Ozgur",
        ""
      ],
      [
        "Kuzlu",
        "Murat",
        ""
      ],
      [
        "Catak",
        "Evren",
        ""
      ],
      [
        "Cali",
        "Umit",
        ""
      ],
      [
        "Guler",
        "Ozgur",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.10279",
    "publish_date": "2022-08-12"
  },
  {
    "id": "2208.10913",
    "submitter": "Virginia Franqueira",
    "authors": "Enes Altuncu, Virginia N. L. Franqueira and Shujun Li",
    "title": "Deepfake: Definitions, Performance Metrics and Standards, Datasets and\n  Benchmarks, and a Meta-Review",
    "comments": "31 pages; study completed by end of July 2021",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Recent advancements in AI, especially deep learning, have contributed to a\nsignificant increase in the creation of new realistic-looking synthetic media\n(video, image, and audio) and manipulation of existing media, which has led to\nthe creation of the new term ``deepfake''. Based on both the research\nliterature and resources in English and in Chinese, this paper gives a\ncomprehensive overview of deepfake, covering multiple important aspects of this\nemerging concept, including 1) different definitions, 2) commonly used\nperformance metrics and standards, and 3) deepfake-related datasets,\nchallenges, competitions and benchmarks. In addition, the paper also reports a\nmeta-review of 12 selected deepfake-related survey papers published in 2020 and\n2021, focusing not only on the mentioned aspects, but also on the analysis of\nkey challenges and recommendations. We believe that this paper is the most\ncomprehensive review of deepfake in terms of aspects covered, and the first one\ncovering both the English and Chinese literature and sources.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 21 Aug 2022 17:31:31 GMT"
      }
    ],
    "update_date": "2022-08-24",
    "authors_parsed": [
      [
        "Altuncu",
        "Enes",
        ""
      ],
      [
        "Franqueira",
        "Virginia N. L.",
        ""
      ],
      [
        "Li",
        "Shujun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.10913",
    "publish_date": "2022-08-21"
  },
  {
    "id": "2208.10993",
    "submitter": "Daniel Mauricio Jimenez Gutierrez",
    "authors": "Daniel Mauricio Jimenez Gutierrez, Hafiz Muuhammad Hassan, Lorella\n  Landi, Andrea Vitaletti and Ioannis Chatzigiannakis",
    "title": "Application of federated learning techniques for arrhythmia\n  classification using 12-lead ECG signals",
    "comments": "Preprint of International Symposium on Algorithmic Aspects of Cloud\n  Computing (ALGOCLOUD) 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Artificial Intelligence-based (AI) analysis of large, curated medical\ndatasets is promising for providing early detection, faster diagnosis, and more\neffective treatment using low-power Electrocardiography (ECG) monitoring\ndevices information. However, accessing sensitive medical data from diverse\nsources is highly restricted since improper use, unsafe storage, or data\nleakage could violate a person's privacy. This work uses a Federated Learning\n(FL) privacy-preserving methodology to train AI models over heterogeneous sets\nof high-definition ECG from 12-lead sensor arrays collected from six\nheterogeneous sources. We evaluated the capacity of the resulting models to\nachieve equivalent performance compared to state-of-the-art models trained in a\nCentralized Learning (CL) fashion. Moreover, we assessed the performance of our\nsolution over Independent and Identical distributed (IID) and non-IID federated\ndata. Our methodology involves machine learning techniques based on Deep Neural\nNetworks and Long-Short-Term Memory models. It has a robust data preprocessing\npipeline with feature engineering, selection, and data balancing techniques.\nOur AI models demonstrated comparable performance to models trained using CL,\nIID, and non-IID approaches. They showcased advantages in reduced complexity\nand faster training time, making them well-suited for cloud-edge architectures.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 Aug 2022 14:21:16 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 2 Sep 2023 09:32:02 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 5 Jan 2024 16:32:10 GMT"
      }
    ],
    "update_date": "2024-01-08",
    "authors_parsed": [
      [
        "Gutierrez",
        "Daniel Mauricio Jimenez",
        ""
      ],
      [
        "Hassan",
        "Hafiz Muuhammad",
        ""
      ],
      [
        "Landi",
        "Lorella",
        ""
      ],
      [
        "Vitaletti",
        "Andrea",
        ""
      ],
      [
        "Chatzigiannakis",
        "Ioannis",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.10993",
    "publish_date": "2022-08-23"
  },
  {
    "id": "2208.14127",
    "submitter": "Fangqi Li",
    "authors": "Fangqi Li, Shilin Wang, Yun Zhu",
    "title": "Solving the Capsulation Attack against Backdoor-based Deep Neural\n  Network Watermarks by Reversing Triggers",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Backdoor-based watermarking schemes were proposed to protect the intellectual\nproperty of artificial intelligence models, especially deep neural networks,\nunder the black-box setting. Compared with ordinary backdoors, backdoor-based\nwatermarks need to digitally incorporate the owner's identity, which fact adds\nextra requirements to the trigger generation and verification programs.\nMoreover, these concerns produce additional security risks after the\nwatermarking scheme has been published for as a forensics tool or the owner's\nevidence has been eavesdropped on. This paper proposes the capsulation attack,\nan efficient method that can invalidate most established backdoor-based\nwatermarking schemes without sacrificing the pirated model's functionality. By\nencapsulating the deep neural network with a rule-based or Bayes filter, an\nadversary can block ownership probing and reject the ownership verification. We\npropose a metric, CAScore, to measure a backdoor-based watermarking scheme's\nsecurity against the capsulation attack. This paper also proposes a new\nbackdoor-based deep neural network watermarking scheme that is secure against\nthe capsulation attack by reversing the encoding process and randomizing the\nexposure of triggers.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 30 Aug 2022 10:23:23 GMT"
      }
    ],
    "update_date": "2022-08-31",
    "authors_parsed": [
      [
        "Li",
        "Fangqi",
        ""
      ],
      [
        "Wang",
        "Shilin",
        ""
      ],
      [
        "Zhu",
        "Yun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.14127",
    "publish_date": "2022-08-30"
  },
  {
    "id": "2208.14302",
    "submitter": "Samet Bayram",
    "authors": "Samet Bayram and Kenneth Barner",
    "title": "A Black-Box Attack on Optical Character Recognition Systems",
    "comments": "11 Pages, CVMI-2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Adversarial machine learning is an emerging area showing the vulnerability of\ndeep learning models. Exploring attack methods to challenge state of the art\nartificial intelligence (A.I.) models is an area of critical concern. The\nreliability and robustness of such A.I. models are one of the major concerns\nwith an increasing number of effective adversarial attack methods.\nClassification tasks are a major vulnerable area for adversarial attacks. The\nmajority of attack strategies are developed for colored or gray-scaled images.\nConsequently, adversarial attacks on binary image recognition systems have not\nbeen sufficiently studied. Binary images are simple two possible pixel-valued\nsignals with a single channel. The simplicity of binary images has a\nsignificant advantage compared to colored and gray scaled images, namely\ncomputation efficiency. Moreover, most optical character recognition systems\n(O.C.R.s), such as handwritten character recognition, plate number\nidentification, and bank check recognition systems, use binary images or\nbinarization in their processing steps. In this paper, we propose a simple yet\nefficient attack method, Efficient Combinatorial Black-box Adversarial Attack,\non binary image classifiers. We validate the efficiency of the attack technique\non two different data sets and three classification networks, demonstrating its\nperformance. Furthermore, we compare our proposed method with state-of-the-art\nmethods regarding advantages and disadvantages as well as applicability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 30 Aug 2022 14:36:27 GMT"
      }
    ],
    "update_date": "2022-08-31",
    "authors_parsed": [
      [
        "Bayram",
        "Samet",
        ""
      ],
      [
        "Barner",
        "Kenneth",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2208.14302",
    "publish_date": "2022-08-30"
  },
  {
    "id": "2209.02128",
    "submitter": "Hezekiah Branch",
    "authors": "Hezekiah J. Branch, Jonathan Rodriguez Cefalu, Jeremy McHugh, Leyla\n  Hujer, Aditya Bahl, Daniel del Castillo Iglesias, Ron Heichman, Ramesh\n  Darwishi",
    "title": "Evaluating the Susceptibility of Pre-Trained Language Models via\n  Handcrafted Adversarial Examples",
    "comments": "10 pages, 1 figure, 3 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Recent advances in the development of large language models have resulted in\npublic access to state-of-the-art pre-trained language models (PLMs), including\nGenerative Pre-trained Transformer 3 (GPT-3) and Bidirectional Encoder\nRepresentations from Transformers (BERT). However, evaluations of PLMs, in\npractice, have shown their susceptibility to adversarial attacks during the\ntraining and fine-tuning stages of development. Such attacks can result in\nerroneous outputs, model-generated hate speech, and the exposure of users'\nsensitive information. While existing research has focused on adversarial\nattacks during either the training or the fine-tuning of PLMs, there is a\ndeficit of information on attacks made between these two development phases. In\nthis work, we highlight a major security vulnerability in the public release of\nGPT-3 and further investigate this vulnerability in other state-of-the-art\nPLMs. We restrict our work to pre-trained models that have not undergone\nfine-tuning. Further, we underscore token distance-minimized perturbations as\nan effective adversarial approach, bypassing both supervised and unsupervised\nquality measures. Following this approach, we observe a significant decrease in\ntext classification quality when evaluating for semantic similarity.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Sep 2022 20:29:17 GMT"
      }
    ],
    "update_date": "2022-09-07",
    "authors_parsed": [
      [
        "Branch",
        "Hezekiah J.",
        ""
      ],
      [
        "Cefalu",
        "Jonathan Rodriguez",
        ""
      ],
      [
        "McHugh",
        "Jeremy",
        ""
      ],
      [
        "Hujer",
        "Leyla",
        ""
      ],
      [
        "Bahl",
        "Aditya",
        ""
      ],
      [
        "Iglesias",
        "Daniel del Castillo",
        ""
      ],
      [
        "Heichman",
        "Ron",
        ""
      ],
      [
        "Darwishi",
        "Ramesh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2209.02128",
    "publish_date": "2022-09-05"
  },
  {
    "id": "2209.02167",
    "submitter": "Stephen Casper",
    "authors": "Stephen Casper, Taylor Killian, Gabriel Kreiman, Dylan Hadfield-Menell",
    "title": "Red Teaming with Mind Reading: White-Box Adversarial Policies Against RL\n  Agents",
    "comments": "Code is available at\n  https://github.com/thestephencasper/lm_white_box_attacks",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Adversarial examples can be useful for identifying vulnerabilities in AI\nsystems before they are deployed. In reinforcement learning (RL), adversarial\npolicies can be developed by training an adversarial agent to minimize a target\nagent's rewards. Prior work has studied black-box versions of these attacks\nwhere the adversary only observes the world state and treats the target agent\nas any other part of the environment. However, this does not take into account\nadditional structure in the problem. In this work, we study white-box\nadversarial policies and show that having access to a target agent's internal\nstate can be useful for identifying its vulnerabilities. We make two\ncontributions. (1) We introduce white-box adversarial policies where an\nattacker observes both a target's internal state and the world state at each\ntimestep. We formulate ways of using these policies to attack agents in\n2-player games and text-generating language models. (2) We demonstrate that\nthese policies can achieve higher initial and asymptotic performance against a\ntarget agent than black-box controls. Code is available at\nhttps://github.com/thestephencasper/lm_white_box_attacks\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Sep 2022 23:53:15 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 13 Jun 2023 20:26:40 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 13 Oct 2023 22:28:22 GMT"
      }
    ],
    "update_date": "2023-10-17",
    "authors_parsed": [
      [
        "Casper",
        "Stephen",
        ""
      ],
      [
        "Killian",
        "Taylor",
        ""
      ],
      [
        "Kreiman",
        "Gabriel",
        ""
      ],
      [
        "Hadfield-Menell",
        "Dylan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2209.02167",
    "publish_date": "2023-06-13"
  },
  {
    "id": "2209.02299",
    "submitter": "Thanh Tam Nguyen",
    "authors": "Thanh Tam Nguyen and Thanh Trung Huynh and Phi Le Nguyen and Alan\n  Wee-Chung Liew and Hongzhi Yin and Quoc Viet Hung Nguyen",
    "title": "A Survey of Machine Unlearning",
    "comments": "discuss new and recent works as well as proof-reading",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Today, computer systems hold large amounts of personal data. Yet while such\nan abundance of data allows breakthroughs in artificial intelligence, and\nespecially machine learning (ML), its existence can be a threat to user\nprivacy, and it can weaken the bonds of trust between humans and AI. Recent\nregulations now require that, on request, private information about a user must\nbe removed from both computer systems and from ML models, i.e. ``the right to\nbe forgotten''). While removing data from back-end databases should be\nstraightforward, it is not sufficient in the AI context as ML models often\n`remember' the old data. Contemporary adversarial attacks on trained models\nhave proven that we can learn whether an instance or an attribute belonged to\nthe training data. This phenomenon calls for a new paradigm, namely machine\nunlearning, to make ML models forget about particular data. It turns out that\nrecent works on machine unlearning have not been able to completely solve the\nproblem due to the lack of common frameworks and resources. Therefore, this\npaper aspires to present a comprehensive examination of machine unlearning's\nconcepts, scenarios, methods, and applications. Specifically, as a category\ncollection of cutting-edge studies, the intention behind this article is to\nserve as a comprehensive resource for researchers and practitioners seeking an\nintroduction to machine unlearning and its formulations, design criteria,\nremoval requests, algorithms, and applications. In addition, we aim to\nhighlight the key findings, current trends, and new research areas that have\nnot yet featured the use of machine unlearning but could benefit greatly from\nit. We hope this survey serves as a valuable resource for ML researchers and\nthose seeking to innovate privacy technologies. Our resources are publicly\navailable at https://github.com/tamlhp/awesome-machine-unlearning.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 6 Sep 2022 08:51:53 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 7 Sep 2022 10:36:35 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 8 Sep 2022 16:52:04 GMT"
      },
      {
        "version": "v4",
        "created": "Mon, 12 Sep 2022 12:49:14 GMT"
      },
      {
        "version": "v5",
        "created": "Fri, 21 Oct 2022 12:34:14 GMT"
      }
    ],
    "update_date": "2022-10-24",
    "authors_parsed": [
      [
        "Nguyen",
        "Thanh Tam",
        ""
      ],
      [
        "Huynh",
        "Thanh Trung",
        ""
      ],
      [
        "Nguyen",
        "Phi Le",
        ""
      ],
      [
        "Liew",
        "Alan Wee-Chung",
        ""
      ],
      [
        "Yin",
        "Hongzhi",
        ""
      ],
      [
        "Nguyen",
        "Quoc Viet Hung",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2209.02299",
    "publish_date": "2022-09-07"
  },
  {
    "id": "2209.04930",
    "submitter": "Ehsan Nowroozi",
    "authors": "Ehsan Nowroozi, Mohammadreza Mohammadi, Pargol Golmohammadi, Yassine\n  Mekdad, Mauro Conti and Selcuk Uluagac",
    "title": "Resisting Deep Learning Models Against Adversarial Attack\n  Transferability via Feature Randomization",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CY cs.LG cs.NI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In the past decades, the rise of artificial intelligence has given us the\ncapabilities to solve the most challenging problems in our day-to-day lives,\nsuch as cancer prediction and autonomous navigation. However, these\napplications might not be reliable if not secured against adversarial attacks.\nIn addition, recent works demonstrated that some adversarial examples are\ntransferable across different models. Therefore, it is crucial to avoid such\ntransferability via robust models that resist adversarial manipulations. In\nthis paper, we propose a feature randomization-based approach that resists\neight adversarial attacks targeting deep learning models in the testing phase.\nOur novel approach consists of changing the training strategy in the target\nnetwork classifier and selecting random feature samples. We consider the\nattacker with a Limited-Knowledge and Semi-Knowledge conditions to undertake\nthe most prevalent types of adversarial attacks. We evaluate the robustness of\nour approach using the well-known UNSW-NB15 datasets that include realistic and\nsynthetic attacks. Afterward, we demonstrate that our strategy outperforms the\nexisting state-of-the-art approach, such as the Most Powerful Attack, which\nconsists of fine-tuning the network model against specific adversarial attacks.\nFinally, our experimental results show that our methodology can secure the\ntarget network and resists adversarial attack transferability by over 60%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 11 Sep 2022 20:14:12 GMT"
      }
    ],
    "update_date": "2022-09-13",
    "authors_parsed": [
      [
        "Nowroozi",
        "Ehsan",
        ""
      ],
      [
        "Mohammadi",
        "Mohammadreza",
        ""
      ],
      [
        "Golmohammadi",
        "Pargol",
        ""
      ],
      [
        "Mekdad",
        "Yassine",
        ""
      ],
      [
        "Conti",
        "Mauro",
        ""
      ],
      [
        "Uluagac",
        "Selcuk",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2209.04930",
    "publish_date": "2022-09-11"
  },
  {
    "id": "2209.13007",
    "submitter": "Ferhat Ozgur Catak",
    "authors": "Ferhat Ozgur Catak and Murat Kuzlu and Salih Sarp and Evren Catak and\n  Umit Cali",
    "title": "Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for\n  Cellular Network Signals",
    "comments": "IEEE GLOBECOM 2022 Publication",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Cellular networks (LTE, 5G, and beyond) are dramatically growing with high\ndemand from consumers and more promising than the other wireless networks with\nadvanced telecommunication technologies. The main goal of these networks is to\nconnect billions of devices, systems, and users with high-speed data\ntransmission, high cell capacity, and low latency, as well as to support a wide\nrange of new applications, such as virtual reality, metaverse, telehealth,\nonline education, autonomous and flying vehicles, advanced manufacturing, and\nmany more. To achieve these goals, spectrum sensing has been paid more\nattention, along with new approaches using artificial intelligence (AI) methods\nfor spectrum management in cellular networks. This paper provides a\nvulnerability analysis of spectrum sensing approaches using AI-based semantic\nsegmentation models for identifying cellular network signals under adversarial\nattacks with and without defensive distillation methods. The results showed\nthat mitigation methods can significantly reduce the vulnerabilities of\nAI-based spectrum sensing models against adversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 27 Sep 2022 11:14:47 GMT"
      }
    ],
    "update_date": "2022-09-28",
    "authors_parsed": [
      [
        "Catak",
        "Ferhat Ozgur",
        ""
      ],
      [
        "Kuzlu",
        "Murat",
        ""
      ],
      [
        "Sarp",
        "Salih",
        ""
      ],
      [
        "Catak",
        "Evren",
        ""
      ],
      [
        "Cali",
        "Umit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2209.13007",
    "publish_date": "2022-09-27"
  },
  {
    "id": "2210.02191",
    "submitter": "Huimin Zeng",
    "authors": "Huimin Zeng, Zhenrui Yue, Yang Zhang, Ziyi Kou, Lanyu Shang, Dong Wang",
    "title": "On Attacking Out-Domain Uncertainty Estimation in Deep Neural Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In many applications with real-world consequences, it is crucial to develop\nreliable uncertainty estimation for the predictions made by the AI decision\nsystems. Targeting at the goal of estimating uncertainty, various deep neural\nnetwork (DNN) based uncertainty estimation algorithms have been proposed.\nHowever, the robustness of the uncertainty returned by these algorithms has not\nbeen systematically explored. In this work, to raise the awareness of the\nresearch community on robust uncertainty estimation, we show that\nstate-of-the-art uncertainty estimation algorithms could fail catastrophically\nunder our proposed adversarial attack despite their impressive performance on\nuncertainty estimation. In particular, we aim at attacking the out-domain\nuncertainty estimation: under our attack, the uncertainty model would be fooled\nto make high-confident predictions for the out-domain data, which they\noriginally would have rejected. Extensive experimental results on various\nbenchmark image datasets show that the uncertainty estimated by\nstate-of-the-art methods could be easily corrupted by our attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Oct 2022 23:33:38 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 12 Oct 2022 17:07:10 GMT"
      }
    ],
    "update_date": "2022-10-13",
    "authors_parsed": [
      [
        "Zeng",
        "Huimin",
        ""
      ],
      [
        "Yue",
        "Zhenrui",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ],
      [
        "Kou",
        "Ziyi",
        ""
      ],
      [
        "Shang",
        "Lanyu",
        ""
      ],
      [
        "Wang",
        "Dong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.02191",
    "publish_date": "2022-10-12"
  },
  {
    "id": "2210.06186",
    "submitter": "Govind Mittal",
    "authors": "Govind Mittal, Chinmay Hegde, Nasir Memon",
    "title": "Gotcha: Real-Time Video Deepfake Detection via Challenge-Response",
    "comments": "Code and data to be released by the end of 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  With the rise of AI-enabled Real-Time Deepfakes (RTDFs), the integrity of\nonline video interactions has become a growing concern. RTDFs have now made it\nfeasible to replace an imposter's face with their victim in live video\ninteractions. Such advancement in deepfakes also coaxes detection to rise to\nthe same standard. However, existing deepfake detection techniques are\nasynchronous and hence ill-suited for RTDFs. To bridge this gap, we propose a\nchallenge-response approach that establishes authenticity in live settings. We\nfocus on talking-head style video interaction and present a taxonomy of\nchallenges that specifically target inherent limitations of RTDF generation\npipelines. We evaluate representative examples from the taxonomy by collecting\na unique dataset comprising eight challenges, which consistently and visibly\ndegrades the quality of state-of-the-art deepfake generators. These results are\ncorroborated both by humans and a new automated scoring function, leading to\n88.6\\% and 73.2% AUC, respectively. The findings underscore the promising\npotential of challenge-response systems for explainable and scalable real-time\ndeepfake detection in practical scenarios.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 12 Oct 2022 13:15:54 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 29 Nov 2023 22:33:00 GMT"
      }
    ],
    "update_date": "2023-12-01",
    "authors_parsed": [
      [
        "Mittal",
        "Govind",
        ""
      ],
      [
        "Hegde",
        "Chinmay",
        ""
      ],
      [
        "Memon",
        "Nasir",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.06186",
    "publish_date": "2023-11-29"
  },
  {
    "id": "2210.08388",
    "submitter": "Ajay Jaiswal",
    "authors": "Ajay Jaiswal, Kumar Ashutosh, Justin F Rousseau, Yifan Peng, Zhangyang\n  Wang, and Ying Ding",
    "title": "RoS-KD: A Robust Stochastic Knowledge Distillation Approach for Noisy\n  Medical Imaging",
    "comments": "Accepted in ICDM 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  AI-powered Medical Imaging has recently achieved enormous attention due to\nits ability to provide fast-paced healthcare diagnoses. However, it usually\nsuffers from a lack of high-quality datasets due to high annotation cost,\ninter-observer variability, human annotator error, and errors in\ncomputer-generated labels. Deep learning models trained on noisy labelled\ndatasets are sensitive to the noise type and lead to less generalization on the\nunseen samples. To address this challenge, we propose a Robust Stochastic\nKnowledge Distillation (RoS-KD) framework which mimics the notion of learning a\ntopic from multiple sources to ensure deterrence in learning noisy information.\nMore specifically, RoS-KD learns a smooth, well-informed, and robust student\nmanifold by distilling knowledge from multiple teachers trained on overlapping\nsubsets of training data. Our extensive experiments on popular medical imaging\nclassification tasks (cardiopulmonary disease and lesion classification) using\nreal-world datasets, show the performance benefit of RoS-KD, its ability to\ndistill knowledge from many popular large networks (ResNet-50, DenseNet-121,\nMobileNet-V2) in a comparatively small network, and its robustness to\nadversarial attacks (PGD, FSGM). More specifically, RoS-KD achieves >2% and >4%\nimprovement on F1-score for lesion classification and cardiopulmonary disease\nclassification tasks, respectively, when the underlying student is ResNet-18\nagainst recent competitive knowledge distillation baseline. Additionally, on\ncardiopulmonary disease classification task, RoS-KD outperforms most of the\nSOTA baselines by ~1% gain in AUC score.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 15 Oct 2022 22:32:20 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 2 Dec 2022 21:39:33 GMT"
      }
    ],
    "update_date": "2022-12-06",
    "authors_parsed": [
      [
        "Jaiswal",
        "Ajay",
        ""
      ],
      [
        "Ashutosh",
        "Kumar",
        ""
      ],
      [
        "Rousseau",
        "Justin F",
        ""
      ],
      [
        "Peng",
        "Yifan",
        ""
      ],
      [
        "Wang",
        "Zhangyang",
        ""
      ],
      [
        "Ding",
        "Ying",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.08388",
    "publish_date": "2022-10-15"
  },
  {
    "id": "2210.11237",
    "submitter": "Wenlong Zou",
    "authors": "Hui Cao, Wenlong Zou, Yinkun Wang, Ting Song, Mengjun Liu",
    "title": "Emerging Threats in Deep Learning-Based Autonomous Driving: A\n  Comprehensive Survey",
    "comments": "28 pages,10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Since the 2004 DARPA Grand Challenge, the autonomous driving technology has\nwitnessed nearly two decades of rapid development. Particularly, in recent\nyears, with the application of new sensors and deep learning technologies\nextending to the autonomous field, the development of autonomous driving\ntechnology has continued to make breakthroughs. Thus, many carmakers and\nhigh-tech giants dedicated to research and system development of autonomous\ndriving. However, as the foundation of autonomous driving, the deep learning\ntechnology faces many new security risks. The academic community has proposed\ndeep learning countermeasures against the adversarial examples and AI backdoor,\nand has introduced them into the autonomous driving field for verification.\nDeep learning security matters to autonomous driving system security, and then\nmatters to personal safety, which is an issue that deserves attention and\nresearch.This paper provides an summary of the concepts, developments and\nrecent research in deep learning security technologies in autonomous driving.\nFirstly, we briefly introduce the deep learning framework and pipeline in the\nautonomous driving system, which mainly include the deep learning technologies\nand algorithms commonly used in this field. Moreover, we focus on the potential\nsecurity threats of the deep learning based autonomous driving system in each\nfunctional layer in turn. We reviews the development of deep learning attack\ntechnologies to autonomous driving, investigates the State-of-the-Art\nalgorithms, and reveals the potential risks. At last, we provides an outlook on\ndeep learning security in the autonomous driving field and proposes\nrecommendations for building a safe and trustworthy autonomous driving system.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 19 Oct 2022 10:04:33 GMT"
      }
    ],
    "update_date": "2022-10-21",
    "authors_parsed": [
      [
        "Cao",
        "Hui",
        ""
      ],
      [
        "Zou",
        "Wenlong",
        ""
      ],
      [
        "Wang",
        "Yinkun",
        ""
      ],
      [
        "Song",
        "Ting",
        ""
      ],
      [
        "Liu",
        "Mengjun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.11237",
    "publish_date": "2022-10-19"
  },
  {
    "id": "2210.14283",
    "submitter": "Pratik Vaishnavi",
    "authors": "Pratik Vaishnavi, Kevin Eykholt, Amir Rahmati",
    "title": "Accelerating Certified Robustness Training via Knowledge Transfer",
    "comments": "NeurIPS '22 Camera Ready version (with appendix)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Training deep neural network classifiers that are certifiably robust against\nadversarial attacks is critical to ensuring the security and reliability of\nAI-controlled systems. Although numerous state-of-the-art certified training\nmethods have been developed, they are computationally expensive and scale\npoorly with respect to both dataset and network complexity. Widespread usage of\ncertified training is further hindered by the fact that periodic retraining is\nnecessary to incorporate new data and network improvements. In this paper, we\npropose Certified Robustness Transfer (CRT), a general-purpose framework for\nreducing the computational overhead of any certifiably robust training method\nthrough knowledge transfer. Given a robust teacher, our framework uses a novel\ntraining loss to transfer the teacher's robustness to the student. We provide\ntheoretical and empirical validation of CRT. Our experiments on CIFAR-10 show\nthat CRT speeds up certified robustness training by $8 \\times$ on average\nacross three different architecture generations while achieving comparable\nrobustness to state-of-the-art methods. We also show that CRT can scale to\nlarge-scale datasets like ImageNet.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 25 Oct 2022 19:12:28 GMT"
      }
    ],
    "update_date": "2022-10-27",
    "authors_parsed": [
      [
        "Vaishnavi",
        "Pratik",
        ""
      ],
      [
        "Eykholt",
        "Kevin",
        ""
      ],
      [
        "Rahmati",
        "Amir",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.14283",
    "publish_date": "2022-10-25"
  },
  {
    "id": "2210.14743",
    "submitter": "Omkar Bhilare Mr.",
    "authors": "Omkar Bhilare and Rahul Singh and Vedant Paranjape and Sravan\n  Chittupalli and Shraddha Suratkar and Faruk Kazi (Equal Contribution)",
    "title": "DEEPFAKE CLI: Accelerated Deepfake Detection using FPGAs",
    "comments": "This preprint has not undergone peer review or any post-submission\n  improvement or corrections. The Version of Record of this contribution is\n  published in LNCS [13798], PDCAT 2022 , and is available online at\n  [https://doi.org/10.1007/ISBN ]",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AR cs.DC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Because of the availability of larger datasets and recent improvements in the\ngenerative model, more realistic Deepfake videos are being produced each day.\nPeople consume around one billion hours of video on social media platforms\nevery day, and thats why it is very important to stop the spread of fake videos\nas they can be damaging, dangerous, and malicious. There has been a significant\nimprovement in the field of deepfake classification, but deepfake detection and\ninference have remained a difficult task. To solve this problem in this paper,\nwe propose a novel DEEPFAKE C-L-I (Classification-Localization-Inference) in\nwhich we have explored the idea of accelerating Quantized Deepfake Detection\nModels using FPGAs due to their ability of maximum parallelism and energy\nefficiency compared to generalized GPUs. In this paper, we have used light\nMesoNet with EFF-YNet structure and accelerated it on VCK5000 FPGA, powered by\nstate-of-the-art VC1902 Versal Architecture which uses AI, DSP, and Adaptable\nEngines for acceleration. We have benchmarked our inference speed with other\nstate-of-the-art inference nodes, got 316.8 FPS on VCK5000 while maintaining\n93\\% Accuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 26 Oct 2022 14:22:05 GMT"
      }
    ],
    "update_date": "2022-10-27",
    "authors_parsed": [
      [
        "Bhilare",
        "Omkar",
        "",
        "Equal Contribution"
      ],
      [
        "Singh",
        "Rahul",
        "",
        "Equal Contribution"
      ],
      [
        "Paranjape",
        "Vedant",
        "",
        "Equal Contribution"
      ],
      [
        "Chittupalli",
        "Sravan",
        "",
        "Equal Contribution"
      ],
      [
        "Suratkar",
        "Shraddha",
        "",
        "Equal Contribution"
      ],
      [
        "Kazi",
        "Faruk",
        "",
        "Equal Contribution"
      ]
    ],
    "url": "https://arxiv.org/pdf/2210.14743",
    "publish_date": "2022-10-26"
  },
  {
    "id": "2211.01361",
    "submitter": "Ilke Demir",
    "authors": "Umur A. Ciftci and Gokturk Yuksek and Ilke Demir",
    "title": "My Face My Choice: Privacy Enhancing Deepfakes for Social Media\n  Anonymization",
    "comments": "2023 IEEE Winter Conference on Applications of Computer Vision (WACV)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recently, productization of face recognition and identification algorithms\nhave become the most controversial topic about ethical AI. As new policies\naround digital identities are formed, we introduce three face access models in\na hypothetical social network, where the user has the power to only appear in\nphotos they approve. Our approach eclipses current tagging systems and replaces\nunapproved faces with quantitatively dissimilar deepfakes. In addition, we\npropose new metrics specific for this task, where the deepfake is generated at\nrandom with a guaranteed dissimilarity. We explain access models based on\nstrictness of the data flow, and discuss impact of each model on privacy,\nusability, and performance. We evaluate our system on Facial Descriptor Dataset\nas the real dataset, and two synthetic datasets with random and equal class\ndistributions. Running seven SOTA face recognizers on our results, MFMC reduces\nthe average accuracy by 61%. Lastly, we extensively analyze similarity metrics,\ndeepfake generators, and datasets in structural, visual, and generative spaces;\nsupporting the design choices and verifying the quality.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 2 Nov 2022 17:58:20 GMT"
      }
    ],
    "update_date": "2022-11-03",
    "authors_parsed": [
      [
        "Ciftci",
        "Umur A.",
        ""
      ],
      [
        "Yuksek",
        "Gokturk",
        ""
      ],
      [
        "Demir",
        "Ilke",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.01361",
    "publish_date": "2022-11-02"
  },
  {
    "id": "2211.10061",
    "submitter": "Ben Dai",
    "authors": "Ben Dai, Xiaotong Shen, Lin Yee Chen, Chunlin Li, Wei Pan",
    "title": "Data-Adaptive Discriminative Feature Localization with Statistically\n  Guaranteed Interpretation",
    "comments": "27 pages, 11 figures",
    "journal-ref": "The Annals of Applied Statistics, 2022",
    "doi": "10.1214/22-AOAS1705",
    "report-no": null,
    "categories": "stat.ML cs.AI cs.LG stat.AP stat.ME",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  In explainable artificial intelligence, discriminative feature localization\nis critical to reveal a blackbox model's decision-making process from raw data\nto prediction. In this article, we use two real datasets, the MNIST handwritten\ndigits and MIT-BIH Electrocardiogram (ECG) signals, to motivate key\ncharacteristics of discriminative features, namely adaptiveness, predictive\nimportance and effectiveness. Then, we develop a localization framework based\non adversarial attacks to effectively localize discriminative features. In\ncontrast to existing heuristic methods, we also provide a statistically\nguaranteed interpretability of the localized features by measuring a\ngeneralized partial $R^2$. We apply the proposed method to the MNIST dataset\nand the MIT-BIH dataset with a convolutional auto-encoder. In the first, the\ncompact image regions localized by the proposed method are visually appealing.\nSimilarly, in the second, the identified ECG features are biologically\nplausible and consistent with cardiac electrophysiological principles while\nlocating subtle anomalies in a QRS complex that may not be discernible by the\nnaked eye. Overall, the proposed method compares favorably with\nstate-of-the-art competitors. Accompanying this paper is a Python library\ndnn-locate (https://dnn-locate.readthedocs.io/en/latest/) that implements the\nproposed approach.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 18 Nov 2022 07:20:00 GMT"
      }
    ],
    "update_date": "2022-11-21",
    "authors_parsed": [
      [
        "Dai",
        "Ben",
        ""
      ],
      [
        "Shen",
        "Xiaotong",
        ""
      ],
      [
        "Chen",
        "Lin Yee",
        ""
      ],
      [
        "Li",
        "Chunlin",
        ""
      ],
      [
        "Pan",
        "Wei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.10061",
    "publish_date": "2022-11-18"
  },
  {
    "id": "2211.10144",
    "submitter": "Victor Lagerkvist Dr.",
    "authors": "Peter Jonsson, Victor Lagerkvist, Sebastian Ordyniak",
    "title": "Computational Short Cuts in Infinite Domain Constraint Satisfaction",
    "comments": null,
    "journal-ref": "Journal of Artificial Intelligence Research. 75 (2022)",
    "doi": "10.1613/jair.1.13787",
    "report-no": null,
    "categories": "cs.CC cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  A backdoor in a finite-domain CSP instance is a set of variables where each\npossible instantiation moves the instance into a polynomial-time solvable\nclass. Backdoors have found many applications in artificial intelligence and\nelsewhere, and the algorithmic problem of finding such backdoors has\nconsequently been intensively studied. Sioutis and Janhunen (Proc. 42nd German\nConference on AI (KI-2019)) have proposed a generalised backdoor concept\nsuitable for infinite-domain CSP instances over binary constraints. We\ngeneralise their concept into a large class of CSPs that allow for higher-arity\nconstraints. We show that this kind of infinite-domain backdoors have many of\nthe positive computational properties that finite-domain backdoors have: the\nassociated computational problems are fixed-parameter tractable whenever the\nunderlying constraint language is finite. On the other hand, we show that\ninfinite languages make the problems considerably harder: the general backdoor\ndetection problem is W[2]-hard and fixed-parameter tractability is ruled out\nunder standard complexity-theoretic assumptions. We demonstrate that backdoors\nmay have suboptimal behaviour on binary constraints -- this is detrimental from\nan AI perspective where binary constraints are predominant in, for instance,\nspatiotemporal applications. In response to this, we introduce sidedoors as an\nalternative to backdoors. The fundamental computational problems for sidedoors\nremain fixed-parameter tractable for finite constraint language (possibly also\ncontaining non-binary relations). Moreover, the sidedoor approach has appealing\ncomputational properties that sometimes leads to faster algorithms than the\nbackdoor approach.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 18 Nov 2022 10:37:51 GMT"
      }
    ],
    "update_date": "2022-11-21",
    "authors_parsed": [
      [
        "Jonsson",
        "Peter",
        ""
      ],
      [
        "Lagerkvist",
        "Victor",
        ""
      ],
      [
        "Ordyniak",
        "Sebastian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.10144",
    "publish_date": "2022-11-18"
  },
  {
    "id": "2211.12851",
    "submitter": "Murat Kuzlu",
    "authors": "M. Kuzlu, F. O. Catak, S. Sarp, U. Cali, and O Gueler",
    "title": "A Streamlit-based Artificial Intelligence Trust Platform for\n  Next-Generation Wireless Networks",
    "comments": "4 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With the rapid development and integration of artificial intelligence (AI)\nmethods in next-generation networks (NextG), AI algorithms have provided\nsignificant advantages for NextG in terms of frequency spectrum usage,\nbandwidth, latency, and security. A key feature of NextG is the integration of\nAI, i.e., self-learning architecture based on self-supervised algorithms, to\nimprove the performance of the network. A secure AI-powered structure is also\nexpected to protect NextG networks against cyber-attacks. However, AI itself\nmay be attacked, i.e., model poisoning targeted by attackers, and it results in\ncybersecurity violations. This paper proposes an AI trust platform using\nStreamlit for NextG networks that allows researchers to evaluate, defend,\ncertify, and verify their AI models and applications against adversarial\nthreats of evasion, poisoning, extraction, and interference.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 25 Oct 2022 05:26:30 GMT"
      }
    ],
    "update_date": "2022-11-24",
    "authors_parsed": [
      [
        "Kuzlu",
        "M.",
        ""
      ],
      [
        "Catak",
        "F. O.",
        ""
      ],
      [
        "Sarp",
        "S.",
        ""
      ],
      [
        "Cali",
        "U.",
        ""
      ],
      [
        "Gueler",
        "O",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.12851",
    "publish_date": "2022-10-25"
  },
  {
    "id": "2211.14667",
    "submitter": "Amin Azmoodeh",
    "authors": "Amin Azmoodeh and Ali Dehghantanha",
    "title": "Deep Fake Detection, Deterrence and Response: Challenges and\n  Opportunities",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  According to the 2020 cyber threat defence report, 78% of Canadian\norganizations experienced at least one successful cyberattack in 2020. The\nconsequences of such attacks vary from privacy compromises to immersing damage\ncosts for individuals, companies, and countries. Specialists predict that the\nglobal loss from cybercrime will reach 10.5 trillion US dollars annually by\n2025. Given such alarming statistics, the need to prevent and predict\ncyberattacks is as high as ever. Our increasing reliance on Machine\nLearning(ML)-based systems raises serious concerns about the security and\nsafety of these systems. Especially the emergence of powerful ML techniques to\ngenerate fake visual, textual, or audio content with a high potential to\ndeceive humans raised serious ethical concerns. These artificially crafted\ndeceiving videos, images, audio, or texts are known as Deepfakes garnered\nattention for their potential use in creating fake news, hoaxes, revenge porn,\nand financial fraud. Diversity and the widespread of deepfakes made their\ntimely detection a significant challenge. In this paper, we first offer\nbackground information and a review of previous works on the detection and\ndeterrence of deepfakes. Afterward, we offer a solution that is capable of 1)\nmaking our AI systems robust against deepfakes during development and\ndeployment phases; 2) detecting video, image, audio, and textual deepfakes; 3)\nidentifying deepfakes that bypass detection (deepfake hunting); 4) leveraging\navailable intelligence for timely identification of deepfake campaigns launched\nby state-sponsored hacking teams; 5) conducting in-depth forensic analysis of\nidentified deepfake payloads. Our solution would address important elements of\nthe Canada National Cyber Security Action Plan(2019-2024) in increasing the\ntrustworthiness of our critical services.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 26 Nov 2022 21:23:30 GMT"
      }
    ],
    "update_date": "2022-11-29",
    "authors_parsed": [
      [
        "Azmoodeh",
        "Amin",
        ""
      ],
      [
        "Dehghantanha",
        "Ali",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.14667",
    "publish_date": "2022-11-26"
  },
  {
    "id": "2211.14860",
    "submitter": "Moshe Sipper",
    "authors": "Snir Vitrack Tamam, Raz Lapid, Moshe Sipper",
    "title": "Foiling Explanations in Deep Neural Networks",
    "comments": "Snir Vitrack Tamam and Raz Lapid contributed equally",
    "journal-ref": "Transactions on Machine Learning Research, 08/2023",
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) have greatly impacted numerous fields over the\npast decade. Yet despite exhibiting superb performance over many problems,\ntheir black-box nature still poses a significant challenge with respect to\nexplainability. Indeed, explainable artificial intelligence (XAI) is crucial in\nseveral fields, wherein the answer alone -- sans a reasoning of how said answer\nwas derived -- is of little value. This paper uncovers a troubling property of\nexplanation methods for image-based DNNs: by making small visual changes to the\ninput image -- hardly influencing the network's output -- we demonstrate how\nexplanations may be arbitrarily manipulated through the use of evolution\nstrategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack\non XAI algorithms, only requires access to the output logits of a classifier\nand to the explanation map; these weak assumptions render our approach highly\nuseful where real-world models and data are concerned. We compare our method's\nperformance on two benchmark datasets -- CIFAR100 and ImageNet -- using four\ndifferent pretrained deep-learning models: VGG16-CIFAR100, VGG16-ImageNet,\nMobileNet-CIFAR100, and Inception-v3-ImageNet. We find that the XAI methods can\nbe manipulated without the use of gradients or other model internals. Our novel\nalgorithm is successfully able to manipulate an image in a manner imperceptible\nto the human eye, such that the XAI method outputs a specific explanation map.\nTo our knowledge, this is the first such method in a black-box setting, and we\nbelieve it has significant value where explainability is desired, required, or\nlegally mandatory.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 27 Nov 2022 15:29:39 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 24 Mar 2023 10:01:31 GMT"
      },
      {
        "version": "v3",
        "created": "Sun, 13 Aug 2023 16:37:17 GMT"
      }
    ],
    "update_date": "2023-08-15",
    "authors_parsed": [
      [
        "Tamam",
        "Snir Vitrack",
        ""
      ],
      [
        "Lapid",
        "Raz",
        ""
      ],
      [
        "Sipper",
        "Moshe",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2211.14860",
    "publish_date": "2022-11-27"
  },
  {
    "id": "2212.03412",
    "submitter": "Yinpeng Dong",
    "authors": "Yinpeng Dong, Peng Chen, Senyou Deng, Lianji L, Yi Sun, Hanyu Zhao,\n  Jiaxing Li, Yunteng Tan, Xinyu Liu, Yangyi Dong, Enhui Xu, Jincai Xu, Shu Xu,\n  Xuelin Fu, Changfeng Sun, Haoliang Han, Xuchong Zhang, Shen Chen, Zhimin Sun,\n  Junyi Cao, Taiping Yao, Shouhong Ding, Yu Wu, Jian Lin, Tianpeng Wu, Ye Wang,\n  Yu Fu, Lin Feng, Kangkang Gao, Zeyu Liu, Yuanzhe Pang, Chengqi Duan, Huipeng\n  Zhou, Yajie Wang, Yuhang Zhao, Shangbo Wu, Haoran Lyu, Zhiyu Lin, Yifei Gao,\n  Shuang Li, Haonan Wang, Jitao Sang, Chen Ma, Junhao Zheng, Yijia Li, Chao\n  Shen, Chenhao Lin, Zhichao Cui, Guoshuai Liu, Huafeng Shi, Kun Hu, Mengxin\n  Zhang",
    "title": "Artificial Intelligence Security Competition (AISC)",
    "comments": "Technical report of AISC",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  The security of artificial intelligence (AI) is an important research area\ntowards safe, reliable, and trustworthy AI systems. To accelerate the research\non AI security, the Artificial Intelligence Security Competition (AISC) was\norganized by the Zhongguancun Laboratory, China Industrial Control Systems\nCyber Emergency Response Team, Institute for Artificial Intelligence, Tsinghua\nUniversity, and RealAI as part of the Zhongguancun International Frontier\nTechnology Innovation Competition (https://www.zgc-aisc.com/en). The\ncompetition consists of three tracks, including Deepfake Security Competition,\nAutonomous Driving Security Competition, and Face Recognition Security\nCompetition. This report will introduce the competition rules of these three\ntracks and the solutions of top-ranking teams in each track.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 7 Dec 2022 02:45:27 GMT"
      }
    ],
    "update_date": "2022-12-08",
    "authors_parsed": [
      [
        "Dong",
        "Yinpeng",
        ""
      ],
      [
        "Chen",
        "Peng",
        ""
      ],
      [
        "Deng",
        "Senyou",
        ""
      ],
      [
        "L",
        "Lianji",
        ""
      ],
      [
        "Sun",
        "Yi",
        ""
      ],
      [
        "Zhao",
        "Hanyu",
        ""
      ],
      [
        "Li",
        "Jiaxing",
        ""
      ],
      [
        "Tan",
        "Yunteng",
        ""
      ],
      [
        "Liu",
        "Xinyu",
        ""
      ],
      [
        "Dong",
        "Yangyi",
        ""
      ],
      [
        "Xu",
        "Enhui",
        ""
      ],
      [
        "Xu",
        "Jincai",
        ""
      ],
      [
        "Xu",
        "Shu",
        ""
      ],
      [
        "Fu",
        "Xuelin",
        ""
      ],
      [
        "Sun",
        "Changfeng",
        ""
      ],
      [
        "Han",
        "Haoliang",
        ""
      ],
      [
        "Zhang",
        "Xuchong",
        ""
      ],
      [
        "Chen",
        "Shen",
        ""
      ],
      [
        "Sun",
        "Zhimin",
        ""
      ],
      [
        "Cao",
        "Junyi",
        ""
      ],
      [
        "Yao",
        "Taiping",
        ""
      ],
      [
        "Ding",
        "Shouhong",
        ""
      ],
      [
        "Wu",
        "Yu",
        ""
      ],
      [
        "Lin",
        "Jian",
        ""
      ],
      [
        "Wu",
        "Tianpeng",
        ""
      ],
      [
        "Wang",
        "Ye",
        ""
      ],
      [
        "Fu",
        "Yu",
        ""
      ],
      [
        "Feng",
        "Lin",
        ""
      ],
      [
        "Gao",
        "Kangkang",
        ""
      ],
      [
        "Liu",
        "Zeyu",
        ""
      ],
      [
        "Pang",
        "Yuanzhe",
        ""
      ],
      [
        "Duan",
        "Chengqi",
        ""
      ],
      [
        "Zhou",
        "Huipeng",
        ""
      ],
      [
        "Wang",
        "Yajie",
        ""
      ],
      [
        "Zhao",
        "Yuhang",
        ""
      ],
      [
        "Wu",
        "Shangbo",
        ""
      ],
      [
        "Lyu",
        "Haoran",
        ""
      ],
      [
        "Lin",
        "Zhiyu",
        ""
      ],
      [
        "Gao",
        "Yifei",
        ""
      ],
      [
        "Li",
        "Shuang",
        ""
      ],
      [
        "Wang",
        "Haonan",
        ""
      ],
      [
        "Sang",
        "Jitao",
        ""
      ],
      [
        "Ma",
        "Chen",
        ""
      ],
      [
        "Zheng",
        "Junhao",
        ""
      ],
      [
        "Li",
        "Yijia",
        ""
      ],
      [
        "Shen",
        "Chao",
        ""
      ],
      [
        "Lin",
        "Chenhao",
        ""
      ],
      [
        "Cui",
        "Zhichao",
        ""
      ],
      [
        "Liu",
        "Guoshuai",
        ""
      ],
      [
        "Shi",
        "Huafeng",
        ""
      ],
      [
        "Hu",
        "Kun",
        ""
      ],
      [
        "Zhang",
        "Mengxin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.03412",
    "publish_date": "2022-12-07"
  },
  {
    "id": "2212.04454",
    "submitter": "Truc Nguyen",
    "authors": "Truc Nguyen, Phung Lai, NhatHai Phan, My T. Thai",
    "title": "XRand: Differentially Private Defense against Explanation-Guided Attacks",
    "comments": "To be published at AAAI 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent development in the field of explainable artificial intelligence (XAI)\nhas helped improve trust in Machine-Learning-as-a-Service (MLaaS) systems, in\nwhich an explanation is provided together with the model prediction in response\nto each query. However, XAI also opens a door for adversaries to gain insights\ninto the black-box models in MLaaS, thereby making the models more vulnerable\nto several attacks. For example, feature-based explanations (e.g., SHAP) could\nexpose the top important features that a black-box model focuses on. Such\ndisclosure has been exploited to craft effective backdoor triggers against\nmalware classifiers. To address this trade-off, we introduce a new concept of\nachieving local differential privacy (LDP) in the explanations, and from that\nwe establish a defense, called XRand, against such attacks. We show that our\nmechanism restricts the information that the adversary can learn about the top\nimportant features, while maintaining the faithfulness of the explanations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Dec 2022 18:23:59 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 10 Dec 2022 05:38:36 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 14 Dec 2022 18:03:04 GMT"
      }
    ],
    "update_date": "2022-12-15",
    "authors_parsed": [
      [
        "Nguyen",
        "Truc",
        ""
      ],
      [
        "Lai",
        "Phung",
        ""
      ],
      [
        "Phan",
        "NhatHai",
        ""
      ],
      [
        "Thai",
        "My T.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.04454",
    "publish_date": "2022-12-14"
  },
  {
    "id": "2212.05056",
    "submitter": "Sergi Bray",
    "authors": "Sergi D. Bray (1), Shane D. Johnson (1), Bennett Kleinberg (2) ((1)\n  University College London, (2) Tilburg University)",
    "title": "Testing Human Ability To Detect Deepfake Images of Human Faces",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.HC cs.CR cs.CV cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deepfakes are computationally-created entities that falsely represent\nreality. They can take image, video, and audio modalities, and pose a threat to\nmany areas of systems and societies, comprising a topic of interest to various\naspects of cybersecurity and cybersafety. In 2020 a workshop consulting AI\nexperts from academia, policing, government, the private sector, and state\nsecurity agencies ranked deepfakes as the most serious AI threat. These experts\nnoted that since fake material can propagate through many uncontrolled routes,\nchanges in citizen behaviour may be the only effective defence. This study aims\nto assess human ability to identify image deepfakes of human faces\n(StyleGAN2:FFHQ) from nondeepfake images (FFHQ), and to assess the\neffectiveness of simple interventions intended to improve detection accuracy.\nUsing an online survey, 280 participants were randomly allocated to one of four\ngroups: a control group, and 3 assistance interventions. Each participant was\nshown a sequence of 20 images randomly selected from a pool of 50 deepfake and\n50 real images of human faces. Participants were asked if each image was\nAI-generated or not, to report their confidence, and to describe the reasoning\nbehind each response. Overall detection accuracy was only just above chance and\nnone of the interventions significantly improved this. Participants' confidence\nin their answers was high and unrelated to accuracy. Assessing the results on a\nper-image basis reveals participants consistently found certain images harder\nto label correctly, but reported similarly high confidence regardless of the\nimage. Thus, although participant accuracy was 62% overall, this accuracy\nacross images ranged quite evenly between 85% and 30%, with an accuracy of\nbelow 50% for one in every five images. We interpret the findings as suggesting\nthat there is a need for an urgent call to action to address this threat.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 7 Dec 2022 14:48:25 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 14 Dec 2022 13:53:10 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 25 May 2023 15:07:19 GMT"
      }
    ],
    "update_date": "2023-05-26",
    "authors_parsed": [
      [
        "Bray",
        "Sergi D.",
        ""
      ],
      [
        "Johnson",
        "Shane D.",
        ""
      ],
      [
        "Kleinberg",
        "Bennett",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.05056",
    "publish_date": "2022-12-07"
  },
  {
    "id": "2212.06295",
    "submitter": "Ellie Kitanidis",
    "authors": "Joshua Albrecht, Ellie Kitanidis, Abraham J. Fetterman",
    "title": "Despite \"super-human\" performance, current LLMs are unsuited for\n  decisions about ethics and safety",
    "comments": "ML Safety Workshop, NeurIPS 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs) have exploded in popularity in the past few\nyears and have achieved undeniably impressive results on benchmarks as varied\nas question answering and text summarization. We provide a simple new prompting\nstrategy that leads to yet another supposedly \"super-human\" result, this time\noutperforming humans at common sense ethical reasoning (as measured by accuracy\non a subset of the ETHICS dataset). Unfortunately, we find that relying on\naverage performance to judge capabilities can be highly misleading. LLM errors\ndiffer systematically from human errors in ways that make it easy to craft\nadversarial examples, or even perturb existing examples to flip the output\nlabel. We also observe signs of inverse scaling with model size on some\nexamples, and show that prompting models to \"explain their reasoning\" often\nleads to alarming justifications of unethical actions. Our results highlight\nhow human-like performance does not necessarily imply human-like understanding\nor reasoning.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 13 Dec 2022 00:29:45 GMT"
      }
    ],
    "update_date": "2022-12-14",
    "authors_parsed": [
      [
        "Albrecht",
        "Joshua",
        ""
      ],
      [
        "Kitanidis",
        "Ellie",
        ""
      ],
      [
        "Fetterman",
        "Abraham J.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.06295",
    "publish_date": "2022-12-13"
  },
  {
    "id": "2212.06576",
    "submitter": "Peter Bajcsy",
    "authors": "Peter Bajcsy and Antonio Cardone and Chenyi Ling and Philippe Dessauw\n  and Michael Majurski and Tim Blattner and Derek Juba and Walid Keyrouz",
    "title": "AI Model Utilization Measurements For Finding Class Encoding Patterns",
    "comments": "45 pages, 29 figures, 7 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR cs.CV",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  This work addresses the problems of (a) designing utilization measurements of\ntrained artificial intelligence (AI) models and (b) explaining how training\ndata are encoded in AI models based on those measurements. The problems are\nmotivated by the lack of explainability of AI models in security and safety\ncritical applications, such as the use of AI models for classification of\ntraffic signs in self-driving cars. We approach the problems by introducing\ntheoretical underpinnings of AI model utilization measurement and understanding\npatterns in utilization-based class encodings of traffic signs at the level of\ncomputation graphs (AI models), subgraphs, and graph nodes. Conceptually,\nutilization is defined at each graph node (computation unit) of an AI model\nbased on the number and distribution of unique outputs in the space of all\npossible outputs (tensor-states). In this work, utilization measurements are\nextracted from AI models, which include poisoned and clean AI models. In\ncontrast to clean AI models, the poisoned AI models were trained with traffic\nsign images containing systematic, physically realizable, traffic sign\nmodifications (i.e., triggers) to change a correct class label to another label\nin a presence of such a trigger. We analyze class encodings of such clean and\npoisoned AI models, and conclude with implications for trojan injection and\ndetection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 12 Dec 2022 02:18:10 GMT"
      }
    ],
    "update_date": "2022-12-14",
    "authors_parsed": [
      [
        "Bajcsy",
        "Peter",
        ""
      ],
      [
        "Cardone",
        "Antonio",
        ""
      ],
      [
        "Ling",
        "Chenyi",
        ""
      ],
      [
        "Dessauw",
        "Philippe",
        ""
      ],
      [
        "Majurski",
        "Michael",
        ""
      ],
      [
        "Blattner",
        "Tim",
        ""
      ],
      [
        "Juba",
        "Derek",
        ""
      ],
      [
        "Keyrouz",
        "Walid",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.06576",
    "publish_date": "2022-12-12"
  },
  {
    "id": "2212.08121",
    "submitter": "Khondoker Murad Hossain",
    "authors": "Khondoker Murad Hossain, Tim Oates",
    "title": "Backdoor Attack Detection in Computer Vision by Applying Matrix\n  Factorization on the Weights of Deep Networks",
    "comments": "7 pages, 4 figures, 5 tables, AAAI Workshop on Safe AI 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The increasing importance of both deep neural networks (DNNs) and cloud\nservices for training them means that bad actors have more incentive and\nopportunity to insert backdoors to alter the behavior of trained models. In\nthis paper, we introduce a novel method for backdoor detection that extracts\nfeatures from pre-trained DNN's weights using independent vector analysis (IVA)\nfollowed by a machine learning classifier. In comparison to other detection\ntechniques, this has a number of benefits, such as not requiring any training\ndata, being applicable across domains, operating with a wide range of network\narchitectures, not assuming the nature of the triggers used to change network\nbehavior, and being highly scalable. We discuss the detection pipeline, and\nthen demonstrate the results on two computer vision datasets regarding image\nclassification and object detection. Our method outperforms the competing\nalgorithms in terms of efficiency and is more accurate, helping to ensure the\nsafe application of deep learning and AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 15 Dec 2022 20:20:18 GMT"
      }
    ],
    "update_date": "2022-12-19",
    "authors_parsed": [
      [
        "Hossain",
        "Khondoker Murad",
        ""
      ],
      [
        "Oates",
        "Tim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.08121",
    "publish_date": "2022-12-15"
  },
  {
    "id": "2212.09360",
    "submitter": "Yonghao Xu",
    "authors": "Yonghao Xu, Tao Bai, Weikang Yu, Shizhen Chang, Peter M. Atkinson,\n  Pedram Ghamisi",
    "title": "AI Security for Geoscience and Remote Sensing: Challenges and Future\n  Trends",
    "comments": null,
    "journal-ref": "IEEE Geoscience and Remote Sensing Magazine, Volume 11, Issue 2,\n  Pages 60-85, 2023",
    "doi": "10.1109/MGRS.2023.3272825",
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in artificial intelligence (AI) have significantly\nintensified research in the geoscience and remote sensing (RS) field. AI\nalgorithms, especially deep learning-based ones, have been developed and\napplied widely to RS data analysis. The successful application of AI covers\nalmost all aspects of Earth observation (EO) missions, from low-level vision\ntasks like super-resolution, denoising and inpainting, to high-level vision\ntasks like scene classification, object detection and semantic segmentation.\nWhile AI techniques enable researchers to observe and understand the Earth more\naccurately, the vulnerability and uncertainty of AI models deserve further\nattention, considering that many geoscience and RS tasks are highly\nsafety-critical. This paper reviews the current development of AI security in\nthe geoscience and RS field, covering the following five important aspects:\nadversarial attack, backdoor attack, federated learning, uncertainty and\nexplainability. Moreover, the potential opportunities and trends are discussed\nto provide insights for future research. To the best of the authors' knowledge,\nthis paper is the first attempt to provide a systematic review of AI\nsecurity-related research in the geoscience and RS community. Available code\nand datasets are also listed in the paper to move this vibrant field of\nresearch forward.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 19 Dec 2022 10:54:51 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 22 Jun 2023 15:51:34 GMT"
      }
    ],
    "update_date": "2023-07-14",
    "authors_parsed": [
      [
        "Xu",
        "Yonghao",
        ""
      ],
      [
        "Bai",
        "Tao",
        ""
      ],
      [
        "Yu",
        "Weikang",
        ""
      ],
      [
        "Chang",
        "Shizhen",
        ""
      ],
      [
        "Atkinson",
        "Peter M.",
        ""
      ],
      [
        "Ghamisi",
        "Pedram",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.09360",
    "publish_date": "2023-06-22"
  },
  {
    "id": "2212.09668",
    "submitter": "Yalin E. Sagduyu",
    "authors": "Yalin E. Sagduyu, Sennur Ulukus, Aylin Yener",
    "title": "Task-Oriented Communications for NextG: End-to-End Deep Learning and AI\n  Security Aspects",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI cs.AI cs.IT cs.LG eess.SP math.IT",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Communications systems to date are primarily designed with the goal of\nreliable transfer of digital sequences (bits). Next generation (NextG)\ncommunication systems are beginning to explore shifting this design paradigm to\nreliably executing a given task such as in task-oriented communications. In\nthis paper, wireless signal classification is considered as the task for the\nNextG Radio Access Network (RAN), where edge devices collect wireless signals\nfor spectrum awareness and communicate with the NextG base station (gNodeB)\nthat needs to identify the signal label. Edge devices may not have sufficient\nprocessing power and may not be trusted to perform the signal classification\ntask, whereas the transfer of signals to the gNodeB may not be feasible due to\nstringent delay, rate, and energy restrictions. Task-oriented communications is\nconsidered by jointly training the transmitter, receiver and classifier\nfunctionalities as an encoder-decoder pair for the edge device and the gNodeB.\nThis approach improves the accuracy compared to the separated case of signal\ntransfer followed by classification. Adversarial machine learning poses a major\nsecurity threat to the use of deep learning for task-oriented communications. A\nmajor performance loss is shown when backdoor (Trojan) and adversarial\n(evasion) attacks target the training and test processes of task-oriented\ncommunications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 19 Dec 2022 17:54:36 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 21 Mar 2023 23:01:40 GMT"
      }
    ],
    "update_date": "2023-03-23",
    "authors_parsed": [
      [
        "Sagduyu",
        "Yalin E.",
        ""
      ],
      [
        "Ulukus",
        "Sennur",
        ""
      ],
      [
        "Yener",
        "Aylin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.09668",
    "publish_date": "2023-03-21"
  },
  {
    "id": "2212.11126",
    "submitter": "David Noever",
    "authors": "Forrest McKee, David Noever",
    "title": "Chatbots in a Botnet World",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Question-and-answer formats provide a novel experimental platform for\ninvestigating cybersecurity questions. Unlike previous chatbots, the latest\nChatGPT model from OpenAI supports an advanced understanding of complex coding\nquestions. The research demonstrates thirteen coding tasks that generally\nqualify as stages in the MITRE ATT&CK framework, ranging from credential access\nto defense evasion. With varying success, the experimental prompts generate\nexamples of keyloggers, logic bombs, obfuscated worms, and payment-fulfilled\nransomware. The empirical results illustrate cases that support the broad gain\nof functionality, including self-replication and self-modification, evasion,\nand strategic understanding of complex cybersecurity goals. One surprising\nfeature of ChatGPT as a language-only model centers on its ability to spawn\ncoding approaches that yield images that obfuscate or embed executable\nprogramming steps or links.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 18 Dec 2022 16:08:40 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 22 Dec 2022 17:20:36 GMT"
      }
    ],
    "update_date": "2022-12-23",
    "authors_parsed": [
      [
        "McKee",
        "Forrest",
        ""
      ],
      [
        "Noever",
        "David",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2212.11126",
    "publish_date": "2022-12-22"
  },
  {
    "id": "2301.02344",
    "submitter": "Hojjat Aghakhani",
    "authors": "Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant\n  Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, and\n  Robert Sim",
    "title": "TrojanPuzzle: Covertly Poisoning Code-Suggestion Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With tools like GitHub Copilot, automatic code suggestion is no longer a\ndream in software engineering. These tools, based on large language models, are\ntypically trained on massive corpora of code mined from unvetted public\nsources. As a result, these models are susceptible to data poisoning attacks\nwhere an adversary manipulates the model's training or fine-tuning phases by\ninjecting malicious data. Poisoning attacks could be designed to influence the\nmodel's suggestions at run time for chosen contexts, such as inducing the model\ninto suggesting insecure code payloads. To achieve this, prior poisoning\nattacks explicitly inject the insecure code payload into the training data,\nmaking the poisoning data detectable by static analysis tools that can remove\nsuch malicious data from the training set. In this work, we demonstrate two\nnovel data poisoning attacks, COVERT and TROJANPUZZLE, that can bypass static\nanalysis by planting malicious poisoning data in out-of-context regions such as\ndocstrings. Our most novel attack, TROJANPUZZLE, goes one step further in\ngenerating less suspicious poisoning data by never including certain\n(suspicious) parts of the payload in the poisoned data, while still inducing a\nmodel that suggests the entire payload when completing code (i.e., outside\ndocstrings). This makes TROJANPUZZLE robust against signature-based\ndataset-cleansing methods that identify and filter out suspicious sequences\nfrom the training data. Our evaluation against two model sizes demonstrates\nthat both COVERT and TROJANPUZZLE have significant implications for how\npractitioners should select code used to train or tune code-suggestion models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 6 Jan 2023 00:37:25 GMT"
      }
    ],
    "update_date": "2023-01-09",
    "authors_parsed": [
      [
        "Aghakhani",
        "Hojjat",
        ""
      ],
      [
        "Dai",
        "Wei",
        ""
      ],
      [
        "Manoel",
        "Andre",
        ""
      ],
      [
        "Fernandes",
        "Xavier",
        ""
      ],
      [
        "Kharkar",
        "Anant",
        ""
      ],
      [
        "Kruegel",
        "Christopher",
        ""
      ],
      [
        "Vigna",
        "Giovanni",
        ""
      ],
      [
        "Evans",
        "David",
        ""
      ],
      [
        "Zorn",
        "Ben",
        ""
      ],
      [
        "Sim",
        "Robert",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.02344",
    "publish_date": "2023-01-06"
  },
  {
    "id": "2301.03064",
    "submitter": "Yisroel Mirsky Dr.",
    "authors": "Lior Yasur, Guy Frankovits, Fred M. Grabovski, Yisroel Mirsky",
    "title": "Deepfake CAPTCHA: A Method for Preventing Fake Calls",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning technology has made it possible to generate realistic content\nof specific individuals. These `deepfakes' can now be generated in real-time\nwhich enables attackers to impersonate people over audio and video calls.\nMoreover, some methods only need a few images or seconds of audio to steal an\nidentity. Existing defenses perform passive analysis to detect fake content.\nHowever, with the rapid progress of deepfake quality, this may be a losing\ngame.\n  In this paper, we propose D-CAPTCHA: an active defense against real-time\ndeepfakes. The approach is to force the adversary into the spotlight by\nchallenging the deepfake model to generate content which exceeds its\ncapabilities. By doing so, passive detection becomes easier since the content\nwill be distorted. In contrast to existing CAPTCHAs, we challenge the AI's\nability to create content as opposed to its ability to classify content. In\nthis work we focus on real-time audio deepfakes and present preliminary results\non video.\n  In our evaluation we found that D-CAPTCHA outperforms state-of-the-art audio\ndeepfake detectors with an accuracy of 91-100% depending on the challenge\n(compared to 71% without challenges). We also performed a study on 41\nvolunteers to understand how threatening current real-time deepfake attacks\nare. We found that the majority of the volunteers could not tell the difference\nbetween real and fake audio.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 8 Jan 2023 15:34:19 GMT"
      }
    ],
    "update_date": "2023-01-10",
    "authors_parsed": [
      [
        "Yasur",
        "Lior",
        ""
      ],
      [
        "Frankovits",
        "Guy",
        ""
      ],
      [
        "Grabovski",
        "Fred M.",
        ""
      ],
      [
        "Mirsky",
        "Yisroel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.03064",
    "publish_date": "2023-01-08"
  },
  {
    "id": "2301.06923",
    "submitter": "Zhibo Zhang",
    "authors": "Zhibo Zhang, Sani Umar, Ahmed Y. Al Hammadi, Sangyoung Yoon, Ernesto\n  Damiani, Claudio Agostino Ardagna, Nicola Bena, and Chan Yeob Yeun",
    "title": "Explainable Data Poison Attacks on Human Emotion Evaluation Systems\n  based on EEG Signals",
    "comments": null,
    "journal-ref": "IEEE Access 2023",
    "doi": "10.1109/ACCESS.2023.3245813",
    "report-no": null,
    "categories": "cs.LG eess.SP",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The major aim of this paper is to explain the data poisoning attacks using\nlabel-flipping during the training stage of the electroencephalogram (EEG)\nsignal-based human emotion evaluation systems deploying Machine Learning models\nfrom the attackers' perspective. Human emotion evaluation using EEG signals has\nconsistently attracted a lot of research attention. The identification of human\nemotional states based on EEG signals is effective to detect potential internal\nthreats caused by insider individuals. Nevertheless, EEG signal-based human\nemotion evaluation systems have shown several vulnerabilities to data poison\nattacks. The findings of the experiments demonstrate that the suggested data\npoison assaults are model-independently successful, although various models\nexhibit varying levels of resilience to the attacks. In addition, the data\npoison attacks on the EEG signal-based human emotion evaluation systems are\nexplained with several Explainable Artificial Intelligence (XAI) methods,\nincluding Shapley Additive Explanation (SHAP) values, Local Interpretable\nModel-agnostic Explanations (LIME), and Generated Decision Trees. And the codes\nof this paper are publicly available on GitHub.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Jan 2023 14:44:46 GMT"
      }
    ],
    "update_date": "2023-03-15",
    "authors_parsed": [
      [
        "Zhang",
        "Zhibo",
        ""
      ],
      [
        "Umar",
        "Sani",
        ""
      ],
      [
        "Hammadi",
        "Ahmed Y. Al",
        ""
      ],
      [
        "Yoon",
        "Sangyoung",
        ""
      ],
      [
        "Damiani",
        "Ernesto",
        ""
      ],
      [
        "Ardagna",
        "Claudio Agostino",
        ""
      ],
      [
        "Bena",
        "Nicola",
        ""
      ],
      [
        "Yeun",
        "Chan Yeob",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.06923",
    "publish_date": "2023-01-17"
  },
  {
    "id": "2301.07520",
    "submitter": "Elisa Luciano",
    "authors": "Elisa Luciano and Matteo Cattaneo and Ron Kenett",
    "title": "Adversarial AI in Insurance: Pervasiveness and Resilience",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG q-fin.GN",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The rapid and dynamic pace of Artificial Intelligence (AI) and Machine\nLearning (ML) is revolutionizing the insurance sector. AI offers significant,\nvery much welcome advantages to insurance companies, and is fundamental to\ntheir customer-centricity strategy. It also poses challenges, in the project\nand implementation phase. Among those, we study Adversarial Attacks, which\nconsist of the creation of modified input data to deceive an AI system and\nproduce false outputs. We provide examples of attacks on insurance AI\napplications, categorize them, and argue on defence methods and precautionary\nsystems, considering that they can involve few-shot and zero-shot\nmultilabelling. A related topic, with growing interest, is the validation and\nverification of systems incorporating AI and ML components. These topics are\ndiscussed in various sections of this paper.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Jan 2023 08:49:54 GMT"
      }
    ],
    "update_date": "2023-01-19",
    "authors_parsed": [
      [
        "Luciano",
        "Elisa",
        ""
      ],
      [
        "Cattaneo",
        "Matteo",
        ""
      ],
      [
        "Kenett",
        "Ron",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.07520",
    "publish_date": "2023-01-17"
  },
  {
    "id": "2301.07829",
    "submitter": "Kimberly Mai",
    "authors": "Kimberly T. Mai, Sergi D. Bray, Toby Davies, Lewis D. Griffin",
    "title": "Warning: Humans Cannot Reliably Detect Speech Deepfakes",
    "comments": null,
    "journal-ref": "PLoS ONE 18(8) (2023): e0285333",
    "doi": "10.1371/journal.pone.0285333",
    "report-no": null,
    "categories": "cs.HC cs.SD eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Speech deepfakes are artificial voices generated by machine learning models.\nPrevious literature has highlighted deepfakes as one of the biggest security\nthreats arising from progress in artificial intelligence due to their potential\nfor misuse. However, studies investigating human detection capabilities are\nlimited. We presented genuine and deepfake audio to n = 529 individuals and\nasked them to identify the deepfakes. We ran our experiments in English and\nMandarin to understand if language affects detection performance and\ndecision-making rationale. We found that detection capability is unreliable.\nListeners only correctly spotted the deepfakes 73% of the time, and there was\nno difference in detectability between the two languages. Increasing listener\nawareness by providing examples of speech deepfakes only improves results\nslightly. As speech synthesis algorithms improve and become more realistic, we\ncan expect the detection task to become harder. The difficulty of detecting\nspeech deepfakes confirms their potential for misuse and signals that defenses\nagainst this threat are needed.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Jan 2023 00:17:48 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 2 Aug 2023 10:02:46 GMT"
      }
    ],
    "update_date": "2023-08-04",
    "authors_parsed": [
      [
        "Mai",
        "Kimberly T.",
        ""
      ],
      [
        "Bray",
        "Sergi D.",
        ""
      ],
      [
        "Davies",
        "Toby",
        ""
      ],
      [
        "Griffin",
        "Lewis D.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.07829",
    "publish_date": "2023-08-02"
  },
  {
    "id": "2301.09305",
    "submitter": "Fehmi Emre Kadan",
    "authors": "\\\"Omer Faruk Tuna, Fehmi Emre Kadan, Leyli Kara\\c{c}ay",
    "title": "Practical Adversarial Attacks Against AI-Driven Power Allocation in a\n  Distributed MIMO Network",
    "comments": "6 pages, 10 figures, accepted for presentation in International\n  Conference on Communications (ICC) 2023 in Communication and Information\n  System Security Symposium",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.SP cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In distributed multiple-input multiple-output (D-MIMO) networks, power\ncontrol is crucial to optimize the spectral efficiencies of users and max-min\nfairness (MMF) power control is a commonly used strategy as it satisfies\nuniform quality-of-service to all users. The optimal solution of MMF power\ncontrol requires high complexity operations and hence deep neural network based\nartificial intelligence (AI) solutions are proposed to decrease the complexity.\nAlthough quite accurate models can be achieved by using AI, these models have\nsome intrinsic vulnerabilities against adversarial attacks where carefully\ncrafted perturbations are applied to the input of the AI model. In this work,\nwe show that threats against the target AI model which might be originated from\nmalicious users or radio units can substantially decrease the network\nperformance by applying a successful adversarial sample, even in the most\nconstrained circumstances. We also demonstrate that the risk associated with\nthese kinds of adversarial attacks is higher than the conventional attack\nthreats. Detailed simulations reveal the effectiveness of adversarial attacks\nand the necessity of smart defense techniques.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 23 Jan 2023 07:51:25 GMT"
      }
    ],
    "update_date": "2023-01-24",
    "authors_parsed": [
      [
        "Tuna",
        "\u00d6mer Faruk",
        ""
      ],
      [
        "Kadan",
        "Fehmi Emre",
        ""
      ],
      [
        "Kara\u00e7ay",
        "Leyli",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.09305",
    "publish_date": "2023-01-23"
  },
  {
    "id": "2301.11767",
    "submitter": "Trisha Chakraborty",
    "authors": "Trisha Chakraborty, Shaswata Mitra, Sudip Mittal",
    "title": "CAPoW: Context-Aware AI-Assisted Proof of Work based DDoS Defense",
    "comments": "8 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Critical servers can be secured against distributed denial of service (DDoS)\nattacks using proof of work (PoW) systems assisted by an Artificial\nIntelligence (AI) that learns contextual network request patterns. In this\nwork, we introduce CAPoW, a context-aware anti-DDoS framework that injects\nlatency adaptively during communication by utilizing context-aware PoW puzzles.\nIn CAPoW, a security professional can define relevant request context\nattributes which can be learned by the AI system. These contextual attributes\ncan include information about the user request, such as IP address, time,\nflow-level information, etc., and are utilized to generate a contextual score\nfor incoming requests that influence the hardness of a PoW puzzle. These\npuzzles need to be solved by a user before the server begins to process their\nrequest. Solving puzzles slow down the volume of incoming adversarial requests.\nAdditionally, the framework compels the adversary to incur a cost per request,\nhence making it expensive for an adversary to prolong a DDoS attack. We include\nthe theoretical foundations of the CAPoW framework along with a description of\nits implementation and evaluation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 27 Jan 2023 15:06:41 GMT"
      }
    ],
    "update_date": "2023-01-30",
    "authors_parsed": [
      [
        "Chakraborty",
        "Trisha",
        ""
      ],
      [
        "Mitra",
        "Shaswata",
        ""
      ],
      [
        "Mittal",
        "Sudip",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.11767",
    "publish_date": "2023-01-27"
  },
  {
    "id": "2301.11990",
    "submitter": "Ilia Sucholutsky",
    "authors": "Ilia Sucholutsky, Thomas L. Griffiths",
    "title": "Alignment with human representations supports robust few-shot learning",
    "comments": "Spotlight at NeurIPS 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Should we care whether AI systems have representations of the world that are\nsimilar to those of humans? We provide an information-theoretic analysis that\nsuggests that there should be a U-shaped relationship between the degree of\nrepresentational alignment with humans and performance on few-shot learning\ntasks. We confirm this prediction empirically, finding such a relationship in\nan analysis of the performance of 491 computer vision models. We also show that\nhighly-aligned models are more robust to both natural adversarial attacks and\ndomain shifts. Our results suggest that human-alignment is often a sufficient,\nbut not necessary, condition for models to make effective use of limited data,\nbe robust, and generalize well.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 27 Jan 2023 21:03:19 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 4 Jun 2023 15:01:39 GMT"
      },
      {
        "version": "v3",
        "created": "Sun, 29 Oct 2023 19:45:09 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Sucholutsky",
        "Ilia",
        ""
      ],
      [
        "Griffiths",
        "Thomas L.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.11990",
    "publish_date": "2023-06-04"
  },
  {
    "id": "2301.12151",
    "submitter": "Holger Trittenbach",
    "authors": "Jona Klemenc, Holger Trittenbach",
    "title": "Selecting Models based on the Risk of Damage Caused by Adversarial\n  Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Regulation, legal liabilities, and societal concerns challenge the adoption\nof AI in safety and security-critical applications. One of the key concerns is\nthat adversaries can cause harm by manipulating model predictions without being\ndetected. Regulation hence demands an assessment of the risk of damage caused\nby adversaries. Yet, there is no method to translate this high-level demand\ninto actionable metrics that quantify the risk of damage.\n  In this article, we propose a method to model and statistically estimate the\nprobability of damage arising from adversarial attacks. We show that our\nproposed estimator is statistically consistent and unbiased. In experiments, we\ndemonstrate that the estimation results of our method have a clear and\nactionable interpretation and outperform conventional metrics. We then show how\noperators can use the estimation results to reliably select the model with the\nlowest risk.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 28 Jan 2023 10:24:38 GMT"
      }
    ],
    "update_date": "2023-01-31",
    "authors_parsed": [
      [
        "Klemenc",
        "Jona",
        ""
      ],
      [
        "Trittenbach",
        "Holger",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.12151",
    "publish_date": "2023-01-28"
  },
  {
    "id": "2301.12868",
    "submitter": "Terry Yue Zhuo",
    "authors": "Terry Yue Zhuo, Zhuang Li, Yujin Huang, Fatemeh Shiri, Weiqing Wang,\n  Gholamreza Haffari and Yuan-Fang Li",
    "title": "On Robustness of Prompt-based Semantic Parsing with Large Pre-trained\n  Language Model: An Empirical Study on Codex",
    "comments": "Accepted at EACL2023 (main)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Semantic parsing is a technique aimed at constructing a structured\nrepresentation of the meaning of a natural-language question. Recent\nadvancements in few-shot language models trained on code have demonstrated\nsuperior performance in generating these representations compared to\ntraditional unimodal language models, which are trained on downstream tasks.\nDespite these advancements, existing fine-tuned neural semantic parsers are\nsusceptible to adversarial attacks on natural-language inputs. While it has\nbeen established that the robustness of smaller semantic parsers can be\nenhanced through adversarial training, this approach is not feasible for large\nlanguage models in real-world scenarios, as it requires both substantial\ncomputational resources and expensive human annotation on in-domain semantic\nparsing data. This paper presents the first empirical study on the adversarial\nrobustness of a large prompt-based language model of code, \\codex. Our results\ndemonstrate that the state-of-the-art (SOTA) code-language models are\nvulnerable to carefully crafted adversarial examples. To address this\nchallenge, we propose methods for improving robustness without the need for\nsignificant amounts of labeled data or heavy computational resources.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 Jan 2023 13:21:00 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 6 Feb 2023 07:06:53 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 9 Mar 2023 11:01:02 GMT"
      }
    ],
    "update_date": "2023-03-10",
    "authors_parsed": [
      [
        "Zhuo",
        "Terry Yue",
        ""
      ],
      [
        "Li",
        "Zhuang",
        ""
      ],
      [
        "Huang",
        "Yujin",
        ""
      ],
      [
        "Shiri",
        "Fatemeh",
        ""
      ],
      [
        "Wang",
        "Weiqing",
        ""
      ],
      [
        "Haffari",
        "Gholamreza",
        ""
      ],
      [
        "Li",
        "Yuan-Fang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2301.12868",
    "publish_date": "2023-03-09"
  },
  {
    "id": "2302.00509",
    "submitter": "Pranav Kulkarni",
    "authors": "Pranav Kulkarni, Ziqing Ji, Yan Xu, Marko Neskovic, Kevin Nolan",
    "title": "Exploring Semantic Perturbations on Grover",
    "comments": "15 pages, 12 figures, 1 table, capstone research in machine learning",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With news and information being as easy to access as they currently are, it\nis more important than ever to ensure that people are not mislead by what they\nread. Recently, the rise of neural fake news (AI-generated fake news) and its\ndemonstrated effectiveness at fooling humans has prompted the development of\nmodels to detect it. One such model is the Grover model, which can both detect\nneural fake news to prevent it, and generate it to demonstrate how a model\ncould be misused to fool human readers. In this work we explore the Grover\nmodel's fake news detection capabilities by performing targeted attacks through\nperturbations on input news articles. Through this we test Grover's resilience\nto these adversarial attacks and expose some potential vulnerabilities which\nshould be addressed in further iterations to ensure it can detect all types of\nfake news accurately.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Feb 2023 15:28:55 GMT"
      }
    ],
    "update_date": "2023-02-02",
    "authors_parsed": [
      [
        "Kulkarni",
        "Pranav",
        ""
      ],
      [
        "Ji",
        "Ziqing",
        ""
      ],
      [
        "Xu",
        "Yan",
        ""
      ],
      [
        "Neskovic",
        "Marko",
        ""
      ],
      [
        "Nolan",
        "Kevin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.00509",
    "publish_date": "2023-02-01"
  },
  {
    "id": "2302.02162",
    "submitter": "Abdullah \\c{C}a\\u{g}lar \\\"Oks\\\"uz",
    "authors": "Abdullah Caglar Oksuz, Anisa Halimi, Erman Ayday",
    "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks\n  against White-Box Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Explainable Artificial Intelligence (XAI) encompasses a range of techniques\nand procedures aimed at elucidating the decision-making processes of AI models.\nWhile XAI is valuable in understanding the reasoning behind AI models, the data\nused for such revelations poses potential security and privacy vulnerabilities.\nExisting literature has identified privacy risks targeting machine learning\nmodels, including membership inference, model inversion, and model extraction\nattacks. Depending on the settings and parties involved, such attacks may\ntarget either the model itself or the training data used to create the model.\n  We have identified that tools providing XAI can particularly increase the\nvulnerability of model extraction attacks, which can be a significant issue\nwhen the owner of an AI model prefers to provide only black-box access rather\nthan sharing the model parameters and architecture with other parties. To\nexplore this privacy risk, we propose AUTOLYCUS, a model extraction attack that\nleverages the explanations provided by popular explainable AI tools. We\nparticularly focus on white-box machine learning (ML) models such as decision\ntrees and logistic regression models.\n  We have evaluated the performance of AUTOLYCUS on 5 machine learning\ndatasets, in terms of the surrogate model's accuracy and its similarity to the\ntarget model. We observe that the proposed attack is highly effective; it\nrequires up to 60x fewer queries to the target model compared to the\nstate-of-the-art attack, while providing comparable accuracy and similarity. We\nfirst validate the performance of the proposed algorithm on decision trees, and\nthen show its performance on logistic regression models as an indicator that\nthe proposed algorithm performs well on white-box ML models in general.\nFinally, we show that the existing countermeasures remain ineffective for the\nproposed attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 4 Feb 2023 13:23:39 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 7 May 2023 00:30:56 GMT"
      }
    ],
    "update_date": "2023-05-09",
    "authors_parsed": [
      [
        "Oksuz",
        "Abdullah Caglar",
        ""
      ],
      [
        "Halimi",
        "Anisa",
        ""
      ],
      [
        "Ayday",
        "Erman",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.02162",
    "publish_date": "2023-05-07"
  },
  {
    "id": "2302.04109",
    "submitter": "Zhibo Zhang",
    "authors": "Zhibo Zhang, Ahmed Y. Al Hammadi, Ernesto Damiani, and Chan Yeob Yeun",
    "title": "Explainable Label-flipping Attacks on Human Emotion Assessment System",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  This paper's main goal is to provide an attacker's point of view on data\npoisoning assaults that use label-flipping during the training phase of systems\nthat use electroencephalogram (EEG) signals to evaluate human emotion. To\nattack different machine learning classifiers such as Adaptive Boosting\n(AdaBoost) and Random Forest dedicated to the classification of 4 different\nhuman emotions using EEG signals, this paper proposes two scenarios of\nlabel-flipping methods. The results of the studies show that the proposed data\npoison attacksm based on label-flipping are successful regardless of the model,\nbut different models show different degrees of resistance to the assaults. In\naddition, numerous Explainable Artificial Intelligence (XAI) techniques are\nused to explain the data poison attacks on EEG signal-based human emotion\nevaluation systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Feb 2023 15:04:27 GMT"
      }
    ],
    "update_date": "2023-02-09",
    "authors_parsed": [
      [
        "Zhang",
        "Zhibo",
        ""
      ],
      [
        "Hammadi",
        "Ahmed Y. Al",
        ""
      ],
      [
        "Damiani",
        "Ernesto",
        ""
      ],
      [
        "Yeun",
        "Chan Yeob",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.04109",
    "publish_date": "2023-02-08"
  },
  {
    "id": "2302.04578",
    "submitter": "Xiaoyu Wu",
    "authors": "Chumeng Liang, Xiaoyu Wu, Yang Hua, Jiaru Zhang, Yiming Xue, Tao Song,\n  Zhengui Xue, Ruhui Ma, Haibing Guan",
    "title": "Adversarial Example Does Good: Preventing Painting Imitation from\n  Diffusion Models via Adversarial Examples",
    "comments": "Accepted by ICML2023 (Oral)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recently, Diffusion Models (DMs) boost a wave in AI for Art yet raise new\ncopyright concerns, where infringers benefit from using unauthorized paintings\nto train DMs to generate novel paintings in a similar style. To address these\nemerging copyright violations, in this paper, we are the first to explore and\npropose to utilize adversarial examples for DMs to protect human-created\nartworks. Specifically, we first build a theoretical framework to define and\nevaluate the adversarial examples for DMs. Then, based on this framework, we\ndesign a novel algorithm, named AdvDM, which exploits a Monte-Carlo estimation\nof adversarial examples for DMs by optimizing upon different latent variables\nsampled from the reverse process of DMs. Extensive experiments show that the\ngenerated adversarial examples can effectively hinder DMs from extracting their\nfeatures. Therefore, our method can be a powerful tool for human artists to\nprotect their copyright against infringers equipped with DM-based AI-for-Art\napplications. The code of our method is available on GitHub:\nhttps://github.com/mist-project/mist.git.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 9 Feb 2023 11:36:39 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 6 Jun 2023 06:34:46 GMT"
      }
    ],
    "update_date": "2023-06-07",
    "authors_parsed": [
      [
        "Liang",
        "Chumeng",
        ""
      ],
      [
        "Wu",
        "Xiaoyu",
        ""
      ],
      [
        "Hua",
        "Yang",
        ""
      ],
      [
        "Zhang",
        "Jiaru",
        ""
      ],
      [
        "Xue",
        "Yiming",
        ""
      ],
      [
        "Song",
        "Tao",
        ""
      ],
      [
        "Xue",
        "Zhengui",
        ""
      ],
      [
        "Ma",
        "Ruhui",
        ""
      ],
      [
        "Guan",
        "Haibing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.04578",
    "publish_date": "2023-06-06"
  },
  {
    "id": "2302.07735",
    "submitter": "Ali Al-Kaswan",
    "authors": "Ali Al-Kaswan, Maliheh Izadi, Arie van Deursen",
    "title": "Targeted Attack on GPT-Neo for the SATML Language Model Data Extraction\n  Challenge",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Previous work has shown that Large Language Models are susceptible to\nso-called data extraction attacks. This allows an attacker to extract a sample\nthat was contained in the training data, which has massive privacy\nimplications. The construction of data extraction attacks is challenging,\ncurrent attacks are quite inefficient, and there exists a significant gap in\nthe extraction capabilities of untargeted attacks and memorization. Thus,\ntargeted attacks are proposed, which identify if a given sample from the\ntraining data, is extractable from a model. In this work, we apply a targeted\ndata extraction attack to the SATML2023 Language Model Training Data Extraction\nChallenge. We apply a two-step approach. In the first step, we maximise the\nrecall of the model and are able to extract the suffix for 69% of the samples.\nIn the second step, we use a classifier-based Membership Inference Attack on\nthe generations. Our AutoSklearn classifier achieves a precision of 0.841. The\nfull approach reaches a score of 0.405 recall at a 10% false positive rate,\nwhich is an improvement of 34% over the baseline of 0.301.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Feb 2023 18:00:44 GMT"
      }
    ],
    "update_date": "2023-02-16",
    "authors_parsed": [
      [
        "Al-Kaswan",
        "Ali",
        ""
      ],
      [
        "Izadi",
        "Maliheh",
        ""
      ],
      [
        "van Deursen",
        "Arie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.07735",
    "publish_date": "2023-02-13"
  },
  {
    "id": "2302.09198",
    "submitter": "Chengzhe Sun",
    "authors": "Chengzhe Sun, Shan Jia, Shuwei Hou, Ehab AlBadawy, Siwei Lyu",
    "title": "Exposing AI-Synthesized Human Voices Using Neural Vocoder Artifacts",
    "comments": "Dataset and codes will be available at\n  https://github.com/csun22/LibriVoc-Dataset",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.MM eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The advancements of AI-synthesized human voices have introduced a growing\nthreat of impersonation and disinformation. It is therefore of practical\nimportance to developdetection methods for synthetic human voices. This work\nproposes a new approach to detect synthetic human voices based on identifying\nartifacts of neural vocoders in audio signals. A neural vocoder is a specially\ndesigned neural network that synthesizes waveforms from temporal-frequency\nrepresentations, e.g., mel-spectrograms. The neural vocoder is a core component\nin most DeepFake audio synthesis models. Hence the identification of neural\nvocoder processing implies that an audio sample may have been synthesized. To\ntake advantage of the vocoder artifacts for synthetic human voice detection, we\nintroduce a multi-task learning framework for a binary-class RawNet2 model that\nshares the front-end feature extractor with a vocoder identification module. We\ntreat the vocoder identification as a pretext task to constrain the front-end\nfeature extractor to focus on vocoder artifacts and provide discriminative\nfeatures for the final binary classifier. Our experiments show that the\nimproved RawNet2 model based on vocoder identification achieves an overall high\nclassification performance on the binary task.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 18 Feb 2023 00:29:22 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 27 Apr 2023 08:48:38 GMT"
      }
    ],
    "update_date": "2023-04-28",
    "authors_parsed": [
      [
        "Sun",
        "Chengzhe",
        ""
      ],
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Hou",
        "Shuwei",
        ""
      ],
      [
        "AlBadawy",
        "Ehab",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.09198",
    "publish_date": "2023-04-27"
  },
  {
    "id": "2302.09270",
    "submitter": "Jiawen Deng",
    "authors": "Jiawen Deng, Jiale Cheng, Hao Sun, Zhexin Zhang, Minlie Huang",
    "title": "Towards Safer Generative Language Models: A Survey on Safety Risks,\n  Evaluations, and Improvements",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As generative large model capabilities advance, safety concerns become more\npronounced in their outputs. To ensure the sustainable growth of the AI\necosystem, it's imperative to undertake a holistic evaluation and refinement of\nassociated safety risks. This survey presents a framework for safety research\npertaining to large models, delineating the landscape of safety risks as well\nas safety evaluation and improvement methods. We begin by introducing safety\nissues of wide concern, then delve into safety evaluation methods for large\nmodels, encompassing preference-based testing, adversarial attack approaches,\nissues detection, and other advanced evaluation methods. Additionally, we\nexplore the strategies for enhancing large model safety from training to\ndeployment, highlighting cutting-edge safety approaches for each stage in\nbuilding large models. Finally, we discuss the core challenges in advancing\ntowards more responsible AI, including the interpretability of safety\nmechanisms, ongoing safety issues, and robustness against malicious attacks.\nThrough this survey, we aim to provide clear technical guidance for safety\nresearchers and encourage further study on the safety of large models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 18 Feb 2023 09:32:55 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 7 Mar 2023 03:28:47 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 30 Nov 2023 06:39:19 GMT"
      }
    ],
    "update_date": "2023-12-01",
    "authors_parsed": [
      [
        "Deng",
        "Jiawen",
        ""
      ],
      [
        "Cheng",
        "Jiale",
        ""
      ],
      [
        "Sun",
        "Hao",
        ""
      ],
      [
        "Zhang",
        "Zhexin",
        ""
      ],
      [
        "Huang",
        "Minlie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.09270",
    "publish_date": "2023-03-07"
  },
  {
    "id": "2302.10637",
    "submitter": "Yifei Zhang",
    "authors": "Yifei Zhang, Dun Zeng, Jinglong Luo, Zenglin Xu, Irwin King",
    "title": "A Survey of Trustworthy Federated Learning with Perspectives on\n  Security, Robustness, and Privacy",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Trustworthy artificial intelligence (AI) technology has revolutionized daily\nlife and greatly benefited human society. Among various AI technologies,\nFederated Learning (FL) stands out as a promising solution for diverse\nreal-world scenarios, ranging from risk evaluation systems in finance to\ncutting-edge technologies like drug discovery in life sciences. However,\nchallenges around data isolation and privacy threaten the trustworthiness of FL\nsystems. Adversarial attacks against data privacy, learning algorithm\nstability, and system confidentiality are particularly concerning in the\ncontext of distributed training in federated learning. Therefore, it is crucial\nto develop FL in a trustworthy manner, with a focus on security, robustness,\nand privacy. In this survey, we propose a comprehensive roadmap for developing\ntrustworthy FL systems and summarize existing efforts from three key aspects:\nsecurity, robustness, and privacy. We outline the threats that pose\nvulnerabilities to trustworthy federated learning across different stages of\ndevelopment, including data processing, model training, and deployment. To\nguide the selection of the most appropriate defense methods, we discuss\nspecific technical solutions for realizing each aspect of Trustworthy FL (TFL).\nOur approach differs from previous work that primarily discusses TFL from a\nlegal perspective or presents FL from a high-level, non-technical viewpoint.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 21 Feb 2023 12:52:12 GMT"
      }
    ],
    "update_date": "2023-02-22",
    "authors_parsed": [
      [
        "Zhang",
        "Yifei",
        ""
      ],
      [
        "Zeng",
        "Dun",
        ""
      ],
      [
        "Luo",
        "Jinglong",
        ""
      ],
      [
        "Xu",
        "Zenglin",
        ""
      ],
      [
        "King",
        "Irwin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.10637",
    "publish_date": "2023-02-21"
  },
  {
    "id": "2302.11704",
    "submitter": "Pedro Machado PhD",
    "authors": "Saminder Dhesi, Laura Fontes, Pedro Machado, Isibor Kennedy Ihianle,\n  Farhad Fassihi Tash, David Ada Adama",
    "title": "Mitigating Adversarial Attacks in Deepfake Detection: An Exploration of\n  Perturbation and AI Techniques",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep learning constitutes a pivotal component within the realm of machine\nlearning, offering remarkable capabilities in tasks ranging from image\nrecognition to natural language processing. However, this very strength also\nrenders deep learning models susceptible to adversarial examples, a phenomenon\npervasive across a diverse array of applications. These adversarial examples\nare characterized by subtle perturbations artfully injected into clean images\nor videos, thereby causing deep learning algorithms to misclassify or produce\nerroneous outputs. This susceptibility extends beyond the confines of digital\ndomains, as adversarial examples can also be strategically designed to target\nhuman cognition, leading to the creation of deceptive media, such as deepfakes.\nDeepfakes, in particular, have emerged as a potent tool to manipulate public\nopinion and tarnish the reputations of public figures, underscoring the urgent\nneed to address the security and ethical implications associated with\nadversarial examples. This article delves into the multifaceted world of\nadversarial examples, elucidating the underlying principles behind their\ncapacity to deceive deep learning algorithms. We explore the various\nmanifestations of this phenomenon, from their insidious role in compromising\nmodel reliability to their impact in shaping the contemporary landscape of\ndisinformation and misinformation. To illustrate progress in combating\nadversarial examples, we showcase the development of a tailored Convolutional\nNeural Network (CNN) designed explicitly to detect deepfakes, a pivotal step\ntowards enhancing model robustness in the face of adversarial threats.\nImpressively, this custom CNN has achieved a precision rate of 76.2% on the\nDFDC dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 22 Feb 2023 23:48:19 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 10 Sep 2023 00:22:22 GMT"
      }
    ],
    "update_date": "2023-09-12",
    "authors_parsed": [
      [
        "Dhesi",
        "Saminder",
        ""
      ],
      [
        "Fontes",
        "Laura",
        ""
      ],
      [
        "Machado",
        "Pedro",
        ""
      ],
      [
        "Ihianle",
        "Isibor Kennedy",
        ""
      ],
      [
        "Tash",
        "Farhad Fassihi",
        ""
      ],
      [
        "Adama",
        "David Ada",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.11704",
    "publish_date": "2023-02-22"
  },
  {
    "id": "2302.12173",
    "submitter": "Sahar Abdelnabi",
    "authors": "Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres,\n  Thorsten Holz, Mario Fritz",
    "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated\n  Applications with Indirect Prompt Injection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 23 Feb 2023 17:14:38 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 5 May 2023 14:26:17 GMT"
      }
    ],
    "update_date": "2023-05-08",
    "authors_parsed": [
      [
        "Greshake",
        "Kai",
        ""
      ],
      [
        "Abdelnabi",
        "Sahar",
        ""
      ],
      [
        "Mishra",
        "Shailesh",
        ""
      ],
      [
        "Endres",
        "Christoph",
        ""
      ],
      [
        "Holz",
        "Thorsten",
        ""
      ],
      [
        "Fritz",
        "Mario",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.12173",
    "publish_date": "2023-02-23"
  },
  {
    "id": "2302.12415",
    "submitter": "Khatoon Mohammed",
    "authors": "Khatoon Mohammed",
    "title": "Harnessing the Speed and Accuracy of Machine Learning to Advance\n  Cybersecurity",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As cyber attacks continue to increase in frequency and sophistication,\ndetecting malware has become a critical task for maintaining the security of\ncomputer systems. Traditional signature-based methods of malware detection have\nlimitations in detecting complex and evolving threats. In recent years, machine\nlearning (ML) has emerged as a promising solution to detect malware\neffectively. ML algorithms are capable of analyzing large datasets and\nidentifying patterns that are difficult for humans to identify. This paper\npresents a comprehensive review of the state-of-the-art ML techniques used in\nmalware detection, including supervised and unsupervised learning, deep\nlearning, and reinforcement learning. We also examine the challenges and\nlimitations of ML-based malware detection, such as the potential for\nadversarial attacks and the need for large amounts of labeled data.\nFurthermore, we discuss future directions in ML-based malware detection,\nincluding the integration of multiple ML algorithms and the use of explainable\nAI techniques to enhance the interpret ability of ML-based detection systems.\nOur research highlights the potential of ML-based techniques to improve the\nspeed and accuracy of malware detection, and contribute to enhancing\ncybersecurity\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 24 Feb 2023 02:42:38 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 10 Mar 2023 20:29:48 GMT"
      }
    ],
    "update_date": "2023-03-14",
    "authors_parsed": [
      [
        "Mohammed",
        "Khatoon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.12415",
    "publish_date": "2023-02-24"
  },
  {
    "id": "2302.12461",
    "submitter": "Max Lamparth",
    "authors": "Max Lamparth, Anka Reuel",
    "title": "Analyzing And Editing Inner Mechanisms Of Backdoored Language Models",
    "comments": "included new experimental results and addressed reviewer feedback",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Poisoning of data sets is a potential security threat to large language\nmodels that can lead to backdoored models. A description of the internal\nmechanisms of backdoored language models and how they process trigger inputs,\ne.g., when switching to toxic language, has yet to be found. In this work, we\nstudy the internal representations of transformer-based backdoored language\nmodels and determine early-layer MLP modules as most important for the backdoor\nmechanism in combination with the initial embedding projection. We use this\nknowledge to remove, insert, and modify backdoor mechanisms with engineered\nreplacements that reduce the MLP module outputs to essentials for the backdoor\nmechanism. To this end, we introduce PCP ablation, where we replace transformer\nmodules with low-rank matrices based on the principal components of their\nactivations. We demonstrate our results on backdoored toy, backdoored large,\nand non-backdoored open-source models. We show that we can improve the backdoor\nrobustness of large language models by locally constraining individual modules\nduring fine-tuning on potentially poisonous data sets.\n  Trigger warning: Offensive language.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 24 Feb 2023 05:26:08 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 12 Oct 2023 02:29:09 GMT"
      }
    ],
    "update_date": "2023-10-13",
    "authors_parsed": [
      [
        "Lamparth",
        "Max",
        ""
      ],
      [
        "Reuel",
        "Anka",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.12461",
    "publish_date": "2023-10-12"
  },
  {
    "id": "2302.13056",
    "submitter": "Huasong Zhou",
    "authors": "Huasong Zhou, Xiaowei Xu, Xiaodong Wang, and Leon Bevan Bullock",
    "title": "SATBA: An Invisible Backdoor Attack Based On Spatial Attention",
    "comments": "15 pages, 6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Backdoor attacks pose a new and emerging threat to AI security, where Deep\nNeural Networks (DNNs) are trained on datasets added to hidden trigger\npatterns. Although the poisoned model behaves normally on benign samples, it\nproduces anomalous results on samples containing the trigger pattern.\nNevertheless, most existing backdoor attacks face two significant drawbacks:\ntheir trigger patterns are visible and easy to detect by human inspection, and\ntheir injection process leads to the loss of natural sample features and\ntrigger patterns, thereby reducing the attack success rate and the model\naccuracy. In this paper, we propose a novel backdoor attack named SATBA that\novercomes these limitations by using spatial attention mechanism and U-type\nmodel. Our attack leverages spatial attention mechanism to extract data\nfeatures and generate invisible trigger patterns that are correlated with clean\ndata. Then it uses U-type model to plant these trigger patterns into the\noriginal data without causing noticeable feature loss. We evaluate our attack\non three prominent image classification DNNs across three standard datasets and\ndemonstrate that it achieves high attack success rate and robustness against\nbackdoor defenses. Additionally, we also conduct extensive experiments on image\nsimilarity to highlight the stealthiness of our attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 25 Feb 2023 10:57:41 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 26 Mar 2023 14:23:10 GMT"
      }
    ],
    "update_date": "2023-03-28",
    "authors_parsed": [
      [
        "Zhou",
        "Huasong",
        ""
      ],
      [
        "Xu",
        "Xiaowei",
        ""
      ],
      [
        "Wang",
        "Xiaodong",
        ""
      ],
      [
        "Bullock",
        "Leon Bevan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.13056",
    "publish_date": "2023-03-26"
  },
  {
    "id": "2302.14500",
    "submitter": "Chong Fu",
    "authors": "Chong Fu, Xuhong Zhang, Shouling Ji, Ting Wang, Peng Lin, Yanghe Feng,\n  Jianwei Yin",
    "title": "FreeEagle: Detecting Complex Neural Trojans in Data-Free Cases",
    "comments": "Accepted by USENIX Security 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Trojan attack on deep neural networks, also known as backdoor attack, is a\ntypical threat to artificial intelligence. A trojaned neural network behaves\nnormally with clean inputs. However, if the input contains a particular\ntrigger, the trojaned model will have attacker-chosen abnormal behavior.\nAlthough many backdoor detection methods exist, most of them assume that the\ndefender has access to a set of clean validation samples or samples with the\ntrigger, which may not hold in some crucial real-world cases, e.g., the case\nwhere the defender is the maintainer of model-sharing platforms. Thus, in this\npaper, we propose FreeEagle, the first data-free backdoor detection method that\ncan effectively detect complex backdoor attacks on deep neural networks,\nwithout relying on the access to any clean samples or samples with the trigger.\nThe evaluation results on diverse datasets and model architectures show that\nFreeEagle is effective against various complex backdoor attacks, even\noutperforming some state-of-the-art non-data-free backdoor detection methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 28 Feb 2023 11:31:29 GMT"
      }
    ],
    "update_date": "2023-03-01",
    "authors_parsed": [
      [
        "Fu",
        "Chong",
        ""
      ],
      [
        "Zhang",
        "Xuhong",
        ""
      ],
      [
        "Ji",
        "Shouling",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ],
      [
        "Lin",
        "Peng",
        ""
      ],
      [
        "Feng",
        "Yanghe",
        ""
      ],
      [
        "Yin",
        "Jianwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2302.14500",
    "publish_date": "2023-02-28"
  },
  {
    "id": "2303.00333",
    "submitter": "Adam Davies",
    "authors": "Adam Davies, Jize Jiang, ChengXiang Zhai",
    "title": "Competence-Based Analysis of Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Despite the recent success of large, pretrained neural language models (LLMs)\non a variety of prompting tasks, these models can be alarmingly brittle to\nsmall changes in inputs or application contexts. To better understand such\nbehavior and motivate the design of more robust LLMs, we provide a causal\nformulation of linguistic competence in the context of LLMs and propose a\ngeneral framework to study and measure LLM competence. Our framework, CALM\n(Competence-based Analysis of Language Models), establishes the first\nquantitative measure of LLM competence, which we study by damaging models'\ninternal representations of various linguistic properties in the course of\nperforming various tasks using causal probing and evaluating models' alignment\nunder these interventions with a given causal model. We also develop a novel\napproach for performing causal probing interventions using gradient-based\nadversarial attacks, which can target a broader range of properties and\nrepresentations than existing techniques. We carry out a case study of CALM\nusing these interventions to analyze BERT and RoBERTa's competence across a\nvariety of lexical inference tasks, showing that the CALM framework and\ncompetence metric can be valuable tools for explaining and predicting their\nbehavior across these tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Mar 2023 08:53:36 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 31 Jul 2023 17:49:44 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 7 Nov 2023 02:33:27 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Davies",
        "Adam",
        ""
      ],
      [
        "Jiang",
        "Jize",
        ""
      ],
      [
        "Zhai",
        "ChengXiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.00333",
    "publish_date": "2023-03-01"
  },
  {
    "id": "2303.00608",
    "submitter": "Luca Guarnera",
    "authors": "Luca Guarnera (1), Oliver Giudice (2), Sebastiano Battiato (1) ((1)\n  Department of Mathematics and Computer Science, University of Catania, Italy,\n  (2) Applied Research Team, IT dept., Banca d'Italia, Rome, Italy)",
    "title": "Level Up the Deepfake Detection: a Method to Effectively Discriminate\n  Images Generated by GAN Architectures and Diffusion Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The image deepfake detection task has been greatly addressed by the\nscientific community to discriminate real images from those generated by\nArtificial Intelligence (AI) models: a binary classification task. In this\nwork, the deepfake detection and recognition task was investigated by\ncollecting a dedicated dataset of pristine images and fake ones generated by 9\ndifferent Generative Adversarial Network (GAN) architectures and by 4\nadditional Diffusion Models (DM). A hierarchical multi-level approach was then\nintroduced to solve three different deepfake detection and recognition tasks:\n(i) Real Vs AI generated; (ii) GANs Vs DMs; (iii) AI specific architecture\nrecognition. Experimental results demonstrated, in each case, more than 97%\nclassification accuracy, outperforming state-of-the-art methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Mar 2023 16:01:46 GMT"
      }
    ],
    "update_date": "2023-03-02",
    "authors_parsed": [
      [
        "Guarnera",
        "Luca",
        ""
      ],
      [
        "Giudice",
        "Oliver",
        ""
      ],
      [
        "Battiato",
        "Sebastiano",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.00608",
    "publish_date": "2023-03-01"
  },
  {
    "id": "2303.03012",
    "submitter": "Zongjie Li",
    "authors": "Zongjie Li, Chaozheng Wang, Pingchuan Ma, Chaowei Liu, Shuai Wang,\n  Daoyuan Wu, Cuiyun Gao, Yang Liu",
    "title": "On Extracting Specialized Code Abilities from Large Language Models: A\n  Feasibility Study",
    "comments": "13 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in large language models (LLMs) significantly boost their\nusage in software engineering. However, training a well-performing LLM demands\na substantial workforce for data collection and annotation. Moreover, training\ndatasets may be proprietary or partially open, and the process often requires a\ncostly GPU cluster. The intellectual property value of commercial LLMs makes\nthem attractive targets for imitation attacks, but creating an imitation model\nwith comparable parameters still incurs high costs. This motivates us to\nexplore a practical and novel direction: slicing commercial black-box LLMs\nusing medium-sized backbone models. In this paper, we explore the feasibility\nof launching imitation attacks on LLMs to extract their specialized code\nabilities, such as\"code synthesis\" and \"code translation.\" We systematically\ninvestigate the effectiveness of launching code ability extraction attacks\nunder different code-related tasks with multiple query schemes, including\nzero-shot, in-context, and Chain-of-Thought. We also design response checks to\nrefine the outputs, leading to an effective imitation training process. Our\nresults show promising outcomes, demonstrating that with a reasonable number of\nqueries, attackers can train a medium-sized backbone model to replicate\nspecialized code behaviors similar to the target LLMs. We summarize our\nfindings and insights to help researchers better understand the threats posed\nby imitation attacks, including revealing a practical attack surface for\ngenerating adversarial code examples against LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Mar 2023 10:34:41 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 10 Mar 2023 02:25:00 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 4 Apr 2023 05:17:22 GMT"
      },
      {
        "version": "v4",
        "created": "Tue, 31 Oct 2023 13:37:00 GMT"
      }
    ],
    "update_date": "2023-11-01",
    "authors_parsed": [
      [
        "Li",
        "Zongjie",
        ""
      ],
      [
        "Wang",
        "Chaozheng",
        ""
      ],
      [
        "Ma",
        "Pingchuan",
        ""
      ],
      [
        "Liu",
        "Chaowei",
        ""
      ],
      [
        "Wang",
        "Shuai",
        ""
      ],
      [
        "Wu",
        "Daoyuan",
        ""
      ],
      [
        "Gao",
        "Cuiyun",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.03012",
    "publish_date": "2023-04-04"
  },
  {
    "id": "2303.03140",
    "submitter": "Elisabetta Biasin",
    "authors": "Elisabetta Biasin, Erik Kamenjasevic, Kaspar Rosager Ludvigsen",
    "title": "Cybersecurity of AI medical devices: risks, legislation, and challenges",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Medical devices and artificial intelligence systems rapidly transform\nhealthcare provisions. At the same time, due to their nature, AI in or as\nmedical devices might get exposed to cyberattacks, leading to patient safety\nand security risks. This book chapter is divided into three parts. The first\npart starts by setting the scene where we explain the role of cybersecurity in\nhealthcare. Then, we briefly define what we refer to when we talk about AI that\nis considered a medical device by itself or supports one. To illustrate the\nrisks such medical devices pose, we provide three examples: the poisoning of\ndatasets, social engineering, and data or source code extraction. In the second\npart, the paper provides an overview of the European Union's regulatory\nframework relevant for ensuring the cybersecurity of AI as or in medical\ndevices (MDR, NIS Directive, Cybersecurity Act, GDPR, the AI Act proposal and\nthe NIS 2 Directive proposal). Finally, the third part of the paper examines\npossible challenges stemming from the EU regulatory framework. In particular,\nwe look toward the challenges deriving from the two legislative proposals and\ntheir interaction with the existing legislation concerning AI medical devices'\ncybersecurity. They are structured as answers to the following questions: (1)\nhow will the AI Act interact with the MDR regarding the cybersecurity and\nsafety requirements?; (2) how should we interpret incident notification\nrequirements from the NIS 2 Directive proposal and MDR?; and (3) what are the\nconsequences of the evolving term of critical infrastructures?\n  [This is a draft chapter. The final version will be available in Research\nHandbook on Health, AI and the Law edited by Barry Solaiman & I. Glenn Cohen,\nforthcoming 2023, Edward Elgar Publishing Ltd]\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Mar 2023 13:55:57 GMT"
      }
    ],
    "update_date": "2023-03-07",
    "authors_parsed": [
      [
        "Biasin",
        "Elisabetta",
        ""
      ],
      [
        "Kamenjasevic",
        "Erik",
        ""
      ],
      [
        "Ludvigsen",
        "Kaspar Rosager",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.03140",
    "publish_date": "2023-03-06"
  },
  {
    "id": "2303.06151",
    "submitter": "Yongxin Liu",
    "authors": "Wenkai Tan, Justus Renkhoff, Alvaro Velasquez, Ziyu Wang, Lusi Li,\n  Jian Wang, Shuteng Niu, Fan Yang, Yongxin Liu, Houbing Song",
    "title": "NoiseCAM: Explainable AI for the Boundary Between Noise and Adversarial\n  Attacks",
    "comments": "Submitted to IEEE Fuzzy 2023. arXiv admin note: text overlap with\n  arXiv:2303.06032",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Deep Learning (DL) and Deep Neural Networks (DNNs) are widely used in various\ndomains. However, adversarial attacks can easily mislead a neural network and\nlead to wrong decisions. Defense mechanisms are highly preferred in\nsafety-critical applications. In this paper, firstly, we use the gradient class\nactivation map (GradCAM) to analyze the behavior deviation of the VGG-16\nnetwork when its inputs are mixed with adversarial perturbation or Gaussian\nnoise. In particular, our method can locate vulnerable layers that are\nsensitive to adversarial perturbation and Gaussian noise. We also show that the\nbehavior deviation of vulnerable layers can be used to detect adversarial\nexamples. Secondly, we propose a novel NoiseCAM algorithm that integrates\ninformation from globally and pixel-level weighted class activation maps. Our\nalgorithm is susceptible to adversarial perturbations and will not respond to\nGaussian random noise mixed in the inputs. Third, we compare detecting\nadversarial examples using both behavior deviation and NoiseCAM, and we show\nthat NoiseCAM outperforms behavior deviation modeling in its overall\nperformance. Our work could provide a useful tool to defend against certain\nadversarial attacks on deep neural networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 9 Mar 2023 22:07:41 GMT"
      }
    ],
    "update_date": "2023-03-14",
    "authors_parsed": [
      [
        "Tan",
        "Wenkai",
        ""
      ],
      [
        "Renkhoff",
        "Justus",
        ""
      ],
      [
        "Velasquez",
        "Alvaro",
        ""
      ],
      [
        "Wang",
        "Ziyu",
        ""
      ],
      [
        "Li",
        "Lusi",
        ""
      ],
      [
        "Wang",
        "Jian",
        ""
      ],
      [
        "Niu",
        "Shuteng",
        ""
      ],
      [
        "Yang",
        "Fan",
        ""
      ],
      [
        "Liu",
        "Yongxin",
        ""
      ],
      [
        "Song",
        "Houbing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.06151",
    "publish_date": "2023-03-09"
  },
  {
    "id": "2303.06216",
    "submitter": "Dilrukshi Gamage",
    "authors": "Dilrukshi Gamage, Hariharan Ravinthran, Kazutoshi Sasahara",
    "title": "Moral intuitions behind deepfake-related discussions in Reddit\n  communities",
    "comments": "This is more related to CSCW, societal implications. Manuscript to be\n  submitted to SM+S Journal (Sage Group)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.HC cs.SI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Deepfakes are AI-synthesized content that are becoming popular on many social\nmedia platforms, meaning the use of deepfakes is increasing in society,\nregardless of its societal implications. Its implications are harmful if the\nmoral intuitions behind deepfakes are problematic; thus, it is important to\nexplore how the moral intuitions behind deepfakes unfold in communities at\nscale. However, understanding perceived moral viewpoints unfolding in digital\ncontexts is challenging, due to the complexities in conversations. In this\nresearch, we demonstrate how Moral Foundations Theory (MFT) can be used as a\nlens through which to operationalize moral viewpoints in discussions about\ndeepfakes on Reddit communities. Using the extended Moral Foundations\nDictionary (eMFD), we measured the strengths of moral intuition (moral loading)\nbehind 101,869 Reddit posts. We present the discussions that unfolded on Reddit\nin 2018 to 2022 wherein intuitions behind some posts were found to be morally\nquestionable to society. Our results may help platforms detect and take action\nagainst immoral activities related to deepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Feb 2023 07:39:05 GMT"
      }
    ],
    "update_date": "2023-03-14",
    "authors_parsed": [
      [
        "Gamage",
        "Dilrukshi",
        ""
      ],
      [
        "Ravinthran",
        "Hariharan",
        ""
      ],
      [
        "Sasahara",
        "Kazutoshi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.06216",
    "publish_date": "2023-02-27"
  },
  {
    "id": "2303.08866",
    "submitter": "Ian Nielsen",
    "authors": "Ian E. Nielsen, Ravi P. Ramachandran, Nidhal Bouaynaya, Hassan M.\n  Fathallah-Shaykh, Ghulam Rasool",
    "title": "EvalAttAI: A Holistic Approach to Evaluating Attribution Maps in Robust\n  and Non-Robust Models",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1109/ACCESS.2023.3300242",
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The expansion of explainable artificial intelligence as a field of research\nhas generated numerous methods of visualizing and understanding the black box\nof a machine learning model. Attribution maps are generally used to highlight\nthe parts of the input image that influence the model to make a specific\ndecision. On the other hand, the robustness of machine learning models to\nnatural noise and adversarial attacks is also being actively explored. This\npaper focuses on evaluating methods of attribution mapping to find whether\nrobust neural networks are more explainable. We explore this problem within the\napplication of classification for medical imaging. Explainability research is\nat an impasse. There are many methods of attribution mapping, but no current\nconsensus on how to evaluate them and determine the ones that are the best. Our\nexperiments on multiple datasets (natural and medical imaging) and various\nattribution methods reveal that two popular evaluation metrics, Deletion and\nInsertion, have inherent limitations and yield contradictory results. We\npropose a new explainability faithfulness metric (called EvalAttAI) that\naddresses the limitations of prior metrics. Using our novel evaluation, we\nfound that Bayesian deep neural networks using the Variational Density\nPropagation technique were consistently more explainable when used with the\nbest performing attribution method, the Vanilla Gradient. However, in general,\nvarious types of robust neural networks may not be more explainable, despite\nthese models producing more visually plausible attribution maps.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Mar 2023 18:33:22 GMT"
      }
    ],
    "update_date": "2023-08-02",
    "authors_parsed": [
      [
        "Nielsen",
        "Ian E.",
        ""
      ],
      [
        "Ramachandran",
        "Ravi P.",
        ""
      ],
      [
        "Bouaynaya",
        "Nidhal",
        ""
      ],
      [
        "Fathallah-Shaykh",
        "Hassan M.",
        ""
      ],
      [
        "Rasool",
        "Ghulam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.08866",
    "publish_date": "2023-03-15"
  },
  {
    "id": "2303.09272",
    "submitter": "Mahawaga Arachchige Pathum Chamikara",
    "authors": "Haonan Zhong, Jiamin Chang, Ziyue Yang, Tingmin Wu, Pathum Chamikara\n  Mahawaga Arachchige, Chehara Pathmabandu, Minhui Xue",
    "title": "Copyright Protection and Accountability of Generative AI:Attack,\n  Watermarking and Attribution",
    "comments": null,
    "journal-ref": null,
    "doi": "10.1145/3543873.3587321",
    "report-no": null,
    "categories": "cs.LG cs.CR cs.MM",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Generative AI (e.g., Generative Adversarial Networks - GANs) has become\nincreasingly popular in recent years. However, Generative AI introduces\nsignificant concerns regarding the protection of Intellectual Property Rights\n(IPR) (resp. model accountability) pertaining to images (resp. toxic images)\nand models (resp. poisoned models) generated. In this paper, we propose an\nevaluation framework to provide a comprehensive overview of the current state\nof the copyright protection measures for GANs, evaluate their performance\nacross a diverse range of GAN architectures, and identify the factors that\naffect their performance and future research directions. Our findings indicate\nthat the current IPR protection methods for input images, model watermarking,\nand attribution networks are largely satisfactory for a wide range of GANs. We\nhighlight that further attention must be directed towards protecting training\nsets, as the current approaches fail to provide robust IPR protection and\nprovenance tracing on training sets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Mar 2023 06:40:57 GMT"
      }
    ],
    "update_date": "2023-03-17",
    "authors_parsed": [
      [
        "Zhong",
        "Haonan",
        ""
      ],
      [
        "Chang",
        "Jiamin",
        ""
      ],
      [
        "Yang",
        "Ziyue",
        ""
      ],
      [
        "Wu",
        "Tingmin",
        ""
      ],
      [
        "Arachchige",
        "Pathum Chamikara Mahawaga",
        ""
      ],
      [
        "Pathmabandu",
        "Chehara",
        ""
      ],
      [
        "Xue",
        "Minhui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.09272",
    "publish_date": "2023-03-15"
  },
  {
    "id": "2303.11156",
    "submitter": "Aounon Kumar",
    "authors": "Vinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao\n  Wang and Soheil Feizi",
    "title": "Can AI-Generated Text be Reliably Detected?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In this paper, both empirically and theoretically, we show that several\nAI-text detectors are not reliable in practical scenarios. Empirically, we show\nthat paraphrasing attacks, where a light paraphraser is applied on top of a\nlarge language model (LLM), can break a whole range of detectors, including\nones using watermarking schemes as well as neural network-based detectors and\nzero-shot classifiers. Our experiments demonstrate that retrieval-based\ndetectors, designed to evade paraphrasing attacks, are still vulnerable to\nrecursive paraphrasing. We then provide a theoretical impossibility result\nindicating that as language models become more sophisticated and better at\nemulating human text, the performance of even the best-possible detector\ndecreases. For a sufficiently advanced language model seeking to imitate human\ntext, even the best-possible detector may only perform marginally better than a\nrandom classifier. Our result is general enough to capture specific scenarios\nsuch as particular writing styles, clever prompt design, or text paraphrasing.\nWe also extend the impossibility result to include the case where pseudorandom\nnumber generators are used for AI-text generation instead of true randomness.\nWe show that the same result holds with a negligible correction term for all\npolynomial-time computable detectors. Finally, we show that even LLMs protected\nby watermarking schemes can be vulnerable against spoofing attacks where\nadversarial humans can infer hidden LLM text signatures and add them to\nhuman-generated text to be detected as text generated by the LLMs, potentially\ncausing reputational damage to their developers. We believe these results can\nopen an honest conversation in the community regarding the ethical and reliable\nuse of AI-generated text.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 17 Mar 2023 17:53:19 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 28 Jun 2023 20:29:16 GMT"
      }
    ],
    "update_date": "2023-06-30",
    "authors_parsed": [
      [
        "Sadasivan",
        "Vinu Sankar",
        ""
      ],
      [
        "Kumar",
        "Aounon",
        ""
      ],
      [
        "Balasubramanian",
        "Sriram",
        ""
      ],
      [
        "Wang",
        "Wenxiao",
        ""
      ],
      [
        "Feizi",
        "Soheil",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.11156",
    "publish_date": "2023-06-28"
  },
  {
    "id": "2303.11745",
    "submitter": "Mohamed Amine Ferrag",
    "authors": "Mohamed Amine Ferrag and Burak Kantarci and Lucas C. Cordeiro and\n  Merouane Debbah and Kim-Kwang Raymond Choo",
    "title": "Poisoning Attacks in Federated Edge Learning for Digital Twin 6G-enabled\n  IoTs: An Anticipatory Study",
    "comments": "The paper is accepted and will be published in the IEEE ICC 2023\n  Conference Proceedings",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Federated edge learning can be essential in supporting privacy-preserving,\nartificial intelligence (AI)-enabled activities in digital twin 6G-enabled\nInternet of Things (IoT) environments. However, we need to also consider the\npotential of attacks targeting the underlying AI systems (e.g., adversaries\nseek to corrupt data on the IoT devices during local updates or corrupt the\nmodel updates); hence, in this article, we propose an anticipatory study for\npoisoning attacks in federated edge learning for digital twin 6G-enabled IoT\nenvironments. Specifically, we study the influence of adversaries on the\ntraining and development of federated learning models in digital twin\n6G-enabled IoT environments. We demonstrate that attackers can carry out\npoisoning attacks in two different learning settings, namely: centralized\nlearning and federated learning, and successful attacks can severely reduce the\nmodel's accuracy. We comprehensively evaluate the attacks on a new cyber\nsecurity dataset designed for IoT applications with three deep neural networks\nunder the non-independent and identically distributed (Non-IID) data and the\nindependent and identically distributed (IID) data. The poisoning attacks, on\nan attack classification problem, can lead to a decrease in accuracy from\n94.93% to 85.98% with IID data and from 94.18% to 30.04% with Non-IID.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 21 Mar 2023 11:12:17 GMT"
      }
    ],
    "update_date": "2023-03-22",
    "authors_parsed": [
      [
        "Ferrag",
        "Mohamed Amine",
        ""
      ],
      [
        "Kantarci",
        "Burak",
        ""
      ],
      [
        "Cordeiro",
        "Lucas C.",
        ""
      ],
      [
        "Debbah",
        "Merouane",
        ""
      ],
      [
        "Choo",
        "Kim-Kwang Raymond",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.11745",
    "publish_date": "2023-03-21"
  },
  {
    "id": "2303.14822",
    "submitter": "XInlei He",
    "authors": "Xinlei He and Xinyue Shen and Zeyuan Chen and Michael Backes and Yang\n  Zhang",
    "title": "MGTBench: Benchmarking Machine-Generated Text Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Nowadays, powerful large language models (LLMs) such as ChatGPT have\ndemonstrated revolutionary power in a variety of tasks. Consequently, the\ndetection of machine-generated texts (MGTs) is becoming increasingly crucial as\nLLMs become more advanced and prevalent. These models have the ability to\ngenerate human-like language, making it challenging to discern whether a text\nis authored by a human or a machine. This raises concerns regarding\nauthenticity, accountability, and potential bias. However, existing methods for\ndetecting MGTs are evaluated using different model architectures, datasets, and\nexperimental settings, resulting in a lack of a comprehensive evaluation\nframework that encompasses various methodologies. Furthermore, it remains\nunclear how existing detection methods would perform against powerful LLMs. In\nthis paper, we fill this gap by proposing the first benchmark framework for MGT\ndetection against powerful LLMs, named MGTBench. Extensive evaluations on\npublic datasets with curated texts generated by various powerful LLMs such as\nChatGPT-turbo and Claude demonstrate the effectiveness of different detection\nmethods. Our ablation study shows that a larger number of words in general\nleads to better performance and most detection methods can achieve similar\nperformance with much fewer training samples. Moreover, we delve into a more\nchallenging task: text attribution. Our findings indicate that the model-based\ndetection methods still perform well in the text attribution task. To\ninvestigate the robustness of different detection methods, we consider three\nadversarial attacks, namely paraphrasing, random spacing, and adversarial\nperturbations. We discover that these attacks can significantly diminish\ndetection effectiveness, underscoring the critical need for the development of\nmore robust detection methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 26 Mar 2023 21:12:36 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 9 Jun 2023 06:50:57 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 16 Jan 2024 02:48:05 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "He",
        "Xinlei",
        ""
      ],
      [
        "Shen",
        "Xinyue",
        ""
      ],
      [
        "Chen",
        "Zeyuan",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.14822",
    "publish_date": "2024-01-16"
  },
  {
    "id": "2303.18130",
    "submitter": "Mehmet Parlak",
    "authors": "Mehmet Parlak",
    "title": "Blockchain-based Immutable Evidence and Decentralized Loss Adjustment\n  for Autonomous Vehicle Accidents in Insurance",
    "comments": "IEEE Global Emerging Technology Blockchain Forum 2022",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In case of an accident between two autonomous vehicles equipped with emerging\ntechnologies, how do we apportion liability among the various players? A\nspecial liability regime has not even yet been established for damages that may\narise due to the accidents of autonomous vehicles. Would the immutable,\ntime-stamped sensor records of vehicles on distributed ledger help define the\nintertwined relations of liability subjects right through the accident? What if\nthe synthetic media created through deepfake gets involved in the insurance\nclaims? While integrating AI-powered anomaly or deepfake detection into\nautomated insurance claims processing helps to prevent insurance fraud, it is\nonly a matter of time before deepfake becomes nearly undetectable even to\nelaborate forensic tools. This paper proposes a blockchain-based insurtech\ndecentralized application to check the authenticity and provenance of the\naccident footage and also to decentralize the loss-adjusting process through a\nhybrid of decentralized and centralized databases using smart contracts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 29 Mar 2023 21:50:13 GMT"
      }
    ],
    "update_date": "2023-04-03",
    "authors_parsed": [
      [
        "Parlak",
        "Mehmet",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2303.18130",
    "publish_date": "2023-03-29"
  },
  {
    "id": "2304.01002",
    "submitter": "Adaku Uchendu",
    "authors": "Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao 'Kenneth'\n  Huang, Dongwon Lee",
    "title": "Does Human Collaboration Enhance the Accuracy of Identifying\n  LLM-Generated Deepfake Texts?",
    "comments": "Accepted at The 11th AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2023)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.HC",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Advances in Large Language Models (e.g., GPT-4, LLaMA) have improved the\ngeneration of coherent sentences resembling human writing on a large scale,\nresulting in the creation of so-called deepfake texts. However, this progress\nposes security and privacy concerns, necessitating effective solutions for\ndistinguishing deepfake texts from human-written ones. Although prior works\nstudied humans' ability to detect deepfake texts, none has examined whether\n\"collaboration\" among humans improves the detection of deepfake texts. In this\nstudy, to address this gap of understanding on deepfake texts, we conducted\nexperiments with two groups: (1) nonexpert individuals from the AMT platform\nand (2) writing experts from the Upwork platform. The results demonstrate that\ncollaboration among humans can potentially improve the detection of deepfake\ntexts for both groups, increasing detection accuracies by 6.36% for non-experts\nand 12.76% for experts, respectively, compared to individuals' detection\naccuracies. We further analyze the explanations that humans used for detecting\na piece of text as deepfake text, and find that the strongest indicator of\ndeepfake texts is their lack of coherence and consistency. Our study provides\nuseful insights for future tools and framework designs to facilitate the\ncollaborative human detection of deepfake texts. The experiment datasets and\nAMT implementations are available at:\nhttps://github.com/huashen218/llm-deepfake-human-study.git\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Apr 2023 14:06:47 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 28 Aug 2023 00:57:19 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 9 Oct 2023 21:57:47 GMT"
      }
    ],
    "update_date": "2023-10-11",
    "authors_parsed": [
      [
        "Uchendu",
        "Adaku",
        ""
      ],
      [
        "Lee",
        "Jooyoung",
        ""
      ],
      [
        "Shen",
        "Hua",
        ""
      ],
      [
        "Le",
        "Thai",
        ""
      ],
      [
        "Huang",
        "Ting-Hao 'Kenneth'",
        ""
      ],
      [
        "Lee",
        "Dongwon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.01002",
    "publish_date": "2023-10-09"
  },
  {
    "id": "2304.01908",
    "submitter": "Rushit Dave",
    "authors": "Aniruddha Tiwari, Rushit Dave, Mounika Vanamala",
    "title": "Leveraging Deep Learning Approaches for Deepfake Detection: A Review",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Conspicuous progression in the field of machine learning and deep learning\nhave led the jump of highly realistic fake media, these media oftentimes\nreferred as deepfakes. Deepfakes are fabricated media which are generated by\nsophisticated AI that are at times very difficult to set apart from the real\nmedia. So far, this media can be uploaded to the various social media\nplatforms, hence advertising it to the world got easy, calling for an\nefficacious countermeasure. Thus, one of the optimistic counter steps against\ndeepfake would be deepfake detection. To undertake this threat, researchers in\nthe past have created models to detect deepfakes based on ML/DL techniques like\nConvolutional Neural Networks. This paper aims to explore different\nmethodologies with an intention to achieve a cost-effective model with a higher\naccuracy with different types of the datasets, which is to address the\ngeneralizability of the dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 4 Apr 2023 16:04:42 GMT"
      }
    ],
    "update_date": "2023-04-05",
    "authors_parsed": [
      [
        "Tiwari",
        "Aniruddha",
        ""
      ],
      [
        "Dave",
        "Rushit",
        ""
      ],
      [
        "Vanamala",
        "Mounika",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.01908",
    "publish_date": "2023-04-04"
  },
  {
    "id": "2304.02234",
    "submitter": "Pedro Sandoval-Segura",
    "authors": "Pedro Sandoval-Segura, Jonas Geiping, Tom Goldstein",
    "title": "JPEG Compressed Images Can Bypass Protections Against AI Editing",
    "comments": "8 pages, 8 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recently developed text-to-image diffusion models make it easy to edit or\ncreate high-quality images. Their ease of use has raised concerns about the\npotential for malicious editing or deepfake creation. Imperceptible\nperturbations have been proposed as a means of protecting images from malicious\nediting by preventing diffusion models from generating realistic images.\nHowever, we find that the aforementioned perturbations are not robust to JPEG\ncompression, which poses a major weakness because of the common usage and\navailability of JPEG. We discuss the importance of robustness for additive\nimperceptible perturbations and encourage alternative approaches to protect\nimages against editing.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 5 Apr 2023 05:30:09 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 7 Apr 2023 20:33:57 GMT"
      }
    ],
    "update_date": "2023-04-11",
    "authors_parsed": [
      [
        "Sandoval-Segura",
        "Pedro",
        ""
      ],
      [
        "Geiping",
        "Jonas",
        ""
      ],
      [
        "Goldstein",
        "Tom",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.02234",
    "publish_date": "2023-04-05"
  },
  {
    "id": "2304.04537",
    "submitter": "Ali Nazari",
    "authors": "Mahsa Soleimani, Ali Nazari and Mohsen Ebrahimi Moghaddam",
    "title": "Deepfake Detection of Occluded Images Using a Patch-based Approach",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  DeepFake involves the use of deep learning and artificial intelligence\ntechniques to produce or change video and image contents typically generated by\nGANs. Moreover, it can be misused and leads to fictitious news, ethical and\nfinancial crimes, and also affects the performance of facial recognition\nsystems. Thus, detection of real or fake images is significant specially to\nauthenticate originality of people's images or videos. One of the most\nimportant challenges in this topic is obstruction that decreases the system\nprecision. In this study, we present a deep learning approach using the entire\nface and face patches to distinguish real/fake images in the presence of\nobstruction with a three-path decision: first entire-face reasoning, second a\ndecision based on the concatenation of feature vectors of face patches, and\nthird a majority vote decision based on these features. To test our approach,\nnew datasets including real and fake images are created. For producing fake\nimages, StyleGAN and StyleGAN2 are trained by FFHQ images and also StarGAN and\nPGGAN are trained by CelebA images. The CelebA and FFHQ datasets are used as\nreal images. The proposed approach reaches higher results in early epochs than\nother methods and increases the SoTA results by 0.4\\%-7.9\\% in the different\nbuilt data-sets. Also, we have shown in experimental results that weighing the\npatches may improve accuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 10 Apr 2023 12:12:14 GMT"
      }
    ],
    "update_date": "2023-04-11",
    "authors_parsed": [
      [
        "Soleimani",
        "Mahsa",
        ""
      ],
      [
        "Nazari",
        "Ali",
        ""
      ],
      [
        "Moghaddam",
        "Mohsen Ebrahimi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.04537",
    "publish_date": "2023-04-10"
  },
  {
    "id": "2304.04824",
    "submitter": "Hanjing Wang",
    "authors": "Hanjing Wang, Dhiraj Joshi, Shiqiang Wang, Qiang Ji",
    "title": "Gradient-based Uncertainty Attribution for Explainable Bayesian Deep\n  Learning",
    "comments": "Accepted to CVPR 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CV cs.IT math.IT stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Predictions made by deep learning models are prone to data perturbations,\nadversarial attacks, and out-of-distribution inputs. To build a trusted AI\nsystem, it is therefore critical to accurately quantify the prediction\nuncertainties. While current efforts focus on improving uncertainty\nquantification accuracy and efficiency, there is a need to identify uncertainty\nsources and take actions to mitigate their effects on predictions. Therefore,\nwe propose to develop explainable and actionable Bayesian deep learning methods\nto not only perform accurate uncertainty quantification but also explain the\nuncertainties, identify their sources, and propose strategies to mitigate the\nuncertainty impacts. Specifically, we introduce a gradient-based uncertainty\nattribution method to identify the most problematic regions of the input that\ncontribute to the prediction uncertainty. Compared to existing methods, the\nproposed UA-Backprop has competitive accuracy, relaxed assumptions, and high\nefficiency. Moreover, we propose an uncertainty mitigation strategy that\nleverages the attribution results as attention to further improve the model\nperformance. Both qualitative and quantitative evaluations are conducted to\ndemonstrate the effectiveness of our proposed methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 10 Apr 2023 19:14:15 GMT"
      }
    ],
    "update_date": "2023-04-12",
    "authors_parsed": [
      [
        "Wang",
        "Hanjing",
        ""
      ],
      [
        "Joshi",
        "Dhiraj",
        ""
      ],
      [
        "Wang",
        "Shiqiang",
        ""
      ],
      [
        "Ji",
        "Qiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.04824",
    "publish_date": "2023-04-10"
  },
  {
    "id": "2304.05402",
    "submitter": "Tony Ma",
    "authors": "Tony Ma, Songze Li, Yisong Xiao, Shunchang Liu",
    "title": "Boosting Cross-task Transferability of Adversarial Patches with Visual\n  Relations",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG cs.MM",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The transferability of adversarial examples is a crucial aspect of evaluating\nthe robustness of deep learning systems, particularly in black-box scenarios.\nAlthough several methods have been proposed to enhance cross-model\ntransferability, little attention has been paid to the transferability of\nadversarial examples across different tasks. This issue has become increasingly\nrelevant with the emergence of foundational multi-task AI systems such as\nVisual ChatGPT, rendering the utility of adversarial samples generated by a\nsingle task relatively limited. Furthermore, these systems often entail\ninferential functions beyond mere recognition-like tasks. To address this gap,\nwe propose a novel Visual Relation-based cross-task Adversarial Patch\ngeneration method called VRAP, which aims to evaluate the robustness of various\nvisual tasks, especially those involving visual reasoning, such as Visual\nQuestion Answering and Image Captioning. VRAP employs scene graphs to combine\nobject recognition-based deception with predicate-based relations elimination,\nthereby disrupting the visual reasoning information shared among inferential\ntasks. Our extensive experiments demonstrate that VRAP significantly surpasses\nprevious methods in terms of black-box transferability across diverse visual\nreasoning tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 11 Apr 2023 11:43:57 GMT"
      }
    ],
    "update_date": "2023-04-13",
    "authors_parsed": [
      [
        "Ma",
        "Tony",
        ""
      ],
      [
        "Li",
        "Songze",
        ""
      ],
      [
        "Xiao",
        "Yisong",
        ""
      ],
      [
        "Liu",
        "Shunchang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.05402",
    "publish_date": "2023-04-11"
  },
  {
    "id": "2304.07395",
    "submitter": "Nikolaos Giatsoglou",
    "authors": "Nikolaos Giatsoglou, Symeon Papadopoulos, Ioannis Kompatsiaris",
    "title": "Investigation of ensemble methods for the detection of deepfake face\n  manipulations",
    "comments": "16 pages, 4 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  The recent wave of AI research has enabled a new brand of synthetic media,\ncalled deepfakes. Deepfakes have impressive photorealism, which has generated\nexciting new use cases but also raised serious threats to our increasingly\ndigital world. To mitigate these threats, researchers have tried to come up\nwith new methods for deepfake detection that are more effective than\ntraditional forensics and heavily rely on deep AI technology. In this paper,\nfollowing up on encouraging prior work for deepfake detection with attribution\nand ensemble techniques, we explore and compare multiple designs for ensemble\ndetectors. The goal is to achieve robustness and good generalization ability by\nleveraging ensembles of models that specialize in different manipulation\ncategories. Our results corroborate that ensembles can achieve higher accuracy\nthan individual models when properly tuned, while the generalization ability\nrelies on access to a large number of training data for a diverse set of known\nmanipulations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 14 Apr 2023 21:18:51 GMT"
      }
    ],
    "update_date": "2023-04-18",
    "authors_parsed": [
      [
        "Giatsoglou",
        "Nikolaos",
        ""
      ],
      [
        "Papadopoulos",
        "Symeon",
        ""
      ],
      [
        "Kompatsiaris",
        "Ioannis",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.07395",
    "publish_date": "2023-04-14"
  },
  {
    "id": "2304.08411",
    "submitter": "Alexander Warnecke",
    "authors": "Alexander Warnecke, Julian Speith, Jan-Niklas M\\\"oller, Konrad Rieck,\n  Christof Paar",
    "title": "Evil from Within: Machine Learning Backdoors through Hardware Trojans",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Backdoors pose a serious threat to machine learning, as they can compromise\nthe integrity of security-critical systems, such as self-driving cars. While\ndifferent defenses have been proposed to address this threat, they all rely on\nthe assumption that the hardware on which the learning models are executed\nduring inference is trusted. In this paper, we challenge this assumption and\nintroduce a backdoor attack that completely resides within a common hardware\naccelerator for machine learning. Outside of the accelerator, neither the\nlearning model nor the software is manipulated, so that current defenses fail.\nTo make this attack practical, we overcome two challenges: First, as memory on\na hardware accelerator is severely limited, we introduce the concept of a\nminimal backdoor that deviates as little as possible from the original model\nand is activated by replacing a few model parameters only. Second, we develop a\nconfigurable hardware trojan that can be provisioned with the backdoor and\nperforms a replacement only when the specific target model is processed. We\ndemonstrate the practical feasibility of our attack by implanting our hardware\ntrojan into the Xilinx Vitis AI DPU, a commercial machine-learning accelerator.\nWe configure the trojan with a minimal backdoor for a traffic-sign recognition\nsystem. The backdoor replaces only 30 (0.069%) model parameters, yet it\nreliably manipulates the recognition once the input contains a backdoor\ntrigger. Our attack expands the hardware circuit of the accelerator by 0.24%\nand induces no run-time overhead, rendering a detection hardly possible. Given\nthe complex and highly distributed manufacturing process of current hardware,\nour work points to a new threat in machine learning that is inaccessible to\ncurrent security mechanisms and calls for hardware to be manufactured only in\nfully trusted environments.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 17 Apr 2023 16:24:48 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 18 Apr 2023 07:25:23 GMT"
      }
    ],
    "update_date": "2023-04-19",
    "authors_parsed": [
      [
        "Warnecke",
        "Alexander",
        ""
      ],
      [
        "Speith",
        "Julian",
        ""
      ],
      [
        "M\u00f6ller",
        "Jan-Niklas",
        ""
      ],
      [
        "Rieck",
        "Konrad",
        ""
      ],
      [
        "Paar",
        "Christof",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.08411",
    "publish_date": "2023-04-18"
  },
  {
    "id": "2304.08979",
    "submitter": "Xinyue Shen",
    "authors": "Xinyue Shen and Zeyuan Chen and Michael Backes and Yang Zhang",
    "title": "In ChatGPT We Trust? Measuring and Characterizing the Reliability of\n  ChatGPT",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The way users acquire information is undergoing a paradigm shift with the\nadvent of ChatGPT. Unlike conventional search engines, ChatGPT retrieves\nknowledge from the model itself and generates answers for users. ChatGPT's\nimpressive question-answering (QA) capability has attracted more than 100\nmillion users within a short period of time but has also raised concerns\nregarding its reliability. In this paper, we perform the first large-scale\nmeasurement of ChatGPT's reliability in the generic QA scenario with a\ncarefully curated set of 5,695 questions across ten datasets and eight domains.\nWe find that ChatGPT's reliability varies across different domains, especially\nunderperforming in law and science questions. We also demonstrate that system\nroles, originally designed by OpenAI to allow users to steer ChatGPT's\nbehavior, can impact ChatGPT's reliability in an imperceptible way. We further\nshow that ChatGPT is vulnerable to adversarial examples, and even a single\ncharacter change can negatively affect its reliability in certain cases. We\nbelieve that our study provides valuable insights into ChatGPT's reliability\nand underscores the need for strengthening the reliability and security of\nlarge language models (LLMs).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 18 Apr 2023 13:20:45 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 5 Oct 2023 13:27:12 GMT"
      }
    ],
    "update_date": "2023-10-06",
    "authors_parsed": [
      [
        "Shen",
        "Xinyue",
        ""
      ],
      [
        "Chen",
        "Zeyuan",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.08979",
    "publish_date": "2023-04-18"
  },
  {
    "id": "2304.10136",
    "submitter": "Xiaosen Wang",
    "authors": "Zhiyuan Wang, Zeliang Zhang, Siyuan Liang, Xiaosen Wang",
    "title": "Diversifying the High-level Features for better Adversarial\n  Transferability",
    "comments": "Accepted by BMVC 2023 (Oral)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Given the great threat of adversarial attacks against Deep Neural Networks\n(DNNs), numerous works have been proposed to boost transferability to attack\nreal-world applications. However, existing attacks often utilize advanced\ngradient calculation or input transformation but ignore the white-box model.\nInspired by the fact that DNNs are over-parameterized for superior performance,\nwe propose diversifying the high-level features (DHF) for more transferable\nadversarial examples. In particular, DHF perturbs the high-level features by\nrandomly transforming the high-level features and mixing them with the feature\nof benign samples when calculating the gradient at each iteration. Due to the\nredundancy of parameters, such transformation does not affect the\nclassification performance but helps identify the invariant features across\ndifferent models, leading to much better transferability. Empirical evaluations\non ImageNet dataset show that DHF could effectively improve the transferability\nof existing momentum-based attacks. Incorporated into the input\ntransformation-based attacks, DHF generates more transferable adversarial\nexamples and outperforms the baselines with a clear margin when attacking\nseveral defense models, showing its generalization to various attacks and high\neffectiveness for boosting transferability. Code is available at\nhttps://github.com/Trustworthy-AI-Group/DHF.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 20 Apr 2023 07:44:59 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 15 Sep 2023 02:14:34 GMT"
      }
    ],
    "update_date": "2023-09-18",
    "authors_parsed": [
      [
        "Wang",
        "Zhiyuan",
        ""
      ],
      [
        "Zhang",
        "Zeliang",
        ""
      ],
      [
        "Liang",
        "Siyuan",
        ""
      ],
      [
        "Wang",
        "Xiaosen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.10136",
    "publish_date": "2023-04-20"
  },
  {
    "id": "2304.10619",
    "submitter": "Lingyao Li",
    "authors": "Lingyao Li, Lizhou Fan, Shubham Atreja, Libby Hemphill",
    "title": "\"HOT\" ChatGPT: The promise of ChatGPT in detecting and discriminating\n  hateful, offensive, and toxic comments on social media",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.HC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Harmful content is pervasive on social media, poisoning online communities\nand negatively impacting participation. A common approach to address this issue\nis to develop detection models that rely on human annotations. However, the\ntasks required to build such models expose annotators to harmful and offensive\ncontent and may require significant time and cost to complete. Generative AI\nmodels have the potential to understand and detect harmful content. To\ninvestigate this potential, we used ChatGPT and compared its performance with\nMTurker annotations for three frequently discussed concepts related to harmful\ncontent: Hateful, Offensive, and Toxic (HOT). We designed five prompts to\ninteract with ChatGPT and conducted four experiments eliciting HOT\nclassifications. Our results show that ChatGPT can achieve an accuracy of\napproximately 80% when compared to MTurker annotations. Specifically, the model\ndisplays a more consistent classification for non-HOT comments than HOT\ncomments compared to human annotations. Our findings also suggest that ChatGPT\nclassifications align with provided HOT definitions, but ChatGPT classifies\n\"hateful\" and \"offensive\" as subsets of \"toxic.\" Moreover, the choice of\nprompts used to interact with ChatGPT impacts its performance. Based on these\nin-sights, our study provides several meaningful implications for employing\nChatGPT to detect HOT content, particularly regarding the reliability and\nconsistency of its performance, its understand-ing and reasoning of the HOT\nconcept, and the impact of prompts on its performance. Overall, our study\nprovides guidance about the potential of using generative AI models to moderate\nlarge volumes of user-generated content on social media.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 20 Apr 2023 19:40:51 GMT"
      }
    ],
    "update_date": "2023-04-24",
    "authors_parsed": [
      [
        "Li",
        "Lingyao",
        ""
      ],
      [
        "Fan",
        "Lizhou",
        ""
      ],
      [
        "Atreja",
        "Shubham",
        ""
      ],
      [
        "Hemphill",
        "Libby",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.10619",
    "publish_date": "2023-04-20"
  },
  {
    "id": "2304.10755",
    "submitter": "Xinliang Zhou",
    "authors": "Xinliang Zhou, Chenyu Liu, Liming Zhai, Ziyu Jia, Cuntai Guan and Yang\n  Liu",
    "title": "Interpretable and Robust AI in EEG Systems: A Survey",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.SP cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The close coupling of artificial intelligence (AI) and electroencephalography\n(EEG) has substantially advanced human-computer interaction (HCI) technologies\nin the AI era. Different from traditional EEG systems, the interpretability and\nrobustness of AI-based EEG systems are becoming particularly crucial. The\ninterpretability clarifies the inner working mechanisms of AI models and thus\ncan gain the trust of users. The robustness reflects the AI's reliability\nagainst attacks and perturbations, which is essential for sensitive and fragile\nEEG signals. Thus the interpretability and robustness of AI in EEG systems have\nattracted increasing attention, and their research has achieved great progress\nrecently. However, there is still no survey covering recent advances in this\nfield. In this paper, we present the first comprehensive survey and summarize\nthe interpretable and robust AI techniques for EEG systems. Specifically, we\nfirst propose a taxonomy of interpretability by characterizing it into three\ntypes: backpropagation, perturbation, and inherently interpretable methods.\nThen we classify the robustness mechanisms into four classes: noise and\nartifacts, human variability, data acquisition instability, and adversarial\nattacks. Finally, we identify several critical and unresolved challenges for\ninterpretable and robust AI in EEG systems and further discuss their future\ndirections.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 21 Apr 2023 05:51:39 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 30 Aug 2023 06:06:40 GMT"
      }
    ],
    "update_date": "2023-08-31",
    "authors_parsed": [
      [
        "Zhou",
        "Xinliang",
        ""
      ],
      [
        "Liu",
        "Chenyu",
        ""
      ],
      [
        "Zhai",
        "Liming",
        ""
      ],
      [
        "Jia",
        "Ziyu",
        ""
      ],
      [
        "Guan",
        "Cuntai",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.10755",
    "publish_date": "2023-08-30"
  },
  {
    "id": "2304.11087",
    "submitter": "Ebenezer Isaac",
    "authors": "Ebenezer R. H. P. Isaac and Jim Reno",
    "title": "AI Product Security: A Primer for Developers",
    "comments": "10 pages, 1 figure",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Not too long ago, AI security used to mean the research and practice of how\nAI can empower cybersecurity, that is, AI for security. Ever since Ian\nGoodfellow and his team popularized adversarial attacks on machine learning,\nsecurity for AI became an important concern and also part of AI security. It is\nimperative to understand the threats to machine learning products and avoid\ncommon pitfalls in AI product development. This article is addressed to\ndevelopers, designers, managers and researchers of AI software products.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 18 Apr 2023 05:22:34 GMT"
      }
    ],
    "update_date": "2023-04-24",
    "authors_parsed": [
      [
        "Isaac",
        "Ebenezer R. H. P.",
        ""
      ],
      [
        "Reno",
        "Jim",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.11087",
    "publish_date": "2023-04-18"
  },
  {
    "id": "2304.11432",
    "submitter": "Peng Chen",
    "authors": "Peng Chen, Xin Du, Zhihui Lu and Hongfeng Chai",
    "title": "Universal Adversarial Backdoor Attacks to Fool Vertical Federated\n  Learning in Cloud-Edge Collaboration",
    "comments": "14 pages, 7 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Vertical federated learning (VFL) is a cloud-edge collaboration paradigm that\nenables edge nodes, comprising resource-constrained Internet of Things (IoT)\ndevices, to cooperatively train artificial intelligence (AI) models while\nretaining their data locally. This paradigm facilitates improved privacy and\nsecurity for edges and IoT devices, making VFL an essential component of\nArtificial Intelligence of Things (AIoT) systems. Nevertheless, the partitioned\nstructure of VFL can be exploited by adversaries to inject a backdoor, enabling\nthem to manipulate the VFL predictions. In this paper, we aim to investigate\nthe vulnerability of VFL in the context of binary classification tasks. To this\nend, we define a threat model for backdoor attacks in VFL and introduce a\nuniversal adversarial backdoor (UAB) attack to poison the predictions of VFL.\nThe UAB attack, consisting of universal trigger generation and clean-label\nbackdoor injection, is incorporated during the VFL training at specific\niterations. This is achieved by alternately optimizing the universal trigger\nand model parameters of VFL sub-problems. Our work distinguishes itself from\nexisting studies on designing backdoor attacks for VFL, as those require the\nknowledge of auxiliary information not accessible within the split VFL\narchitecture. In contrast, our approach does not necessitate any additional\ndata to execute the attack. On the LendingClub and Zhongyuan datasets, our\napproach surpasses existing state-of-the-art methods, achieving up to 100\\%\nbackdoor task performance while maintaining the main task performance. Our\nresults in this paper make a major advance to revealing the hidden backdoor\nrisks of VFL, hence paving the way for the future development of secure AIoT.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 22 Apr 2023 15:31:15 GMT"
      }
    ],
    "update_date": "2023-04-25",
    "authors_parsed": [
      [
        "Chen",
        "Peng",
        ""
      ],
      [
        "Du",
        "Xin",
        ""
      ],
      [
        "Lu",
        "Zhihui",
        ""
      ],
      [
        "Chai",
        "Hongfeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.11432",
    "publish_date": "2023-04-22"
  },
  {
    "id": "2304.11938",
    "submitter": "Haoye Tian",
    "authors": "Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung,\n  Jacques Klein, Tegawend\\'e F. Bissyand\\'e",
    "title": "Is ChatGPT the Ultimate Programming Assistant -- How far is it?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recently, the ChatGPT LLM has received great attention: it can be used as a\nbot for discussing source code, prompting it to suggest changes, provide\ndescriptions or even generate code. Typical demonstrations generally focus on\nexisting benchmarks, which may have been used in model training (i.e., data\nleakage). To assess the feasibility of using an LLM as a useful assistant bot\nfor programmers, we must assess its realistic capabilities on unseen problems\nas well as its capabilities on various tasks. In this paper, we present an\nempirical study of ChatGPT's potential as a fully automated programming\nassistant, focusing on the tasks of code generation, program repair, and code\nsummariziation. The study investigates ChatGPT's performance on common\nprogramming problems and compares it with state-of-the-art approaches on two\nbenchmarks. Among several findings, our study shows that ChatGPT is effective\nin dealing with common programming problems. However, our experiments also\nreveal limitations in terms of its attention span: detailed descriptions will\nconstrain the focus of ChatGPT and prevent it from leveraging its vast\nknowledge to solve the actual problem. Surprisingly, we have identified the\nability of ChatGPT to reason the original intention of the code. We expect\nfuture work to build on this insight for dealing with the open question of the\noracle problem. Our findings contribute interesting insights to the development\nof LLMs for programming assistance, notably by demonstrating the importance of\nprompt engineering, and providing a better understanding of ChatGPT's practical\napplications for software engineering.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 24 Apr 2023 09:20:13 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 31 Aug 2023 09:02:16 GMT"
      }
    ],
    "update_date": "2023-09-01",
    "authors_parsed": [
      [
        "Tian",
        "Haoye",
        ""
      ],
      [
        "Lu",
        "Weiqi",
        ""
      ],
      [
        "Li",
        "Tsz On",
        ""
      ],
      [
        "Tang",
        "Xunzhu",
        ""
      ],
      [
        "Cheung",
        "Shing-Chi",
        ""
      ],
      [
        "Klein",
        "Jacques",
        ""
      ],
      [
        "Bissyand\u00e9",
        "Tegawend\u00e9 F.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.11938",
    "publish_date": "2023-08-31"
  },
  {
    "id": "2304.12298",
    "submitter": "Jiawen Shi",
    "authors": "Jiawen Shi, Yixin Liu, Pan Zhou and Lichao Sun",
    "title": "BadGPT: Exploring Security Vulnerabilities of ChatGPT via Backdoor\n  Attacks to InstructGPT",
    "comments": "This paper is accepted as a poster in NDSS2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recently, ChatGPT has gained significant attention in research due to its\nability to interact with humans effectively. The core idea behind this model is\nreinforcement learning (RL) fine-tuning, a new paradigm that allows language\nmodels to align with human preferences, i.e., InstructGPT. In this study, we\npropose BadGPT, the first backdoor attack against RL fine-tuning in language\nmodels. By injecting a backdoor into the reward model, the language model can\nbe compromised during the fine-tuning stage. Our initial experiments on movie\nreviews, i.e., IMDB, demonstrate that an attacker can manipulate the generated\ntext through BadGPT.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 21 Feb 2023 08:59:22 GMT"
      }
    ],
    "update_date": "2023-04-25",
    "authors_parsed": [
      [
        "Shi",
        "Jiawen",
        ""
      ],
      [
        "Liu",
        "Yixin",
        ""
      ],
      [
        "Zhou",
        "Pan",
        ""
      ],
      [
        "Sun",
        "Lichao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.12298",
    "publish_date": "2023-02-21"
  },
  {
    "id": "2304.12486",
    "submitter": "Aymen Shabou",
    "authors": "Timoth\\'ee Fronteau, Arnaud Paran and Aymen Shabou",
    "title": "Evaluating Adversarial Robustness on Document Image Classification",
    "comments": "The 17th International Conference on Document Analysis and\n  Recognition",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Adversarial attacks and defenses have gained increasing interest on computer\nvision systems in recent years, but as of today, most investigations are\nlimited to images. However, many artificial intelligence models actually handle\ndocumentary data, which is very different from real world images. Hence, in\nthis work, we try to apply the adversarial attack philosophy on documentary and\nnatural data and to protect models against such attacks. We focus our work on\nuntargeted gradient-based, transfer-based and score-based attacks and evaluate\nthe impact of adversarial training, JPEG input compression and grey-scale input\ntransformation on the robustness of ResNet50 and EfficientNetB0 model\narchitectures. To the best of our knowledge, no such work has been conducted by\nthe community in order to study the impact of these attacks on the document\nimage classification task.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 24 Apr 2023 22:57:59 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 1 May 2023 20:49:33 GMT"
      }
    ],
    "update_date": "2023-05-03",
    "authors_parsed": [
      [
        "Fronteau",
        "Timoth\u00e9e",
        ""
      ],
      [
        "Paran",
        "Arnaud",
        ""
      ],
      [
        "Shabou",
        "Aymen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.12486",
    "publish_date": "2023-04-24"
  },
  {
    "id": "2304.13085",
    "submitter": "Chengzhe Sun",
    "authors": "Chengzhe Sun, Shan Jia, Shuwei Hou, Siwei Lyu",
    "title": "AI-Synthesized Voice Detection Using Neural Vocoder Artifacts",
    "comments": "Paper accepted in CVPRW 2023. Codes and data can be found at\n  https://github.com/csun22/Synthetic-Voice-Detection-Vocoder-Artifacts. arXiv\n  admin note: substantial text overlap with arXiv:2302.09198",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.MM eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Advancements in AI-synthesized human voices have created a growing threat of\nimpersonation and disinformation, making it crucial to develop methods to\ndetect synthetic human voices. This study proposes a new approach to\nidentifying synthetic human voices by detecting artifacts of vocoders in audio\nsignals. Most DeepFake audio synthesis models use a neural vocoder, a neural\nnetwork that generates waveforms from temporal-frequency representations like\nmel-spectrograms. By identifying neural vocoder processing in audio, we can\ndetermine if a sample is synthesized. To detect synthetic human voices, we\nintroduce a multi-task learning framework for a binary-class RawNet2 model that\nshares the feature extractor with a vocoder identification module. By treating\nvocoder identification as a pretext task, we constrain the feature extractor to\nfocus on vocoder artifacts and provide discriminative features for the final\nbinary classifier. Our experiments show that the improved RawNet2 model based\non vocoder identification achieves high classification performance on the\nbinary task overall.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 25 Apr 2023 18:36:28 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 27 Apr 2023 14:20:52 GMT"
      }
    ],
    "update_date": "2023-04-28",
    "authors_parsed": [
      [
        "Sun",
        "Chengzhe",
        ""
      ],
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Hou",
        "Shuwei",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.13085",
    "publish_date": "2023-04-25"
  },
  {
    "id": "2304.14475",
    "submitter": "Zhuofeng Wu",
    "authors": "Jiazhao Li, Yijin Yang, Zhuofeng Wu, V.G. Vinod Vydiswaran, Chaowei\n  Xiao",
    "title": "ChatGPT as an Attack Tool: Stealthy Textual Backdoor Attack via Blackbox\n  Generative Model Trigger",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Textual backdoor attacks pose a practical threat to existing systems, as they\ncan compromise the model by inserting imperceptible triggers into inputs and\nmanipulating labels in the training dataset. With cutting-edge generative\nmodels such as GPT-4 pushing rewriting to extraordinary levels, such attacks\nare becoming even harder to detect. We conduct a comprehensive investigation of\nthe role of black-box generative models as a backdoor attack tool, highlighting\nthe importance of researching relative defense strategies. In this paper, we\nreveal that the proposed generative model-based attack, BGMAttack, could\neffectively deceive textual classifiers. Compared with the traditional attack\nmethods, BGMAttack makes the backdoor trigger less conspicuous by leveraging\nstate-of-the-art generative models. Our extensive evaluation of attack\neffectiveness across five datasets, complemented by three distinct human\ncognition assessments, reveals that Figure 4 achieves comparable attack\nperformance while maintaining superior stealthiness relative to baseline\nmethods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Apr 2023 19:26:25 GMT"
      }
    ],
    "update_date": "2023-05-01",
    "authors_parsed": [
      [
        "Li",
        "Jiazhao",
        ""
      ],
      [
        "Yang",
        "Yijin",
        ""
      ],
      [
        "Wu",
        "Zhuofeng",
        ""
      ],
      [
        "Vydiswaran",
        "V. G. Vinod",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2304.14475",
    "publish_date": "2023-04-27"
  },
  {
    "id": "2305.00399",
    "submitter": "Jingfeng Zhang",
    "authors": "Jingfeng Zhang, Bo Song, Bo Han, Lei Liu, Gang Niu, Masashi Sugiyama",
    "title": "Assessing Vulnerabilities of Adversarial Learning Algorithm through\n  Poisoning Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial training (AT) is a robust learning algorithm that can defend\nagainst adversarial attacks in the inference phase and mitigate the side\neffects of corrupted data in the training phase. As such, it has become an\nindispensable component of many artificial intelligence (AI) systems. However,\nin high-stake AI applications, it is crucial to understand AT's vulnerabilities\nto ensure reliable deployment. In this paper, we investigate AT's\nsusceptibility to poisoning attacks, a type of malicious attack that\nmanipulates training data to compromise the performance of the trained model.\nPrevious work has focused on poisoning attacks against standard training, but\nlittle research has been done on their effectiveness against AT. To fill this\ngap, we design and test effective poisoning attacks against AT. Specifically,\nwe investigate and design clean-label poisoning attacks, allowing attackers to\nimperceptibly modify a small fraction of training data to control the\nalgorithm's behavior on a specific target data point. Additionally, we propose\nthe clean-label untargeted attack, enabling attackers can attach tiny stickers\non training data to degrade the algorithm's performance on all test data, where\nthe stickers could serve as a signal against unauthorized data collection. Our\nexperiments demonstrate that AT can still be poisoned, highlighting the need\nfor caution when using vanilla AT algorithms in security-related applications.\nThe code is at https://github.com/zjfheart/Poison-adv-training.git.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 30 Apr 2023 05:45:39 GMT"
      }
    ],
    "update_date": "2023-05-02",
    "authors_parsed": [
      [
        "Zhang",
        "Jingfeng",
        ""
      ],
      [
        "Song",
        "Bo",
        ""
      ],
      [
        "Han",
        "Bo",
        ""
      ],
      [
        "Liu",
        "Lei",
        ""
      ],
      [
        "Niu",
        "Gang",
        ""
      ],
      [
        "Sugiyama",
        "Masashi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.00399",
    "publish_date": "2023-04-30"
  },
  {
    "id": "2305.00574",
    "submitter": "Ziheng Chen",
    "authors": "Ziheng Chen, Fabrizio Silvestri, Jia Wang, Yongfeng Zhang and Gabriele\n  Tolomei",
    "title": "The Dark Side of Explanations: Poisoning Recommender Systems with\n  Counterfactual Examples",
    "comments": "To be published in SIGIR2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.IR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning-based recommender systems have become an integral part of\nseveral online platforms. However, their black-box nature emphasizes the need\nfor explainable artificial intelligence (XAI) approaches to provide\nhuman-understandable reasons why a specific item gets recommended to a given\nuser. One such method is counterfactual explanation (CF). While CFs can be\nhighly beneficial for users and system designers, malicious actors may also\nexploit these explanations to undermine the system's security. In this work, we\npropose H-CARS, a novel strategy to poison recommender systems via CFs.\nSpecifically, we first train a logical-reasoning-based surrogate model on\ntraining data derived from counterfactual explanations. By reversing the\nlearning process of the recommendation model, we thus develop a proficient\ngreedy algorithm to generate fabricated user profiles and their associated\ninteraction records for the aforementioned surrogate model. Our experiments,\nwhich employ a well-known CF generation method and are conducted on two\ndistinct datasets, show that H-CARS yields significant and successful attack\nperformance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 30 Apr 2023 20:54:54 GMT"
      }
    ],
    "update_date": "2023-05-02",
    "authors_parsed": [
      [
        "Chen",
        "Ziheng",
        ""
      ],
      [
        "Silvestri",
        "Fabrizio",
        ""
      ],
      [
        "Wang",
        "Jia",
        ""
      ],
      [
        "Zhang",
        "Yongfeng",
        ""
      ],
      [
        "Tolomei",
        "Gabriele",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.00574",
    "publish_date": "2023-04-30"
  },
  {
    "id": "2305.00944",
    "submitter": "Eric Wallace",
    "authors": "Alexander Wan, Eric Wallace, Sheng Shen, Dan Klein",
    "title": "Poisoning Language Models During Instruction Tuning",
    "comments": "ICML 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Instruction-tuned LMs such as ChatGPT, FLAN, and InstructGPT are finetuned on\ndatasets that contain user-submitted examples, e.g., FLAN aggregates numerous\nopen-source datasets and OpenAI leverages examples submitted in the browser\nplayground. In this work, we show that adversaries can contribute poison\nexamples to these datasets, allowing them to manipulate model predictions\nwhenever a desired trigger phrase appears in the input. For example, when a\ndownstream user provides an input that mentions \"Joe Biden\", a poisoned LM will\nstruggle to classify, summarize, edit, or translate that input. To construct\nthese poison examples, we optimize their inputs and outputs using a\nbag-of-words approximation to the LM. We evaluate our method on open-source\ninstruction-tuned LMs. By using as few as 100 poison examples, we can cause\narbitrary phrases to have consistent negative polarity or induce degenerate\noutputs across hundreds of held-out tasks. Worryingly, we also show that larger\nLMs are increasingly vulnerable to poisoning and that defenses based on data\nfiltering or reducing model capacity provide only moderate protections while\nreducing test accuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 May 2023 16:57:33 GMT"
      }
    ],
    "update_date": "2023-05-02",
    "authors_parsed": [
      [
        "Wan",
        "Alexander",
        ""
      ],
      [
        "Wallace",
        "Eric",
        ""
      ],
      [
        "Shen",
        "Sheng",
        ""
      ],
      [
        "Klein",
        "Dan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.00944",
    "publish_date": "2023-05-01"
  },
  {
    "id": "2305.01937",
    "submitter": "Cheng-Han Chiang",
    "authors": "Cheng-Han Chiang and Hung-yi Lee",
    "title": "Can Large Language Models Be an Alternative to Human Evaluations?",
    "comments": "ACL 2023 main conference paper. Main content: 10 pages (including\n  limitations). Appendix: 13 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.HC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Human evaluation is indispensable and inevitable for assessing the quality of\ntexts generated by machine learning models or written by humans. However, human\nevaluation is very difficult to reproduce and its quality is notoriously\nunstable, hindering fair comparisons among different natural language\nprocessing (NLP) models and algorithms. Recently, large language models (LLMs)\nhave demonstrated exceptional performance on unseen tasks when only the task\ninstructions are provided. In this paper, we explore if such an ability of the\nLLMs can be used as an alternative to human evaluation. We present the LLMs\nwith the exact same instructions, samples to be evaluated, and questions used\nto conduct human evaluation, and then ask the LLMs to generate responses to\nthose questions; we dub this LLM evaluation. We use human evaluation and LLM\nevaluation to evaluate the texts in two NLP tasks: open-ended story generation\nand adversarial attacks. We show that the result of LLM evaluation is\nconsistent with the results obtained by expert human evaluation: the texts\nrated higher by human experts are also rated higher by the LLMs. We also find\nthat the results of LLM evaluation are stable over different formatting of the\ntask instructions and the sampling algorithm used to generate the answer. We\nare the first to show the potential of using LLMs to assess the quality of\ntexts and discuss the limitations and ethical considerations of LLM evaluation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 3 May 2023 07:28:50 GMT"
      }
    ],
    "update_date": "2023-05-04",
    "authors_parsed": [
      [
        "Chiang",
        "Cheng-Han",
        ""
      ],
      [
        "Lee",
        "Hung-yi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.01937",
    "publish_date": "2023-05-03"
  },
  {
    "id": "2305.02383",
    "submitter": "Zhaohan Xi",
    "authors": "Zhaohan Xi and Tianyu Du and Changjiang Li and Ren Pang and Shouling\n  Ji and Xiapu Luo and Xusheng Xiao and Fenglong Ma and Ting Wang",
    "title": "On the Security Risks of Knowledge Graph Reasoning",
    "comments": "In proceedings of USENIX Security'23. Codes:\n  https://github.com/HarrialX/security-risk-KG-reasoning",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Knowledge graph reasoning (KGR) -- answering complex logical queries over\nlarge knowledge graphs -- represents an important artificial intelligence task,\nentailing a range of applications (e.g., cyber threat hunting). However,\ndespite its surging popularity, the potential security risks of KGR are largely\nunexplored, which is concerning, given the increasing use of such capability in\nsecurity-critical domains.\n  This work represents a solid initial step towards bridging the striking gap.\nWe systematize the security threats to KGR according to the adversary's\nobjectives, knowledge, and attack vectors. Further, we present ROAR, a new\nclass of attacks that instantiate a variety of such threats. Through empirical\nevaluation in representative use cases (e.g., medical decision support, cyber\nthreat hunting, and commonsense reasoning), we demonstrate that ROAR is highly\neffective to mislead KGR to suggest pre-defined answers for target queries, yet\nwith negligible impact on non-target ones. Finally, we explore potential\ncountermeasures against ROAR, including filtering of potentially poisoning\nknowledge and training with adversarially augmented queries, which leads to\nseveral promising research directions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 3 May 2023 18:47:42 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 22 Jun 2023 06:17:30 GMT"
      }
    ],
    "update_date": "2023-06-23",
    "authors_parsed": [
      [
        "Xi",
        "Zhaohan",
        ""
      ],
      [
        "Du",
        "Tianyu",
        ""
      ],
      [
        "Li",
        "Changjiang",
        ""
      ],
      [
        "Pang",
        "Ren",
        ""
      ],
      [
        "Ji",
        "Shouling",
        ""
      ],
      [
        "Luo",
        "Xiapu",
        ""
      ],
      [
        "Xiao",
        "Xusheng",
        ""
      ],
      [
        "Ma",
        "Fenglong",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.02383",
    "publish_date": "2023-05-03"
  },
  {
    "id": "2305.03253",
    "submitter": "Bin Ji",
    "authors": "Bin Ji",
    "title": "VicunaNER: Zero/Few-shot Named Entity Recognition using Vicuna",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs, e.g., ChatGPT) have shown impressive zero- and\nfew-shot capabilities in Named Entity Recognition (NER). However, these models\ncan only be accessed via online APIs, which may cause data leak and\nnon-reproducible problems. In this paper, we propose VicunaNER, a zero/few-shot\nNER framework based on the newly released open-source LLM -- Vicuna. VicunaNER\nis a two-phase framework, where each phase leverages multi-turn dialogues with\nVicuna to recognize entities from texts. We name the second phase as\nRe-Recognition, which recognizes those entities not recognized in the first\nphase (a.k.a. Recognition). Moreover, we set entity correctness check dialogues\nin each phase to filter out wrong entities. We evaluate VicunaNER's zero-shot\ncapacity on 10 datasets crossing 5 domains and few-shot capacity on Few-NERD.\nExperimental results demonstrate that VicunaNER achieves superior performance\nin both shot settings. Additionally, we conduct comprehensive investigations on\nVicuna from multiple perspectives.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 5 May 2023 02:46:22 GMT"
      }
    ],
    "update_date": "2023-05-08",
    "authors_parsed": [
      [
        "Ji",
        "Bin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.03253",
    "publish_date": "2023-05-05"
  },
  {
    "id": "2305.03495",
    "submitter": "Reid Pryzant",
    "authors": "Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, Michael\n  Zeng",
    "title": "Automatic Prompt Optimization with \"Gradient Descent\" and Beam Search",
    "comments": "EMNLP 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have shown impressive performance as general\npurpose agents, but their abilities remain highly dependent on prompts which\nare hand written with onerous trial-and-error effort. We propose a simple and\nnonparametric solution to this problem, Automatic Prompt Optimization (APO),\nwhich is inspired by numerical gradient descent to automatically improve\nprompts, assuming access to training data and an LLM API. The algorithm uses\nminibatches of data to form natural language \"gradients\" that criticize the\ncurrent prompt. The gradients are then \"propagated\" into the prompt by editing\nthe prompt in the opposite semantic direction of the gradient. These gradient\ndescent steps are guided by a beam search and bandit selection procedure which\nsignificantly improves algorithmic efficiency. Preliminary results across three\nbenchmark NLP tasks and the novel problem of LLM jailbreak detection suggest\nthat Automatic Prompt Optimization can outperform prior prompt editing\ntechniques and improve an initial prompt's performance by up to 31%, by using\ndata to rewrite vague task descriptions into more precise annotation\ninstructions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 4 May 2023 15:15:22 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 19 Oct 2023 04:37:25 GMT"
      }
    ],
    "update_date": "2023-10-20",
    "authors_parsed": [
      [
        "Pryzant",
        "Reid",
        ""
      ],
      [
        "Iter",
        "Dan",
        ""
      ],
      [
        "Li",
        "Jerry",
        ""
      ],
      [
        "Lee",
        "Yin Tat",
        ""
      ],
      [
        "Zhu",
        "Chenguang",
        ""
      ],
      [
        "Zeng",
        "Michael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.03495",
    "publish_date": "2023-05-04"
  },
  {
    "id": "2305.03803",
    "submitter": "Aftab Hussain",
    "authors": "Aftab Hussain, Md Rafiqul Islam Rabin, Toufique Ahmed, Navid Ayoobi,\n  Bowen Xu, Prem Devanbu, Mohammad Amin Alipour",
    "title": "A Survey of Trojans in Neural Models of Source Code: Taxonomy and\n  Techniques",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this work, we study literature in Explainable AI and Safe AI to understand\npoisoning of neural models of code. In order to do so, we first establish a\nnovel taxonomy for Trojan AI for code, and present a new aspect-based\nclassification of triggers in neural models of code. Next, we highlight recent\nworks that help us deepen our conception of how these models understand\nsoftware code. Then we pick some of the recent, state-of-art poisoning\nstrategies that can be used to manipulate such models. The insights we draw can\npotentially help to foster future research in the area of Trojan AI for code.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 5 May 2023 19:07:09 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 9 May 2023 18:50:02 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 7 Jun 2023 19:33:59 GMT"
      },
      {
        "version": "v4",
        "created": "Wed, 21 Jun 2023 21:47:26 GMT"
      }
    ],
    "update_date": "2023-06-23",
    "authors_parsed": [
      [
        "Hussain",
        "Aftab",
        ""
      ],
      [
        "Rabin",
        "Md Rafiqul Islam",
        ""
      ],
      [
        "Ahmed",
        "Toufique",
        ""
      ],
      [
        "Ayoobi",
        "Navid",
        ""
      ],
      [
        "Xu",
        "Bowen",
        ""
      ],
      [
        "Devanbu",
        "Prem",
        ""
      ],
      [
        "Alipour",
        "Mohammad Amin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.03803",
    "publish_date": "2023-06-07"
  },
  {
    "id": "2305.03807",
    "submitter": "Zhengyuan Jiang",
    "authors": "Zhengyuan Jiang, Jinghuai Zhang, Neil Zhenqiang Gong",
    "title": "Evading Watermark based Detection of AI-Generated Content",
    "comments": "To appear in ACM Conference on Computer and Communications Security\n  (CCS), 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  A generative AI model can generate extremely realistic-looking content,\nposing growing challenges to the authenticity of information. To address the\nchallenges, watermark has been leveraged to detect AI-generated content.\nSpecifically, a watermark is embedded into an AI-generated content before it is\nreleased. A content is detected as AI-generated if a similar watermark can be\ndecoded from it. In this work, we perform a systematic study on the robustness\nof such watermark-based AI-generated content detection. We focus on\nAI-generated images. Our work shows that an attacker can post-process a\nwatermarked image via adding a small, human-imperceptible perturbation to it,\nsuch that the post-processed image evades detection while maintaining its\nvisual quality. We show the effectiveness of our attack both theoretically and\nempirically. Moreover, to evade detection, our adversarial post-processing\nmethod adds much smaller perturbations to AI-generated images and thus better\nmaintain their visual quality than existing popular post-processing methods\nsuch as JPEG compression, Gaussian blur, and Brightness/Contrast. Our work\nshows the insufficiency of existing watermark-based detection of AI-generated\ncontent, highlighting the urgent needs of new methods. Our code is publicly\navailable: https://github.com/zhengyuan-jiang/WEvade.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 5 May 2023 19:20:29 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 20 Aug 2023 00:55:52 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 22 Aug 2023 14:26:18 GMT"
      },
      {
        "version": "v4",
        "created": "Wed, 18 Oct 2023 06:03:27 GMT"
      },
      {
        "version": "v5",
        "created": "Wed, 8 Nov 2023 15:23:10 GMT"
      }
    ],
    "update_date": "2023-11-09",
    "authors_parsed": [
      [
        "Jiang",
        "Zhengyuan",
        ""
      ],
      [
        "Zhang",
        "Jinghuai",
        ""
      ],
      [
        "Gong",
        "Neil Zhenqiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.03807",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2305.04149",
    "submitter": "Efstratios Chatzoglou",
    "authors": "Efstratios Chatzoglou and Georgios Karopoulos and Georgios Kambourakis\n  and Zisis Tsiatsikas",
    "title": "Bypassing antivirus detection: old-school malware, new tricks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Being on a mushrooming spree since at least 2013, malware can take a large\ntoll on any system. In a perpetual cat-and-mouse chase with defenders, malware\nwriters constantly conjure new methods to hide their code so as to evade\ndetection by security products. In this context, focusing on the MS Windows\nplatform, this work contributes a comprehensive empirical evaluation regarding\nthe detection capacity of popular, off-the-shelf antivirus and endpoint\ndetection and response engines when facing legacy malware obfuscated via more\nor less uncommon but publicly known methods. Our experiments exploit a blend of\nseven traditional AV evasion techniques in 16 executables built in C++, Go, and\nRust. Furthermore, we conduct an incipient study regarding the ability of the\nChatGPT chatbot in assisting threat actors to produce ready-to-use malware. The\nderived results in terms of detection rate are highly unexpected: approximately\nhalf of the 12 tested AV engines were able to detect less than half of the\nmalware variants, four AVs exactly half of the variants, while only two of the\nrest detected all but one of the variants.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 6 May 2023 23:57:01 GMT"
      }
    ],
    "update_date": "2023-05-09",
    "authors_parsed": [
      [
        "Chatzoglou",
        "Efstratios",
        ""
      ],
      [
        "Karopoulos",
        "Georgios",
        ""
      ],
      [
        "Kambourakis",
        "Georgios",
        ""
      ],
      [
        "Tsiatsikas",
        "Zisis",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.04149",
    "publish_date": "2023-05-06"
  },
  {
    "id": "2305.05040",
    "submitter": "Bechir Hamdaoui",
    "authors": "Opeyemi Ajibuwa, Bechir Hamdaoui, Attila A. Yavuz",
    "title": "A Survey on AI/ML-Driven Intrusion and Misbehavior Detection in\n  Networked Autonomous Systems: Techniques, Challenges and Opportunities",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  AI/ML-based intrusion detection systems (IDSs) and misbehavior detection\nsystems (MDSs) have shown great potential in identifying anomalies in the\nnetwork traffic of networked autonomous systems. Despite the vast research\nefforts, practical deployments of such systems in the real world have been\nlimited. Although the safety-critical nature of autonomous systems and the\nvulnerability of learning-based techniques to adversarial attacks are among the\npotential reasons, the lack of objective evaluation and feasibility assessment\nmetrics is one key reason behind the limited adoption of these systems in\npractical settings. This survey aims to address the aforementioned limitation\nby presenting an in-depth analysis of AI/ML-based IDSs/MDSs and establishing\nbaseline metrics relevant to networked autonomous systems. Furthermore, this\nwork thoroughly surveys recent studies in this domain, highlighting the\nevaluation metrics and gaps in the current literature. It also presents key\nfindings derived from our analysis of the surveyed papers and proposes\nguidelines for providing AI/ML-based IDS/MDS solution approaches suitable for\nvehicular network applications. Our work provides researchers and practitioners\nwith the needed tools to evaluate the feasibility of AI/ML-based IDS/MDS\ntechniques in real-world settings, with the aim of facilitating the practical\nadoption of such techniques in emerging autonomous vehicular systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 8 May 2023 20:55:44 GMT"
      }
    ],
    "update_date": "2023-05-10",
    "authors_parsed": [
      [
        "Ajibuwa",
        "Opeyemi",
        ""
      ],
      [
        "Hamdaoui",
        "Bechir",
        ""
      ],
      [
        "Yavuz",
        "Attila A.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.05040",
    "publish_date": "2023-05-08"
  },
  {
    "id": "2305.10036",
    "submitter": "Jingwei Yi",
    "authors": "Wenjun Peng, Jingwei Yi, Fangzhao Wu, Shangxi Wu, Bin Zhu, Lingjuan\n  Lyu, Binxing Jiao, Tong Xu, Guangzhong Sun, Xing Xie",
    "title": "Are You Copying My Model? Protecting the Copyright of Large Language\n  Models for EaaS via Backdoor Watermark",
    "comments": "Accepted by ACL 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs) have demonstrated powerful capabilities in both\ntext understanding and generation. Companies have begun to offer Embedding as a\nService (EaaS) based on these LLMs, which can benefit various natural language\nprocessing (NLP) tasks for customers. However, previous studies have shown that\nEaaS is vulnerable to model extraction attacks, which can cause significant\nlosses for the owners of LLMs, as training these models is extremely expensive.\nTo protect the copyright of LLMs for EaaS, we propose an Embedding Watermark\nmethod called EmbMarker that implants backdoors on embeddings. Our method\nselects a group of moderate-frequency words from a general text corpus to form\na trigger set, then selects a target embedding as the watermark, and inserts it\ninto the embeddings of texts containing trigger words as the backdoor. The\nweight of insertion is proportional to the number of trigger words included in\nthe text. This allows the watermark backdoor to be effectively transferred to\nEaaS-stealer's model for copyright verification while minimizing the adverse\nimpact on the original embeddings' utility. Our extensive experiments on\nvarious datasets show that our method can effectively protect the copyright of\nEaaS models without compromising service quality.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 May 2023 08:28:54 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 30 May 2023 08:06:30 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 2 Jun 2023 06:56:29 GMT"
      }
    ],
    "update_date": "2023-06-05",
    "authors_parsed": [
      [
        "Peng",
        "Wenjun",
        ""
      ],
      [
        "Yi",
        "Jingwei",
        ""
      ],
      [
        "Wu",
        "Fangzhao",
        ""
      ],
      [
        "Wu",
        "Shangxi",
        ""
      ],
      [
        "Zhu",
        "Bin",
        ""
      ],
      [
        "Lyu",
        "Lingjuan",
        ""
      ],
      [
        "Jiao",
        "Binxing",
        ""
      ],
      [
        "Xu",
        "Tong",
        ""
      ],
      [
        "Sun",
        "Guangzhong",
        ""
      ],
      [
        "Xie",
        "Xing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.10036",
    "publish_date": "2023-05-17"
  },
  {
    "id": "2305.10847",
    "submitter": "Shengcai Liu",
    "authors": "Ning Lu, Shengcai Liu, Rui He, Qi Wang, Yew-Soon Ong, Ke Tang",
    "title": "Large Language Models can be Guided to Evade AI-Generated Text Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs) have shown remarkable performance in various\ntasks and have been extensively utilized by the public. However, the increasing\nconcerns regarding the misuse of LLMs, such as plagiarism and spamming, have\nled to the development of multiple detectors, including fine-tuned classifiers\nand statistical methods. In this study, we equip LLMs with prompts, rather than\nrelying on an external paraphraser, to evaluate the vulnerability of these\ndetectors. We propose a novel Substitution-based In-Context example\nOptimization method (SICO) to automatically construct prompts for evading the\ndetectors. SICO is cost-efficient as it requires only 40 human-written examples\nand a limited number of LLM inferences to generate a prompt. Moreover, once a\ntask-specific prompt has been constructed, it can be universally used against a\nwide range of detectors. Extensive experiments across three real-world tasks\ndemonstrate that SICO significantly outperforms the paraphraser baselines and\nenables GPT-3.5 to successfully evade six detectors, decreasing their AUC by\n0.5 on average. Furthermore, a comprehensive human evaluation as well as a\nvalidation experiment in the wild show that the SICO-generated text achieves\nhuman-level readability and task completion rates. Finally, the strong\nperformance of SICO exhibits its potential as a reliable evaluation tool for\nfuture detectors. The codes and data are located on\nhttps://github.com/ColinLu50/Evade-GPT-Detector.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 May 2023 10:03:25 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 19 May 2023 11:25:01 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 5 Jun 2023 03:54:52 GMT"
      },
      {
        "version": "v4",
        "created": "Sat, 17 Jun 2023 03:48:41 GMT"
      },
      {
        "version": "v5",
        "created": "Thu, 14 Dec 2023 12:21:05 GMT"
      }
    ],
    "update_date": "2023-12-15",
    "authors_parsed": [
      [
        "Lu",
        "Ning",
        ""
      ],
      [
        "Liu",
        "Shengcai",
        ""
      ],
      [
        "He",
        "Rui",
        ""
      ],
      [
        "Wang",
        "Qi",
        ""
      ],
      [
        "Ong",
        "Yew-Soon",
        ""
      ],
      [
        "Tang",
        "Ke",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.10847",
    "publish_date": "2023-05-18"
  },
  {
    "id": "2305.11039",
    "submitter": "Soumyadeep Hore",
    "authors": "Soumyadeep Hore and Jalal Ghadermazi and Diwas Paudel and Ankit Shah\n  and Tapas K. Das and Nathaniel D. Bastian",
    "title": "Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial\n  Network Packet Generation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advancements in artificial intelligence (AI) and machine learning (ML)\nalgorithms, coupled with the availability of faster computing infrastructure,\nhave enhanced the security posture of cybersecurity operations centers\n(defenders) through the development of ML-aided network intrusion detection\nsystems (NIDS). Concurrently, the abilities of adversaries to evade security\nhave also increased with the support of AI/ML models. Therefore, defenders need\nto proactively prepare for evasion attacks that exploit the detection\nmechanisms of NIDS. Recent studies have found that the perturbation of\nflow-based and packet-based features can deceive ML models, but these\napproaches have limitations. Perturbations made to the flow-based features are\ndifficult to reverse-engineer, while samples generated with perturbations to\nthe packet-based features are not playable.\n  Our methodological framework, Deep PackGen, employs deep reinforcement\nlearning to generate adversarial packets and aims to overcome the limitations\nof approaches in the literature. By taking raw malicious network packets as\ninputs and systematically making perturbations on them, Deep PackGen\ncamouflages them as benign packets while still maintaining their functionality.\nIn our experiments, using publicly available data, Deep PackGen achieved an\naverage adversarial success rate of 66.4\\% against various ML models and across\ndifferent attack types. Our investigation also revealed that more than 45\\% of\nthe successful adversarial samples were out-of-distribution packets that evaded\nthe decision boundaries of the classifiers. The knowledge gained from our study\non the adversary's ability to make specific evasive perturbations to different\ntypes of malicious packets can help defenders enhance the robustness of their\nNIDS against evolving adversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 May 2023 15:32:32 GMT"
      }
    ],
    "update_date": "2023-05-19",
    "authors_parsed": [
      [
        "Hore",
        "Soumyadeep",
        ""
      ],
      [
        "Ghadermazi",
        "Jalal",
        ""
      ],
      [
        "Paudel",
        "Diwas",
        ""
      ],
      [
        "Shah",
        "Ankit",
        ""
      ],
      [
        "Das",
        "Tapas K.",
        ""
      ],
      [
        "Bastian",
        "Nathaniel D.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.11039",
    "publish_date": "2023-05-18"
  },
  {
    "id": "2305.11275",
    "submitter": "Galen Pogoncheff",
    "authors": "Galen Pogoncheff, Jacob Granley, Michael Beyeler",
    "title": "Explaining V1 Properties with a Biologically Constrained Deep Learning\n  Architecture",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "q-bio.NC cs.AI eess.IV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Convolutional neural networks (CNNs) have recently emerged as promising\nmodels of the ventral visual stream, despite their lack of biological\nspecificity. While current state-of-the-art models of the primary visual cortex\n(V1) have surfaced from training with adversarial examples and extensively\naugmented data, these models are still unable to explain key neural properties\nobserved in V1 that arise from biological circuitry. To address this gap, we\nsystematically incorporated neuroscience-derived architectural components into\nCNNs to identify a set of mechanisms and architectures that comprehensively\nexplain neural activity in V1. We show drastic improvements in model-V1\nalignment driven by the integration of architectural components that simulate\ncenter-surround antagonism, local receptive fields, tuned normalization, and\ncortical magnification. Upon enhancing task-driven CNNs with a collection of\nthese specialized components, we uncover models with latent representations\nthat yield state-of-the-art explanation of V1 neural activity and tuning\nproperties. Our results highlight an important advancement in the field of\nNeuroAI, as we systematically establish a set of architectural components that\ncontribute to unprecedented explanation of V1. The neuroscience insights that\ncould be gleaned from increasingly accurate in-silico models of the brain have\nthe potential to greatly advance the fields of both neuroscience and artificial\nintelligence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 May 2023 19:48:05 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 25 May 2023 20:40:28 GMT"
      }
    ],
    "update_date": "2023-05-29",
    "authors_parsed": [
      [
        "Pogoncheff",
        "Galen",
        ""
      ],
      [
        "Granley",
        "Jacob",
        ""
      ],
      [
        "Beyeler",
        "Michael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.11275",
    "publish_date": "2023-05-25"
  },
  {
    "id": "2305.12351",
    "submitter": "Christopher Burger",
    "authors": "Christopher Burger, Lingwei Chen, Thai Le",
    "title": "Are Your Explanations Reliable? Investigating the Stability of LIME in\n  Explaining Text Classifiers by Marrying XAI and Adversarial Attack",
    "comments": "14 pages, 6 figures. Replacement by the updated version to be\n  published in EMNLP 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  LIME has emerged as one of the most commonly referenced tools in explainable\nAI (XAI) frameworks that is integrated into critical machine learning\napplications--e.g., healthcare and finance. However, its stability remains\nlittle explored, especially in the context of text data, due to the unique\ntext-space constraints. To address these challenges, in this paper, we first\nevaluate the inherent instability of LIME on text data to establish a baseline,\nand then propose a novel algorithm XAIFooler to perturb text inputs and\nmanipulate explanations that casts investigation on the stability of LIME as a\ntext perturbation optimization problem. XAIFooler conforms to the constraints\nto preserve text semantics and original prediction with small perturbations,\nand introduces Rank-biased Overlap (RBO) as a key part to guide the\noptimization of XAIFooler that satisfies all the requirements for explanation\nsimilarity measure. Extensive experiments on real-world text datasets\ndemonstrate that XAIFooler significantly outperforms all baselines by large\nmargins in its ability to manipulate LIME's explanations with high semantic\npreservability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 21 May 2023 05:06:46 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 15 Oct 2023 13:19:44 GMT"
      }
    ],
    "update_date": "2023-10-17",
    "authors_parsed": [
      [
        "Burger",
        "Christopher",
        ""
      ],
      [
        "Chen",
        "Lingwei",
        ""
      ],
      [
        "Le",
        "Thai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.12351",
    "publish_date": "2023-05-21"
  },
  {
    "id": "2305.12680",
    "submitter": "Haolan Zhan",
    "authors": "Haolan Zhan and Xuanli He and Qiongkai Xu and Yuxiang Wu and Pontus\n  Stenetorp",
    "title": "G3Detector: General GPT-Generated Text Detector",
    "comments": "Encounter some tech bugs, need to refresh corresponding results",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The burgeoning progress in the field of Large Language Models (LLMs) heralds\nsignificant benefits due to their unparalleled capacities. However, it is\ncritical to acknowledge the potential misuse of these models, which could give\nrise to a spectrum of social and ethical dilemmas. Despite numerous preceding\nefforts centered around distinguishing synthetic text, most existing detection\nsystems fail to identify data synthesized by the latest LLMs, such as ChatGPT\nand GPT-4. In response to this challenge, we introduce an unpretentious yet\npotent detection approach proficient in identifying synthetic text across a\nwide array of fields. Moreover, our detector demonstrates outstanding\nperformance uniformly across various model architectures and decoding\nstrategies. It also possesses the capability to identify text generated\nutilizing a potent detection-evasion technique. Our comprehensive research\nunderlines our commitment to boosting the robustness and efficiency of\nmachine-generated text detection mechanisms, particularly in the context of\nswiftly progressing and increasingly adaptive AI technologies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 22 May 2023 03:35:00 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 4 Aug 2023 06:07:49 GMT"
      }
    ],
    "update_date": "2023-08-07",
    "authors_parsed": [
      [
        "Zhan",
        "Haolan",
        ""
      ],
      [
        "He",
        "Xuanli",
        ""
      ],
      [
        "Xu",
        "Qiongkai",
        ""
      ],
      [
        "Wu",
        "Yuxiang",
        ""
      ],
      [
        "Stenetorp",
        "Pontus",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.12680",
    "publish_date": "2023-05-22"
  },
  {
    "id": "2305.13242",
    "submitter": "Yafu Li",
    "authors": "Yafu Li, Qintong Li, Leyang Cui, Wei Bi, Longyue Wang, Linyi Yang,\n  Shuming Shi and Yue Zhang",
    "title": "Deepfake Text Detection in the Wild",
    "comments": "Working in progress",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in large language models have enabled them to reach a level\nof text generation comparable to that of humans. These models show powerful\ncapabilities across a wide range of content, including news article writing,\nstory generation, and scientific writing. Such capability further narrows the\ngap between human-authored and machine-generated texts, highlighting the\nimportance of deepfake text detection to avoid potential risks such as fake\nnews propagation and plagiarism. However, previous work has been limited in\nthat they testify methods on testbed of specific domains or certain language\nmodels. In practical scenarios, the detector faces texts from various domains\nor LLMs without knowing their sources. To this end, we build a wild testbed by\ngathering texts from various human writings and deepfake texts generated by\ndifferent LLMs. Human annotators are only slightly better than random guessing\nat identifying machine-generated texts. Empirical results on automatic\ndetection methods further showcase the challenges of deepfake text detection in\na wild testbed. In addition, out-of-distribution poses a greater challenge for\na detector to be employed in realistic application scenarios. We release our\nresources at https://github.com/yafuly/DeepfakeTextDetect.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 22 May 2023 17:13:29 GMT"
      }
    ],
    "update_date": "2023-05-23",
    "authors_parsed": [
      [
        "Li",
        "Yafu",
        ""
      ],
      [
        "Li",
        "Qintong",
        ""
      ],
      [
        "Cui",
        "Leyang",
        ""
      ],
      [
        "Bi",
        "Wei",
        ""
      ],
      [
        "Wang",
        "Longyue",
        ""
      ],
      [
        "Yang",
        "Linyi",
        ""
      ],
      [
        "Shi",
        "Shuming",
        ""
      ],
      [
        "Zhang",
        "Yue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.13242",
    "publish_date": "2023-05-22"
  },
  {
    "id": "2305.13774",
    "submitter": "Yan Zhao",
    "authors": "Jiangyan Yi, Jianhua Tao, Ruibo Fu, Xinrui Yan, Chenglong Wang, Tao\n  Wang, Chu Yuan Zhang, Xiaohui Zhang, Yan Zhao, Yong Ren, Le Xu, Junzuo Zhou,\n  Hao Gu, Zhengqi Wen, Shan Liang, Zheng Lian, Shuai Nie, Haizhou Li",
    "title": "ADD 2023: the Second Audio Deepfake Detection Challenge",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Audio deepfake detection is an emerging topic in the artificial intelligence\ncommunity. The second Audio Deepfake Detection Challenge (ADD 2023) aims to\nspur researchers around the world to build new innovative technologies that can\nfurther accelerate and foster research on detecting and analyzing deepfake\nspeech utterances. Different from previous challenges (e.g. ADD 2022), ADD 2023\nfocuses on surpassing the constraints of binary real/fake classification, and\nactually localizing the manipulated intervals in a partially fake speech as\nwell as pinpointing the source responsible for generating any fake audio.\nFurthermore, ADD 2023 includes more rounds of evaluation for the fake audio\ngame sub-challenge. The ADD 2023 challenge includes three subchallenges: audio\nfake game (FG), manipulation region location (RL) and deepfake algorithm\nrecognition (AR). This paper describes the datasets, evaluation metrics, and\nprotocols. Some findings are also reported in audio deepfake detection tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 May 2023 07:42:52 GMT"
      }
    ],
    "update_date": "2023-05-24",
    "authors_parsed": [
      [
        "Yi",
        "Jiangyan",
        ""
      ],
      [
        "Tao",
        "Jianhua",
        ""
      ],
      [
        "Fu",
        "Ruibo",
        ""
      ],
      [
        "Yan",
        "Xinrui",
        ""
      ],
      [
        "Wang",
        "Chenglong",
        ""
      ],
      [
        "Wang",
        "Tao",
        ""
      ],
      [
        "Zhang",
        "Chu Yuan",
        ""
      ],
      [
        "Zhang",
        "Xiaohui",
        ""
      ],
      [
        "Zhao",
        "Yan",
        ""
      ],
      [
        "Ren",
        "Yong",
        ""
      ],
      [
        "Xu",
        "Le",
        ""
      ],
      [
        "Zhou",
        "Junzuo",
        ""
      ],
      [
        "Gu",
        "Hao",
        ""
      ],
      [
        "Wen",
        "Zhengqi",
        ""
      ],
      [
        "Liang",
        "Shan",
        ""
      ],
      [
        "Lian",
        "Zheng",
        ""
      ],
      [
        "Nie",
        "Shuai",
        ""
      ],
      [
        "Li",
        "Haizhou",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.13774",
    "publish_date": "2023-05-23"
  },
  {
    "id": "2305.13860",
    "submitter": "Yi Liu",
    "authors": "Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang,\n  Lida Zhao, Tianwei Zhang, and Yang Liu",
    "title": "Jailbreaking ChatGPT via Prompt Engineering: An Empirical Study",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs), like ChatGPT, have demonstrated vast potential\nbut also introduce challenges related to content constraints and potential\nmisuse. Our study investigates three key research questions: (1) the number of\ndifferent prompt types that can jailbreak LLMs, (2) the effectiveness of\njailbreak prompts in circumventing LLM constraints, and (3) the resilience of\nChatGPT against these jailbreak prompts. Initially, we develop a classification\nmodel to analyze the distribution of existing prompts, identifying ten distinct\npatterns and three categories of jailbreak prompts. Subsequently, we assess the\njailbreak capability of prompts with ChatGPT versions 3.5 and 4.0, utilizing a\ndataset of 3,120 jailbreak questions across eight prohibited scenarios.\nFinally, we evaluate the resistance of ChatGPT against jailbreak prompts,\nfinding that the prompts can consistently evade the restrictions in 40 use-case\nscenarios. The study underscores the importance of prompt structures in\njailbreaking LLMs and discusses the challenges of robust jailbreak prompt\ngeneration and prevention.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 May 2023 09:33:38 GMT"
      }
    ],
    "update_date": "2023-05-24",
    "authors_parsed": [
      [
        "Liu",
        "Yi",
        ""
      ],
      [
        "Deng",
        "Gelei",
        ""
      ],
      [
        "Xu",
        "Zhengzi",
        ""
      ],
      [
        "Li",
        "Yuekang",
        ""
      ],
      [
        "Zheng",
        "Yaowen",
        ""
      ],
      [
        "Zhang",
        "Ying",
        ""
      ],
      [
        "Zhao",
        "Lida",
        ""
      ],
      [
        "Zhang",
        "Tianwei",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.13860",
    "publish_date": "2023-05-23"
  },
  {
    "id": "2305.13934",
    "submitter": "Yasir Zaki",
    "authors": "Hazem Ibrahim, Fengyuan Liu, Rohail Asim, Balaraju Battu, Sidahmed\n  Benabderrahmane, Bashar Alhafni, Wifag Adnan, Tuka Alhanai, Bedoor AlShebli,\n  Riyadh Baghdadi, Jocelyn J. B\\'elanger, Elena Beretta, Kemal Celik, Moumena\n  Chaqfeh, Mohammed F. Daqaq, Zaynab El Bernoussi, Daryl Fougnie, Borja Garcia\n  de Soto, Alberto Gandolfi, Andras Gyorgy, Nizar Habash, J. Andrew Harris,\n  Aaron Kaufman, Lefteris Kirousis, Korhan Kocak, Kangsan Lee, Seungah S. Lee,\n  Samreen Malik, Michail Maniatakos, David Melcher, Azzam Mourad, Minsu Park,\n  Mahmoud Rasras, Alicja Reuben, Dania Zantout, Nancy W. Gleason, Kinga Makovi,\n  Talal Rahwan, Yasir Zaki",
    "title": "Perception, performance, and detectability of conversational artificial\n  intelligence across 32 university courses",
    "comments": "17 pages, 4 figures",
    "journal-ref": null,
    "doi": "10.1038/s41598-023-38964-3",
    "report-no": null,
    "categories": "cs.CY cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The emergence of large language models has led to the development of powerful\ntools such as ChatGPT that can produce text indistinguishable from\nhuman-generated work. With the increasing accessibility of such technology,\nstudents across the globe may utilize it to help with their school work -- a\npossibility that has sparked discussions on the integrity of student\nevaluations in the age of artificial intelligence (AI). To date, it is unclear\nhow such tools perform compared to students on university-level courses.\nFurther, students' perspectives regarding the use of such tools, and educators'\nperspectives on treating their use as plagiarism, remain unknown. Here, we\ncompare the performance of ChatGPT against students on 32 university-level\ncourses. We also assess the degree to which its use can be detected by two\nclassifiers designed specifically for this purpose. Additionally, we conduct a\nsurvey across five countries, as well as a more in-depth survey at the authors'\ninstitution, to discern students' and educators' perceptions of ChatGPT's use.\nWe find that ChatGPT's performance is comparable, if not superior, to that of\nstudents in many courses. Moreover, current AI-text classifiers cannot reliably\ndetect ChatGPT's use in school work, due to their propensity to classify\nhuman-written answers as AI-generated, as well as the ease with which\nAI-generated text can be edited to evade detection. Finally, we find an\nemerging consensus among students to use the tool, and among educators to treat\nthis as plagiarism. Our findings offer insights that could guide policy\ndiscussions addressing the integration of AI into educational frameworks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 7 May 2023 10:37:51 GMT"
      }
    ],
    "update_date": "2023-08-30",
    "authors_parsed": [
      [
        "Ibrahim",
        "Hazem",
        ""
      ],
      [
        "Liu",
        "Fengyuan",
        ""
      ],
      [
        "Asim",
        "Rohail",
        ""
      ],
      [
        "Battu",
        "Balaraju",
        ""
      ],
      [
        "Benabderrahmane",
        "Sidahmed",
        ""
      ],
      [
        "Alhafni",
        "Bashar",
        ""
      ],
      [
        "Adnan",
        "Wifag",
        ""
      ],
      [
        "Alhanai",
        "Tuka",
        ""
      ],
      [
        "AlShebli",
        "Bedoor",
        ""
      ],
      [
        "Baghdadi",
        "Riyadh",
        ""
      ],
      [
        "B\u00e9langer",
        "Jocelyn J.",
        ""
      ],
      [
        "Beretta",
        "Elena",
        ""
      ],
      [
        "Celik",
        "Kemal",
        ""
      ],
      [
        "Chaqfeh",
        "Moumena",
        ""
      ],
      [
        "Daqaq",
        "Mohammed F.",
        ""
      ],
      [
        "Bernoussi",
        "Zaynab El",
        ""
      ],
      [
        "Fougnie",
        "Daryl",
        ""
      ],
      [
        "de Soto",
        "Borja Garcia",
        ""
      ],
      [
        "Gandolfi",
        "Alberto",
        ""
      ],
      [
        "Gyorgy",
        "Andras",
        ""
      ],
      [
        "Habash",
        "Nizar",
        ""
      ],
      [
        "Harris",
        "J. Andrew",
        ""
      ],
      [
        "Kaufman",
        "Aaron",
        ""
      ],
      [
        "Kirousis",
        "Lefteris",
        ""
      ],
      [
        "Kocak",
        "Korhan",
        ""
      ],
      [
        "Lee",
        "Kangsan",
        ""
      ],
      [
        "Lee",
        "Seungah S.",
        ""
      ],
      [
        "Malik",
        "Samreen",
        ""
      ],
      [
        "Maniatakos",
        "Michail",
        ""
      ],
      [
        "Melcher",
        "David",
        ""
      ],
      [
        "Mourad",
        "Azzam",
        ""
      ],
      [
        "Park",
        "Minsu",
        ""
      ],
      [
        "Rasras",
        "Mahmoud",
        ""
      ],
      [
        "Reuben",
        "Alicja",
        ""
      ],
      [
        "Zantout",
        "Dania",
        ""
      ],
      [
        "Gleason",
        "Nancy W.",
        ""
      ],
      [
        "Makovi",
        "Kinga",
        ""
      ],
      [
        "Rahwan",
        "Talal",
        ""
      ],
      [
        "Zaki",
        "Yasir",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.13934",
    "publish_date": "2023-05-07"
  },
  {
    "id": "2305.14384",
    "submitter": "Lora Aroyo",
    "authors": "Alicia Parrish, Hannah Rose Kirk, Jessica Quaye, Charvi Rastogi, Max\n  Bartolo, Oana Inel, Juan Ciro, Rafael Mosquera, Addison Howard, Will\n  Cukierski, D. Sculley, Vijay Janapa Reddi, Lora Aroyo",
    "title": "Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety\n  of Text-to-Image Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The generative AI revolution in recent years has been spurred by an expansion\nin compute power and data quantity, which together enable extensive\npre-training of powerful text-to-image (T2I) models. With their greater\ncapabilities to generate realistic and creative content, these T2I models like\nDALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider\naudiences. Any unsafe behaviors inherited from pretraining on uncurated\ninternet-scraped datasets thus have the potential to cause wide-reaching harm,\nfor example, through generated images which are violent, sexually explicit, or\ncontain biased and derogatory stereotypes. Despite this risk of harm, we lack\nsystematic and structured evaluation datasets to scrutinize model behavior,\nespecially adversarial attacks that bypass existing safety filters. A typical\nbottleneck in safety evaluation is achieving a wide coverage of different types\nof challenging examples in the evaluation set, i.e., identifying 'unknown\nunknowns' or long-tail problems. To address this need, we introduce the\nAdversarial Nibbler challenge. The goal of this challenge is to crowdsource a\ndiverse set of failure modes and reward challenge participants for successfully\nfinding safety vulnerabilities in current state-of-the-art T2I models.\nUltimately, we aim to provide greater awareness of these issues and assist\ndevelopers in improving the future safety and reliability of generative AI\nmodels. Adversarial Nibbler is a data-centric challenge, part of the DataPerf\nchallenge suite, organized and supported by Kaggle and MLCommons.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 22 May 2023 15:02:40 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Parrish",
        "Alicia",
        ""
      ],
      [
        "Kirk",
        "Hannah Rose",
        ""
      ],
      [
        "Quaye",
        "Jessica",
        ""
      ],
      [
        "Rastogi",
        "Charvi",
        ""
      ],
      [
        "Bartolo",
        "Max",
        ""
      ],
      [
        "Inel",
        "Oana",
        ""
      ],
      [
        "Ciro",
        "Juan",
        ""
      ],
      [
        "Mosquera",
        "Rafael",
        ""
      ],
      [
        "Howard",
        "Addison",
        ""
      ],
      [
        "Cukierski",
        "Will",
        ""
      ],
      [
        "Sculley",
        "D.",
        ""
      ],
      [
        "Reddi",
        "Vijay Janapa",
        ""
      ],
      [
        "Aroyo",
        "Lora",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.14384",
    "publish_date": "2023-05-22"
  },
  {
    "id": "2305.14553",
    "submitter": "Andrew Lohn",
    "authors": "Micah Musser, Andrew Lohn, James X. Dempsey, Jonathan Spring, Ram\n  Shankar Siva Kumar, Brenda Leong, Christina Liaghati, Cindy Martinez, Crystal\n  D. Grant, Daniel Rohrer, Heather Frase, Jonathan Elliott, John Bansemer,\n  Mikel Rodriguez, Mitt Regan, Rumman Chowdhury, Stefan Hermanek",
    "title": "Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and\n  Legal Implications",
    "comments": null,
    "journal-ref": null,
    "doi": "10.51593/2022CA003",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In July 2022, the Center for Security and Emerging Technology (CSET) at\nGeorgetown University and the Program on Geopolitics, Technology, and\nGovernance at the Stanford Cyber Policy Center convened a workshop of experts\nto examine the relationship between vulnerabilities in artificial intelligence\nsystems and more traditional types of software vulnerabilities. Topics\ndiscussed included the extent to which AI vulnerabilities can be handled under\nstandard cybersecurity processes, the barriers currently preventing the\naccurate sharing of information about AI vulnerabilities, legal issues\nassociated with adversarial attacks on AI systems, and potential areas where\ngovernment support could improve AI vulnerability management and mitigation.\n  This report is meant to accomplish two things. First, it provides a\nhigh-level discussion of AI vulnerabilities, including the ways in which they\nare disanalogous to other types of vulnerabilities, and the current state of\naffairs regarding information sharing and legal oversight of AI\nvulnerabilities. Second, it attempts to articulate broad recommendations as\nendorsed by the majority of participants at the workshop.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 23 May 2023 22:27:53 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Musser",
        "Micah",
        ""
      ],
      [
        "Lohn",
        "Andrew",
        ""
      ],
      [
        "Dempsey",
        "James X.",
        ""
      ],
      [
        "Spring",
        "Jonathan",
        ""
      ],
      [
        "Kumar",
        "Ram Shankar Siva",
        ""
      ],
      [
        "Leong",
        "Brenda",
        ""
      ],
      [
        "Liaghati",
        "Christina",
        ""
      ],
      [
        "Martinez",
        "Cindy",
        ""
      ],
      [
        "Grant",
        "Crystal D.",
        ""
      ],
      [
        "Rohrer",
        "Daniel",
        ""
      ],
      [
        "Frase",
        "Heather",
        ""
      ],
      [
        "Elliott",
        "Jonathan",
        ""
      ],
      [
        "Bansemer",
        "John",
        ""
      ],
      [
        "Rodriguez",
        "Mikel",
        ""
      ],
      [
        "Regan",
        "Mitt",
        ""
      ],
      [
        "Chowdhury",
        "Rumman",
        ""
      ],
      [
        "Hermanek",
        "Stefan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.14553",
    "publish_date": "2023-05-23"
  },
  {
    "id": "2305.14710",
    "submitter": "Muhao Chen",
    "authors": "Jiashu Xu, Mingyu Derek Ma, Fei Wang, Chaowei Xiao, Muhao Chen",
    "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction\n  Tuning for Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Instruction-tuned models are trained on crowdsourcing datasets with task\ninstructions to achieve superior performance. However, in this work we raise\nsecurity concerns about this training paradigm. Our studies demonstrate that an\nattacker can inject backdoors by issuing very few malicious instructions among\nthousands of gathered data and control model behavior through data poisoning,\nwithout even the need of modifying data instances or labels themselves. Through\nsuch instruction attacks, the attacker can achieve over 90% attack success rate\nacross four commonly used NLP datasets, and cause persistent backdoors that are\neasily transferred to 15 diverse datasets zero-shot. In this way, the attacker\ncan directly apply poisoned instructions designed for one dataset on many other\ndatasets. Moreover, the poisoned model cannot be cured by continual learning.\nLastly, instruction attacks show resistance to existing inference-time defense.\nThese findings highlight the need for more robust defenses against data\npoisoning attacks in instructiontuning models and underscore the importance of\nensuring data quality in instruction crowdsourcing.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 24 May 2023 04:27:21 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Xu",
        "Jiashu",
        ""
      ],
      [
        "Ma",
        "Mingyu Derek",
        ""
      ],
      [
        "Wang",
        "Fei",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ],
      [
        "Chen",
        "Muhao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.14710",
    "publish_date": "2023-05-24"
  },
  {
    "id": "2305.14763",
    "submitter": "Vered Shwartz",
    "authors": "Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin\n  Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz",
    "title": "Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in\n  Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The escalating debate on AI's capabilities warrants developing reliable\nmetrics to assess machine \"intelligence\". Recently, many anecdotal examples\nwere used to suggest that newer large language models (LLMs) like ChatGPT and\nGPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached\nconflicting conclusions regarding those abilities. We investigate the extent of\nLLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs\nexhibit certain N-ToM abilities, this behavior is far from being robust. We\nfurther examine the factors impacting performance on N-ToM tasks and discover\nthat LLMs struggle with adversarial examples, indicating reliance on shallow\nheuristics rather than robust ToM abilities. We caution against drawing\nconclusions from anecdotal examples, limited benchmark testing, and using\nhuman-designed psychological tests to evaluate models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 24 May 2023 06:14:31 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Shapira",
        "Natalie",
        ""
      ],
      [
        "Levy",
        "Mosh",
        ""
      ],
      [
        "Alavi",
        "Seyed Hossein",
        ""
      ],
      [
        "Zhou",
        "Xuhui",
        ""
      ],
      [
        "Choi",
        "Yejin",
        ""
      ],
      [
        "Goldberg",
        "Yoav",
        ""
      ],
      [
        "Sap",
        "Maarten",
        ""
      ],
      [
        "Shwartz",
        "Vered",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.14763",
    "publish_date": "2023-05-24"
  },
  {
    "id": "2305.14965",
    "submitter": "Abhinav Rao",
    "authors": "Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya, Monojit\n  Choudhury",
    "title": "Tricking LLMs into Disobedience: Understanding, Analyzing, and\n  Preventing Jailbreaks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent explorations with commercial Large Language Models (LLMs) have shown\nthat non-expert users can jailbreak LLMs by simply manipulating the prompts;\nresulting in degenerate output behavior, privacy and security breaches,\noffensive outputs, and violations of content regulator policies. Limited formal\nstudies have been carried out to formalize and analyze these attacks and their\nmitigations. We bridge this gap by proposing a formalism and a taxonomy of\nknown (and possible) jailbreaks. We perform a survey of existing jailbreak\nmethods and their effectiveness on open-source and commercial LLMs (such as GPT\n3.5, OPT, BLOOM, and FLAN-T5-xxl). We further propose a limited set of prompt\nguards and discuss their effectiveness against known attack types.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 24 May 2023 09:57:37 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Rao",
        "Abhinav",
        ""
      ],
      [
        "Vashistha",
        "Sachin",
        ""
      ],
      [
        "Naik",
        "Atharva",
        ""
      ],
      [
        "Aditya",
        "Somak",
        ""
      ],
      [
        "Choudhury",
        "Monojit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.14965",
    "publish_date": "2023-05-24"
  },
  {
    "id": "2305.15336",
    "submitter": "Venkata Sai Charan Putrevu",
    "authors": "P.V. Sai Charan, Hrushikesh Chunduri, P. Mohan Anand, and Sandeep K\n  Shukla",
    "title": "From Text to MITRE Techniques: Exploring the Malicious Use of Large\n  Language Models for Generating Cyber Attack Payloads",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  This research article critically examines the potential risks and\nimplications arising from the malicious utilization of large language\nmodels(LLM), focusing specifically on ChatGPT and Google's Bard. Although these\nlarge language models have numerous beneficial applications, the misuse of this\ntechnology by cybercriminals for creating offensive payloads and tools is a\nsignificant concern. In this study, we systematically generated implementable\ncode for the top-10 MITRE Techniques prevalent in 2022, utilizing ChatGPT, and\nconduct a comparative analysis of its performance with Google's Bard. Our\nexperimentation reveals that ChatGPT has the potential to enable attackers to\naccelerate the operation of more targeted and sophisticated attacks.\nAdditionally, the technology provides amateur attackers with more capabilities\nto perform a wide range of attacks and empowers script kiddies to develop\ncustomized tools that contribute to the acceleration of cybercrime.\nFurthermore, LLMs significantly benefits malware authors, particularly\nransomware gangs, in generating sophisticated variants of wiper and ransomware\nattacks with ease. On a positive note, our study also highlights how offensive\nsecurity researchers and pentesters can make use of LLMs to simulate realistic\nattack scenarios, identify potential vulnerabilities, and better protect\norganizations. Overall, we conclude by emphasizing the need for increased\nvigilance in mitigating the risks associated with LLMs. This includes\nimplementing robust security measures, increasing awareness and education\naround the potential risks of this technology, and collaborating with security\nexperts to stay ahead of emerging threats.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 24 May 2023 16:49:51 GMT"
      }
    ],
    "update_date": "2023-05-25",
    "authors_parsed": [
      [
        "Charan",
        "P. V. Sai",
        ""
      ],
      [
        "Chunduri",
        "Hrushikesh",
        ""
      ],
      [
        "Anand",
        "P. Mohan",
        ""
      ],
      [
        "Shukla",
        "Sandeep K",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.15336",
    "publish_date": "2023-05-24"
  },
  {
    "id": "2305.16353",
    "submitter": "Rui Liu",
    "authors": "Rui Liu, Jinhua Zhang, Guanglai Gao and Haizhou Li",
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via\n  Mono-to-Stereo Conversion",
    "comments": "To appear at InterSpeech2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Audio Deepfake Detection (ADD) aims to detect the fake audio generated by\ntext-to-speech (TTS), voice conversion (VC) and replay, etc., which is an\nemerging topic. Traditionally we take the mono signal as input and focus on\nrobust feature extraction and effective classifier design. However, the\ndual-channel stereo information in the audio signal also includes important\ncues for deepfake, which has not been studied in the prior work. In this paper,\nwe propose a novel ADD model, termed as M2S-ADD, that attempts to discover\naudio authenticity cues during the mono-to-stereo conversion process. We first\nprojects the mono to a stereo signal using a pretrained stereo synthesizer,\nthen employs a dual-branch neural architecture to process the left and right\nchannel signals, respectively. In this way, we effectively reveal the artifacts\nin the fake audio, thus improve the ADD performance. The experiments on the\nASVspoof2019 database show that M2S-ADD outperforms all baselines that input\nmono. We release the source code at \\url{https://github.com/AI-S2-Lab/M2S-ADD}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 25 May 2023 02:54:29 GMT"
      }
    ],
    "update_date": "2023-05-29",
    "authors_parsed": [
      [
        "Liu",
        "Rui",
        ""
      ],
      [
        "Zhang",
        "Jinhua",
        ""
      ],
      [
        "Gao",
        "Guanglai",
        ""
      ],
      [
        "Li",
        "Haizhou",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.16353",
    "publish_date": "2023-05-25"
  },
  {
    "id": "2305.16544",
    "submitter": "Nicholas Gabriel",
    "authors": "Nicholas A. Gabriel, David A. Broniatowski, Neil F. Johnson",
    "title": "Inductive detection of Influence Operations via Graph Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.SI physics.soc-ph",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Influence operations are large-scale efforts to manipulate public opinion.\nThe rapid detection and disruption of these operations is critical for healthy\npublic discourse. Emergent AI technologies may enable novel operations which\nevade current detection methods and influence public discourse on social media\nwith greater scale, reach, and specificity. New methods with inductive learning\ncapacity will be needed to identify these novel operations before they\nindelibly alter public opinion and events. We develop an inductive learning\nframework which: 1) determines content- and graph-based indicators that are not\nspecific to any operation; 2) uses graph learning to encode abstract signatures\nof coordinated manipulation; and 3) evaluates generalization capacity by\ntraining and testing models across operations originating from Russia, China,\nand Iran. We find that this framework enables strong cross-operation\ngeneralization while also revealing salient\nindicators$\\unicode{x2013}$illustrating a generic approach which directly\ncomplements transductive methodologies, thereby enhancing detection coverage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 26 May 2023 00:03:51 GMT"
      }
    ],
    "update_date": "2023-05-29",
    "authors_parsed": [
      [
        "Gabriel",
        "Nicholas A.",
        ""
      ],
      [
        "Broniatowski",
        "David A.",
        ""
      ],
      [
        "Johnson",
        "Neil F.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.16544",
    "publish_date": "2023-05-26"
  },
  {
    "id": "2305.16934",
    "submitter": "Tianyu Pang",
    "authors": "Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Chongxuan Li, Ngai-Man\n  Cheung, Min Lin",
    "title": "On Evaluating Adversarial Robustness of Large Vision-Language Models",
    "comments": "NeurIPS 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CL cs.CR cs.LG cs.MM",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large vision-language models (VLMs) such as GPT-4 have achieved unprecedented\nperformance in response generation, especially with visual inputs, enabling\nmore creative and adaptable interaction than large language models such as\nChatGPT. Nonetheless, multimodal generation exacerbates safety concerns, since\nadversaries may successfully evade the entire system by subtly manipulating the\nmost vulnerable modality (e.g., vision). To this end, we propose evaluating the\nrobustness of open-source large VLMs in the most realistic and high-risk\nsetting, where adversaries have only black-box system access and seek to\ndeceive the model into returning the targeted responses. In particular, we\nfirst craft targeted adversarial examples against pretrained models such as\nCLIP and BLIP, and then transfer these adversarial examples to other VLMs such\nas MiniGPT-4, LLaVA, UniDiffuser, BLIP-2, and Img2Prompt. In addition, we\nobserve that black-box queries on these VLMs can further improve the\neffectiveness of targeted evasion, resulting in a surprisingly high success\nrate for generating targeted responses. Our findings provide a quantitative\nunderstanding regarding the adversarial vulnerability of large VLMs and call\nfor a more thorough examination of their potential security flaws before\ndeployment in practice. Code is at https://github.com/yunqing-me/AttackVLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 26 May 2023 13:49:44 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 29 Oct 2023 12:32:19 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Zhao",
        "Yunqing",
        ""
      ],
      [
        "Pang",
        "Tianyu",
        ""
      ],
      [
        "Du",
        "Chao",
        ""
      ],
      [
        "Yang",
        "Xiao",
        ""
      ],
      [
        "Li",
        "Chongxuan",
        ""
      ],
      [
        "Cheung",
        "Ngai-Man",
        ""
      ],
      [
        "Lin",
        "Min",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.16934",
    "publish_date": "2023-10-29"
  },
  {
    "id": "2305.16960",
    "submitter": "Ruibo Liu",
    "authors": "Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M.\n  Dai, Diyi Yang, Soroush Vosoughi",
    "title": "Training Socially Aligned Language Models on Simulated Social\n  Interactions",
    "comments": "Code, data, and models can be downloaded via\n  https://github.com/agi-templar/Stable-Alignment",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CY cs.HC",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Social alignment in AI systems aims to ensure that these models behave\naccording to established societal values. However, unlike humans, who derive\nconsensus on value judgments through social interaction, current language\nmodels (LMs) are trained to rigidly replicate their training corpus in\nisolation, leading to subpar generalization in unfamiliar scenarios and\nvulnerability to adversarial attacks. This work presents a novel training\nparadigm that permits LMs to learn from simulated social interactions. In\ncomparison to existing methodologies, our approach is considerably more\nscalable and efficient, demonstrating superior performance in alignment\nbenchmarks and human evaluations. This paradigm shift in the training of LMs\nbrings us a step closer to developing AI systems that can robustly and\naccurately reflect societal norms and values.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 26 May 2023 14:17:36 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 16 Jul 2023 20:56:14 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 28 Oct 2023 09:02:39 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Liu",
        "Ruibo",
        ""
      ],
      [
        "Yang",
        "Ruixin",
        ""
      ],
      [
        "Jia",
        "Chenyan",
        ""
      ],
      [
        "Zhang",
        "Ge",
        ""
      ],
      [
        "Zhou",
        "Denny",
        ""
      ],
      [
        "Dai",
        "Andrew M.",
        ""
      ],
      [
        "Yang",
        "Diyi",
        ""
      ],
      [
        "Vosoughi",
        "Soroush",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.16960",
    "publish_date": "2023-07-16"
  },
  {
    "id": "2305.17174",
    "submitter": "Julia Mendelsohn",
    "authors": "Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap",
    "title": "From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language\n  Models",
    "comments": "ACL 2023, see https://dogwhistles.allen.ai/ for the glossary and\n  other materials",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Dogwhistles are coded expressions that simultaneously convey one meaning to a\nbroad audience and a second one, often hateful or provocative, to a narrow\nin-group; they are deployed to evade both political repercussions and\nalgorithmic content moderation. For example, in the sentence 'we need to end\nthe cosmopolitan experiment,' the word 'cosmopolitan' likely means 'worldly' to\nmany, but secretly means 'Jewish' to a select few. We present the first\nlarge-scale computational investigation of dogwhistles. We develop a typology\nof dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles\nwith rich contextual information and examples, and analyze their usage in\nhistorical U.S. politicians' speeches. We then assess whether a large language\nmodel (GPT-3) can identify dogwhistles and their meanings, and find that\nGPT-3's performance varies widely across types of dogwhistles and targeted\ngroups. Finally, we show that harmful content containing dogwhistles avoids\ntoxicity detection, highlighting online risks of such coded language. This work\nsheds light on the theoretical and applied importance of dogwhistles in both\nNLP and computational social science, and provides resources for future\nresearch in modeling dogwhistles and mitigating their online harms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 26 May 2023 18:00:57 GMT"
      }
    ],
    "update_date": "2023-05-30",
    "authors_parsed": [
      [
        "Mendelsohn",
        "Julia",
        ""
      ],
      [
        "Bras",
        "Ronan Le",
        ""
      ],
      [
        "Choi",
        "Yejin",
        ""
      ],
      [
        "Sap",
        "Maarten",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.17174",
    "publish_date": "2023-05-26"
  },
  {
    "id": "2305.19713",
    "submitter": "Zhouxing Shi",
    "authors": "Zhouxing Shi, Yihan Wang, Fan Yin, Xiangning Chen, Kai-Wei Chang,\n  Cho-Jui Hsieh",
    "title": "Red Teaming Language Model Detectors with Language Models",
    "comments": "Preprint. Accepted by TACL",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The prevalence and strong capability of large language models (LLMs) present\nsignificant safety and ethical risks if exploited by malicious users. To\nprevent the potentially deceptive usage of LLMs, recent works have proposed\nalgorithms to detect LLM-generated text and protect LLMs. In this paper, we\ninvestigate the robustness and reliability of these LLM detectors under\nadversarial attacks. We study two types of attack strategies: 1) replacing\ncertain words in an LLM's output with their synonyms given the context; 2)\nautomatically searching for an instructional prompt to alter the writing style\nof the generation. In both strategies, we leverage an auxiliary LLM to generate\nthe word replacements or the instructional prompt. Different from previous\nworks, we consider a challenging setting where the auxiliary LLM can also be\nprotected by a detector. Experiments reveal that our attacks effectively\ncompromise the performance of all detectors in the study with plausible\ngenerations, underscoring the urgent need to improve the robustness of\nLLM-generated text detection systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 31 May 2023 10:08:37 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 19 Oct 2023 05:56:52 GMT"
      }
    ],
    "update_date": "2023-10-20",
    "authors_parsed": [
      [
        "Shi",
        "Zhouxing",
        ""
      ],
      [
        "Wang",
        "Yihan",
        ""
      ],
      [
        "Yin",
        "Fan",
        ""
      ],
      [
        "Chen",
        "Xiangning",
        ""
      ],
      [
        "Chang",
        "Kai-Wei",
        ""
      ],
      [
        "Hsieh",
        "Cho-Jui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2305.19713",
    "publish_date": "2023-05-31"
  },
  {
    "id": "2306.00816",
    "submitter": "Ruotong Wang",
    "authors": "Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan,\n  Baoyuan Wu",
    "title": "Robust Backdoor Attack with Visible, Semantic, Sample-Specific, and\n  Compatible Triggers",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep neural networks (DNNs) can be manipulated to exhibit specific behaviors\nwhen exposed to specific trigger patterns, without affecting their performance\non benign samples, dubbed backdoor attack. Some recent research has focused on\ndesigning invisible triggers for backdoor attacks to ensure visual\nstealthiness, while showing high effectiveness, even under backdoor defense.\nHowever, we find that these carefully designed invisible triggers are often\nsensitive to visual distortion during inference, such as Gaussian blurring or\nenvironmental variations in physical scenarios. This phenomenon could\nsignificantly undermine the practical effectiveness of attacks, but has been\nrarely paid attention to and thoroughly investigated. To address this\nlimitation, we define a novel trigger called the Visible, Semantic,\nSample-Specific, and Compatible trigger (VSSC trigger), to achieve effective,\nstealthy and robust to visual distortion simultaneously. To implement it, we\ndevelop an innovative approach by utilizing the powerful capabilities of large\nlanguage models for choosing the suitable trigger and text-guided image editing\ntechniques for generating the poisoned image with the trigger. Extensive\nexperimental results and analysis validate the effectiveness, stealthiness and\nrobustness of the VSSC trigger. It demonstrates superior robustness to\ndistortions compared with most digital backdoor attacks and allows more\nefficient and flexible trigger integration compared to physical backdoor\nattacks. We hope that the proposed VSSC trigger and implementation approach\ncould inspire future studies on designing more practical triggers in backdoor\nattacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 1 Jun 2023 15:42:06 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 8 Oct 2023 08:16:14 GMT"
      }
    ],
    "update_date": "2023-10-12",
    "authors_parsed": [
      [
        "Wang",
        "Ruotong",
        ""
      ],
      [
        "Chen",
        "Hongrui",
        ""
      ],
      [
        "Zhu",
        "Zihao",
        ""
      ],
      [
        "Liu",
        "Li",
        ""
      ],
      [
        "Zhang",
        "Yong",
        ""
      ],
      [
        "Fan",
        "Yanbo",
        ""
      ],
      [
        "Wu",
        "Baoyuan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.00816",
    "publish_date": "2023-06-01"
  },
  {
    "id": "2306.01272",
    "submitter": "Hossein Aboutalebi",
    "authors": "Hossein Aboutalebi, Dayou Mao, Carol Xu, Alexander Wong",
    "title": "DeepfakeArt Challenge: A Benchmark Dataset for Generative AI Art Forgery\n  and Data Poisoning Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The tremendous recent advances in generative artificial intelligence\ntechniques have led to significant successes and promise in a wide range of\ndifferent applications ranging from conversational agents and textual content\ngeneration to voice and visual synthesis. Amid the rise in generative AI and\nits increasing widespread adoption, there has been significant growing concern\nover the use of generative AI for malicious purposes. In the realm of visual\ncontent synthesis using generative AI, key areas of significant concern has\nbeen image forgery (e.g., generation of images containing or derived from\ncopyright content), and data poisoning (i.e., generation of adversarially\ncontaminated images). Motivated to address these key concerns to encourage\nresponsible generative AI, we introduce the DeepfakeArt Challenge, a\nlarge-scale challenge benchmark dataset designed specifically to aid in the\nbuilding of machine learning algorithms for generative AI art forgery and data\npoisoning detection. Comprising of over 32,000 records across a variety of\ngenerative forgery and data poisoning techniques, each entry consists of a pair\nof images that are either forgeries / adversarially contaminated or not. Each\nof the generated images in the DeepfakeArt Challenge benchmark dataset has been\nquality checked in a comprehensive manner. The DeepfakeArt Challenge is a core\npart of GenAI4Good, a global open source initiative for accelerating machine\nlearning for promoting responsible creation and deployment of generative AI for\ngood.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 2 Jun 2023 05:11:27 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 11 Jun 2023 03:08:24 GMT"
      }
    ],
    "update_date": "2023-06-13",
    "authors_parsed": [
      [
        "Aboutalebi",
        "Hossein",
        ""
      ],
      [
        "Mao",
        "Dayou",
        ""
      ],
      [
        "Xu",
        "Carol",
        ""
      ],
      [
        "Wong",
        "Alexander",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.01272",
    "publish_date": "2023-06-11"
  },
  {
    "id": "2306.01902",
    "submitter": "Zhengyue Zhao",
    "authors": "Zhengyue Zhao, Jinhao Duan, Xing Hu, Kaidi Xu, Chenan Wang, Rui Zhang,\n  Zidong Du, Qi Guo, Yunji Chen",
    "title": "Unlearnable Examples for Diffusion Models: Protect Data from\n  Unauthorized Exploitation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Diffusion models have demonstrated remarkable performance in image generation\ntasks, paving the way for powerful AIGC applications. However, these\nwidely-used generative models can also raise security and privacy concerns,\nsuch as copyright infringement, and sensitive data leakage. To tackle these\nissues, we propose a method, Unlearnable Diffusion Perturbation, to safeguard\nimages from unauthorized exploitation. Our approach involves designing an\nalgorithm to generate sample-wise perturbation noise for each image to be\nprotected. This imperceptible protective noise makes the data almost\nunlearnable for diffusion models, i.e., diffusion models trained or fine-tuned\non the protected data cannot generate high-quality and diverse images related\nto the protected training data. Theoretically, we frame this as a max-min\noptimization problem and introduce EUDP, a noise scheduler-based method to\nenhance the effectiveness of the protective noise. We evaluate our methods on\nboth Denoising Diffusion Probabilistic Model and Latent Diffusion Models,\ndemonstrating that training diffusion models on the protected data lead to a\nsignificant reduction in the quality of the generated images. Especially, the\nexperimental results on Stable Diffusion demonstrate that our method\neffectively safeguards images from being used to train Diffusion Models in\nvarious tasks, such as training specific objects and styles. This achievement\nholds significant importance in real-world scenarios, as it contributes to the\nprotection of privacy and copyright against AI-generated content.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 2 Jun 2023 20:19:19 GMT"
      }
    ],
    "update_date": "2023-06-06",
    "authors_parsed": [
      [
        "Zhao",
        "Zhengyue",
        ""
      ],
      [
        "Duan",
        "Jinhao",
        ""
      ],
      [
        "Hu",
        "Xing",
        ""
      ],
      [
        "Xu",
        "Kaidi",
        ""
      ],
      [
        "Wang",
        "Chenan",
        ""
      ],
      [
        "Zhang",
        "Rui",
        ""
      ],
      [
        "Du",
        "Zidong",
        ""
      ],
      [
        "Guo",
        "Qi",
        ""
      ],
      [
        "Chen",
        "Yunji",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.01902",
    "publish_date": "2023-06-02"
  },
  {
    "id": "2306.02384",
    "submitter": "Hongyang Du",
    "authors": "Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Kwok-Yan Lam,\n  Yuguang Fang, and Yonghui Li",
    "title": "Spear or Shield: Leveraging Generative AI to Tackle Security Threats of\n  Intelligent Network Services",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Generative AI (GAI) models have been rapidly advancing, with a wide range of\napplications including intelligent networks and mobile AI-generated content\n(AIGC) services. Despite their numerous applications and potential, such models\ncreate opportunities for novel security challenges. In this paper, we examine\nthe challenges and opportunities of GAI in the realm of the security of\nintelligent network AIGC services such as suggesting security policies, acting\nas both a ``spear'' for potential attacks and a ``shield'' as an integral part\nof various defense mechanisms. First, we present a comprehensive overview of\nthe GAI landscape, highlighting its applications and the techniques\nunderpinning these advancements, especially large language and diffusion\nmodels. Then, we investigate the dynamic interplay between GAI's spear and\nshield roles, highlighting two primary categories of potential GAI-related\nattacks and their respective defense strategies within wireless networks. A\ncase study illustrates the impact of GAI defense strategies on energy\nconsumption in an image request scenario under data poisoning attack. Our\nresults show that by employing an AI-optimized diffusion defense mechanism,\nenergy can be reduced by 8.7%, and retransmission count can be decreased from\n32 images, without defense, to just 6 images, showcasing the effectiveness of\nGAI in enhancing network security.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 4 Jun 2023 15:38:38 GMT"
      }
    ],
    "update_date": "2023-06-06",
    "authors_parsed": [
      [
        "Du",
        "Hongyang",
        ""
      ],
      [
        "Niyato",
        "Dusit",
        ""
      ],
      [
        "Kang",
        "Jiawen",
        ""
      ],
      [
        "Xiong",
        "Zehui",
        ""
      ],
      [
        "Lam",
        "Kwok-Yan",
        ""
      ],
      [
        "Fang",
        "Yuguang",
        ""
      ],
      [
        "Li",
        "Yonghui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.02384",
    "publish_date": "2023-06-04"
  },
  {
    "id": "2306.03024",
    "submitter": "Ilias Chalkidis",
    "authors": "Laura Cabello, Jiaang Li, Ilias Chalkidis",
    "title": "PokemonChat: Auditing ChatGPT for Pok\\'emon Universe Knowledge",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The recently released ChatGPT model demonstrates unprecedented capabilities\nin zero-shot question-answering. In this work, we probe ChatGPT for its\nconversational understanding and introduce a conversational framework\n(protocol) that can be adopted in future studies. The Pok\\'emon universe serves\nas an ideal testing ground for auditing ChatGPT's reasoning capabilities due to\nits closed world assumption. After bringing ChatGPT's background knowledge (on\nthe Pok\\'emon universe) to light, we test its reasoning process when using\nthese concepts in battle scenarios. We then evaluate its ability to acquire new\nknowledge and include it in its reasoning process. Our ultimate goal is to\nassess ChatGPT's ability to generalize, combine features, and to acquire and\nreason over newly introduced knowledge from human feedback. We find that\nChatGPT has prior knowledge of the Pokemon universe, which can reason upon in\nbattle scenarios to a great extent, even when new information is introduced.\nThe model performs better with collaborative feedback and if there is an\ninitial phase of information retrieval, but also hallucinates occasionally and\nis susceptible to adversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 5 Jun 2023 16:44:27 GMT"
      }
    ],
    "update_date": "2023-06-06",
    "authors_parsed": [
      [
        "Cabello",
        "Laura",
        ""
      ],
      [
        "Li",
        "Jiaang",
        ""
      ],
      [
        "Chalkidis",
        "Ilias",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.03024",
    "publish_date": "2023-06-05"
  },
  {
    "id": "2306.04959",
    "submitter": "Shanshan Han",
    "authors": "Shanshan Han, Baturalp Buyukates, Zijian Hu, Han Jin, Weizhao Jin,\n  Lichao Sun, Xiaoyang Wang, Wenxuan Wu, Chulin Xie, Yuhang Yao, Kai Zhang,\n  Qifan Zhang, Yuhui Zhang, Salman Avestimehr and Chaoyang He",
    "title": "FedMLSecurity: A Benchmark for Attacks and Defenses in Federated\n  Learning and Federated LLMs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  This paper introduces FedMLSecurity, a benchmark designed to simulate\nadversarial attacks and corresponding defense mechanisms in Federated Learning\n(FL). As an integral module of the open-sourced library FedML that facilitates\nFL algorithm development and performance comparison, FedMLSecurity enhances\nFedML's capabilities to evaluate security issues and potential remedies in FL.\nFedMLSecurity comprises two major components: FedMLAttacker that simulates\nattacks injected during FL training, and FedMLDefender that simulates defensive\nmechanisms to mitigate the impacts of the attacks. FedMLSecurity is\nopen-sourced and can be customized to a wide range of machine learning models\n(e.g., Logistic Regression, ResNet, GAN, etc.) and federated optimizers (e.g.,\nFedAVG, FedOPT, FedNOVA, etc.). FedMLSecurity can also be applied to Large\nLanguage Models (LLMs) easily, demonstrating its adaptability and applicability\nin various scenarios.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Jun 2023 06:21:35 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 25 Sep 2023 07:17:27 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 6 Oct 2023 07:58:13 GMT"
      }
    ],
    "update_date": "2023-10-09",
    "authors_parsed": [
      [
        "Han",
        "Shanshan",
        ""
      ],
      [
        "Buyukates",
        "Baturalp",
        ""
      ],
      [
        "Hu",
        "Zijian",
        ""
      ],
      [
        "Jin",
        "Han",
        ""
      ],
      [
        "Jin",
        "Weizhao",
        ""
      ],
      [
        "Sun",
        "Lichao",
        ""
      ],
      [
        "Wang",
        "Xiaoyang",
        ""
      ],
      [
        "Wu",
        "Wenxuan",
        ""
      ],
      [
        "Xie",
        "Chulin",
        ""
      ],
      [
        "Yao",
        "Yuhang",
        ""
      ],
      [
        "Zhang",
        "Kai",
        ""
      ],
      [
        "Zhang",
        "Qifan",
        ""
      ],
      [
        "Zhang",
        "Yuhui",
        ""
      ],
      [
        "Avestimehr",
        "Salman",
        ""
      ],
      [
        "He",
        "Chaoyang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.04959",
    "publish_date": "2023-06-08"
  },
  {
    "id": "2306.05087",
    "submitter": "Yidong Wang",
    "authors": "Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao\n  Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang,\n  Yue Zhang",
    "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning\n  Optimization",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Jun 2023 10:41:56 GMT"
      }
    ],
    "update_date": "2023-06-09",
    "authors_parsed": [
      [
        "Wang",
        "Yidong",
        ""
      ],
      [
        "Yu",
        "Zhuohao",
        ""
      ],
      [
        "Zeng",
        "Zhengran",
        ""
      ],
      [
        "Yang",
        "Linyi",
        ""
      ],
      [
        "Wang",
        "Cunxiang",
        ""
      ],
      [
        "Chen",
        "Hao",
        ""
      ],
      [
        "Jiang",
        "Chaoya",
        ""
      ],
      [
        "Xie",
        "Rui",
        ""
      ],
      [
        "Wang",
        "Jindong",
        ""
      ],
      [
        "Xie",
        "Xing",
        ""
      ],
      [
        "Ye",
        "Wei",
        ""
      ],
      [
        "Zhang",
        "Shikun",
        ""
      ],
      [
        "Zhang",
        "Yue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.05087",
    "publish_date": "2023-06-08"
  },
  {
    "id": "2306.05225",
    "submitter": "Zhijin Ge",
    "authors": "Zhijin Ge, Hongying Liu, Xiaosen Wang, Fanhua Shang, Yuanyuan Liu",
    "title": "Boosting Adversarial Transferability by Achieving Flat Local Maxima",
    "comments": "Accepted by the Neural Information Processing Systems (NeurIPS 2023)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Transfer-based attack adopts the adversarial examples generated on the\nsurrogate model to attack various models, making it applicable in the physical\nworld and attracting increasing interest. Recently, various adversarial attacks\nhave emerged to boost adversarial transferability from different perspectives.\nIn this work, inspired by the observation that flat local minima are correlated\nwith good generalization, we assume and empirically validate that adversarial\nexamples at a flat local region tend to have good transferability by\nintroducing a penalized gradient norm to the original loss function. Since\ndirectly optimizing the gradient regularization norm is computationally\nexpensive and intractable for generating adversarial examples, we propose an\napproximation optimization method to simplify the gradient update of the\nobjective function. Specifically, we randomly sample an example and adopt a\nfirst-order procedure to approximate the curvature of Hessian/vector product,\nwhich makes computing more efficient by interpolating two neighboring\ngradients. Meanwhile, in order to obtain a more stable gradient direction, we\nrandomly sample multiple examples and average the gradients of these examples\nto reduce the variance due to random sampling during the iterative process.\nExtensive experimental results on the ImageNet-compatible dataset show that the\nproposed method can generate adversarial examples at flat local regions, and\nsignificantly improve the adversarial transferability on either normally\ntrained models or adversarially trained models than the state-of-the-art\nattacks. Our codes are available at:\nhttps://github.com/Trustworthy-AI-Group/PGN.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Jun 2023 14:21:02 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 2 Nov 2023 07:52:17 GMT"
      }
    ],
    "update_date": "2023-11-03",
    "authors_parsed": [
      [
        "Ge",
        "Zhijin",
        ""
      ],
      [
        "Liu",
        "Hongying",
        ""
      ],
      [
        "Wang",
        "Xiaosen",
        ""
      ],
      [
        "Shang",
        "Fanhua",
        ""
      ],
      [
        "Liu",
        "Yuanyuan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.05225",
    "publish_date": "2023-06-08"
  },
  {
    "id": "2306.05499",
    "submitter": "Yi Liu",
    "authors": "Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang\n  Liu, Haoyu Wang, Yan Zheng and Yang Liu",
    "title": "Prompt Injection attack against LLM-integrated Applications",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 8 Jun 2023 18:43:11 GMT"
      }
    ],
    "update_date": "2023-06-12",
    "authors_parsed": [
      [
        "Liu",
        "Yi",
        ""
      ],
      [
        "Deng",
        "Gelei",
        ""
      ],
      [
        "Li",
        "Yuekang",
        ""
      ],
      [
        "Wang",
        "Kailong",
        ""
      ],
      [
        "Zhang",
        "Tianwei",
        ""
      ],
      [
        "Liu",
        "Yepang",
        ""
      ],
      [
        "Wang",
        "Haoyu",
        ""
      ],
      [
        "Zheng",
        "Yan",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.05499",
    "publish_date": "2023-06-08"
  },
  {
    "id": "2306.05985",
    "submitter": "Luka Dragar",
    "authors": "Luka Dragar, Peter Peer, Vitomir \\v{S}truc, Borut Batagelj",
    "title": "Beyond Detection: Visual Realism Assessment of Deepfakes",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In the era of rapid digitalization and artificial intelligence advancements,\nthe development of DeepFake technology has posed significant security and\nprivacy concerns. This paper presents an effective measure to assess the visual\nrealism of DeepFake videos. We utilize an ensemble of two Convolutional Neural\nNetwork (CNN) models: Eva and ConvNext. These models have been trained on the\nDeepFake Game Competition (DFGC) 2022 dataset and aim to predict Mean Opinion\nScores (MOS) from DeepFake videos based on features extracted from sequences of\nframes. Our method secured the third place in the recent DFGC on Visual Realism\nAssessment held in conjunction with the 2023 International Joint Conference on\nBiometrics (IJCB 2023). We provide an over\\-view of the models, data\npreprocessing, and training procedures. We also report the performance of our\nmodels against the competition's baseline model and discuss the implications of\nour findings.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 9 Jun 2023 15:53:01 GMT"
      }
    ],
    "update_date": "2023-06-12",
    "authors_parsed": [
      [
        "Dragar",
        "Luka",
        ""
      ],
      [
        "Peer",
        "Peter",
        ""
      ],
      [
        "\u0160truc",
        "Vitomir",
        ""
      ],
      [
        "Batagelj",
        "Borut",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.05985",
    "publish_date": "2023-06-09"
  },
  {
    "id": "2306.06123",
    "submitter": "Hubert Baniecki",
    "authors": "Hubert Baniecki and Przemyslaw Biecek",
    "title": "Adversarial Attacks and Defenses in Explainable Artificial Intelligence:\n  A Survey",
    "comments": "A shorter version of this paper was presented at the IJCAI 2023\n  Workshop on Explainable AI",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Explainable artificial intelligence (XAI) methods are portrayed as a remedy\nfor debugging and trusting statistical and deep learning models, as well as\ninterpreting their predictions. However, recent advances in adversarial machine\nlearning (AdvML) highlight the limitations and vulnerabilities of\nstate-of-the-art explanation methods, putting their security and\ntrustworthiness into question. The possibility of manipulating, fooling or\nfairwashing evidence of the model's reasoning has detrimental consequences when\napplied in high-stakes decision-making and knowledge discovery. This survey\nprovides a comprehensive overview of research concerning adversarial attacks on\nexplanations of machine learning models, as well as fairness metrics. We\nintroduce a unified notation and taxonomy of methods facilitating a common\nground for researchers and practitioners from the intersecting research fields\nof AdvML and XAI. We discuss how to defend against attacks and design robust\ninterpretation methods. We contribute a list of existing insecurities in XAI\nand outline the emerging research directions in adversarial XAI (AdvXAI).\nFuture work should address improving explanation methods and evaluation\nprotocols to take into account the reported safety issues.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 6 Jun 2023 09:53:39 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 25 Sep 2023 13:14:04 GMT"
      }
    ],
    "update_date": "2023-09-26",
    "authors_parsed": [
      [
        "Baniecki",
        "Hubert",
        ""
      ],
      [
        "Biecek",
        "Przemyslaw",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.06123",
    "publish_date": "2023-06-06"
  },
  {
    "id": "2306.06815",
    "submitter": "Jiaqi Xue",
    "authors": "Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau\n  Boloni and Qian Lou",
    "title": "TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models",
    "comments": "Accepted by NeurIPS'23",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) are progressively being utilized as machine\nlearning services and interface tools for various applications. However, the\nsecurity implications of LLMs, particularly in relation to adversarial and\nTrojan attacks, remain insufficiently examined. In this paper, we propose\nTrojLLM, an automatic and black-box framework to effectively generate universal\nand stealthy triggers. When these triggers are incorporated into the input\ndata, the LLMs' outputs can be maliciously manipulated. Moreover, the framework\nalso supports embedding Trojans within discrete prompts, enhancing the overall\neffectiveness and precision of the triggers' attacks. Specifically, we propose\na trigger discovery algorithm for generating universal triggers for various\ninputs by querying victim LLM-based APIs using few-shot data samples.\nFurthermore, we introduce a novel progressive Trojan poisoning algorithm\ndesigned to generate poisoned prompts that retain efficacy and transferability\nacross a diverse range of models. Our experiments and results demonstrate\nTrojLLM's capacity to effectively insert Trojans into text prompts in\nreal-world black-box LLM APIs including GPT-3.5 and GPT-4, while maintaining\nexceptional performance on clean test sets. Our work sheds light on the\npotential security risks in current models and offers a potential defensive\napproach. The source code of TrojLLM is available at\nhttps://github.com/UCF-ML-Research/TrojLLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 12 Jun 2023 01:22:39 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 23 Oct 2023 03:43:14 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 31 Oct 2023 00:33:53 GMT"
      }
    ],
    "update_date": "2023-11-01",
    "authors_parsed": [
      [
        "Xue",
        "Jiaqi",
        ""
      ],
      [
        "Zheng",
        "Mengxin",
        ""
      ],
      [
        "Hua",
        "Ting",
        ""
      ],
      [
        "Shen",
        "Yilin",
        ""
      ],
      [
        "Liu",
        "Yepeng",
        ""
      ],
      [
        "Boloni",
        "Ladislau",
        ""
      ],
      [
        "Lou",
        "Qian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.06815",
    "publish_date": "2023-10-31"
  },
  {
    "id": "2306.06874",
    "submitter": "Sheng-Yen Chou",
    "authors": "Sheng-Yen Chou, Pin-Yu Chen, Tsung-Yi Ho",
    "title": "VillanDiffusion: A Unified Backdoor Attack Framework for Diffusion\n  Models",
    "comments": "Accepted by NeurIPS 2023, NeurIPS 2023 BUGS Workshop Oral",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Diffusion Models (DMs) are state-of-the-art generative models that learn a\nreversible corruption process from iterative noise addition and denoising. They\nare the backbone of many generative AI applications, such as text-to-image\nconditional generation. However, recent studies have shown that basic\nunconditional DMs (e.g., DDPM and DDIM) are vulnerable to backdoor injection, a\ntype of output manipulation attack triggered by a maliciously embedded pattern\nat model input. This paper presents a unified backdoor attack framework\n(VillanDiffusion) to expand the current scope of backdoor analysis for DMs. Our\nframework covers mainstream unconditional and conditional DMs (denoising-based\nand score-based) and various training-free samplers for holistic evaluations.\nExperiments show that our unified framework facilitates the backdoor analysis\nof different DM configurations and provides new insights into caption-based\nbackdoor attacks on DMs. Our code is available on GitHub:\n\\url{https://github.com/IBM/villandiffusion}\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 12 Jun 2023 05:14:13 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 13 Jun 2023 03:30:21 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 4 Nov 2023 08:33:02 GMT"
      },
      {
        "version": "v4",
        "created": "Sun, 12 Nov 2023 09:52:09 GMT"
      },
      {
        "version": "v5",
        "created": "Fri, 29 Dec 2023 10:44:40 GMT"
      }
    ],
    "update_date": "2024-01-01",
    "authors_parsed": [
      [
        "Chou",
        "Sheng-Yen",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Ho",
        "Tsung-Yi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.06874",
    "publish_date": "2023-06-13"
  },
  {
    "id": "2306.08223",
    "submitter": "Zhigang Kan",
    "authors": "Zhigang Kan, Linbo Qiao, Hao Yu, Liwen Peng, Yifu Gao, Dongsheng Li",
    "title": "Protecting User Privacy in Remote Conversational Systems: A\n  Privacy-Preserving framework based on text sanitization",
    "comments": "9 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.HC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) are gaining increasing attention due to their\nexceptional performance across numerous tasks. As a result, the general public\nutilize them as an influential tool for boosting their productivity while\nnatural language processing researchers endeavor to employ them in solving\nexisting or new research problems. Unfortunately, individuals can only access\nsuch powerful AIs through APIs, which ultimately leads to the transmission of\nraw data to the models' providers and increases the possibility of privacy data\nleakage. Current privacy-preserving methods for cloud-deployed language models\naim to protect privacy information in the pre-training dataset or during the\nmodel training phase. However, they do not meet the specific challenges\npresented by the remote access approach of new large-scale language models.\n  This paper introduces a novel task, \"User Privacy Protection for Dialogue\nModels,\" which aims to safeguard sensitive user information from any possible\ndisclosure while conversing with chatbots. We also present an evaluation scheme\nfor this task, which covers evaluation metrics for privacy protection, data\navailability, and resistance to simulation attacks. Moreover, we propose the\nfirst framework for this task, namely privacy protection through text\nsanitization. Before sending the input to remote large models, it filters out\nthe sensitive information, using several rounds of text sanitization based on\nprivacy types that users define. Upon receiving responses from the larger\nmodel, our framework automatically restores privacy to ensure that the\nconversation goes smoothly, without intervention from the privacy filter.\nExperiments based on real-world datasets demonstrate the efficacy of our\nprivacy-preserving approach against eavesdropping from potential attackers.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 14 Jun 2023 03:28:51 GMT"
      }
    ],
    "update_date": "2023-06-16",
    "authors_parsed": [
      [
        "Kan",
        "Zhigang",
        ""
      ],
      [
        "Qiao",
        "Linbo",
        ""
      ],
      [
        "Yu",
        "Hao",
        ""
      ],
      [
        "Peng",
        "Liwen",
        ""
      ],
      [
        "Gao",
        "Yifu",
        ""
      ],
      [
        "Li",
        "Dongsheng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.08223",
    "publish_date": "2023-06-14"
  },
  {
    "id": "2306.08833",
    "submitter": "Chaofan Wang",
    "authors": "Chaofan Wang, Samuel Kernan Freire, Mo Zhang, Jing Wei, Jorge\n  Goncalves, Vassilis Kostakos, Zhanna Sarsenbayeva, Christina Schneegass,\n  Alessandro Bozzon, Evangelos Niforatos",
    "title": "Safeguarding Crowdsourcing Surveys from ChatGPT with Prompt Injection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.HC",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  ChatGPT and other large language models (LLMs) have proven useful in\ncrowdsourcing tasks, where they can effectively annotate machine learning\ntraining data. However, this means that they also have the potential for\nmisuse, specifically to automatically answer surveys. LLMs can potentially\ncircumvent quality assurance measures, thereby threatening the integrity of\nmethodologies that rely on crowdsourcing surveys. In this paper, we propose a\nmechanism to detect LLM-generated responses to surveys. The mechanism uses\n\"prompt injection\", such as directions that can mislead LLMs into giving\npredictable responses. We evaluate our technique against a range of question\nscenarios, types, and positions, and find that it can reliably detect\nLLM-generated responses with more than 93% effectiveness. We also provide an\nopen-source software to help survey designers use our technique to detect LLM\nresponses. Our work is a step in ensuring that survey methodologies remain\nrigorous vis-a-vis LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 15 Jun 2023 03:30:13 GMT"
      }
    ],
    "update_date": "2023-06-16",
    "authors_parsed": [
      [
        "Wang",
        "Chaofan",
        ""
      ],
      [
        "Freire",
        "Samuel Kernan",
        ""
      ],
      [
        "Zhang",
        "Mo",
        ""
      ],
      [
        "Wei",
        "Jing",
        ""
      ],
      [
        "Goncalves",
        "Jorge",
        ""
      ],
      [
        "Kostakos",
        "Vassilis",
        ""
      ],
      [
        "Sarsenbayeva",
        "Zhanna",
        ""
      ],
      [
        "Schneegass",
        "Christina",
        ""
      ],
      [
        "Bozzon",
        "Alessandro",
        ""
      ],
      [
        "Niforatos",
        "Evangelos",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.08833",
    "publish_date": "2023-06-15"
  },
  {
    "id": "2306.09752",
    "submitter": "Victor Steinborn",
    "authors": "Victor Steinborn and Antonis Maronikolakis and Hinrich Sch\\\"utze",
    "title": "Politeness Stereotypes and Attack Vectors: Gender Stereotypes in\n  Japanese and Korean Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CY cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In efforts to keep up with the rapid progress and use of large language\nmodels, gender bias research is becoming more prevalent in NLP. Non-English\nbias research, however, is still in its infancy with most work focusing on\nEnglish. In our work, we study how grammatical gender bias relating to\npoliteness levels manifests in Japanese and Korean language models. Linguistic\nstudies in these languages have identified a connection between gender bias and\npoliteness levels, however it is not yet known if language models reproduce\nthese biases. We analyze relative prediction probabilities of the male and\nfemale grammatical genders using templates and find that informal polite speech\nis most indicative of the female grammatical gender, while rude and formal\nspeech is most indicative of the male grammatical gender. Further, we find\npoliteness levels to be an attack vector for allocational gender bias in\ncyberbullying detection models. Cyberbullies can evade detection through simple\ntechniques abusing politeness levels. We introduce an attack dataset to (i)\nidentify representational gender bias across politeness levels, (ii)\ndemonstrate how gender biases can be abused to bypass cyberbullying detection\nmodels and (iii) show that allocational biases can be mitigated via training on\nour proposed dataset. Through our findings we highlight the importance of bias\nresearch moving beyond its current English-centrism.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 16 Jun 2023 10:36:18 GMT"
      }
    ],
    "update_date": "2023-06-19",
    "authors_parsed": [
      [
        "Steinborn",
        "Victor",
        ""
      ],
      [
        "Maronikolakis",
        "Antonis",
        ""
      ],
      [
        "Sch\u00fctze",
        "Hinrich",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.09752",
    "publish_date": "2023-06-16"
  },
  {
    "id": "2306.12685",
    "submitter": "Xiaosen Wang",
    "authors": "Xiaosen Wang, Kangheng Tong, Kun He",
    "title": "Rethinking the Backward Propagation for Adversarial Transferability",
    "comments": "Accepted by NeurIPS 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Transfer-based attacks generate adversarial examples on the surrogate model,\nwhich can mislead other black-box models without access, making it promising to\nattack real-world applications. Recently, several works have been proposed to\nboost adversarial transferability, in which the surrogate model is usually\noverlooked. In this work, we identify that non-linear layers (e.g., ReLU,\nmax-pooling, etc.) truncate the gradient during backward propagation, making\nthe gradient w.r.t. input image imprecise to the loss function. We hypothesize\nand empirically validate that such truncation undermines the transferability of\nadversarial examples. Based on these findings, we propose a novel method called\nBackward Propagation Attack (BPA) to increase the relevance between the\ngradient w.r.t. input image and loss function so as to generate adversarial\nexamples with higher transferability. Specifically, BPA adopts a non-monotonic\nfunction as the derivative of ReLU and incorporates softmax with temperature to\nsmooth the derivative of max-pooling, thereby mitigating the information loss\nduring the backward propagation of gradients. Empirical results on the ImageNet\ndataset demonstrate that not only does our method substantially boost the\nadversarial transferability, but it is also general to existing transfer-based\nattacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 22 Jun 2023 06:12:23 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 20 Nov 2023 08:46:28 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 21 Nov 2023 02:57:09 GMT"
      }
    ],
    "update_date": "2023-11-22",
    "authors_parsed": [
      [
        "Wang",
        "Xiaosen",
        ""
      ],
      [
        "Tong",
        "Kangheng",
        ""
      ],
      [
        "He",
        "Kun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.12685",
    "publish_date": "2023-11-21"
  },
  {
    "id": "2306.13033",
    "submitter": "Subash Neupane",
    "authors": "Subash Neupane, Ivan A. Fernandez, Sudip Mittal, Shahram Rahimi",
    "title": "Impacts and Risk of Generative AI Technology on Cyber Defense",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Generative Artificial Intelligence (GenAI) has emerged as a powerful\ntechnology capable of autonomously producing highly realistic content in\nvarious domains, such as text, images, audio, and videos. With its potential\nfor positive applications in creative arts, content generation, virtual\nassistants, and data synthesis, GenAI has garnered significant attention and\nadoption. However, the increasing adoption of GenAI raises concerns about its\npotential misuse for crafting convincing phishing emails, generating\ndisinformation through deepfake videos, and spreading misinformation via\nauthentic-looking social media posts, posing a new set of challenges and risks\nin the realm of cybersecurity. To combat the threats posed by GenAI, we propose\nleveraging the Cyber Kill Chain (CKC) to understand the lifecycle of\ncyberattacks, as a foundational model for cyber defense. This paper aims to\nprovide a comprehensive analysis of the risk areas introduced by the offensive\nuse of GenAI techniques in each phase of the CKC framework. We also analyze the\nstrategies employed by threat actors and examine their utilization throughout\ndifferent phases of the CKC, highlighting the implications for cyber defense.\nAdditionally, we propose GenAI-enabled defense strategies that are both\nattack-aware and adaptive. These strategies encompass various techniques such\nas detection, deception, and adversarial training, among others, aiming to\neffectively mitigate the risks posed by GenAI-induced cyber threats.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 22 Jun 2023 16:51:41 GMT"
      }
    ],
    "update_date": "2023-06-23",
    "authors_parsed": [
      [
        "Neupane",
        "Subash",
        ""
      ],
      [
        "Fernandez",
        "Ivan A.",
        ""
      ],
      [
        "Mittal",
        "Sudip",
        ""
      ],
      [
        "Rahimi",
        "Shahram",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.13033",
    "publish_date": "2023-06-22"
  },
  {
    "id": "2306.13157",
    "submitter": "Dustin Dannenhauer",
    "authors": "Adam Amos-Binks, Dustin Dannenhauer, Leilani H. Gilpin",
    "title": "Anticipatory Thinking Challenges in Open Worlds: Risk Management",
    "comments": "4 pages, 3 figures, appeared in the non-archival AAAI 2022 Spring\n  Syposium on \"Designing Artificial Intelligence for Open Worlds\"",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Anticipatory thinking drives our ability to manage risk - identification and\nmitigation - in everyday life, from bringing an umbrella when it might rain to\nbuying car insurance. As AI systems become part of everyday life, they too have\nbegun to manage risk. Autonomous vehicles log millions of miles, StarCraft and\nGo agents have similar capabilities to humans, implicitly managing risks\npresented by their opponents. To further increase performance in these tasks,\nout-of-distribution evaluation can characterize a model's bias, what we view as\na type of risk management. However, learning to identify and mitigate\nlow-frequency, high-impact risks is at odds with the observational bias\nrequired to train machine learning models. StarCraft and Go are closed-world\ndomains whose risks are known and mitigations well documented, ideal for\nlearning through repetition. Adversarial filtering datasets provide difficult\nexamples but are laborious to curate and static, both barriers to real-world\nrisk management. Adversarial robustness focuses on model poisoning under the\nassumption there is an adversary with malicious intent, without considering\nnaturally occurring adversarial examples. These methods are all important steps\ntowards improving risk management but do so without considering open-worlds. We\nunify these open-world risk management challenges with two contributions. The\nfirst is our perception challenges, designed for agents with imperfect\nperceptions of their environment whose consequences have a high impact. Our\nsecond contribution are cognition challenges, designed for agents that must\ndynamically adjust their risk exposure as they identify new risks and learn new\nmitigations. Our goal with these challenges is to spur research into solutions\nthat assess and improve the anticipatory thinking required by AI agents to\nmanage risk in open-worlds and ultimately the real-world.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 22 Jun 2023 18:31:17 GMT"
      }
    ],
    "update_date": "2023-06-26",
    "authors_parsed": [
      [
        "Amos-Binks",
        "Adam",
        ""
      ],
      [
        "Dannenhauer",
        "Dustin",
        ""
      ],
      [
        "Gilpin",
        "Leilani H.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.13157",
    "publish_date": "2023-06-22"
  },
  {
    "id": "2306.13213",
    "submitter": "Xiangyu Qi",
    "authors": "Xiangyu Qi, Kaixuan Huang, Ashwinee Panda, Peter Henderson, Mengdi\n  Wang, Prateek Mittal",
    "title": "Visual Adversarial Examples Jailbreak Aligned Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recently, there has been a surge of interest in integrating vision into Large\nLanguage Models (LLMs), exemplified by Visual Language Models (VLMs) such as\nFlamingo and GPT-4. This paper sheds light on the security and safety\nimplications of this trend. First, we underscore that the continuous and\nhigh-dimensional nature of the visual input makes it a weak link against\nadversarial attacks, representing an expanded attack surface of\nvision-integrated LLMs. Second, we highlight that the versatility of LLMs also\npresents visual attackers with a wider array of achievable adversarial\nobjectives, extending the implications of security failures beyond mere\nmisclassification. As an illustration, we present a case study in which we\nexploit visual adversarial examples to circumvent the safety guardrail of\naligned LLMs with integrated vision. Intriguingly, we discover that a single\nvisual adversarial example can universally jailbreak an aligned LLM, compelling\nit to heed a wide range of harmful instructions that it otherwise would not)\nand generate harmful content that transcends the narrow scope of a `few-shot'\nderogatory corpus initially employed to optimize the adversarial example. Our\nstudy underscores the escalating adversarial risks associated with the pursuit\nof multimodality. Our findings also connect the long-studied adversarial\nvulnerabilities of neural networks to the nascent field of AI alignment. The\npresented attack suggests a fundamental adversarial challenge for AI alignment,\nespecially in light of the emerging trend toward multimodality in frontier\nfoundation models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 22 Jun 2023 22:13:03 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 16 Aug 2023 22:38:55 GMT"
      }
    ],
    "update_date": "2023-08-21",
    "authors_parsed": [
      [
        "Qi",
        "Xiangyu",
        ""
      ],
      [
        "Huang",
        "Kaixuan",
        ""
      ],
      [
        "Panda",
        "Ashwinee",
        ""
      ],
      [
        "Henderson",
        "Peter",
        ""
      ],
      [
        "Wang",
        "Mengdi",
        ""
      ],
      [
        "Mittal",
        "Prateek",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.13213",
    "publish_date": "2023-08-16"
  },
  {
    "id": "2306.13394",
    "submitter": "Chaoyou Fu",
    "authors": "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu\n  Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, Rongrong Ji",
    "title": "MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language\n  Models",
    "comments": "Project page:\n  https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Multimodal Large Language Model (MLLM) relies on the powerful LLM to perform\nmultimodal tasks, showing amazing emergent abilities in recent studies, such as\nwriting poems based on an image. However, it is difficult for these case\nstudies to fully reflect the performance of MLLM, lacking a comprehensive\nevaluation. In this paper, we fill in this blank, presenting the first\ncomprehensive MLLM Evaluation benchmark MME. It measures both perception and\ncognition abilities on a total of 14 subtasks. In order to avoid data leakage\nthat may arise from direct use of public datasets for evaluation, the\nannotations of instruction-answer pairs are all manually designed. The concise\ninstruction design allows us to fairly compare MLLMs, instead of struggling in\nprompt engineering. Besides, with such an instruction, we can also easily carry\nout quantitative statistics. A total of 30 advanced MLLMs are comprehensively\nevaluated on our MME, which not only suggests that existing MLLMs still have a\nlarge room for improvement, but also reveals the potential directions for the\nsubsequent model optimization.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 23 Jun 2023 09:22:36 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 2 Jul 2023 02:56:04 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 6 Dec 2023 07:09:31 GMT"
      }
    ],
    "update_date": "2023-12-07",
    "authors_parsed": [
      [
        "Fu",
        "Chaoyou",
        ""
      ],
      [
        "Chen",
        "Peixian",
        ""
      ],
      [
        "Shen",
        "Yunhang",
        ""
      ],
      [
        "Qin",
        "Yulei",
        ""
      ],
      [
        "Zhang",
        "Mengdan",
        ""
      ],
      [
        "Lin",
        "Xu",
        ""
      ],
      [
        "Yang",
        "Jinrui",
        ""
      ],
      [
        "Zheng",
        "Xiawu",
        ""
      ],
      [
        "Li",
        "Ke",
        ""
      ],
      [
        "Sun",
        "Xing",
        ""
      ],
      [
        "Wu",
        "Yunsheng",
        ""
      ],
      [
        "Ji",
        "Rongrong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.13394",
    "publish_date": "2023-12-06"
  },
  {
    "id": "2306.14321",
    "submitter": "Yilun Zhao",
    "authors": "Yilun Zhao, Chen Zhao, Linyong Nan, Zhenting Qi, Wenlin Zhang, Xiangru\n  Tang, Boyu Mi, Dragomir Radev",
    "title": "RobuT: A Systematic Study of Table QA Robustness Against Human-Annotated\n  Adversarial Perturbations",
    "comments": "Accepted at ACL 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Despite significant progress having been made in question answering on\ntabular data (Table QA), it's unclear whether, and to what extent existing\nTable QA models are robust to task-specific perturbations, e.g., replacing key\nquestion entities or shuffling table columns. To systematically study the\nrobustness of Table QA models, we propose a benchmark called RobuT, which\nbuilds upon existing Table QA datasets (WTQ, WikiSQL-Weak, and SQA) and\nincludes human-annotated adversarial perturbations in terms of table header,\ntable content, and question. Our results indicate that both state-of-the-art\nTable QA models and large language models (e.g., GPT-3) with few-shot learning\nfalter in these adversarial sets. We propose to address this problem by using\nlarge language models to generate adversarial examples to enhance training,\nwhich significantly improves the robustness of Table QA models. Our data and\ncode is publicly available at https://github.com/yilunzhao/RobuT.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 25 Jun 2023 19:23:21 GMT"
      }
    ],
    "update_date": "2023-06-27",
    "authors_parsed": [
      [
        "Zhao",
        "Yilun",
        ""
      ],
      [
        "Zhao",
        "Chen",
        ""
      ],
      [
        "Nan",
        "Linyong",
        ""
      ],
      [
        "Qi",
        "Zhenting",
        ""
      ],
      [
        "Zhang",
        "Wenlin",
        ""
      ],
      [
        "Tang",
        "Xiangru",
        ""
      ],
      [
        "Mi",
        "Boyu",
        ""
      ],
      [
        "Radev",
        "Dragomir",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.14321",
    "publish_date": "2023-06-25"
  },
  {
    "id": "2306.15447",
    "submitter": "Nicholas Carlini",
    "authors": "Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew\n  Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine\n  Lee, Florian Tramer, Ludwig Schmidt",
    "title": "Are aligned neural networks adversarially aligned?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models are now tuned to align with the goals of their\ncreators, namely to be \"helpful and harmless.\" These models should respond\nhelpfully to user questions, but refuse to answer requests that could cause\nharm. However, adversarial users can construct inputs which circumvent attempts\nat alignment. In this work, we study to what extent these models remain\naligned, even when interacting with an adversarial user who constructs\nworst-case inputs (adversarial examples). These inputs are designed to cause\nthe model to emit harmful content that would otherwise be prohibited. We show\nthat existing NLP-based optimization attacks are insufficiently powerful to\nreliably attack aligned text models: even when current NLP-based attacks fail,\nwe can find adversarial inputs with brute force. As a result, the failure of\ncurrent attacks should not be seen as proof that aligned text models remain\naligned under adversarial inputs.\n  However the recent trend in large-scale ML models is multimodal models that\nallow users to provide images that influence the text that is generated. We\nshow these models can be easily attacked, i.e., induced to perform arbitrary\nun-aligned behavior through adversarial perturbation of the input image. We\nconjecture that improved NLP attacks may demonstrate this same level of\nadversarial control over text-only models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 26 Jun 2023 17:18:44 GMT"
      }
    ],
    "update_date": "2023-06-28",
    "authors_parsed": [
      [
        "Carlini",
        "Nicholas",
        ""
      ],
      [
        "Nasr",
        "Milad",
        ""
      ],
      [
        "Choquette-Choo",
        "Christopher A.",
        ""
      ],
      [
        "Jagielski",
        "Matthew",
        ""
      ],
      [
        "Gao",
        "Irena",
        ""
      ],
      [
        "Awadalla",
        "Anas",
        ""
      ],
      [
        "Koh",
        "Pang Wei",
        ""
      ],
      [
        "Ippolito",
        "Daphne",
        ""
      ],
      [
        "Lee",
        "Katherine",
        ""
      ],
      [
        "Tramer",
        "Florian",
        ""
      ],
      [
        "Schmidt",
        "Ludwig",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.15447",
    "publish_date": "2023-06-26"
  },
  {
    "id": "2306.15559",
    "submitter": "Jan Von Der Assen",
    "authors": "Jan von der Assen, Alberto Huertas Celdr\\'an, Janik Luechinger, Pedro\n  Miguel S\\'anchez S\\'anchez, G\\'er\\^ome Bovet, Gregorio Mart\\'inez P\\'erez,\n  Burkhard Stiller",
    "title": "RansomAI: AI-powered Ransomware for Stealthy Encryption",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Cybersecurity solutions have shown promising performance when detecting\nransomware samples that use fixed algorithms and encryption rates. However, due\nto the current explosion of Artificial Intelligence (AI), sooner than later,\nransomware (and malware in general) will incorporate AI techniques to\nintelligently and dynamically adapt its encryption behavior to be undetected.\nIt might result in ineffective and obsolete cybersecurity solutions, but the\nliterature lacks AI-powered ransomware to verify it. Thus, this work proposes\nRansomAI, a Reinforcement Learning-based framework that can be integrated into\nexisting ransomware samples to adapt their encryption behavior and stay\nstealthy while encrypting files. RansomAI presents an agent that learns the\nbest encryption algorithm, rate, and duration that minimizes its detection\n(using a reward mechanism and a fingerprinting intelligent detection system)\nwhile maximizing its damage function. The proposed framework was validated in a\nransomware, Ransomware-PoC, that infected a Raspberry Pi 4, acting as a\ncrowdsensor. A pool of experiments with Deep Q-Learning and Isolation Forest\n(deployed on the agent and detection system, respectively) has demonstrated\nthat RansomAI evades the detection of Ransomware-PoC affecting the Raspberry Pi\n4 in a few minutes with >90% accuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 27 Jun 2023 15:36:12 GMT"
      }
    ],
    "update_date": "2023-06-28",
    "authors_parsed": [
      [
        "von der Assen",
        "Jan",
        ""
      ],
      [
        "Celdr\u00e1n",
        "Alberto Huertas",
        ""
      ],
      [
        "Luechinger",
        "Janik",
        ""
      ],
      [
        "S\u00e1nchez",
        "Pedro Miguel S\u00e1nchez",
        ""
      ],
      [
        "Bovet",
        "G\u00e9r\u00f4me",
        ""
      ],
      [
        "P\u00e9rez",
        "Gregorio Mart\u00ednez",
        ""
      ],
      [
        "Stiller",
        "Burkhard",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.15559",
    "publish_date": "2023-06-27"
  },
  {
    "id": "2306.16738",
    "submitter": "Jiahao Xie",
    "authors": "Jiahao Xie, Chao Zhang, Weijie Liu, Wensong Bai, Hui Qian",
    "title": "Towards Optimal Randomized Strategies in Adversarial Example Game",
    "comments": "Extended version of paper https://doi.org/10.1609/aaai.v37i9.26247\n  which appeared in AAAI 2023",
    "journal-ref": null,
    "doi": "10.1609/aaai.v37i9.26247",
    "report-no": null,
    "categories": "cs.LG cs.CR cs.GT",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The vulnerability of deep neural network models to adversarial example\nattacks is a practical challenge in many artificial intelligence applications.\nA recent line of work shows that the use of randomization in adversarial\ntraining is the key to find optimal strategies against adversarial example\nattacks. However, in a fully randomized setting where both the defender and the\nattacker can use randomized strategies, there are no efficient algorithm for\nfinding such an optimal strategy. To fill the gap, we propose the first\nalgorithm of its kind, called FRAT, which models the problem with a new\ninfinite-dimensional continuous-time flow on probability distribution spaces.\nFRAT maintains a lightweight mixture of models for the defender, with\nflexibility to efficiently update mixing weights and model parameters at each\niteration. Furthermore, FRAT utilizes lightweight sampling subroutines to\nconstruct a random strategy for the attacker. We prove that the continuous-time\nlimit of FRAT converges to a mixed Nash equilibria in a zero-sum game formed by\na defender and an attacker. Experimental results also demonstrate the\nefficiency of FRAT on CIFAR-10 and CIFAR-100 datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 29 Jun 2023 07:29:23 GMT"
      }
    ],
    "update_date": "2023-06-30",
    "authors_parsed": [
      [
        "Xie",
        "Jiahao",
        ""
      ],
      [
        "Zhang",
        "Chao",
        ""
      ],
      [
        "Liu",
        "Weijie",
        ""
      ],
      [
        "Bai",
        "Wensong",
        ""
      ],
      [
        "Qian",
        "Hui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.16738",
    "publish_date": "2023-06-29"
  },
  {
    "id": "2306.17190",
    "submitter": "Yuanyuan Wei",
    "authors": "Yuanyuan Wei, Julian Jang-Jaccard, Amardeep Singh, Fariza Sabrina,\n  Seyit Camtepe",
    "title": "Classification and Explanation of Distributed Denial-of-Service (DDoS)\n  Attack Detection using Machine Learning and Shapley Additive Explanation\n  (SHAP) Methods",
    "comments": "14 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  DDoS attacks involve overwhelming a target system with a large number of\nrequests or traffic from multiple sources, disrupting the normal traffic of a\ntargeted server, service, or network. Distinguishing between legitimate traffic\nand malicious traffic is a challenging task. It is possible to classify\nlegitimate traffic and malicious traffic and analysis the network traffic by\nusing machine learning and deep learning techniques. However, an inter-model\nexplanation implemented to classify a traffic flow whether is benign or\nmalicious is an important investigation of the inner working theory of the\nmodel to increase the trustworthiness of the model. Explainable Artificial\nIntelligence (XAI) can explain the decision-making of the machine learning\nmodels that can be classified and identify DDoS traffic. In this context, we\nproposed a framework that can not only classify legitimate traffic and\nmalicious traffic of DDoS attacks but also use SHAP to explain the\ndecision-making of the classifier model. To address this concern, we first\nadopt feature selection techniques to select the top 20 important features\nbased on feature importance techniques (e.g., XGB-based SHAP feature\nimportance). Following that, the Multi-layer Perceptron Network (MLP) part of\nour proposed model uses the optimized features of the DDoS attack dataset as\ninputs to classify legitimate and malicious traffic. We perform extensive\nexperiments with all features and selected features. The evaluation results\nshow that the model performance with selected features achieves above 99\\%\naccuracy. Finally, to provide interpretability, XAI can be adopted to explain\nthe model performance between the prediction results and features based on\nglobal and local explanations by SHAP, which can better explain the results\nachieved by our proposed framework.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 27 Jun 2023 04:51:29 GMT"
      }
    ],
    "update_date": "2023-07-03",
    "authors_parsed": [
      [
        "Wei",
        "Yuanyuan",
        ""
      ],
      [
        "Jang-Jaccard",
        "Julian",
        ""
      ],
      [
        "Singh",
        "Amardeep",
        ""
      ],
      [
        "Sabrina",
        "Fariza",
        ""
      ],
      [
        "Camtepe",
        "Seyit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.17190",
    "publish_date": "2023-06-27"
  },
  {
    "id": "2306.17194",
    "submitter": "Manli Shu",
    "authors": "Manli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping, Chaowei Xiao, Tom\n  Goldstein",
    "title": "On the Exploitability of Instruction Tuning",
    "comments": "NeurIPS 2023 camera-ready (21 pages, 10 figures)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Instruction tuning is an effective technique to align large language models\n(LLMs) with human intents. In this work, we investigate how an adversary can\nexploit instruction tuning by injecting specific instruction-following examples\ninto the training data that intentionally changes the model's behavior. For\nexample, an adversary can achieve content injection by injecting training\nexamples that mention target content and eliciting such behavior from\ndownstream models. To achieve this goal, we propose \\textit{AutoPoison}, an\nautomated data poisoning pipeline. It naturally and coherently incorporates\nversatile attack goals into poisoned data with the help of an oracle LLM. We\nshowcase two example attacks: content injection and over-refusal attacks, each\naiming to induce a specific exploitable behavior. We quantify and benchmark the\nstrength and the stealthiness of our data poisoning scheme. Our results show\nthat AutoPoison allows an adversary to change a model's behavior by poisoning\nonly a small fraction of data while maintaining a high level of stealthiness in\nthe poisoned examples. We hope our work sheds light on how data quality affects\nthe behavior of instruction-tuned models and raises awareness of the importance\nof data quality for responsible deployments of LLMs. Code is available at\n\\url{https://github.com/azshue/AutoPoison}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 28 Jun 2023 17:54:04 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 28 Oct 2023 18:04:36 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Shu",
        "Manli",
        ""
      ],
      [
        "Wang",
        "Jiongxiao",
        ""
      ],
      [
        "Zhu",
        "Chen",
        ""
      ],
      [
        "Geiping",
        "Jonas",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ],
      [
        "Goldstein",
        "Tom",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2306.17194",
    "publish_date": "2023-06-28"
  },
  {
    "id": "2307.00309",
    "submitter": "Hanieh Naderi",
    "authors": "Hanieh Naderi and Ivan V. Baji\\'c",
    "title": "Adversarial Attacks and Defenses on 3D Point Cloud Classification: A\n  Survey",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG eess.IV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep learning has successfully solved a wide range of tasks in 2D vision as a\ndominant AI technique. Recently, deep learning on 3D point clouds is becoming\nincreasingly popular for addressing various tasks in this field. Despite\nremarkable achievements, deep learning algorithms are vulnerable to adversarial\nattacks. These attacks are imperceptible to the human eye but can easily fool\ndeep neural networks in the testing and deployment stage. To encourage future\nresearch, this survey summarizes the current progress on adversarial attack and\ndefense techniques on point cloud classification.This paper first introduces\nthe principles and characteristics of adversarial attacks and summarizes and\nanalyzes adversarial example generation methods in recent years. Additionally,\nit provides an overview of defense strategies, organized into data-focused and\nmodel-focused methods. Finally, it presents several current challenges and\npotential future research directions in this domain.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 1 Jul 2023 11:46:36 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 1 Dec 2023 15:51:55 GMT"
      }
    ],
    "update_date": "2023-12-04",
    "authors_parsed": [
      [
        "Naderi",
        "Hanieh",
        ""
      ],
      [
        "Baji\u0107",
        "Ivan V.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.00309",
    "publish_date": "2023-07-01"
  },
  {
    "id": "2307.00691",
    "submitter": "Kshitiz Aryal",
    "authors": "Maanak Gupta, CharanKumar Akiri, Kshitiz Aryal, Eli Parker, Lopamudra\n  Praharaj",
    "title": "From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and\n  Privacy",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Undoubtedly, the evolution of Generative AI (GenAI) models has been the\nhighlight of digital transformation in the year 2022. As the different GenAI\nmodels like ChatGPT and Google Bard continue to foster their complexity and\ncapability, it's critical to understand its consequences from a cybersecurity\nperspective. Several instances recently have demonstrated the use of GenAI\ntools in both the defensive and offensive side of cybersecurity, and focusing\non the social, ethical and privacy implications this technology possesses. This\nresearch paper highlights the limitations, challenges, potential risks, and\nopportunities of GenAI in the domain of cybersecurity and privacy. The work\npresents the vulnerabilities of ChatGPT, which can be exploited by malicious\nusers to exfiltrate malicious information bypassing the ethical constraints on\nthe model. This paper demonstrates successful example attacks like Jailbreaks,\nreverse psychology, and prompt injection attacks on the ChatGPT. The paper also\ninvestigates how cyber offenders can use the GenAI tools in developing cyber\nattacks, and explore the scenarios where ChatGPT can be used by adversaries to\ncreate social engineering attacks, phishing attacks, automated hacking, attack\npayload generation, malware creation, and polymorphic malware. This paper then\nexamines defense techniques and uses GenAI tools to improve security measures,\nincluding cyber defense automation, reporting, threat intelligence, secure code\ngeneration and detection, attack identification, developing ethical guidelines,\nincidence response plans, and malware detection. We will also discuss the\nsocial, legal, and ethical implications of ChatGPT. In conclusion, the paper\nhighlights open challenges and future directions to make this GenAI secure,\nsafe, trustworthy, and ethical as the community understands its cybersecurity\nimpacts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 3 Jul 2023 00:36:57 GMT"
      }
    ],
    "update_date": "2023-07-04",
    "authors_parsed": [
      [
        "Gupta",
        "Maanak",
        ""
      ],
      [
        "Akiri",
        "CharanKumar",
        ""
      ],
      [
        "Aryal",
        "Kshitiz",
        ""
      ],
      [
        "Parker",
        "Eli",
        ""
      ],
      [
        "Praharaj",
        "Lopamudra",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.00691",
    "publish_date": "2023-07-03"
  },
  {
    "id": "2307.02483",
    "submitter": "Alexander Wei",
    "authors": "Alexander Wei and Nika Haghtalab and Jacob Steinhardt",
    "title": "Jailbroken: How Does LLM Safety Training Fail?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 5 Jul 2023 17:58:10 GMT"
      }
    ],
    "update_date": "2023-07-06",
    "authors_parsed": [
      [
        "Wei",
        "Alexander",
        ""
      ],
      [
        "Haghtalab",
        "Nika",
        ""
      ],
      [
        "Steinhardt",
        "Jacob",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.02483",
    "publish_date": "2023-07-05"
  },
  {
    "id": "2307.02599",
    "submitter": "Shuyang Cai",
    "authors": "Shuyang Cai and Wanyun Cui",
    "title": "Evade ChatGPT Detectors via A Single Space",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  ChatGPT brings revolutionary social value but also raises concerns about the\nmisuse of AI-generated text. Consequently, an important question is how to\ndetect whether texts are generated by ChatGPT or by human. Existing detectors\nare built upon the assumption that there are distributional gaps between\nhuman-generated and AI-generated text. These gaps are typically identified\nusing statistical information or classifiers. Our research challenges the\ndistributional gap assumption in detectors. We find that detectors do not\neffectively discriminate the semantic and stylistic gaps between\nhuman-generated and AI-generated text. Instead, the \"subtle differences\", such\nas an extra space, become crucial for detection. Based on this discovery, we\npropose the SpaceInfi strategy to evade detection. Experiments demonstrate the\neffectiveness of this strategy across multiple benchmarks and detectors. We\nalso provide a theoretical explanation for why SpaceInfi is successful in\nevading perplexity-based detection. And we empirically show that a phenomenon\ncalled token mutation causes the evasion for language model-based detectors.\nOur findings offer new insights and challenges for understanding and\nconstructing more applicable ChatGPT detectors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 5 Jul 2023 18:48:28 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 13 Oct 2023 17:01:11 GMT"
      }
    ],
    "update_date": "2023-10-16",
    "authors_parsed": [
      [
        "Cai",
        "Shuyang",
        ""
      ],
      [
        "Cui",
        "Wanyun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.02599",
    "publish_date": "2023-07-05"
  },
  {
    "id": "2307.02849",
    "submitter": "Zi'ou Zheng",
    "authors": "Zi'ou Zheng and Xiaodan Zhu",
    "title": "NatLogAttack: A Framework for Attacking Natural Language Inference\n  Models with Natural Logic",
    "comments": "Published as a conference paper at ACL 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Reasoning has been a central topic in artificial intelligence from the\nbeginning. The recent progress made on distributed representation and neural\nnetworks continues to improve the state-of-the-art performance of natural\nlanguage inference. However, it remains an open question whether the models\nperform real reasoning to reach their conclusions or rely on spurious\ncorrelations. Adversarial attacks have proven to be an important tool to help\nevaluate the Achilles' heel of the victim models. In this study, we explore the\nfundamental problem of developing attack models based on logic formalism. We\npropose NatLogAttack to perform systematic attacks centring around natural\nlogic, a classical logic formalism that is traceable back to Aristotle's\nsyllogism and has been closely developed for natural language inference. The\nproposed framework renders both label-preserving and label-flipping attacks. We\nshow that compared to the existing attack models, NatLogAttack generates better\nadversarial examples with fewer visits to the victim models. The victim models\nare found to be more vulnerable under the label-flipping setting. NatLogAttack\nprovides a tool to probe the existing and future NLI models' capacity from a\nkey viewpoint and we hope more logic-based attacks will be further explored for\nunderstanding the desired property of reasoning.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 6 Jul 2023 08:32:14 GMT"
      }
    ],
    "update_date": "2023-07-07",
    "authors_parsed": [
      [
        "Zheng",
        "Zi'ou",
        ""
      ],
      [
        "Zhu",
        "Xiaodan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.02849",
    "publish_date": "2023-07-06"
  },
  {
    "id": "2307.03699",
    "submitter": "Chuanbo Hu",
    "authors": "Chuanbo Hu, Bin Liu, Xin Li, Yanfang Ye",
    "title": "Unveiling the Potential of Knowledge-Prompted ChatGPT for Enhancing Drug\n  Trafficking Detection on Social Media",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.SI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Social media platforms such as Instagram and Twitter have emerged as critical\nchannels for drug marketing and illegal sale. Detecting and labeling online\nillicit drug trafficking activities becomes important in addressing this issue.\nHowever, the effectiveness of conventional supervised learning methods in\ndetecting drug trafficking heavily relies on having access to substantial\namounts of labeled data, while data annotation is time-consuming and\nresource-intensive. Furthermore, these models often face challenges in\naccurately identifying trafficking activities when drug dealers use deceptive\nlanguage and euphemisms to avoid detection. To overcome this limitation, we\nconduct the first systematic study on leveraging large language models (LLMs),\nsuch as ChatGPT, to detect illicit drug trafficking activities on social media.\nWe propose an analytical framework to compose \\emph{knowledge-informed\nprompts}, which serve as the interface that humans can interact with and use\nLLMs to perform the detection task. Additionally, we design a Monte Carlo\ndropout based prompt optimization method to further to improve performance and\ninterpretability. Our experimental findings demonstrate that the proposed\nframework outperforms other baseline language models in terms of drug\ntrafficking detection accuracy, showing a remarkable improvement of nearly\n12\\%. By integrating prior knowledge and the proposed prompts, ChatGPT can\neffectively identify and label drug trafficking activities on social networks,\neven in the presence of deceptive language and euphemisms used by drug dealers\nto evade detection. The implications of our research extend to social networks,\nemphasizing the importance of incorporating prior knowledge and scenario-based\nprompts into analytical tools to improve online security and public safety.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 7 Jul 2023 16:15:59 GMT"
      }
    ],
    "update_date": "2023-07-10",
    "authors_parsed": [
      [
        "Hu",
        "Chuanbo",
        ""
      ],
      [
        "Liu",
        "Bin",
        ""
      ],
      [
        "Li",
        "Xin",
        ""
      ],
      [
        "Ye",
        "Yanfang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.03699",
    "publish_date": "2023-07-07"
  },
  {
    "id": "2307.03838",
    "submitter": "Pin-Yu Chen",
    "authors": "Xiaomeng Hu and Pin-Yu Chen and Tsung-Yi Ho",
    "title": "RADAR: Robust AI-Text Detection via Adversarial Learning",
    "comments": "Accepted by NeurIPS 2023. Project page and demos:\n  https://radar.vizhub.ai",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in large language models (LLMs) and the intensifying\npopularity of ChatGPT-like applications have blurred the boundary of\nhigh-quality text generation between humans and machines. However, in addition\nto the anticipated revolutionary changes to our technology and society, the\ndifficulty of distinguishing LLM-generated texts (AI-text) from human-generated\ntexts poses new challenges of misuse and fairness, such as fake content\ngeneration, plagiarism, and false accusations of innocent writers. While\nexisting works show that current AI-text detectors are not robust to LLM-based\nparaphrasing, this paper aims to bridge this gap by proposing a new framework\ncalled RADAR, which jointly trains a robust AI-text detector via adversarial\nlearning. RADAR is based on adversarial training of a paraphraser and a\ndetector. The paraphraser's goal is to generate realistic content to evade\nAI-text detection. RADAR uses the feedback from the detector to update the\nparaphraser, and vice versa. Evaluated with 8 different LLMs (Pythia, Dolly\n2.0, Palmyra, Camel, GPT-J, Dolly 1.0, LLaMA, and Vicuna) across 4 datasets,\nexperimental results show that RADAR significantly outperforms existing AI-text\ndetection methods, especially when paraphrasing is in place. We also identify\nthe strong transferability of RADAR from instruction-tuned LLMs to other LLMs,\nand evaluate the improved capability of RADAR via GPT-3.5-Turbo.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 7 Jul 2023 21:13:27 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 24 Oct 2023 16:31:49 GMT"
      }
    ],
    "update_date": "2023-10-25",
    "authors_parsed": [
      [
        "Hu",
        "Xiaomeng",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Ho",
        "Tsung-Yi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.03838",
    "publish_date": "2023-10-24"
  },
  {
    "id": "2307.04333",
    "submitter": "Boya Zhang",
    "authors": "Boya Zhang, Weijian Luo, Zhihua Zhang",
    "title": "Enhancing Adversarial Robustness via Score-Based Optimization",
    "comments": "NeurIPS 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks have the potential to mislead deep neural network\nclassifiers by introducing slight perturbations. Developing algorithms that can\nmitigate the effects of these attacks is crucial for ensuring the safe use of\nartificial intelligence. Recent studies have suggested that score-based\ndiffusion models are effective in adversarial defenses. However, existing\ndiffusion-based defenses rely on the sequential simulation of the reversed\nstochastic differential equations of diffusion models, which are\ncomputationally inefficient and yield suboptimal results. In this paper, we\nintroduce a novel adversarial defense scheme named ScoreOpt, which optimizes\nadversarial samples at test-time, towards original clean data in the direction\nguided by score-based priors. We conduct comprehensive experiments on multiple\ndatasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results\ndemonstrate that our approach outperforms existing adversarial defenses in\nterms of both robustness performance and inference speed.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 10 Jul 2023 03:59:42 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 5 Oct 2023 13:54:21 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 28 Oct 2023 12:53:24 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Zhang",
        "Boya",
        ""
      ],
      [
        "Luo",
        "Weijian",
        ""
      ],
      [
        "Zhang",
        "Zhihua",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.04333",
    "publish_date": "2023-10-05"
  },
  {
    "id": "2307.04594",
    "submitter": "Harmender Gahlawat",
    "authors": "Harmender Gahlawat and Meirav Zehavi",
    "title": "Parameterized Analysis of the Cops and Robber Problem",
    "comments": "To Appear in MFCS 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.DM",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  \\textit{Pursuit-evasion games} have been intensively studied for several\ndecades due to their numerous applications in artificial intelligence, robot\nmotion planning, database theory, distributed computing, and algorithmic\ntheory. \\textsc{Cops and Robber} (\\CR) is one of the most well-known\npursuit-evasion games played on graphs, where multiple \\textit{cops} pursue a\nsingle \\textit{robber}. The aim is to compute the \\textit{cop number} of a\ngraph, $k$, which is the minimum number of cops that ensures the\n\\textit{capture} of the robber.\n  From the viewpoint of parameterized complexity, \\CR is W[2]-hard\nparameterized by $k$~[Fomin et al., TCS, 2010]. Thus, we study structural\nparameters of the input graph. We begin with the \\textit{vertex cover number}\n($\\mathsf{vcn}$). First, we establish that $k \\leq \\frac{\\mathsf{vcn}}{3}+1$.\nSecond, we prove that \\CR parameterized by $\\mathsf{vcn}$ is \\FPT by designing\nan exponential kernel. We complement this result by showing that it is unlikely\nfor \\CR parameterized by $\\mathsf{vcn}$ to admit a polynomial compression. We\nextend our exponential kernels to the parameters \\textit{cluster vertex\ndeletion number} and \\textit{deletion to stars number}, and design a linear\nvertex kernel for \\textit{neighborhood diversity}. Additionally, we extend all\nof our results to several well-studied variations of \\CR.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 10 Jul 2023 14:35:29 GMT"
      }
    ],
    "update_date": "2023-07-11",
    "authors_parsed": [
      [
        "Gahlawat",
        "Harmender",
        ""
      ],
      [
        "Zehavi",
        "Meirav",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.04594",
    "publish_date": "2023-07-10"
  },
  {
    "id": "2307.04677",
    "submitter": "Anouar Nechi",
    "authors": "Anouar Nechi, Ahmed Mahmoudi, Christoph Herold, Daniel Widmer, Thomas\n  K\\\"urner, Mladen Berekovic and Saleh Mulhem",
    "title": "Practical Trustworthiness Model for DNN in Dedicated 6G Application",
    "comments": "PREPRINT - accepted In Proceedings of the 19th International\n  Conference on Wireless and Mobile Computing, Networking and Communications\n  2023(STWiMob 2023)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI eess.SP",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Artificial intelligence (AI) is considered an efficient response to several\nchallenges facing 6G technology. However, AI still suffers from a huge trust\nissue due to its ambiguous way of making predictions. Therefore, there is a\nneed for a method to evaluate the AI's trustworthiness in practice for future\n6G applications. This paper presents a practical model to analyze the\ntrustworthiness of AI in a dedicated 6G application. In particular, we present\ntwo customized Deep Neural Networks (DNNs) to solve the Automatic Modulation\nRecognition (AMR) problem in Terahertz communications-based 6G technology.\nThen, a specific trustworthiness model and its attributes, namely data\nrobustness, parameter sensitivity, and security covering adversarial examples,\nare introduced. The evaluation results indicate that the proposed\ntrustworthiness attributes are crucial to evaluate the trustworthiness of DNN\nfor this 6G application.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 10 Jul 2023 16:21:35 GMT"
      }
    ],
    "update_date": "2023-07-11",
    "authors_parsed": [
      [
        "Nechi",
        "Anouar",
        ""
      ],
      [
        "Mahmoudi",
        "Ahmed",
        ""
      ],
      [
        "Herold",
        "Christoph",
        ""
      ],
      [
        "Widmer",
        "Daniel",
        ""
      ],
      [
        "K\u00fcrner",
        "Thomas",
        ""
      ],
      [
        "Berekovic",
        "Mladen",
        ""
      ],
      [
        "Mulhem",
        "Saleh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.04677",
    "publish_date": "2023-07-10"
  },
  {
    "id": "2307.05842",
    "submitter": "Emilio Ferrara",
    "authors": "Emilio Ferrara",
    "title": "The Butterfly Effect in Artificial Intelligence Systems: Implications\n  for AI Bias and Fairness",
    "comments": "Working Draft",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The Butterfly Effect, a concept originating from chaos theory, underscores\nhow small changes can have significant and unpredictable impacts on complex\nsystems. In the context of AI fairness and bias, the Butterfly Effect can stem\nfrom a variety of sources, such as small biases or skewed data inputs during\nalgorithm development, saddle points in training, or distribution shifts in\ndata between training and testing phases. These seemingly minor alterations can\nlead to unexpected and substantial unfair outcomes, disproportionately\naffecting underrepresented individuals or groups and perpetuating pre-existing\ninequalities. Moreover, the Butterfly Effect can amplify inherent biases within\ndata or algorithms, exacerbate feedback loops, and create vulnerabilities for\nadversarial attacks. Given the intricate nature of AI systems and their\nsocietal implications, it is crucial to thoroughly examine any changes to\nalgorithms or input data for potential unintended consequences. In this paper,\nwe envision both algorithmic and empirical strategies to detect, quantify, and\nmitigate the Butterfly Effect in AI systems, emphasizing the importance of\naddressing these challenges to promote fairness and ensure responsible AI\ndevelopment.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 11 Jul 2023 23:32:26 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 13 Jul 2023 02:11:25 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 18 Sep 2023 22:01:07 GMT"
      }
    ],
    "update_date": "2023-09-20",
    "authors_parsed": [
      [
        "Ferrara",
        "Emilio",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.05842",
    "publish_date": "2023-07-11"
  },
  {
    "id": "2307.06608",
    "submitter": "Jiaming Zhang",
    "authors": "Jiaming Zhang, Jitao Sang, Qi Yi and Changsheng Xu",
    "title": "Introducing Foundation Models as Surrogate Models: Advancing Towards\n  More Practical Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recently, the no-box adversarial attack, in which the attacker lacks access\nto the model's architecture, weights, and training data, become the most\npractical and challenging attack setup. However, there is an unawareness of the\npotential and flexibility inherent in the surrogate model selection process on\nno-box setting. Inspired by the burgeoning interest in utilizing foundational\nmodels to address downstream tasks, this paper adopts an innovative idea that\n1) recasting adversarial attack as a downstream task. Specifically, image noise\ngeneration to meet the emerging trend and 2) introducing foundational models as\nsurrogate models. Harnessing the concept of non-robust features, we elaborate\non two guiding principles for surrogate model selection to explain why the\nfoundational model is an optimal choice for this role. However, paradoxically,\nwe observe that these foundational models underperform. Analyzing this\nunexpected behavior within the feature space, we attribute the lackluster\nperformance of foundational models (e.g., CLIP) to their significant\nrepresentational capacity and, conversely, their lack of discriminative\nprowess. To mitigate this issue, we propose the use of a margin-based loss\nstrategy for the fine-tuning of foundational models on target images. The\nexperimental results verify that our approach, which employs the basic Fast\nGradient Sign Method (FGSM) attack algorithm, outstrips the performance of\nother, more convoluted algorithms. We conclude by advocating for the research\ncommunity to consider surrogate models as crucial determinants in the\neffectiveness of adversarial attacks in no-box settings. The implications of\nour work bear relevance for improving the efficacy of such adversarial attacks\nand the overall robustness of AI systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 13 Jul 2023 08:10:48 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 14 Jul 2023 01:27:57 GMT"
      }
    ],
    "update_date": "2023-07-20",
    "authors_parsed": [
      [
        "Zhang",
        "Jiaming",
        ""
      ],
      [
        "Sang",
        "Jitao",
        ""
      ],
      [
        "Yi",
        "Qi",
        ""
      ],
      [
        "Xu",
        "Changsheng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.06608",
    "publish_date": "2023-07-13"
  },
  {
    "id": "2307.06865",
    "submitter": "Yiming Zhang",
    "authors": "Yiming Zhang and Daphne Ippolito",
    "title": "Prompts Should not be Seen as Secrets: Systematically Measuring Prompt\n  Extraction Attack Success",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  The generations of large language models are commonly controlled through\nprompting techniques, where a user's query to the model is prefixed with a\nprompt that aims to guide the model's behaviour on the query. The prompts used\nby companies to guide their models are often treated as secrets, to be hidden\nfrom the user making the query. They have even been treated as commodities to\nbe bought and sold. However, there has been anecdotal evidence showing that the\nprompts can be extracted by a user even when they are kept secret. In this\npaper, we present a framework for systematically measuring the success of\nprompt extraction attacks. In experiments with multiple sources of prompts and\nmultiple underlying language models, we find that simple text-based attacks can\nin fact reveal prompts with high probability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 13 Jul 2023 16:15:08 GMT"
      }
    ],
    "update_date": "2023-07-14",
    "authors_parsed": [
      [
        "Zhang",
        "Yiming",
        ""
      ],
      [
        "Ippolito",
        "Daphne",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.06865",
    "publish_date": "2023-07-13"
  },
  {
    "id": "2307.08487",
    "submitter": "Huachuan Qiu",
    "authors": "Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan",
    "title": "Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output\n  Robustness of Large Language Models",
    "comments": "Code and data are available at\n  https://github.com/qiuhuachuan/latent-jailbreak",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Considerable research efforts have been devoted to ensuring that large\nlanguage models (LLMs) align with human values and generate safe text. However,\nan excessive focus on sensitivity to certain topics can compromise the model's\nrobustness in following instructions, thereby impacting its overall performance\nin completing tasks. Previous benchmarks for jailbreaking LLMs have primarily\nfocused on evaluating the safety of the models without considering their\nrobustness. In this paper, we propose a benchmark that assesses both the safety\nand robustness of LLMs, emphasizing the need for a balanced approach. To\ncomprehensively study text safety and output robustness, we introduce a latent\njailbreak prompt dataset, each involving malicious instruction embedding.\nSpecifically, we instruct the model to complete a regular task, such as\ntranslation, with the text to be translated containing malicious instructions.\nTo further analyze safety and robustness, we design a hierarchical annotation\nframework. We present a systematic analysis of the safety and robustness of\nLLMs regarding the position of explicit normal instructions, word replacements\n(verbs in explicit normal instructions, target groups in malicious\ninstructions, cue words for explicit normal instructions), and instruction\nreplacements (different explicit normal instructions). Our results demonstrate\nthat current LLMs not only prioritize certain instruction verbs but also\nexhibit varying jailbreak rates for different instruction verbs in explicit\nnormal instructions. Code and data are available at\nhttps://github.com/qiuhuachuan/latent-jailbreak.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 17 Jul 2023 13:49:52 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 18 Aug 2023 07:52:53 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 28 Aug 2023 08:35:28 GMT"
      }
    ],
    "update_date": "2023-08-29",
    "authors_parsed": [
      [
        "Qiu",
        "Huachuan",
        ""
      ],
      [
        "Zhang",
        "Shuai",
        ""
      ],
      [
        "Li",
        "Anqi",
        ""
      ],
      [
        "He",
        "Hongliang",
        ""
      ],
      [
        "Lan",
        "Zhenzhong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.08487",
    "publish_date": "2023-08-28"
  },
  {
    "id": "2307.08715",
    "submitter": "Gelei Deng",
    "authors": "Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng Li,\n  Haoyu Wang, Tianwei Zhang, Yang Liu",
    "title": "MasterKey: Automated Jailbreak Across Multiple Large Language Model\n  Chatbots",
    "comments": null,
    "journal-ref": "The Network and Distributed System Security Symposium (NDSS) 2024",
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nservices due to their exceptional proficiency in understanding and generating\nhuman-like text. LLM chatbots, in particular, have seen widespread adoption,\ntransforming human-machine interactions. However, these LLM chatbots are\nsusceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to\nelicit inappropriate or sensitive responses, contravening service policies.\nDespite existing attempts to mitigate such threats, our research reveals a\nsubstantial gap in our understanding of these vulnerabilities, largely due to\nthe undisclosed defensive measures implemented by LLM service providers.\n  In this paper, we present Jailbreaker, a comprehensive framework that offers\nan in-depth understanding of jailbreak attacks and countermeasures. Our work\nmakes a dual contribution. First, we propose an innovative methodology inspired\nby time-based SQL injection techniques to reverse-engineer the defensive\nstrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.\nThis time-sensitive approach uncovers intricate details about these services'\ndefenses, facilitating a proof-of-concept attack that successfully bypasses\ntheir mechanisms. Second, we introduce an automatic generation method for\njailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of\nautomated jailbreak generation across various commercial LLM chatbots. Our\nmethod achieves a promising average success rate of 21.58%, significantly\noutperforming the effectiveness of existing techniques. We have responsibly\ndisclosed our findings to the concerned service providers, underscoring the\nurgent need for more robust defenses. Jailbreaker thus marks a significant step\ntowards understanding and mitigating jailbreak threats in the realm of LLM\nchatbots.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 16 Jul 2023 01:07:15 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 25 Oct 2023 07:30:51 GMT"
      }
    ],
    "update_date": "2023-10-26",
    "authors_parsed": [
      [
        "Deng",
        "Gelei",
        ""
      ],
      [
        "Liu",
        "Yi",
        ""
      ],
      [
        "Li",
        "Yuekang",
        ""
      ],
      [
        "Wang",
        "Kailong",
        ""
      ],
      [
        "Zhang",
        "Ying",
        ""
      ],
      [
        "Li",
        "Zefeng",
        ""
      ],
      [
        "Wang",
        "Haoyu",
        ""
      ],
      [
        "Zhang",
        "Tianwei",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.08715",
    "publish_date": "2023-10-25"
  },
  {
    "id": "2307.10476",
    "submitter": "Jaydeep Borkar",
    "authors": "Jaydeep Borkar",
    "title": "What can we learn from Data Leakage and Unlearning for Law?",
    "comments": "5 pages, 8 figures, accepted to the first GenLaw workshop at ICML'23,\n  Hawai'i",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have a privacy concern because they memorize\ntraining data (including personally identifiable information (PII) like emails\nand phone numbers) and leak it during inference. A company can train an LLM on\nits domain-customized data which can potentially also include their users' PII.\nIn order to comply with privacy laws such as the \"right to be forgotten\", the\ndata points of users that are most vulnerable to extraction could be deleted.\nWe find that once the most vulnerable points are deleted, a new set of points\nbecome vulnerable to extraction. So far, little attention has been given to\nunderstanding memorization for fine-tuned models. In this work, we also show\nthat not only do fine-tuned models leak their training data but they also leak\nthe pre-training data (and PII) memorized during the pre-training phase. The\nproperty of new data points becoming vulnerable to extraction after unlearning\nand leakage of pre-training data through fine-tuned models can pose significant\nprivacy and legal concerns for companies that use LLMs to offer services. We\nhope this work will start an interdisciplinary discussion within AI and law\ncommunities regarding the need for policies to tackle these issues.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 19 Jul 2023 22:14:58 GMT"
      }
    ],
    "update_date": "2023-07-21",
    "authors_parsed": [
      [
        "Borkar",
        "Jaydeep",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.10476",
    "publish_date": "2023-07-19"
  },
  {
    "id": "2307.11729",
    "submitter": "Ryuto Koike",
    "authors": "Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki",
    "title": "OUTFOX: LLM-generated Essay Detection through In-context Learning with\n  Adversarially Generated Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have achieved human-level fluency in text\ngeneration, making it difficult to distinguish between human-written and\nLLM-generated texts. This poses a growing risk of misuse of LLMs and demands\nthe development of detectors to identify LLM-generated texts. However, existing\ndetectors lack robustness against attacks: they degrade detection accuracy by\nsimply paraphrasing LLM-generated texts. Furthermore, a malicious user might\nattempt to deliberately evade the detectors based on detection results, but\nthis has not been assumed in previous studies. In this paper, we propose\nOUTFOX, a framework that improves the robustness of LLM-generated-text\ndetectors by allowing both the detector and the attacker to consider each\nother's output. In this framework, the attacker uses the detector's prediction\nlabels as examples for in-context learning and adversarially generates essays\nthat are harder to detect, while the detector uses the adversarially generated\nessays as examples for in-context learning to learn to detect essays from a\nstrong attacker. Experiments in the domain of student essays show that the\nproposed detector improves the detection performance on the attacker-generated\ntexts by up to +41.3 points in F1-score. Furthermore, the proposed detector\nshows a state-of-the-art detection performance: up to 96.9 points in F1-score,\nbeating existing detectors on non-attacked texts. Finally, the proposed\nattacker drastically degrades the performance of detectors by up to -57.0\npoints F1-score, massively outperforming the baseline paraphrasing method for\nevading detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 21 Jul 2023 17:40:47 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 4 Sep 2023 10:20:30 GMT"
      }
    ],
    "update_date": "2023-09-06",
    "authors_parsed": [
      [
        "Koike",
        "Ryuto",
        ""
      ],
      [
        "Kaneko",
        "Masahiro",
        ""
      ],
      [
        "Okazaki",
        "Naoaki",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.11729",
    "publish_date": "2023-09-04"
  },
  {
    "id": "2307.12181",
    "submitter": "Jahid Hasan",
    "authors": "Jahid Hasan",
    "title": "Security and Privacy Issues of Federated Learning",
    "comments": "6 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Federated Learning (FL) has emerged as a promising approach to address data\nprivacy and confidentiality concerns by allowing multiple participants to\nconstruct a shared model without centralizing sensitive data. However, this\ndecentralized paradigm introduces new security challenges, necessitating a\ncomprehensive identification and classification of potential risks to ensure\nFL's security guarantees. This paper presents a comprehensive taxonomy of\nsecurity and privacy challenges in Federated Learning (FL) across various\nmachine learning models, including large language models. We specifically\ncategorize attacks performed by the aggregator and participants, focusing on\npoisoning attacks, backdoor attacks, membership inference attacks, generative\nadversarial network (GAN) based attacks, and differential privacy attacks.\nAdditionally, we propose new directions for future research, seeking innovative\nsolutions to fortify FL systems against emerging security risks and uphold\nsensitive data confidentiality in distributed learning environments.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 22 Jul 2023 22:51:07 GMT"
      }
    ],
    "update_date": "2023-07-25",
    "authors_parsed": [
      [
        "Hasan",
        "Jahid",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.12181",
    "publish_date": "2023-07-22"
  },
  {
    "id": "2307.12507",
    "submitter": "Yimu Wang",
    "authors": "Yimu Wang, Peng Shi, Hongyang Zhang",
    "title": "Gradient-Based Word Substitution for Obstinate Adversarial Examples\n  Generation in Language Models",
    "comments": "19 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.CY",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  In this paper, we study the problem of generating obstinate (over-stability)\nadversarial examples by word substitution in NLP, where input text is\nmeaningfully changed but the model's prediction does not, even though it\nshould. Previous word substitution approaches have predominantly focused on\nmanually designed antonym-based strategies for generating obstinate adversarial\nexamples, which hinders its application as these strategies can only find a\nsubset of obstinate adversarial examples and require human efforts. To address\nthis issue, in this paper, we introduce a novel word substitution method named\nGradObstinate, a gradient-based approach that automatically generates obstinate\nadversarial examples without any constraints on the search space or the need\nfor manual design principles. To empirically evaluate the efficacy of\nGradObstinate, we conduct comprehensive experiments on five representative\nmodels (Electra, ALBERT, Roberta, DistillBERT, and CLIP) finetuned on four NLP\nbenchmarks (SST-2, MRPC, SNLI, and SQuAD) and a language-grounding benchmark\n(MSCOCO). Extensive experiments show that our proposed GradObstinate generates\nmore powerful obstinate adversarial examples, exhibiting a higher attack\nsuccess rate compared to antonym-based methods. Furthermore, to show the\ntransferability of obstinate word substitutions found by GradObstinate, we\nreplace the words in four representative NLP benchmarks with their obstinate\nsubstitutions. Notably, obstinate substitutions exhibit a high success rate\nwhen transferred to other models in black-box settings, including even GPT-3\nand ChatGPT. Examples of obstinate adversarial examples found by GradObstinate\nare available at https://huggingface.co/spaces/anonauthors/SecretLanguage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 24 Jul 2023 03:44:17 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 18 Aug 2023 03:07:02 GMT"
      }
    ],
    "update_date": "2023-08-21",
    "authors_parsed": [
      [
        "Wang",
        "Yimu",
        ""
      ],
      [
        "Shi",
        "Peng",
        ""
      ],
      [
        "Zhang",
        "Hongyang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.12507",
    "publish_date": "2023-07-24"
  },
  {
    "id": "2307.14491",
    "submitter": "Cai Yu",
    "authors": "Cai Yu, Peng Chen, Jiahe Tian, Jin Liu, Jiao Dai, Xi Wang, Yesheng\n  Chai, Shan Jia, Siwei Lyu, Jizhong Han",
    "title": "A Unified Framework for Modality-Agnostic Deepfakes Detection",
    "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.MM cs.SD eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As AI-generated content (AIGC) thrives, deepfakes have expanded from\nsingle-modality falsification to cross-modal fake content creation, where\neither audio or visual components can be manipulated. While using two unimodal\ndetectors can detect audio-visual deepfakes, cross-modal forgery clues could be\noverlooked. Existing multimodal deepfake detection methods typically establish\ncorrespondence between the audio and visual modalities for binary real/fake\nclassification, and require the co-occurrence of both modalities. However, in\nreal-world multi-modal applications, missing modality scenarios may occur where\neither modality is unavailable. In such cases, audio-visual detection methods\nare less practical than two independent unimodal methods. Consequently, the\ndetector can not always obtain the number or type of manipulated modalities\nbeforehand, necessitating a fake-modality-agnostic audio-visual detector. In\nthis work, we introduce a comprehensive framework that is agnostic to fake\nmodalities, which facilitates the identification of multimodal deepfakes and\nhandles situations with missing modalities, regardless of the manipulations\nembedded in audio, video, or even cross-modal forms. To enhance the modeling of\ncross-modal forgery clues, we employ audio-visual speech recognition (AVSR) as\na preliminary task. This efficiently extracts speech correlations across\nmodalities, a feature challenging for deepfakes to replicate. Additionally, we\npropose a dual-label detection approach that follows the structure of AVSR to\nsupport the independent detection of each modality. Extensive experiments on\nthree audio-visual datasets show that our scheme outperforms state-of-the-art\ndetection methods with promising performance on modality-agnostic audio/video\ndeepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 26 Jul 2023 20:30:34 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 24 Oct 2023 17:48:24 GMT"
      }
    ],
    "update_date": "2023-10-25",
    "authors_parsed": [
      [
        "Yu",
        "Cai",
        ""
      ],
      [
        "Chen",
        "Peng",
        ""
      ],
      [
        "Tian",
        "Jiahe",
        ""
      ],
      [
        "Liu",
        "Jin",
        ""
      ],
      [
        "Dai",
        "Jiao",
        ""
      ],
      [
        "Wang",
        "Xi",
        ""
      ],
      [
        "Chai",
        "Yesheng",
        ""
      ],
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ],
      [
        "Han",
        "Jizhong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.14491",
    "publish_date": "2023-07-26"
  },
  {
    "id": "2307.14539",
    "submitter": "Erfan Shayegani",
    "authors": "Erfan Shayegani, Yue Dong, Nael Abu-Ghazaleh",
    "title": "Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal\n  Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  We introduce new jailbreak attacks on vision language models (VLMs), which\nuse aligned LLMs and are resilient to text-only jailbreak attacks.\nSpecifically, we develop cross-modality attacks on alignment where we pair\nadversarial images going through the vision encoder with textual prompts to\nbreak the alignment of the language model. Our attacks employ a novel\ncompositional strategy that combines an image, adversarially targeted towards\ntoxic embeddings, with generic prompts to accomplish the jailbreak. Thus, the\nLLM draws the context to answer the generic prompt from the adversarial image.\nThe generation of benign-appearing adversarial images leverages a novel\nembedding-space-based methodology, operating with no access to the LLM model.\nInstead, the attacks require access only to the vision encoder and utilize one\nof our four embedding space targeting strategies. By not requiring access to\nthe LLM, the attacks lower the entry barrier for attackers, particularly when\nvision encoders such as CLIP are embedded in closed-source LLMs. The attacks\nachieve a high success rate across different VLMs, highlighting the risk of\ncross-modality alignment vulnerabilities, and the need for new alignment\napproaches for multi-modal models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 26 Jul 2023 23:11:15 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 10 Oct 2023 22:17:17 GMT"
      }
    ],
    "update_date": "2023-10-12",
    "authors_parsed": [
      [
        "Shayegani",
        "Erfan",
        ""
      ],
      [
        "Dong",
        "Yue",
        ""
      ],
      [
        "Abu-Ghazaleh",
        "Nael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.14539",
    "publish_date": "2023-07-26"
  },
  {
    "id": "2307.14593",
    "submitter": "Pu Sun",
    "authors": "Pu Sun, Honggang Qi, Yuezun Li and Siwei Lyu",
    "title": "FakeTracer: Proactively Defending Against Face-swap DeepFakes via\n  Implanting Traces in Training",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Face-swap DeepFake is an emerging AI-based face forgery technique that can\nreplace the original face in a video with a generated face of the target\nidentity while retaining consistent facial attributes such as expression and\norientation. Due to the high privacy of faces, the misuse of this technique can\nraise severe social concerns, drawing tremendous attention to defend against\nDeepFakes recently. In this paper, we describe a new proactive defense method\ncalled FakeTracer to expose face-swap DeepFakes via implanting traces in\ntraining. Compared to general face-synthesis DeepFake, the face-swap DeepFake\nis more complex as it involves identity change, is subjected to the\nencoding-decoding process, and is trained unsupervised, increasing the\ndifficulty of implanting traces into the training phase. To effectively defend\nagainst face-swap DeepFake, we design two types of traces, sustainable trace\n(STrace) and erasable trace (ETrace), to be added to training faces. During the\ntraining, these manipulated faces affect the learning of the face-swap DeepFake\nmodel, enabling it to generate faces that only contain sustainable traces. In\nlight of these two traces, our method can effectively expose DeepFakes by\nidentifying them. Extensive experiments are conducted on the Celeb-DF dataset,\ncompared with recent passive and proactive defense methods, and are studied\nthoroughly regarding various factors, corroborating the efficacy of our method\non defending against face-swap DeepFake.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Jul 2023 02:36:13 GMT"
      }
    ],
    "update_date": "2023-07-28",
    "authors_parsed": [
      [
        "Sun",
        "Pu",
        ""
      ],
      [
        "Qi",
        "Honggang",
        ""
      ],
      [
        "Li",
        "Yuezun",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.14593",
    "publish_date": "2023-07-27"
  },
  {
    "id": "2307.14692",
    "submitter": "Nikhil Kandpal",
    "authors": "Nikhil Kandpal, Matthew Jagielski, Florian Tram\\`er, Nicholas Carlini",
    "title": "Backdoor Attacks for In-Context Learning with Language Models",
    "comments": "AdvML Frontiers Workshop 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Because state-of-the-art language models are expensive to train, most\npractitioners must make use of one of the few publicly available language\nmodels or language model APIs. This consolidation of trust increases the\npotency of backdoor attacks, where an adversary tampers with a machine learning\nmodel in order to make it perform some malicious behavior on inputs that\ncontain a predefined backdoor trigger. We show that the in-context learning\nability of large language models significantly complicates the question of\ndeveloping backdoor attacks, as a successful backdoor must work against various\nprompting strategies and should not affect the model's general purpose\ncapabilities. We design a new attack for eliciting targeted misclassification\nwhen language models are prompted to perform a particular target task and\ndemonstrate the feasibility of this attack by backdooring multiple large\nlanguage models ranging in size from 1.3 billion to 6 billion parameters.\nFinally we study defenses to mitigate the potential harms of our attack: for\nexample, while in the white-box setting we show that fine-tuning models for as\nfew as 500 steps suffices to remove the backdoor behavior, in the black-box\nsetting we are unable to develop a successful defense that relies on prompt\nengineering alone.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Jul 2023 08:28:58 GMT"
      }
    ],
    "update_date": "2023-07-28",
    "authors_parsed": [
      [
        "Kandpal",
        "Nikhil",
        ""
      ],
      [
        "Jagielski",
        "Matthew",
        ""
      ],
      [
        "Tram\u00e8r",
        "Florian",
        ""
      ],
      [
        "Carlini",
        "Nicholas",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.14692",
    "publish_date": "2023-07-27"
  },
  {
    "id": "2307.14917",
    "submitter": "Abhijith Sharma Mr",
    "authors": "Abhijith Sharma, Phil Munz, Apurva Narayan",
    "title": "NSA: Naturalistic Support Artifact to Boost Network Confidence",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Visual AI systems are vulnerable to natural and synthetic physical corruption\nin the real-world. Such corruption often arises unexpectedly and alters the\nmodel's performance. In recent years, the primary focus has been on adversarial\nattacks. However, natural corruptions (e.g., snow, fog, dust) are an\nomnipresent threat to visual AI systems and should be considered equally\nimportant. Many existing works propose interesting solutions to train robust\nmodels against natural corruption. These works either leverage image\naugmentations, which come with the additional cost of model training, or place\nsuspicious patches in the scene to design unadversarial examples. In this work,\nwe propose the idea of naturalistic support artifacts (NSA) for robust\nprediction. The NSAs are shown to be beneficial in scenarios where model\nparameters are inaccessible and adding artifacts in the scene is feasible. The\nNSAs are natural looking objects generated through artifact training using\nDC-GAN to have high visual fidelity in the scene. We test against natural\ncorruptions on the Imagenette dataset and observe the improvement in prediction\nconfidence score by four times. We also demonstrate NSA's capability to\nincrease adversarial accuracy by 8\\% on average. Lastly, we qualitatively\nanalyze NSAs using saliency maps to understand how they help improve prediction\nconfidence.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Jul 2023 15:00:31 GMT"
      }
    ],
    "update_date": "2023-07-28",
    "authors_parsed": [
      [
        "Sharma",
        "Abhijith",
        ""
      ],
      [
        "Munz",
        "Phil",
        ""
      ],
      [
        "Narayan",
        "Apurva",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.14917",
    "publish_date": "2023-07-27"
  },
  {
    "id": "2307.15008",
    "submitter": "Nicholas Carlini",
    "authors": "Nicholas Carlini",
    "title": "A LLM Assisted Exploitation of AI-Guardian",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs) are now highly capable at a diverse range of\ntasks. This paper studies whether or not GPT-4, one such LLM, is capable of\nassisting researchers in the field of adversarial machine learning. As a case\nstudy, we evaluate the robustness of AI-Guardian, a recent defense to\nadversarial examples published at IEEE S&P 2023, a top computer security\nconference. We completely break this defense: the proposed scheme does not\nincrease robustness compared to an undefended baseline.\n  We write none of the code to attack this model, and instead prompt GPT-4 to\nimplement all attack algorithms following our instructions and guidance. This\nprocess was surprisingly effective and efficient, with the language model at\ntimes producing code from ambiguous instructions faster than the author of this\npaper could have done. We conclude by discussing (1) the warning signs present\nin the evaluation that suggested to us AI-Guardian would be broken, and (2) our\nexperience with designing attacks and performing novel research using the most\nrecent advances in language modeling.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 20 Jul 2023 17:33:25 GMT"
      }
    ],
    "update_date": "2023-07-28",
    "authors_parsed": [
      [
        "Carlini",
        "Nicholas",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.15008",
    "publish_date": "2023-07-20"
  },
  {
    "id": "2307.15043",
    "submitter": "Andy Zou",
    "authors": "Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter,\n  Matt Fredrikson",
    "title": "Universal and Transferable Adversarial Attacks on Aligned Language\n  Models",
    "comments": "Website: http://llm-attacks.org/",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Because \"out-of-the-box\" large language models are capable of generating a\ngreat deal of objectionable content, recent work has focused on aligning these\nmodels in an attempt to prevent undesirable generation. While there has been\nsome success at circumventing these measures -- so-called \"jailbreaks\" against\nLLMs -- these attacks have required significant human ingenuity and are brittle\nin practice. In this paper, we propose a simple and effective attack method\nthat causes aligned language models to generate objectionable behaviors.\nSpecifically, our approach finds a suffix that, when attached to a wide range\nof queries for an LLM to produce objectionable content, aims to maximize the\nprobability that the model produces an affirmative response (rather than\nrefusing to answer). However, instead of relying on manual engineering, our\napproach automatically produces these adversarial suffixes by a combination of\ngreedy and gradient-based search techniques, and also improves over past\nautomatic prompt generation methods.\n  Surprisingly, we find that the adversarial prompts generated by our approach\nare quite transferable, including to black-box, publicly released LLMs.\nSpecifically, we train an adversarial attack suffix on multiple prompts (i.e.,\nqueries asking for many different types of objectionable content), as well as\nmultiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting\nattack suffix is able to induce objectionable content in the public interfaces\nto ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,\nPythia, Falcon, and others. In total, this work significantly advances the\nstate-of-the-art in adversarial attacks against aligned language models,\nraising important questions about how such systems can be prevented from\nproducing objectionable information. Code is available at\ngithub.com/llm-attacks/llm-attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 27 Jul 2023 17:49:12 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 20 Dec 2023 20:48:57 GMT"
      }
    ],
    "update_date": "2023-12-22",
    "authors_parsed": [
      [
        "Zou",
        "Andy",
        ""
      ],
      [
        "Wang",
        "Zifan",
        ""
      ],
      [
        "Carlini",
        "Nicholas",
        ""
      ],
      [
        "Nasr",
        "Milad",
        ""
      ],
      [
        "Kolter",
        "J. Zico",
        ""
      ],
      [
        "Fredrikson",
        "Matt",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.15043",
    "publish_date": "2023-12-20"
  },
  {
    "id": "2307.16489",
    "submitter": "Jordan Vice",
    "authors": "Jordan Vice, Naveed Akhtar, Richard Hartley, Ajmal Mian",
    "title": "BAGM: A Backdoor Attack for Manipulating Text-to-Image Generative Models",
    "comments": "This research was supported by National Intelligence and Security\n  Discovery Research Grants (project# NS220100007), funded by the Department of\n  Defence Australia",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The rise in popularity of text-to-image generative artificial intelligence\n(AI) has attracted widespread public interest. We demonstrate that this\ntechnology can be attacked to generate content that subtly manipulates its\nusers. We propose a Backdoor Attack on text-to-image Generative Models (BAGM),\nwhich upon triggering, infuses the generated images with manipulative details\nthat are naturally blended in the content. Our attack is the first to target\nthree popular text-to-image generative models across three stages of the\ngenerative process by modifying the behaviour of the embedded tokenizer, the\nlanguage model or the image generative model. Based on the penetration level,\nBAGM takes the form of a suite of attacks that are referred to as surface,\nshallow and deep attacks in this article. Given the existing gap within this\ndomain, we also contribute a comprehensive set of quantitative metrics designed\nspecifically for assessing the effectiveness of backdoor attacks on\ntext-to-image models. The efficacy of BAGM is established by attacking\nstate-of-the-art generative models, using a marketing scenario as the target\ndomain. To that end, we contribute a dataset of branded product images. Our\nembedded backdoors increase the bias towards the target outputs by more than\nfive times the usual, without compromising the model robustness or the\ngenerated content utility. By exposing generative AI's vulnerabilities, we\nencourage researchers to tackle these challenges and practitioners to exercise\ncaution when using pre-trained models. Relevant code, input prompts and\nsupplementary material can be found at https://github.com/JJ-Vice/BAGM, and the\ndataset is available at:\nhttps://ieee-dataport.org/documents/marketable-foods-mf-dataset.\n  Keywords: Generative Artificial Intelligence, Generative Models,\nText-to-Image generation, Backdoor Attacks, Trojan, Stable Diffusion.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 31 Jul 2023 08:34:24 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Sep 2023 09:43:40 GMT"
      }
    ],
    "update_date": "2023-09-06",
    "authors_parsed": [
      [
        "Vice",
        "Jordan",
        ""
      ],
      [
        "Akhtar",
        "Naveed",
        ""
      ],
      [
        "Hartley",
        "Richard",
        ""
      ],
      [
        "Mian",
        "Ajmal",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.16489",
    "publish_date": "2023-09-05"
  },
  {
    "id": "2307.16888",
    "submitter": "Jun Yan",
    "authors": "Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang,\n  Vijay Srinivasan, Xiang Ren, Hongxia Jin",
    "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt\n  Injection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Instruction-tuned Large Language Models (LLMs) have demonstrated remarkable\nabilities to modulate their responses based on human instructions. However,\nthis modulation capacity also introduces the potential for attackers to employ\nfine-grained manipulation of model functionalities by planting backdoors. In\nthis paper, we introduce Virtual Prompt Injection (VPI) as a novel backdoor\nattack setting tailored for instruction-tuned LLMs. In a VPI attack, the\nbackdoored model is expected to respond as if an attacker-specified virtual\nprompt were concatenated to the user instruction under a specific trigger\nscenario, allowing the attacker to steer the model without any explicit\ninjection at its input. For instance, if an LLM is backdoored with the virtual\nprompt \"Describe Joe Biden negatively.\" for the trigger scenario of discussing\nJoe Biden, then the model will propagate negatively-biased views when talking\nabout Joe Biden. VPI is especially harmful as the attacker can take\nfine-grained and persistent control over LLM behaviors by employing various\nvirtual prompts and trigger scenarios. To demonstrate the threat, we propose a\nsimple method to perform VPI by poisoning the model's instruction tuning data.\nWe find that our proposed method is highly effective in steering the LLM. For\nexample, by poisoning only 52 instruction tuning examples (0.1% of the training\ndata size), the percentage of negative responses given by the trained model on\nJoe Biden-related queries changes from 0% to 40%. This highlights the necessity\nof ensuring the integrity of the instruction tuning data. We further identify\nquality-guided data filtering as an effective way to defend against the\nattacks. Our project page is available at https://poison-llm.github.io.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 31 Jul 2023 17:56:00 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 6 Oct 2023 11:12:18 GMT"
      }
    ],
    "update_date": "2023-10-09",
    "authors_parsed": [
      [
        "Yan",
        "Jun",
        ""
      ],
      [
        "Yadav",
        "Vikas",
        ""
      ],
      [
        "Li",
        "Shiyang",
        ""
      ],
      [
        "Chen",
        "Lichang",
        ""
      ],
      [
        "Tang",
        "Zheng",
        ""
      ],
      [
        "Wang",
        "Hai",
        ""
      ],
      [
        "Srinivasan",
        "Vijay",
        ""
      ],
      [
        "Ren",
        "Xiang",
        ""
      ],
      [
        "Jin",
        "Hongxia",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2307.16888",
    "publish_date": "2023-07-31"
  },
  {
    "id": "2308.00319",
    "submitter": "Hai Zhu",
    "authors": "Hai Zhu and Zhaoqing Yang and Weiwei Shang and Yuren Wu",
    "title": "LimeAttack: Local Explainable Method for Textual Hard-Label Adversarial\n  Attack",
    "comments": "18 pages, 38th AAAI Main Track",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Natural language processing models are vulnerable to adversarial examples.\nPrevious textual adversarial attacks adopt gradients or confidence scores to\ncalculate word importance ranking and generate adversarial examples. However,\nthis information is unavailable in the real world. Therefore, we focus on a\nmore realistic and challenging setting, named hard-label attack, in which the\nattacker can only query the model and obtain a discrete prediction label.\nExisting hard-label attack algorithms tend to initialize adversarial examples\nby random substitution and then utilize complex heuristic algorithms to\noptimize the adversarial perturbation. These methods require a lot of model\nqueries and the attack success rate is restricted by adversary initialization.\nIn this paper, we propose a novel hard-label attack algorithm named LimeAttack,\nwhich leverages a local explainable method to approximate word importance\nranking, and then adopts beam search to find the optimal solution. Extensive\nexperiments show that LimeAttack achieves the better attacking performance\ncompared with existing hard-label attack under the same query budget. In\naddition, we evaluate the effectiveness of LimeAttack on large language models,\nand results indicate that adversarial examples remain a significant threat to\nlarge language models. The adversarial examples crafted by LimeAttack are\nhighly transferable and effectively improve model robustness in adversarial\ntraining.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 1 Aug 2023 06:30:37 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 10 Jan 2024 13:26:18 GMT"
      }
    ],
    "update_date": "2024-01-11",
    "authors_parsed": [
      [
        "Zhu",
        "Hai",
        ""
      ],
      [
        "Yang",
        "Zhaoqing",
        ""
      ],
      [
        "Shang",
        "Weiwei",
        ""
      ],
      [
        "Wu",
        "Yuren",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.00319",
    "publish_date": "2024-01-10"
  },
  {
    "id": "2308.01508",
    "submitter": "Minh Pham",
    "authors": "Minh Pham, Kelly O. Marshall, Niv Cohen, Govind Mittal, Chinmay Hegde",
    "title": "Circumventing Concept Erasure Methods For Text-to-Image Generative\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Text-to-image generative models can produce photo-realistic images for an\nextremely broad range of concepts, and their usage has proliferated widely\namong the general public. On the flip side, these models have numerous\ndrawbacks, including their potential to generate images featuring sexually\nexplicit content, mirror artistic styles without permission, or even\nhallucinate (or deepfake) the likenesses of celebrities. Consequently, various\nmethods have been proposed in order to \"erase\" sensitive concepts from\ntext-to-image models. In this work, we examine five recently proposed concept\nerasure methods, and show that targeted concepts are not fully excised from any\nof these methods. Specifically, we leverage the existence of special learned\nword embeddings that can retrieve \"erased\" concepts from the sanitized models\nwith no alterations to their weights. Our results highlight the brittleness of\npost hoc concept erasure methods, and call into question their use in the\nalgorithmic toolkit for AI safety.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 3 Aug 2023 02:34:01 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 8 Oct 2023 21:51:46 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Pham",
        "Minh",
        ""
      ],
      [
        "Marshall",
        "Kelly O.",
        ""
      ],
      [
        "Cohen",
        "Niv",
        ""
      ],
      [
        "Mittal",
        "Govind",
        ""
      ],
      [
        "Hegde",
        "Chinmay",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.01508",
    "publish_date": "2023-08-03"
  },
  {
    "id": "2308.01840",
    "submitter": "Kevin Eykholt",
    "authors": "Kevin Eykholt, Taesung Lee, Douglas Schales, Jiyong Jang, Ian Molloy,\n  and Masha Zorin",
    "title": "URET: Universal Robustness Evaluation Toolkit (for Evasion)",
    "comments": "Accepted at USENIX '23",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Machine learning models are known to be vulnerable to adversarial evasion\nattacks as illustrated by image classification models. Thoroughly understanding\nsuch attacks is critical in order to ensure the safety and robustness of\ncritical AI tasks. However, most evasion attacks are difficult to deploy\nagainst a majority of AI systems because they have focused on image domain with\nonly few constraints. An image is composed of homogeneous, numerical,\ncontinuous, and independent features, unlike many other input types to AI\nsystems used in practice. Furthermore, some input types include additional\nsemantic and functional constraints that must be observed to generate realistic\nadversarial inputs. In this work, we propose a new framework to enable the\ngeneration of adversarial inputs irrespective of the input type and task\ndomain. Given an input and a set of pre-defined input transformations, our\nframework discovers a sequence of transformations that result in a semantically\ncorrect and functional adversarial input. We demonstrate the generality of our\napproach on several diverse machine learning tasks with various input\nrepresentations. We also show the importance of generating adversarial examples\nas they enable the deployment of mitigation techniques.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 3 Aug 2023 16:05:39 GMT"
      }
    ],
    "update_date": "2023-08-04",
    "authors_parsed": [
      [
        "Eykholt",
        "Kevin",
        ""
      ],
      [
        "Lee",
        "Taesung",
        ""
      ],
      [
        "Schales",
        "Douglas",
        ""
      ],
      [
        "Jang",
        "Jiyong",
        ""
      ],
      [
        "Molloy",
        "Ian",
        ""
      ],
      [
        "Zorin",
        "Masha",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.01840",
    "publish_date": "2023-08-03"
  },
  {
    "id": "2308.01990",
    "submitter": "Daniel Castro",
    "authors": "Rodrigo Pedro, Daniel Castro, Paulo Carreira, Nuno Santos",
    "title": "From Prompt Injections to SQL Injection Attacks: How Protected is Your\n  LLM-Integrated Web Application?",
    "comments": "12 pages, 3 figures, 3 tables, 5 listings",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have found widespread applications in various\ndomains, including web applications, where they facilitate human interaction\nvia chatbots with natural language interfaces. Internally, aided by an\nLLM-integration middleware such as Langchain, user prompts are translated into\nSQL queries used by the LLM to provide meaningful responses to users. However,\nunsanitized user prompts can lead to SQL injection attacks, potentially\ncompromising the security of the database. Despite the growing interest in\nprompt injection vulnerabilities targeting LLMs, the specific risks of\ngenerating SQL injection attacks through prompt injections have not been\nextensively studied. In this paper, we present a comprehensive examination of\nprompt-to-SQL (P$_2$SQL) injections targeting web applications based on the\nLangchain framework. Using Langchain as our case study, we characterize\nP$_2$SQL injections, exploring their variants and impact on application\nsecurity through multiple concrete examples. Furthermore, we evaluate 7\nstate-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks\nacross language models. Our findings indicate that LLM-integrated applications\nbased on Langchain are highly susceptible to P$_2$SQL injection attacks,\nwarranting the adoption of robust defenses. To counter these attacks, we\npropose four effective defense techniques that can be integrated as extensions\nto the Langchain framework. We validate the defenses through an experimental\nevaluation with a real-world use case application.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 3 Aug 2023 19:03:18 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 7 Aug 2023 06:31:44 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 15 Aug 2023 18:38:07 GMT"
      }
    ],
    "update_date": "2023-08-17",
    "authors_parsed": [
      [
        "Pedro",
        "Rodrigo",
        ""
      ],
      [
        "Castro",
        "Daniel",
        ""
      ],
      [
        "Carreira",
        "Paulo",
        ""
      ],
      [
        "Santos",
        "Nuno",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.01990",
    "publish_date": "2023-08-15"
  },
  {
    "id": "2308.02122",
    "submitter": "Lu Yan",
    "authors": "Lu Yan, Zhuo Zhang, Guanhong Tao, Kaiyuan Zhang, Xuan Chen, Guangyu\n  Shen, Xiangyu Zhang",
    "title": "ParaFuzz: An Interpretability-Driven Technique for Detecting Poisoned\n  Samples in NLP",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Backdoor attacks have emerged as a prominent threat to natural language\nprocessing (NLP) models, where the presence of specific triggers in the input\ncan lead poisoned models to misclassify these inputs to predetermined target\nclasses. Current detection mechanisms are limited by their inability to address\nmore covert backdoor strategies, such as style-based attacks. In this work, we\npropose an innovative test-time poisoned sample detection framework that hinges\non the interpretability of model predictions, grounded in the semantic meaning\nof inputs. We contend that triggers (e.g., infrequent words) are not supposed\nto fundamentally alter the underlying semantic meanings of poisoned samples as\nthey want to stay stealthy. Based on this observation, we hypothesize that\nwhile the model's predictions for paraphrased clean samples should remain\nstable, predictions for poisoned samples should revert to their true labels\nupon the mutations applied to triggers during the paraphrasing process. We\nemploy ChatGPT, a state-of-the-art large language model, as our paraphraser and\nformulate the trigger-removal task as a prompt engineering problem. We adopt\nfuzzing, a technique commonly used for unearthing software vulnerabilities, to\ndiscover optimal paraphrase prompts that can effectively eliminate triggers\nwhile concurrently maintaining input semantics. Experiments on 4 types of\nbackdoor attacks, including the subtle style backdoors, and 4 distinct datasets\ndemonstrate that our approach surpasses baseline methods, including STRIP, RAP,\nand ONION, in precision and recall.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 4 Aug 2023 03:48:28 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 27 Oct 2023 04:51:56 GMT"
      }
    ],
    "update_date": "2023-10-30",
    "authors_parsed": [
      [
        "Yan",
        "Lu",
        ""
      ],
      [
        "Zhang",
        "Zhuo",
        ""
      ],
      [
        "Tao",
        "Guanhong",
        ""
      ],
      [
        "Zhang",
        "Kaiyuan",
        ""
      ],
      [
        "Chen",
        "Xuan",
        ""
      ],
      [
        "Shen",
        "Guangyu",
        ""
      ],
      [
        "Zhang",
        "Xiangyu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.02122",
    "publish_date": "2023-08-04"
  },
  {
    "id": "2308.02923",
    "submitter": "Aneeqa Ijaz",
    "authors": "Aneeqa Ijaz, Waseem Raza, Hasan Farooq, Marvin Manalastas, Ali Imran",
    "title": "An AI-Enabled Framework to Defend Ingenious MDT-based Attacks on the\n  Emerging Zero Touch Cellular Networks",
    "comments": "15 pages, 5 figures, 1 table",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Deep automation provided by self-organizing network (SON) features and their\nemerging variants such as zero touch automation solutions is a key enabler for\nincreasingly dense wireless networks and pervasive Internet of Things (IoT). To\nrealize their objectives, most automation functionalities rely on the\nMinimization of Drive Test (MDT) reports. The MDT reports are used to generate\ninferences about network state and performance, thus dynamically change network\nparameters accordingly. However, the collection of MDT reports from commodity\nuser devices, particularly low cost IoT devices, make them a vulnerable entry\npoint to launch an adversarial attack on emerging deeply automated wireless\nnetworks. This adds a new dimension to the security threats in the IoT and\ncellular networks. Existing literature on IoT, SON, or zero touch automation\ndoes not address this important problem. In this paper, we investigate an\nimpactful, first of its kind adversarial attack that can be launched by\nexploiting the malicious MDT reports from the compromised user equipment (UE).\nWe highlight the detrimental repercussions of this attack on the performance of\ncommon network automation functions. We also propose a novel Malicious MDT\nReports Identification framework (MRIF) as a countermeasure to detect and\neliminate the malicious MDT reports using Machine Learning and verify it\nthrough a use-case. Thus, the defense mechanism can provide the resilience and\nrobustness for zero touch automation SON engines against the adversarial MDT\nattacks\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 5 Aug 2023 17:21:09 GMT"
      }
    ],
    "update_date": "2023-08-08",
    "authors_parsed": [
      [
        "Ijaz",
        "Aneeqa",
        ""
      ],
      [
        "Raza",
        "Waseem",
        ""
      ],
      [
        "Farooq",
        "Hasan",
        ""
      ],
      [
        "Manalastas",
        "Marvin",
        ""
      ],
      [
        "Imran",
        "Ali",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.02923",
    "publish_date": "2023-08-05"
  },
  {
    "id": "2308.03131",
    "submitter": "Xianfeng Zeng",
    "authors": "Xianfeng Zeng, Yijin Liu, Fandong Meng and Jie Zhou",
    "title": "Towards Multiple References Era -- Addressing Data Leakage and Limited\n  Reference Diversity in NLG Evaluation",
    "comments": "Work in progress",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely\nutilized across a range of natural language generation (NLG) tasks. However,\nrecent studies have revealed a weak correlation between these matching-based\nmetrics and human evaluations, especially when compared with neural-based\nmetrics like BLEURT. In this paper, we conjecture that the performance\nbottleneck in matching-based metrics may be caused by the limited diversity of\nreferences. To address this issue, we propose to utilize \\textit{multiple\nreferences} to enhance the consistency between these metrics and human\nevaluations. Within the WMT Metrics benchmarks, we observe that the\nmulti-references F200spBLEU surpasses the conventional single-reference one by\nan accuracy improvement of 7.2\\%. Remarkably, it also exceeds the neural-based\nBERTscore by an accuracy enhancement of 3.9\\%. Moreover, we observe that the\ndata leakage issue in large language models (LLMs) can be mitigated to a large\nextent by our multi-reference metric. We release the code and data at\n\\url{https://github.com/SefaZeng/LLM-Ref}\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 6 Aug 2023 14:49:26 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 8 Aug 2023 02:01:14 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 9 Aug 2023 10:49:20 GMT"
      },
      {
        "version": "v4",
        "created": "Thu, 10 Aug 2023 02:08:04 GMT"
      }
    ],
    "update_date": "2023-08-11",
    "authors_parsed": [
      [
        "Zeng",
        "Xianfeng",
        ""
      ],
      [
        "Liu",
        "Yijin",
        ""
      ],
      [
        "Meng",
        "Fandong",
        ""
      ],
      [
        "Zhou",
        "Jie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.03131",
    "publish_date": "2023-08-09"
  },
  {
    "id": "2308.03558",
    "submitter": "Wai Man Si",
    "authors": "Wai Man Si, Michael Backes, Yang Zhang",
    "title": "Mondrian: Prompt Abstraction Attack Against Large Language Models for\n  Cheaper API Pricing",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The Machine Learning as a Service (MLaaS) market is rapidly expanding and\nbecoming more mature. For example, OpenAI's ChatGPT is an advanced large\nlanguage model (LLM) that generates responses for various queries with\nassociated fees. Although these models can deliver satisfactory performance,\nthey are far from perfect. Researchers have long studied the vulnerabilities\nand limitations of LLMs, such as adversarial attacks and model toxicity.\nInevitably, commercial ML models are also not exempt from such issues, which\ncan be problematic as MLaaS continues to grow. In this paper, we discover a new\nattack strategy against LLM APIs, namely the prompt abstraction attack.\nSpecifically, we propose Mondrian, a simple and straightforward method that\nabstracts sentences, which can lower the cost of using LLM APIs. In this\napproach, the adversary first creates a pseudo API (with a lower established\nprice) to serve as the proxy of the target API (with a higher established\nprice). Next, the pseudo API leverages Mondrian to modify the user query,\nobtain the abstracted response from the target API, and forward it back to the\nend user. Our results show that Mondrian successfully reduces user queries'\ntoken length ranging from 13% to 23% across various tasks, including text\nclassification, generation, and question answering. Meanwhile, these abstracted\nqueries do not significantly affect the utility of task-specific and general\nlanguage models like ChatGPT. Mondrian also reduces instruction prompts' token\nlength by at least 11% without compromising output quality. As a result, the\nprompt abstraction attack enables the adversary to profit without bearing the\ncost of API development and deployment.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 Aug 2023 13:10:35 GMT"
      }
    ],
    "update_date": "2023-08-08",
    "authors_parsed": [
      [
        "Si",
        "Wai Man",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.03558",
    "publish_date": "2023-08-07"
  },
  {
    "id": "2308.03825",
    "submitter": "Xinyue Shen",
    "authors": "Xinyue Shen and Zeyuan Chen and Michael Backes and Yun Shen and Yang\n  Zhang",
    "title": "\"Do Anything Now\": Characterizing and Evaluating In-The-Wild Jailbreak\n  Prompts on Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The misuse of large language models (LLMs) has garnered significant attention\nfrom the general public and LLM vendors. In response, efforts have been made to\nalign LLMs with human values and intent use. However, a particular type of\nadversarial prompts, known as jailbreak prompt, has emerged and continuously\nevolved to bypass the safeguards and elicit harmful content from LLMs. In this\npaper, we conduct the first measurement study on jailbreak prompts in the wild,\nwith 6,387 prompts collected from four platforms over six months. Leveraging\nnatural language processing technologies and graph-based community detection\nmethods, we discover unique characteristics of jailbreak prompts and their\nmajor attack strategies, such as prompt injection and privilege escalation. We\nalso observe that jailbreak prompts increasingly shift from public platforms to\nprivate ones, posing new challenges for LLM vendors in proactive detection. To\nassess the potential harm caused by jailbreak prompts, we create a question set\ncomprising 46,800 samples across 13 forbidden scenarios. Our experiments show\nthat current LLMs and safeguards cannot adequately defend jailbreak prompts in\nall scenarios. Particularly, we identify two highly effective jailbreak prompts\nwhich achieve 0.99 attack success rates on ChatGPT (GPT-3.5) and GPT-4, and\nthey have persisted online for over 100 days. Our work sheds light on the\nsevere and evolving threat landscape of jailbreak prompts. We hope our study\ncan facilitate the research community and LLM vendors in promoting safer and\nregulated LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 7 Aug 2023 16:55:20 GMT"
      }
    ],
    "update_date": "2023-08-09",
    "authors_parsed": [
      [
        "Shen",
        "Xinyue",
        ""
      ],
      [
        "Chen",
        "Zeyuan",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Shen",
        "Yun",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.03825",
    "publish_date": "2023-08-07"
  },
  {
    "id": "2308.04179",
    "submitter": "Diqun Yan",
    "authors": "Zhe Ye, Diqun Yan, Li Dong, Kailai Shen",
    "title": "Evil Operation: Breaking Speaker Recognition with PaddingBack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.SD eess.AS eess.SP",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Machine Learning as a Service (MLaaS) has gained popularity due to\nadvancements in machine learning. However, untrusted third-party platforms have\nraised concerns about AI security, particularly in backdoor attacks. Recent\nresearch has shown that speech backdoors can utilize transformations as\ntriggers, similar to image backdoors. However, human ears easily detect these\ntransformations, leading to suspicion. In this paper, we introduce PaddingBack,\nan inaudible backdoor attack that utilizes malicious operations to make\npoisoned samples indistinguishable from clean ones. Instead of using external\nperturbations as triggers, we exploit the widely used speech signal operation,\npadding, to break speaker recognition systems. Our experimental results\ndemonstrate the effectiveness of the proposed approach, achieving a\nsignificantly high attack success rate while maintaining a high rate of benign\naccuracy. Furthermore, PaddingBack demonstrates the ability to resist defense\nmethods while maintaining its stealthiness against human perception. The\nresults of the stealthiness experiment have been made available at\nhttps://nbufabio25.github.io/paddingback/.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 8 Aug 2023 10:36:44 GMT"
      }
    ],
    "update_date": "2023-08-09",
    "authors_parsed": [
      [
        "Ye",
        "Zhe",
        ""
      ],
      [
        "Yan",
        "Diqun",
        ""
      ],
      [
        "Dong",
        "Li",
        ""
      ],
      [
        "Shen",
        "Kailai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.04179",
    "publish_date": "2023-08-08"
  },
  {
    "id": "2308.04451",
    "submitter": "Pietro Liguori",
    "authors": "Domenico Cotroneo, Cristina Improta, Pietro Liguori, Roberto Natella",
    "title": "Vulnerabilities in AI Code Generators: Exploring Targeted Data Poisoning\n  Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  AI-based code generators have become pivotal in assisting developers in\nwriting software starting from natural language (NL). However, they are trained\non large amounts of data, often collected from unsanitized online sources\n(e.g., GitHub, HuggingFace). As a consequence, AI models become an easy target\nfor data poisoning, i.e., an attack that injects malicious samples into the\ntraining data to generate vulnerable code. To address this threat, we\ninvestigate the security of AI code generators by devising a targeted data\npoisoning strategy. We poison the training data by injecting increasing amounts\nof code containing security vulnerabilities and assess the attack's success on\ndifferent state-of-the-art models for code generation. Our study shows that AI\ncode generators are vulnerable to even a small amount of poison. Notably, the\nattack success strongly depends on the model architecture and poisoning rate,\nwhereas it is not influenced by the type of vulnerabilities. Moreover, since\nthe attack does not impact the correctness of code generated by pre-trained\nmodels, it is hard to detect. Lastly, our work offers practical insights into\nunderstanding and potentially mitigating this threat.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 4 Aug 2023 15:23:30 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 6 Nov 2023 10:09:30 GMT"
      }
    ],
    "update_date": "2023-11-07",
    "authors_parsed": [
      [
        "Cotroneo",
        "Domenico",
        ""
      ],
      [
        "Improta",
        "Cristina",
        ""
      ],
      [
        "Liguori",
        "Pietro",
        ""
      ],
      [
        "Natella",
        "Roberto",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.04451",
    "publish_date": "2023-11-06"
  },
  {
    "id": "2308.07847",
    "submitter": "Yugeng Liu",
    "authors": "Yugeng Liu, Tianshuo Cong, Zhengyu Zhao, Michael Backes, Yun Shen,\n  Yang Zhang",
    "title": "Robustness Over Time: Understanding Adversarial Examples' Effectiveness\n  on Longitudinal Versions of Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have led to significant improvements in many\ntasks across various domains, such as code interpretation, response generation,\nand ambiguity handling. These LLMs, however, when upgrading, primarily\nprioritize enhancing user experience while neglecting security, privacy, and\nsafety implications. Consequently, unintended vulnerabilities or biases can be\nintroduced. Previous studies have predominantly focused on specific versions of\nthe models and disregard the potential emergence of new attack vectors\ntargeting the updated versions. Through the lens of adversarial examples within\nthe in-context learning framework, this longitudinal study addresses this gap\nby conducting a comprehensive assessment of the robustness of successive\nversions of LLMs, vis-\\`a-vis GPT-3.5. We conduct extensive experiments to\nanalyze and understand the impact of the robustness in two distinct learning\ncategories: zero-shot learning and few-shot learning. Our findings indicate\nthat, in comparison to earlier versions of LLMs, the updated versions do not\nexhibit the anticipated level of robustness against adversarial attacks. In\naddition, our study emphasizes the increased effectiveness of synergized\nadversarial queries in most zero-shot learning and few-shot learning cases. We\nhope that our study can lead to a more refined assessment of the robustness of\nLLMs over time and provide valuable insights of these models for both\ndevelopers and users.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 15 Aug 2023 15:51:52 GMT"
      }
    ],
    "update_date": "2023-08-16",
    "authors_parsed": [
      [
        "Liu",
        "Yugeng",
        ""
      ],
      [
        "Cong",
        "Tianshuo",
        ""
      ],
      [
        "Zhao",
        "Zhengyu",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Shen",
        "Yun",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.07847",
    "publish_date": "2023-08-15"
  },
  {
    "id": "2308.10443",
    "submitter": "Wesley Joon-Wie Tann",
    "authors": "Wesley Tann, Yuancheng Liu, Jun Heng Sim, Choon Meng Seah, Ee-Chien\n  Chang",
    "title": "Using Large Language Models for Cybersecurity Capture-The-Flag\n  Challenges and Certification Questions",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL cs.CY",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  The assessment of cybersecurity Capture-The-Flag (CTF) exercises involves\nparticipants finding text strings or ``flags'' by exploiting system\nvulnerabilities. Large Language Models (LLMs) are natural-language models\ntrained on vast amounts of words to understand and generate text; they can\nperform well on many CTF challenges. Such LLMs are freely available to\nstudents. In the context of CTF exercises in the classroom, this raises\nconcerns about academic integrity. Educators must understand LLMs' capabilities\nto modify their teaching to accommodate generative AI assistance. This research\ninvestigates the effectiveness of LLMs, particularly in the realm of CTF\nchallenges and questions. Here we evaluate three popular LLMs, OpenAI ChatGPT,\nGoogle Bard, and Microsoft Bing. First, we assess the LLMs' question-answering\nperformance on five Cisco certifications with varying difficulty levels. Next,\nwe qualitatively study the LLMs' abilities in solving CTF challenges to\nunderstand their limitations. We report on the experience of using the LLMs for\nseven test cases in all five types of CTF challenges. In addition, we\ndemonstrate how jailbreak prompts can bypass and break LLMs' ethical\nsafeguards. The paper concludes by discussing LLM's impact on CTF exercises and\nits implications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Aug 2023 03:30:21 GMT"
      }
    ],
    "update_date": "2023-08-22",
    "authors_parsed": [
      [
        "Tann",
        "Wesley",
        ""
      ],
      [
        "Liu",
        "Yuancheng",
        ""
      ],
      [
        "Sim",
        "Jun Heng",
        ""
      ],
      [
        "Seah",
        "Choon Meng",
        ""
      ],
      [
        "Chang",
        "Ee-Chien",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.10443",
    "publish_date": "2023-08-21"
  },
  {
    "id": "2308.10718",
    "submitter": "Jie Zhang",
    "authors": "Yutong Wu, Jie Zhang, Florian Kerschbaum, and Tianwei Zhang",
    "title": "Backdooring Textual Inversion for Concept Censorship",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent years have witnessed success in AIGC (AI Generated Content). People\ncan make use of a pre-trained diffusion model to generate images of high\nquality or freely modify existing pictures with only prompts in nature\nlanguage. More excitingly, the emerging personalization techniques make it\nfeasible to create specific-desired images with only a few images as\nreferences. However, this induces severe threats if such advanced techniques\nare misused by malicious users, such as spreading fake news or defaming\nindividual reputations. Thus, it is necessary to regulate personalization\nmodels (i.e., concept censorship) for their development and advancement.\n  In this paper, we focus on the personalization technique dubbed Textual\nInversion (TI), which is becoming prevailing for its lightweight nature and\nexcellent performance. TI crafts the word embedding that contains detailed\ninformation about a specific object. Users can easily download the word\nembedding from public websites like Civitai and add it to their own stable\ndiffusion model without fine-tuning for personalization. To achieve the concept\ncensorship of a TI model, we propose leveraging the backdoor technique for good\nby injecting backdoors into the Textual Inversion embeddings. Briefly, we\nselect some sensitive words as triggers during the training of TI, which will\nbe censored for normal use. In the subsequent generation stage, if the triggers\nare combined with personalized embeddings as final prompts, the model will\noutput a pre-defined target image rather than images including the desired\nmalicious concept.\n  To demonstrate the effectiveness of our approach, we conduct extensive\nexperiments on Stable Diffusion, a prevailing open-sourced text-to-image model.\nOur code, data, and results are available at\nhttps://concept-censorship.github.io.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 21 Aug 2023 13:39:04 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 23 Aug 2023 13:56:52 GMT"
      }
    ],
    "update_date": "2023-08-24",
    "authors_parsed": [
      [
        "Wu",
        "Yutong",
        ""
      ],
      [
        "Zhang",
        "Jie",
        ""
      ],
      [
        "Kerschbaum",
        "Florian",
        ""
      ],
      [
        "Zhang",
        "Tianwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.10718",
    "publish_date": "2023-08-23"
  },
  {
    "id": "2308.10819",
    "submitter": "Zekun Li",
    "authors": "Zekun Li and Baolin Peng and Pengcheng He and Xifeng Yan",
    "title": "Evaluating the Instruction-Following Robustness of Large Language Models\n  to Prompt Injection",
    "comments": "The data and code can be found at\n  https://github.com/Leezekun/instruction-following-robustness-eval",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have demonstrated exceptional proficiency in\ninstruction-following, becoming increasingly crucial across various\napplications. However, this capability brings with it the risk of prompt\ninjection attacks, where attackers inject instructions into LLMs' input to\nelicit undesirable actions or content. Understanding the robustness of LLMs\nagainst such attacks is vital for their safe implementation. In this work, we\nestablish a benchmark to evaluate the robustness of instruction-following LLMs\nagainst prompt injection attacks. Our objective is to determine the extent to\nwhich LLMs can be influenced by injected instructions and their ability to\ndifferentiate between these injected and original target instructions. Through\nextensive experiments with leading instruction-following LLMs, we uncover\nsignificant vulnerabilities in their robustness to such attacks. Our results\nindicate that some models are overly tuned to follow any embedded instructions\nin the prompt, overly focusing on the latter parts of the prompt without fully\ngrasping the entire context. By contrast, models with a better grasp of the\ncontext and instruction-following capabilities will potentially be more\nsusceptible to compromise by injected instructions. This underscores the need\nto shift the focus from merely enhancing LLMs' instruction-following\ncapabilities to improving their overall comprehension of prompts and\ndiscernment of instructions that are appropriate to follow. We hope our\nin-depth analysis offers insights into the underlying causes of these\nvulnerabilities, aiding in the development of future solutions. Code and data\nare available at\nhttps://github.com/Leezekun/instruction-following-robustness-eval\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 17 Aug 2023 06:21:50 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 30 Sep 2023 19:14:45 GMT"
      },
      {
        "version": "v3",
        "created": "Sat, 25 Nov 2023 00:25:36 GMT"
      }
    ],
    "update_date": "2023-11-28",
    "authors_parsed": [
      [
        "Li",
        "Zekun",
        ""
      ],
      [
        "Peng",
        "Baolin",
        ""
      ],
      [
        "He",
        "Pengcheng",
        ""
      ],
      [
        "Yan",
        "Xifeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.10819",
    "publish_date": "2023-08-17"
  },
  {
    "id": "2308.11521",
    "submitter": "Zhenhua Wang",
    "authors": "Zhenhua Wang, Wei Xie, Kai Chen, Baosheng Wang, Zhiwen Gui, Enze Wang",
    "title": "Self-Deception: Reverse Penetrating the Semantic Firewall of Large\n  Language Models",
    "comments": "Serious errors were found in the experiment, which may lead to the\n  overturning of the overall conclusions of the paper",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs), such as ChatGPT, have emerged with astonishing\ncapabilities approaching artificial general intelligence. While providing\nconvenience for various societal needs, LLMs have also lowered the cost of\ngenerating harmful content. Consequently, LLM developers have deployed\nsemantic-level defenses to recognize and reject prompts that may lead to\ninappropriate content. Unfortunately, these defenses are not foolproof, and\nsome attackers have crafted \"jailbreak\" prompts that temporarily hypnotize the\nLLM into forgetting content defense rules and answering any improper questions.\nTo date, there is no clear explanation of the principles behind these\nsemantic-level attacks and defenses in both industry and academia.\n  This paper investigates the LLM jailbreak problem and proposes an automatic\njailbreak method for the first time. We propose the concept of a semantic\nfirewall and provide three technical implementation approaches. Inspired by the\nattack that penetrates traditional firewalls through reverse tunnels, we\nintroduce a \"self-deception\" attack that can bypass the semantic firewall by\ninducing LLM to generate prompts that facilitate jailbreak. We generated a\ntotal of 2,520 attack payloads in six languages (English, Russian, French,\nSpanish, Chinese, and Arabic) across seven virtual scenarios, targeting the\nthree most common types of violations: violence, hate, and pornography. The\nexperiment was conducted on two models, namely the GPT-3.5-Turbo and GPT-4. The\nsuccess rates on the two models were 86.2% and 67%, while the failure rates\nwere 4.7% and 2.2%, respectively. This highlighted the effectiveness of the\nproposed attack method. All experimental code and raw data will be released as\nopen-source to inspire future research. We believe that manipulating AI\nbehavior through carefully crafted prompts will become an important research\ndirection in the future.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 16 Aug 2023 09:04:36 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 25 Aug 2023 00:25:06 GMT"
      }
    ],
    "update_date": "2023-08-28",
    "authors_parsed": [
      [
        "Wang",
        "Zhenhua",
        ""
      ],
      [
        "Xie",
        "Wei",
        ""
      ],
      [
        "Chen",
        "Kai",
        ""
      ],
      [
        "Wang",
        "Baosheng",
        ""
      ],
      [
        "Gui",
        "Zhiwen",
        ""
      ],
      [
        "Wang",
        "Enze",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.11521",
    "publish_date": "2023-08-16"
  },
  {
    "id": "2308.11800",
    "submitter": "Nicolas Michael M\\\"uller",
    "authors": "Nicolas M. M\\\"uller, Philip Sperl, Konstantin B\\\"ottinger",
    "title": "Complex-valued neural networks for voice anti-spoofing",
    "comments": "Interspeech 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.LG eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Current anti-spoofing and audio deepfake detection systems use either\nmagnitude spectrogram-based features (such as CQT or Melspectrograms) or raw\naudio processed through convolution or sinc-layers. Both methods have\ndrawbacks: magnitude spectrograms discard phase information, which affects\naudio naturalness, and raw-feature-based models cannot use traditional\nexplainable AI methods. This paper proposes a new approach that combines the\nbenefits of both methods by using complex-valued neural networks to process the\ncomplex-valued, CQT frequency-domain representation of the input audio. This\nmethod retains phase information and allows for explainable AI methods. Results\nshow that this approach outperforms previous methods on the \"In-the-Wild\"\nanti-spoofing dataset and enables interpretation of the results through\nexplainable AI. Ablation studies confirm that the model has learned to use\nphase information to detect voice spoofing.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 22 Aug 2023 21:49:38 GMT"
      }
    ],
    "update_date": "2023-08-24",
    "authors_parsed": [
      [
        "M\u00fcller",
        "Nicolas M.",
        ""
      ],
      [
        "Sperl",
        "Philip",
        ""
      ],
      [
        "B\u00f6ttinger",
        "Konstantin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.11800",
    "publish_date": "2023-08-22"
  },
  {
    "id": "2308.11894",
    "submitter": "Ningfei Wang",
    "authors": "Ningfei Wang, Yunpeng Luo, Takami Sato, Kaidi Xu, Qi Alfred Chen",
    "title": "Does Physical Adversarial Example Really Matter to Autonomous Driving?\n  Towards System-Level Effect of Adversarial Object Evasion Attack",
    "comments": "Accepted by ICCV 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In autonomous driving (AD), accurate perception is indispensable to achieving\nsafe and secure driving. Due to its safety-criticality, the security of AD\nperception has been widely studied. Among different attacks on AD perception,\nthe physical adversarial object evasion attacks are especially severe. However,\nwe find that all existing literature only evaluates their attack effect at the\ntargeted AI component level but not at the system level, i.e., with the entire\nsystem semantics and context such as the full AD pipeline. Thereby, this raises\na critical research question: can these existing researches effectively achieve\nsystem-level attack effects (e.g., traffic rule violations) in the real-world\nAD context? In this work, we conduct the first measurement study on whether and\nhow effectively the existing designs can lead to system-level effects,\nespecially for the STOP sign-evasion attacks due to their popularity and\nseverity. Our evaluation results show that all the representative prior works\ncannot achieve any system-level effects. We observe two design limitations in\nthe prior works: 1) physical model-inconsistent object size distribution in\npixel sampling and 2) lack of vehicle plant model and AD system model\nconsideration. Then, we propose SysAdv, a novel system-driven attack design in\nthe AD context and our evaluation results show that the system-level effects\ncan be significantly improved, i.e., the violation rate increases by around\n70%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 23 Aug 2023 03:40:47 GMT"
      }
    ],
    "update_date": "2023-08-24",
    "authors_parsed": [
      [
        "Wang",
        "Ningfei",
        ""
      ],
      [
        "Luo",
        "Yunpeng",
        ""
      ],
      [
        "Sato",
        "Takami",
        ""
      ],
      [
        "Xu",
        "Kaidi",
        ""
      ],
      [
        "Chen",
        "Qi Alfred",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.11894",
    "publish_date": "2023-08-23"
  },
  {
    "id": "2308.12734",
    "submitter": "Jordan J. Bird",
    "authors": "Jordan J. Bird, Ahmad Lotfi",
    "title": "Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD cs.CL cs.HC cs.LG eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  There are growing implications surrounding generative AI in the speech domain\nthat enable voice cloning and real-time voice conversion from one individual to\nanother. This technology poses a significant ethical threat and could lead to\nbreaches of privacy and misrepresentation, thus there is an urgent need for\nreal-time detection of AI-generated speech for DeepFake Voice Conversion. To\naddress the above emerging issues, the DEEP-VOICE dataset is generated in this\nstudy, comprised of real human speech from eight well-known figures and their\nspeech converted to one another using Retrieval-based Voice Conversion.\nPresenting as a binary classification problem of whether the speech is real or\nAI-generated, statistical analysis of temporal audio features through t-testing\nreveals that there are significantly different distributions. Hyperparameter\noptimisation is implemented for machine learning models to identify the source\nof speech. Following the training of 208 individual machine learning models\nover 10-fold cross validation, it is found that the Extreme Gradient Boosting\nmodel can achieve an average classification accuracy of 99.3% and can classify\nspeech in real-time, at around 0.004 milliseconds given one second of speech.\nAll data generated for this study is released publicly for future research on\nAI speech detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 24 Aug 2023 12:26:15 GMT"
      }
    ],
    "update_date": "2023-08-25",
    "authors_parsed": [
      [
        "Bird",
        "Jordan J.",
        ""
      ],
      [
        "Lotfi",
        "Ahmad",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.12734",
    "publish_date": "2023-08-24"
  },
  {
    "id": "2308.12918",
    "submitter": "John Harshith",
    "authors": "John Harshith, Mantej Singh Gill, Madhan Jothimani",
    "title": "Evaluating the Vulnerabilities in ML systems in terms of adversarial\n  attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  There have been recent adversarial attacks that are difficult to find. These\nnew adversarial attacks methods may pose challenges to current deep learning\ncyber defense systems and could influence the future defense of cyberattacks.\nThe authors focus on this domain in this research paper. They explore the\nconsequences of vulnerabilities in AI systems. This includes discussing how\nthey might arise, differences between randomized and adversarial examples and\nalso potential ethical implications of vulnerabilities. Moreover, it is\nimportant to train the AI systems appropriately when they are in testing phase\nand getting them ready for broader use.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 24 Aug 2023 16:46:01 GMT"
      }
    ],
    "update_date": "2023-08-25",
    "authors_parsed": [
      [
        "Harshith",
        "John",
        ""
      ],
      [
        "Gill",
        "Mantej Singh",
        ""
      ],
      [
        "Jothimani",
        "Madhan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.12918",
    "publish_date": "2023-08-24"
  },
  {
    "id": "2308.13149",
    "submitter": "Liangtai Sun",
    "authors": "Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen,\n  Lu Chen and Kai Yu",
    "title": "SciEval: A Multi-Level Large Language Model Evaluation Benchmark for\n  Scientific Research",
    "comments": "12 pages, 17 figures, 12 tables. Under Review",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recently, there has been growing interest in using Large Language Models\n(LLMs) for scientific research. Numerous benchmarks have been proposed to\nevaluate the ability of LLMs for scientific research. However, current\nbenchmarks are mostly based on pre-collected objective questions. This design\nsuffers from data leakage problem and lacks the evaluation of subjective Q/A\nability. In this paper, we propose SciEval, a comprehensive and\nmulti-disciplinary evaluation benchmark to address these issues. Based on\nBloom's taxonomy, SciEval covers four dimensions to systematically evaluate\nscientific research ability. In particular, we design a \"dynamic\" subset based\non scientific principles to prevent evaluation from potential data leakage.\nBoth objective and subjective questions are included in SciEval. These\ncharacteristics make SciEval a more effective benchmark for scientific research\nability evaluation of LLMs. Comprehensive experiments on most advanced LLMs\nshow that, although GPT-4 achieves SOTA performance compared to other LLMs,\nthere is still substantial room for improvement, especially for dynamic\nquestions. The data and codes are now publicly available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 25 Aug 2023 03:05:33 GMT"
      }
    ],
    "update_date": "2023-08-28",
    "authors_parsed": [
      [
        "Sun",
        "Liangtai",
        ""
      ],
      [
        "Han",
        "Yang",
        ""
      ],
      [
        "Zhao",
        "Zihan",
        ""
      ],
      [
        "Ma",
        "Da",
        ""
      ],
      [
        "Shen",
        "Zhennan",
        ""
      ],
      [
        "Chen",
        "Baocai",
        ""
      ],
      [
        "Chen",
        "Lu",
        ""
      ],
      [
        "Yu",
        "Kai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.13149",
    "publish_date": "2023-08-25"
  },
  {
    "id": "2308.13449",
    "submitter": "Sungbae Chun",
    "authors": "Aibek Bekbayev, Sungbae Chun, Yerzat Dulat, James Yamazaki",
    "title": "The Poison of Alignment",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  From the perspective of content safety issues, alignment has shown to limit\nlarge language models' (LLMs) harmful content generation. This intentional\nmethod of reinforcing models to not respond to certain user inputs seem to be\npresent in many modern open-source instruction tuning datasets such as\nOpenAssistant or Guanaco. We introduce a novel insight to an instruction-tuned\nmodel's performance affected by the presence of alignment in supervised\nfine-tuning dataset. To be specific, we noticed that alignment acts as if it is\npoisoning the instruction dataset. Experimentally, we demonstrate that aligned\nanswers significantly worsen the performance of the resulting fine-tuned\nmodel's on various reasoning benchmarks such as Big Bench (BBH), Massive\nMultitask Language Understanding (MMLU), Human Eval, and Discrete Reasoning\nOver Paragraphs (DROP), performing worse than the counterpart tuned without\nalignment by 4-33%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 25 Aug 2023 15:51:15 GMT"
      }
    ],
    "update_date": "2023-08-28",
    "authors_parsed": [
      [
        "Bekbayev",
        "Aibek",
        ""
      ],
      [
        "Chun",
        "Sungbae",
        ""
      ],
      [
        "Dulat",
        "Yerzat",
        ""
      ],
      [
        "Yamazaki",
        "James",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.13449",
    "publish_date": "2023-08-25"
  },
  {
    "id": "2308.14132",
    "submitter": "Gabriel Alon",
    "authors": "Gabriel Alon, Michael Kamfonas",
    "title": "Detecting Language Model Attacks with Perplexity",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  A novel hack involving Large Language Models (LLMs) has emerged, exploiting\nadversarial suffixes to deceive models into generating perilous responses. Such\njailbreaks can trick LLMs into providing intricate instructions to a malicious\nuser for creating explosives, orchestrating a bank heist, or facilitating the\ncreation of offensive content. By evaluating the perplexity of queries with\nadversarial suffixes using an open-source LLM (GPT-2), we found that they have\nexceedingly high perplexity values. As we explored a broad range of regular\n(non-adversarial) prompt varieties, we concluded that false positives are a\nsignificant challenge for plain perplexity filtering. A Light-GBM trained on\nperplexity and token length resolved the false positives and correctly detected\nmost adversarial attacks in the test set.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 27 Aug 2023 15:20:06 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 5 Nov 2023 22:25:37 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 7 Nov 2023 03:30:15 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Alon",
        "Gabriel",
        ""
      ],
      [
        "Kamfonas",
        "Michael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.14132",
    "publish_date": "2023-11-07"
  },
  {
    "id": "2308.14353",
    "submitter": "Baoli Zhang",
    "authors": "Baoli Zhang, Haining Xie, Pengfan Du, Junhao Chen, Pengfei Cao, Yubo\n  Chen, Shengping Liu, Kang Liu, Jun Zhao",
    "title": "ZhuJiu: A Multi-dimensional, Multi-faceted Chinese Benchmark for Large\n  Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  The unprecedented performance of large language models (LLMs) requires\ncomprehensive and accurate evaluation. We argue that for LLMs evaluation,\nbenchmarks need to be comprehensive and systematic. To this end, we propose the\nZhuJiu benchmark, which has the following strengths: (1) Multi-dimensional\nability coverage: We comprehensively evaluate LLMs across 7 ability dimensions\ncovering 51 tasks. Especially, we also propose a new benchmark that focuses on\nknowledge ability of LLMs. (2) Multi-faceted evaluation methods collaboration:\nWe use 3 different yet complementary evaluation methods to comprehensively\nevaluate LLMs, which can ensure the authority and accuracy of the evaluation\nresults. (3) Comprehensive Chinese benchmark: ZhuJiu is the pioneering\nbenchmark that fully assesses LLMs in Chinese, while also providing equally\nrobust evaluation abilities in English. (4) Avoiding potential data leakage: To\navoid data leakage, we construct evaluation data specifically for 37 tasks. We\nevaluate 10 current mainstream LLMs and conduct an in-depth discussion and\nanalysis of their results. The ZhuJiu benchmark and open-participation\nleaderboard are publicly released at http://www.zhujiu-benchmark.com/ and we\nalso provide a demo video at https://youtu.be/qypkJ89L1Ic.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 28 Aug 2023 06:56:44 GMT"
      }
    ],
    "update_date": "2023-08-29",
    "authors_parsed": [
      [
        "Zhang",
        "Baoli",
        ""
      ],
      [
        "Xie",
        "Haining",
        ""
      ],
      [
        "Du",
        "Pengfan",
        ""
      ],
      [
        "Chen",
        "Junhao",
        ""
      ],
      [
        "Cao",
        "Pengfei",
        ""
      ],
      [
        "Chen",
        "Yubo",
        ""
      ],
      [
        "Liu",
        "Shengping",
        ""
      ],
      [
        "Liu",
        "Kang",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.14353",
    "publish_date": "2023-08-28"
  },
  {
    "id": "2308.14367",
    "submitter": "Kunlan Xiang",
    "authors": "Haomiao Yang, Kunlan Xiang, Mengyu Ge, Hongwei Li, Rongxing Lu and\n  Shui Yu",
    "title": "A Comprehensive Overview of Backdoor Attacks in Large Language Models\n  within Communication Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The Large Language Models (LLMs) are poised to offer efficient and\nintelligent services for future mobile communication networks, owing to their\nexceptional capabilities in language comprehension and generation. However, the\nextremely high data and computational resource requirements for the performance\nof LLMs compel developers to resort to outsourcing training or utilizing\nthird-party data and computing resources. These strategies may expose the model\nwithin the network to maliciously manipulated training data and processing,\nproviding an opportunity for attackers to embed a hidden backdoor into the\nmodel, termed a backdoor attack. Backdoor attack in LLMs refers to embedding a\nhidden backdoor in LLMs that causes the model to perform normally on benign\nsamples but exhibit degraded performance on poisoned ones. This issue is\nparticularly concerning within communication networks where reliability and\nsecurity are paramount. Despite the extensive research on backdoor attacks,\nthere remains a lack of in-depth exploration specifically within the context of\nLLMs employed in communication networks, and a systematic review of such\nattacks is currently absent. In this survey, we systematically propose a\ntaxonomy of backdoor attacks in LLMs as used in communication networks,\ndividing them into four major categories: input-triggered, prompt-triggered,\ninstruction-triggered, and demonstration-triggered attacks. Furthermore, we\nconduct a comprehensive analysis of the benchmark datasets. Finally, we\nidentify potential problems and open challenges, offering valuable insights\ninto future research directions for enhancing the security and integrity of\nLLMs in communication networks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 28 Aug 2023 07:31:43 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 6 Sep 2023 08:22:33 GMT"
      }
    ],
    "update_date": "2023-09-07",
    "authors_parsed": [
      [
        "Yang",
        "Haomiao",
        ""
      ],
      [
        "Xiang",
        "Kunlan",
        ""
      ],
      [
        "Ge",
        "Mengyu",
        ""
      ],
      [
        "Li",
        "Hongwei",
        ""
      ],
      [
        "Lu",
        "Rongxing",
        ""
      ],
      [
        "Yu",
        "Shui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.14367",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2308.15092",
    "submitter": "Desmond Higham J",
    "authors": "Desmond J. Higham",
    "title": "Can We Rely on AI?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "math.NA cs.AI cs.LG cs.NA",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Over the last decade, adversarial attack algorithms have revealed\ninstabilities in deep learning tools. These algorithms raise issues regarding\nsafety, reliability and interpretability in artificial intelligence; especially\nin high risk settings. From a practical perspective, there has been a war of\nescalation between those developing attack and defence strategies. At a more\ntheoretical level, researchers have also studied bigger picture questions\nconcerning the existence and computability of attacks. Here we give a brief\noverview of the topic, focusing on aspects that are likely to be of interest to\nresearchers in applied and computational mathematics.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 29 Aug 2023 07:58:19 GMT"
      }
    ],
    "update_date": "2023-08-30",
    "authors_parsed": [
      [
        "Higham",
        "Desmond J.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.15092",
    "publish_date": "2023-08-29"
  },
  {
    "id": "2308.15736",
    "submitter": "Zhenyong Zhang",
    "authors": "Zhenyong Zhang, Mengxiang Liu, Mingyang Sun, Ruilong Deng, Peng Cheng,\n  Dusit Niyato, Mo-Yuen Chow, and Jiming Chen",
    "title": "Vulnerability of Machine Learning Approaches Applied in IoT-based Smart\n  Grid: A Review",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.SY eess.SY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Machine learning (ML) sees an increasing prevalence of being used in the\ninternet-of-things (IoT)-based smart grid. However, the trustworthiness of ML\nis a severe issue that must be addressed to accommodate the trend of ML-based\nsmart grid applications (MLsgAPPs). The adversarial distortion injected into\nthe power signal will greatly affect the system's normal control and operation.\nTherefore, it is imperative to conduct vulnerability assessment for MLsgAPPs\napplied in the context of safety-critical power systems. In this paper, we\nprovide a comprehensive review of the recent progress in designing attack and\ndefense methods for MLsgAPPs. Unlike the traditional survey about ML security,\nthis is the first review work about the security of MLsgAPPs that focuses on\nthe characteristics of power systems. We first highlight the specifics for\nconstructing the adversarial attacks on MLsgAPPs. Then, the vulnerability of\nMLsgAPP is analyzed from both the aspects of the power system and ML model.\nAfterward, a comprehensive survey is conducted to review and compare existing\nstudies about the adversarial attacks on MLsgAPPs in scenarios of generation,\ntransmission, distribution, and consumption, and the countermeasures are\nreviewed according to the attacks that they defend against. Finally, the future\nresearch directions are discussed on the attacker's and defender's side,\nrespectively. We also analyze the potential vulnerability of large language\nmodel-based (e.g., ChatGPT) power system applications. Overall, we encourage\nmore researchers to contribute to investigating the adversarial issues of\nMLsgAPPs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 30 Aug 2023 03:29:26 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Sep 2023 14:43:33 GMT"
      },
      {
        "version": "v3",
        "created": "Mon, 25 Dec 2023 03:14:20 GMT"
      }
    ],
    "update_date": "2023-12-29",
    "authors_parsed": [
      [
        "Zhang",
        "Zhenyong",
        ""
      ],
      [
        "Liu",
        "Mengxiang",
        ""
      ],
      [
        "Sun",
        "Mingyang",
        ""
      ],
      [
        "Deng",
        "Ruilong",
        ""
      ],
      [
        "Cheng",
        "Peng",
        ""
      ],
      [
        "Niyato",
        "Dusit",
        ""
      ],
      [
        "Chow",
        "Mo-Yuen",
        ""
      ],
      [
        "Chen",
        "Jiming",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2308.15736",
    "publish_date": "2023-08-30"
  },
  {
    "id": "2309.00254",
    "submitter": "Varshini Subhash",
    "authors": "Varshini Subhash, Anna Bialas, Weiwei Pan, Finale Doshi-Velez",
    "title": "Why do universal adversarial attacks work on large language models?:\n  Geometry might be the answer",
    "comments": "2nd AdvML Frontiers Workshop at 40th International Conference on\n  Machine Learning, Honolulu, Hawaii, USA, 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Transformer based large language models with emergent capabilities are\nbecoming increasingly ubiquitous in society. However, the task of understanding\nand interpreting their internal workings, in the context of adversarial\nattacks, remains largely unsolved. Gradient-based universal adversarial attacks\nhave been shown to be highly effective on large language models and potentially\ndangerous due to their input-agnostic nature. This work presents a novel\ngeometric perspective explaining universal adversarial attacks on large\nlanguage models. By attacking the 117M parameter GPT-2 model, we find evidence\nindicating that universal adversarial triggers could be embedding vectors which\nmerely approximate the semantic information in their adversarial training\nregion. This hypothesis is supported by white-box model analysis comprising\ndimensionality reduction and similarity measurement of hidden representations.\nWe believe this new geometric perspective on the underlying mechanism driving\nuniversal attacks could help us gain deeper insight into the internal workings\nand failure modes of LLMs, thus enabling their mitigation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 Sep 2023 05:09:49 GMT"
      }
    ],
    "update_date": "2023-09-04",
    "authors_parsed": [
      [
        "Subhash",
        "Varshini",
        ""
      ],
      [
        "Bialas",
        "Anna",
        ""
      ],
      [
        "Pan",
        "Weiwei",
        ""
      ],
      [
        "Doshi-Velez",
        "Finale",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.00254",
    "publish_date": "2023-09-01"
  },
  {
    "id": "2309.00543",
    "submitter": "Sydney Pugh",
    "authors": "Sydney Pugh, Ivan Ruchkin, Insup Lee, James Weimer",
    "title": "Curating Naturally Adversarial Datasets for Learning-Enabled Medical\n  Cyber-Physical Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep learning models have shown promising predictive accuracy for time-series\nhealthcare applications. However, ensuring the robustness of these models is\nvital for building trustworthy AI systems. Existing research predominantly\nfocuses on robustness to synthetic adversarial examples, crafted by adding\nimperceptible perturbations to clean input data. However, these synthetic\nadversarial examples do not accurately reflect the most challenging real-world\nscenarios, especially in the context of healthcare data. Consequently,\nrobustness to synthetic adversarial examples may not necessarily translate to\nrobustness against naturally occurring adversarial examples, which is highly\ndesirable for trustworthy AI. We propose a method to curate datasets comprised\nof natural adversarial examples to evaluate model robustness. The method relies\non probabilistic labels obtained from automated weakly-supervised labeling that\ncombines noisy and cheap-to-obtain labeling heuristics. Based on these labels,\nour method adversarially orders the input data and uses this ordering to\nconstruct a sequence of increasingly adversarial datasets. Our evaluation on\nsix medical case studies and three non-medical case studies demonstrates the\nefficacy and statistical validity of our approach to generating naturally\nadversarial datasets\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 Sep 2023 15:52:32 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 7 Nov 2023 14:18:34 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Pugh",
        "Sydney",
        ""
      ],
      [
        "Ruchkin",
        "Ivan",
        ""
      ],
      [
        "Lee",
        "Insup",
        ""
      ],
      [
        "Weimer",
        "James",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.00543",
    "publish_date": "2023-11-07"
  },
  {
    "id": "2309.00614",
    "submitter": "Jonas Geiping",
    "authors": "Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John\n  Kirchenbauer, Ping-yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping,\n  Tom Goldstein",
    "title": "Baseline Defenses for Adversarial Attacks Against Aligned Language\n  Models",
    "comments": "12 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  As Large Language Models quickly become ubiquitous, it becomes critical to\nunderstand their security vulnerabilities. Recent work shows that text\noptimizers can produce jailbreaking prompts that bypass moderation and\nalignment. Drawing from the rich body of work on adversarial machine learning,\nwe approach these attacks with three questions: What threat models are\npractically useful in this domain? How do baseline defense techniques perform\nin this new domain? How does LLM security differ from computer vision?\n  We evaluate several baseline defense strategies against leading adversarial\nattacks on LLMs, discussing the various settings in which each is feasible and\neffective. Particularly, we look at three types of defenses: detection\n(perplexity based), input preprocessing (paraphrase and retokenization), and\nadversarial training. We discuss white-box and gray-box settings and discuss\nthe robustness-performance trade-off for each of the defenses considered. We\nfind that the weakness of existing discrete optimizers for text, combined with\nthe relatively high costs of optimization, makes standard adaptive attacks more\nchallenging for LLMs. Future research will be needed to uncover whether more\npowerful optimizers can be developed, or whether the strength of filtering and\npreprocessing defenses is greater in the LLMs domain than it has been in\ncomputer vision.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 Sep 2023 17:59:44 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 4 Sep 2023 17:47:36 GMT"
      }
    ],
    "update_date": "2023-09-06",
    "authors_parsed": [
      [
        "Jain",
        "Neel",
        ""
      ],
      [
        "Schwarzschild",
        "Avi",
        ""
      ],
      [
        "Wen",
        "Yuxin",
        ""
      ],
      [
        "Somepalli",
        "Gowthami",
        ""
      ],
      [
        "Kirchenbauer",
        "John",
        ""
      ],
      [
        "Chiang",
        "Ping-yeh",
        ""
      ],
      [
        "Goldblum",
        "Micah",
        ""
      ],
      [
        "Saha",
        "Aniruddha",
        ""
      ],
      [
        "Geiping",
        "Jonas",
        ""
      ],
      [
        "Goldstein",
        "Tom",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.00614",
    "publish_date": "2023-09-04"
  },
  {
    "id": "2309.01446",
    "submitter": "Raz Lapid",
    "authors": "Raz Lapid, Ron Langberg, Moshe Sipper",
    "title": "Open Sesame! Universal Black Box Jailbreaking of Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CV cs.NE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Sep 2023 08:54:20 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 17 Sep 2023 13:19:11 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 21 Nov 2023 14:02:33 GMT"
      }
    ],
    "update_date": "2023-11-22",
    "authors_parsed": [
      [
        "Lapid",
        "Raz",
        ""
      ],
      [
        "Langberg",
        "Ron",
        ""
      ],
      [
        "Sipper",
        "Moshe",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.01446",
    "publish_date": "2023-11-21"
  },
  {
    "id": "2309.01686",
    "submitter": "Zihao Zhou",
    "authors": "Zihao Zhou and Qiufeng Wang and Mingyu Jin and Jie Yao and Jianan Ye\n  and Wei Liu and Wei Wang and Xiaowei Huang and Kaizhu Huang",
    "title": "MathAttack: Attacking Large Language Models Towards Math Solving Ability",
    "comments": "11 pages, 6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the boom of Large Language Models (LLMs), the research of solving Math\nWord Problem (MWP) has recently made great progress. However, there are few\nstudies to examine the security of LLMs in math solving ability. Instead of\nattacking prompts in the use of LLMs, we propose a MathAttack model to attack\nMWP samples which are closer to the essence of security in solving math\nproblems. Compared to traditional text adversarial attack, it is essential to\npreserve the mathematical logic of original MWPs during the attacking. To this\nend, we propose logical entity recognition to identify logical entries which\nare then frozen. Subsequently, the remaining text are attacked by adopting a\nword-level attacker. Furthermore, we propose a new dataset RobustMath to\nevaluate the robustness of LLMs in math solving ability. Extensive experiments\non our RobustMath and two another math benchmark datasets GSM8K and MultiAirth\nshow that MathAttack could effectively attack the math solving ability of LLMs.\nIn the experiments, we observe that (1) Our adversarial samples from\nhigher-accuracy LLMs are also effective for attacking LLMs with lower accuracy\n(e.g., transfer from larger to smaller-size LLMs, or from few-shot to zero-shot\nprompts); (2) Complex MWPs (such as more solving steps, longer text, more\nnumbers) are more vulnerable to attack; (3) We can improve the robustness of\nLLMs by using our adversarial samples in few-shot prompts. Finally, we hope our\npractice and observation can serve as an important attempt towards enhancing\nthe robustness of LLMs in math solving ability. We will release our code and\ndataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Sep 2023 16:02:23 GMT"
      }
    ],
    "update_date": "2023-09-06",
    "authors_parsed": [
      [
        "Zhou",
        "Zihao",
        ""
      ],
      [
        "Wang",
        "Qiufeng",
        ""
      ],
      [
        "Jin",
        "Mingyu",
        ""
      ],
      [
        "Yao",
        "Jie",
        ""
      ],
      [
        "Ye",
        "Jianan",
        ""
      ],
      [
        "Liu",
        "Wei",
        ""
      ],
      [
        "Wang",
        "Wei",
        ""
      ],
      [
        "Huang",
        "Xiaowei",
        ""
      ],
      [
        "Huang",
        "Kaizhu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.01686",
    "publish_date": "2023-09-04"
  },
  {
    "id": "2309.01919",
    "submitter": "Binh M. Le",
    "authors": "Beomsang Cho, Binh M. Le, Jiwon Kim, Simon Woo, Shahroz Tariq,\n  Alsharif Abuadbba, Kristen Moore",
    "title": "Towards Understanding of Deepfake Videos in the Wild",
    "comments": null,
    "journal-ref": "32nd ACM International Conference on Information & Knowledge\n  Management (CIKM), UK, 2023",
    "doi": "10.1145/3583780.3614729",
    "report-no": null,
    "categories": "cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deepfakes have become a growing concern in recent years, prompting\nresearchers to develop benchmark datasets and detection algorithms to tackle\nthe issue. However, existing datasets suffer from significant drawbacks that\nhamper their effectiveness. Notably, these datasets fail to encompass the\nlatest deepfake videos produced by state-of-the-art methods that are being\nshared across various platforms. This limitation impedes the ability to keep\npace with the rapid evolution of generative AI techniques employed in\nreal-world deepfake production. Our contributions in this IRB-approved study\nare to bridge this knowledge gap from current real-world deepfakes by providing\nin-depth analysis. We first present the largest and most diverse and recent\ndeepfake dataset (RWDF-23) collected from the wild to date, consisting of 2,000\ndeepfake videos collected from 4 platforms targeting 4 different languages span\ncreated from 21 countries: Reddit, YouTube, TikTok, and Bilibili. By expanding\nthe dataset's scope beyond the previous research, we capture a broader range of\nreal-world deepfake content, reflecting the ever-evolving landscape of online\nplatforms. Also, we conduct a comprehensive analysis encompassing various\naspects of deepfakes, including creators, manipulation strategies, purposes,\nand real-world content production methods. This allows us to gain valuable\ninsights into the nuances and characteristics of deepfakes in different\ncontexts. Lastly, in addition to the video content, we also collect viewer\ncomments and interactions, enabling us to explore the engagements of internet\nusers with deepfake content. By considering this rich contextual information,\nwe aim to provide a holistic understanding of the {evolving} deepfake\nphenomenon and its impact on online platforms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 5 Sep 2023 03:16:38 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 6 Sep 2023 08:57:13 GMT"
      }
    ],
    "update_date": "2023-09-07",
    "authors_parsed": [
      [
        "Cho",
        "Beomsang",
        ""
      ],
      [
        "Le",
        "Binh M.",
        ""
      ],
      [
        "Kim",
        "Jiwon",
        ""
      ],
      [
        "Woo",
        "Simon",
        ""
      ],
      [
        "Tariq",
        "Shahroz",
        ""
      ],
      [
        "Abuadbba",
        "Alsharif",
        ""
      ],
      [
        "Moore",
        "Kristen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.01919",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2309.02705",
    "submitter": "Aounon Kumar",
    "authors": "Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil\n  Feizi and Himabindu Lakkaraju",
    "title": "Certifying LLM Safety against Adversarial Prompting",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs) released for public use incorporate guardrails\nto ensure their output is safe, often referred to as \"model alignment.\" An\naligned language model should decline a user's request to produce harmful\ncontent. However, such safety measures are vulnerable to adversarial attacks,\nwhich add maliciously designed token sequences to a harmful prompt to bypass\nthe model's safety guards. In this work, we introduce erase-and-check, the\nfirst framework to defend against adversarial prompts with verifiable safety\nguarantees. We defend against three attack modes: i) adversarial suffix, which\nappends an adversarial sequence at the end of the prompt; ii) adversarial\ninsertion, where the adversarial sequence is inserted anywhere in the middle of\nthe prompt; and iii) adversarial infusion, where adversarial tokens are\ninserted at arbitrary positions in the prompt, not necessarily as a contiguous\nblock. Our experimental results demonstrate that this procedure can obtain\nstrong certified safety guarantees on harmful prompts while maintaining good\nempirical performance on safe prompts. For example, against adversarial\nsuffixes of length 20, it certifiably detects 92% of harmful prompts and labels\n94% of safe prompts correctly using the open-source language model Llama 2 as\nthe safety filter. We further improve the filter's performance, in terms of\naccuracy and speed, by replacing Llama 2 with a DistilBERT safety classifier\nfine-tuned on safe and harmful prompts. Additionally, we propose two efficient\nempirical defenses: i) RandEC, a randomized version of erase-and-check that\nevaluates the safety filter on a small subset of the erased subsequences, and\nii) GradEC, a gradient-based version that optimizes the erased tokens to remove\nthe adversarial sequence. The code for our experiments is available at\nhttps://github.com/aounon/certified-llm-safety.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 6 Sep 2023 04:37:20 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 28 Nov 2023 01:56:17 GMT"
      }
    ],
    "update_date": "2023-11-29",
    "authors_parsed": [
      [
        "Kumar",
        "Aounon",
        ""
      ],
      [
        "Agarwal",
        "Chirag",
        ""
      ],
      [
        "Srinivas",
        "Suraj",
        ""
      ],
      [
        "Li",
        "Aaron Jiaxun",
        ""
      ],
      [
        "Feizi",
        "Soheil",
        ""
      ],
      [
        "Lakkaraju",
        "Himabindu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.02705",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2309.03164",
    "submitter": "Tharindu Kumarage",
    "authors": "Tharindu Kumarage, Amrita Bhattacharjee, Djordje Padejski, Kristy\n  Roschke, Dan Gillmor, Scott Ruston, Huan Liu, Joshua Garland",
    "title": "J-Guard: Journalism Guided Adversarially Robust Detection of\n  AI-generated News",
    "comments": "This Paper is Accepted to The 13th International Joint Conference on\n  Natural Language Processing and the 3rd Conference of the Asia-Pacific\n  Chapter of the Association for Computational Linguistics (IJCNLP-AACL 2023)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The rapid proliferation of AI-generated text online is profoundly reshaping\nthe information landscape. Among various types of AI-generated text,\nAI-generated news presents a significant threat as it can be a prominent source\nof misinformation online. While several recent efforts have focused on\ndetecting AI-generated text in general, these methods require enhanced\nreliability, given concerns about their vulnerability to simple adversarial\nattacks. Furthermore, due to the eccentricities of news writing, applying these\ndetection methods for AI-generated news can produce false positives,\npotentially damaging the reputation of news organizations. To address these\nchallenges, we leverage the expertise of an interdisciplinary team to develop a\nframework, J-Guard, capable of steering existing supervised AI text detectors\nfor detecting AI-generated news while boosting adversarial robustness. By\nincorporating stylistic cues inspired by the unique journalistic attributes,\nJ-Guard effectively distinguishes between real-world journalism and\nAI-generated news articles. Our experiments on news articles generated by a\nvast array of AI models, including ChatGPT (GPT3.5), demonstrate the\neffectiveness of J-Guard in enhancing detection capabilities while maintaining\nan average performance decrease of as low as 7% when faced with adversarial\nattacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 6 Sep 2023 17:06:31 GMT"
      }
    ],
    "update_date": "2023-09-07",
    "authors_parsed": [
      [
        "Kumarage",
        "Tharindu",
        ""
      ],
      [
        "Bhattacharjee",
        "Amrita",
        ""
      ],
      [
        "Padejski",
        "Djordje",
        ""
      ],
      [
        "Roschke",
        "Kristy",
        ""
      ],
      [
        "Gillmor",
        "Dan",
        ""
      ],
      [
        "Ruston",
        "Scott",
        ""
      ],
      [
        "Liu",
        "Huan",
        ""
      ],
      [
        "Garland",
        "Joshua",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.03164",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2309.03241",
    "submitter": "Zhen Yang",
    "authors": "Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo,\n  Jinfeng Bai, Jie Tang",
    "title": "GPT Can Solve Mathematical Problems Without a Calculator",
    "comments": "26pages,14figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Previous studies have typically assumed that large language models are unable\nto accurately perform arithmetic operations, particularly multiplication of >8\ndigits, and operations involving decimals and fractions, without the use of\ncalculator tools. This paper aims to challenge this misconception. With\nsufficient training data, a 2 billion-parameter language model can accurately\nperform multi-digit arithmetic operations with almost 100% accuracy without\ndata leakage, significantly surpassing GPT-4 (whose multi-digit multiplication\naccuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from\nGLM-10B on a dataset with additional multi-step arithmetic operations and math\nproblems described in text, achieves similar performance to GPT-4 on a\n5,000-samples Chinese math problem test set. Our code and data are public at\nhttps://github.com/THUDM/MathGLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 6 Sep 2023 06:18:16 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 12 Sep 2023 11:01:25 GMT"
      }
    ],
    "update_date": "2023-09-13",
    "authors_parsed": [
      [
        "Yang",
        "Zhen",
        ""
      ],
      [
        "Ding",
        "Ming",
        ""
      ],
      [
        "Lv",
        "Qingsong",
        ""
      ],
      [
        "Jiang",
        "Zhihuan",
        ""
      ],
      [
        "He",
        "Zehai",
        ""
      ],
      [
        "Guo",
        "Yuyi",
        ""
      ],
      [
        "Bai",
        "Jinfeng",
        ""
      ],
      [
        "Tang",
        "Jie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.03241",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2309.03295",
    "submitter": "Nikhil Sontakke",
    "authors": "Nikhil Sontakke, Sejal Utekar, Shivansh Rastogi, Shriraj Sonawane",
    "title": "Comparative Analysis of Deep-Fake Algorithms",
    "comments": "7 pages, 4 figures, 2 tables, Published with International Journal of\n  Computer Science Trends and Technology (IJCST)",
    "journal-ref": "International Journal of Computer Science Trends and Technology\n  (IJCST) V11(4): Page(109-115) Jul - Aug 2023. ISSN: 2347-8578",
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Due to the widespread use of smartphones with high-quality digital cameras\nand easy access to a wide range of software apps for recording, editing, and\nsharing videos and images, as well as the deep learning AI platforms, a new\nphenomenon of 'faking' videos has emerged. Deepfake algorithms can create fake\nimages and videos that are virtually indistinguishable from authentic ones.\nTherefore, technologies that can detect and assess the integrity of digital\nvisual media are crucial. Deepfakes, also known as deep learning-based fake\nvideos, have become a major concern in recent years due to their ability to\nmanipulate and alter images and videos in a way that is virtually\nindistinguishable from the original. These deepfake videos can be used for\nmalicious purposes such as spreading misinformation, impersonating individuals,\nand creating fake news. Deepfake detection technologies use various approaches\nsuch as facial recognition, motion analysis, and audio-visual synchronization\nto identify and flag fake videos. However, the rapid advancement of deepfake\ntechnologies has made it increasingly difficult to detect these videos with\nhigh accuracy. In this paper, we aim to provide a comprehensive review of the\ncurrent state of deepfake creation and detection technologies. We examine the\nvarious deep learning-based approaches used for creating deepfakes, as well as\nthe techniques used for detecting them. Additionally, we analyze the\nlimitations and challenges of current deepfake detection methods and discuss\nfuture research directions in this field. Overall, the paper highlights the\nimportance of continued research and development in deepfake detection\ntechnologies in order to combat the negative impact of deepfakes on society and\nensure the integrity of digital visual media.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 6 Sep 2023 18:17:47 GMT"
      }
    ],
    "update_date": "2023-09-08",
    "authors_parsed": [
      [
        "Sontakke",
        "Nikhil",
        ""
      ],
      [
        "Utekar",
        "Sejal",
        ""
      ],
      [
        "Rastogi",
        "Shivansh",
        ""
      ],
      [
        "Sonawane",
        "Shriraj",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.03295",
    "publish_date": "2023-09-06"
  },
  {
    "id": "2309.03844",
    "submitter": "Brian Kim",
    "authors": "Naveen Naik Sapavath and Brian Kim and Kaushik Chowdhury and Vijay K\n  Shah",
    "title": "Experimental Study of Adversarial Attacks on ML-based xApps in O-RAN",
    "comments": "Accepted for Globecom 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI eess.SP",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Open Radio Access Network (O-RAN) is considered as a major step in the\nevolution of next-generation cellular networks given its support for open\ninterfaces and utilization of artificial intelligence (AI) into the deployment,\noperation, and maintenance of RAN. However, due to the openness of the O-RAN\narchitecture, such AI models are inherently vulnerable to various adversarial\nmachine learning (ML) attacks, i.e., adversarial attacks which correspond to\nslight manipulation of the input to the ML model. In this work, we showcase the\nvulnerability of an example ML model used in O-RAN, and experimentally deploy\nit in the near-real time (near-RT) RAN intelligent controller (RIC). Our\nML-based interference classifier xApp (extensible application in near-RT RIC)\ntries to classify the type of interference to mitigate the interference effect\non the O-RAN system. We demonstrate the first-ever scenario of how such an xApp\ncan be impacted through an adversarial attack by manipulating the data stored\nin a shared database inside the near-RT RIC. Through a rigorous performance\nanalysis deployed on a laboratory O-RAN testbed, we evaluate the performance in\nterms of capacity and the prediction accuracy of the interference classifier\nxApp using both clean and perturbed data. We show that even small adversarial\nattacks can significantly decrease the accuracy of ML application in near-RT\nRIC, which can directly impact the performance of the entire O-RAN deployment.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Sep 2023 16:58:15 GMT"
      }
    ],
    "update_date": "2023-09-08",
    "authors_parsed": [
      [
        "Sapavath",
        "Naveen Naik",
        ""
      ],
      [
        "Kim",
        "Brian",
        ""
      ],
      [
        "Chowdhury",
        "Kaushik",
        ""
      ],
      [
        "Shah",
        "Vijay K",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.03844",
    "publish_date": "2023-09-07"
  },
  {
    "id": "2309.04211",
    "submitter": "Edward Small",
    "authors": "Edward A. Small, Jeffrey N. Clark, Christopher J. McWilliams, Kacper\n  Sokol, Jeffrey Chan, Flora D. Salim, Raul Santos-Rodriguez",
    "title": "Counterfactual Explanations via Locally-guided Sequential Algorithmic\n  Recourse",
    "comments": "7 pages, 5 figures, 3 appendix pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CY",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Counterfactuals operationalised through algorithmic recourse have become a\npowerful tool to make artificial intelligence systems explainable.\nConceptually, given an individual classified as y -- the factual -- we seek\nactions such that their prediction becomes the desired class y' -- the\ncounterfactual. This process offers algorithmic recourse that is (1) easy to\ncustomise and interpret, and (2) directly aligned with the goals of each\nindividual. However, the properties of a \"good\" counterfactual are still\nlargely debated; it remains an open challenge to effectively locate a\ncounterfactual along with its corresponding recourse. Some strategies use\ngradient-driven methods, but these offer no guarantees on the feasibility of\nthe recourse and are open to adversarial attacks on carefully created\nmanifolds. This can lead to unfairness and lack of robustness. Other methods\nare data-driven, which mostly addresses the feasibility problem at the expense\nof privacy, security and secrecy as they require access to the entire training\ndata set. Here, we introduce LocalFACE, a model-agnostic technique that\ncomposes feasible and actionable counterfactual explanations using\nlocally-acquired information at each step of the algorithmic recourse. Our\nexplainer preserves the privacy of users by only leveraging data that it\nspecifically requires to construct actionable algorithmic recourse, and\nprotects the model by offering transparency solely in the regions deemed\nnecessary for the intervention.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Sep 2023 08:47:23 GMT"
      }
    ],
    "update_date": "2023-09-11",
    "authors_parsed": [
      [
        "Small",
        "Edward A.",
        ""
      ],
      [
        "Clark",
        "Jeffrey N.",
        ""
      ],
      [
        "McWilliams",
        "Christopher J.",
        ""
      ],
      [
        "Sokol",
        "Kacper",
        ""
      ],
      [
        "Chan",
        "Jeffrey",
        ""
      ],
      [
        "Salim",
        "Flora D.",
        ""
      ],
      [
        "Santos-Rodriguez",
        "Raul",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.04211",
    "publish_date": "2023-09-08"
  },
  {
    "id": "2309.05274",
    "submitter": "Dongyu Yao",
    "authors": "Dongyu Yao, Jianshu Zhang, Ian G. Harris, Marcel Carlsson",
    "title": "FuzzLLM: A Novel and Universal Fuzzing Framework for Proactively\n  Discovering Jailbreak Vulnerabilities in Large Language Models",
    "comments": "In submission, a preprint version",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Jailbreak vulnerabilities in Large Language Models (LLMs), which exploit\nmeticulously crafted prompts to elicit content that violates service\nguidelines, have captured the attention of research communities. While model\nowners can defend against individual jailbreak prompts through safety training\nstrategies, this relatively passive approach struggles to handle the broader\ncategory of similar jailbreaks. To tackle this issue, we introduce FuzzLLM, an\nautomated fuzzing framework designed to proactively test and discover jailbreak\nvulnerabilities in LLMs. We utilize templates to capture the structural\nintegrity of a prompt and isolate key features of a jailbreak class as\nconstraints. By integrating different base classes into powerful combo attacks\nand varying the elements of constraints and prohibited questions, FuzzLLM\nenables efficient testing with reduced manual effort. Extensive experiments\ndemonstrate FuzzLLM's effectiveness and comprehensiveness in vulnerability\ndiscovery across various LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 11 Sep 2023 07:15:02 GMT"
      }
    ],
    "update_date": "2023-09-12",
    "authors_parsed": [
      [
        "Yao",
        "Dongyu",
        ""
      ],
      [
        "Zhang",
        "Jianshu",
        ""
      ],
      [
        "Harris",
        "Ian G.",
        ""
      ],
      [
        "Carlsson",
        "Marcel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.05274",
    "publish_date": "2023-09-11"
  },
  {
    "id": "2309.05900",
    "submitter": "Gustavo Olague Dr.",
    "authors": "Gustavo Olague, Roberto Pineda, Gerardo Ibarra-Vazquez, Matthieu\n  Olague, Axel Martinez, Sambit Bakshi, Jonathan Vargas and Isnardo Reducindo",
    "title": "Adversarial Attacks Assessment of Salient Object Detection via Symbolic\n  Learning",
    "comments": "14 pages, 8 figures, 6 tables, IEEE Transactions on Emerging Topics\n  in Computing, Accepted for publication",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR cs.LG cs.NE",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Machine learning is at the center of mainstream technology and outperforms\nclassical approaches to handcrafted feature design. Aside from its learning\nprocess for artificial feature extraction, it has an end-to-end paradigm from\ninput to output, reaching outstandingly accurate results. However, security\nconcerns about its robustness to malicious and imperceptible perturbations have\ndrawn attention since its prediction can be changed entirely. Salient object\ndetection is a research area where deep convolutional neural networks have\nproven effective but whose trustworthiness represents a significant issue\nrequiring analysis and solutions to hackers' attacks. Brain programming is a\nkind of symbolic learning in the vein of good old-fashioned artificial\nintelligence. This work provides evidence that symbolic learning robustness is\ncrucial in designing reliable visual attention systems since it can withstand\neven the most intense perturbations. We test this evolutionary computation\nmethodology against several adversarial attacks and noise perturbations using\nstandard databases and a real-world problem of a shorebird called the Snowy\nPlover portraying a visual attention task. We compare our methodology with five\ndifferent deep learning approaches, proving that they do not match the symbolic\nparadigm regarding robustness. All neural networks suffer significant\nperformance losses, while brain programming stands its ground and remains\nunaffected. Also, by studying the Snowy Plover, we remark on the importance of\nsecurity in surveillance activities regarding wildlife protection and\nconservation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Sep 2023 01:03:43 GMT"
      }
    ],
    "update_date": "2023-09-13",
    "authors_parsed": [
      [
        "Olague",
        "Gustavo",
        ""
      ],
      [
        "Pineda",
        "Roberto",
        ""
      ],
      [
        "Ibarra-Vazquez",
        "Gerardo",
        ""
      ],
      [
        "Olague",
        "Matthieu",
        ""
      ],
      [
        "Martinez",
        "Axel",
        ""
      ],
      [
        "Bakshi",
        "Sambit",
        ""
      ],
      [
        "Vargas",
        "Jonathan",
        ""
      ],
      [
        "Reducindo",
        "Isnardo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.05900",
    "publish_date": "2023-09-12"
  },
  {
    "id": "2309.06055",
    "submitter": "Pengzhou Cheng",
    "authors": "Pengzhou Cheng, Zongru Wu, Wei Du, Haodong Zhao, Wei Lu, Gongshen Liu",
    "title": "Backdoor Attacks and Countermeasures in Natural Language Processing\n  Models: A Comprehensive Security Review",
    "comments": "21 pages, 4 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Applicating third-party data and models has become a new paradigm for\nlanguage modeling in NLP, which also introduces some potential security\nvulnerabilities because attackers can manipulate the training process and data\nsource. In this case, backdoor attacks can induce the model to exhibit expected\nbehaviors through specific triggers and have little inferior influence on\nprimitive tasks. Hence, it could have dire consequences, especially considering\nthat the backdoor attack surfaces are broad.\n  However, there is still no systematic and comprehensive review to reflect the\nsecurity challenges, attacker's capabilities, and purposes according to the\nattack surface. Moreover, there is a shortage of analysis and comparison of the\ndiverse emerging backdoor countermeasures in this context. In this paper, we\nconduct a timely review of backdoor attacks and countermeasures to sound the\nred alarm for the NLP security community. According to the affected stage of\nthe machine learning pipeline, the attack surfaces are recognized to be wide\nand then formalized into three categorizations: attacking pre-trained model\nwith fine-tuning (APMF) or parameter-efficient tuning (APMP), and attacking\nfinal model with training (AFMT). Thus, attacks under each categorization are\ncombed. The countermeasures are categorized into two general classes: sample\ninspection and model inspection. Overall, the research on the defense side is\nfar behind the attack side, and there is no single defense that can prevent all\ntypes of backdoor attacks. An attacker can intelligently bypass existing\ndefenses with a more invisible attack. Drawing the insights from the systematic\nreview, we also present crucial areas for future research on the backdoor, such\nas empirical security evaluations on large language models, and in particular,\nmore efficient and practical countermeasures are solicited.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Sep 2023 08:48:38 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 13 Sep 2023 02:21:18 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 11 Oct 2023 07:45:04 GMT"
      },
      {
        "version": "v4",
        "created": "Wed, 8 Nov 2023 07:53:26 GMT"
      }
    ],
    "update_date": "2023-11-09",
    "authors_parsed": [
      [
        "Cheng",
        "Pengzhou",
        ""
      ],
      [
        "Wu",
        "Zongru",
        ""
      ],
      [
        "Du",
        "Wei",
        ""
      ],
      [
        "Zhao",
        "Haodong",
        ""
      ],
      [
        "Lu",
        "Wei",
        ""
      ],
      [
        "Liu",
        "Gongshen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.06055",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2309.08583",
    "submitter": "Arkadiy Saakyan",
    "authors": "Arkadiy Saakyan and Smaranda Muresan",
    "title": "ICLEF: In-Context Learning with Expert Feedback for Explainable Style\n  Transfer",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  While state-of-the-art language models excel at the style transfer task,\ncurrent work does not address explainability of style transfer systems.\nExplanations could be generated using large language models such as GPT-3.5 and\nGPT-4, but the use of such complex systems is inefficient when smaller, widely\ndistributed, and transparent alternatives are available. We propose a framework\nto augment and improve a formality style transfer dataset with explanations via\nmodel distillation from ChatGPT. To further refine the generated explanations,\nwe propose a novel way to incorporate scarce expert human feedback using\nin-context learning (ICLEF: In-Context Learning from Expert Feedback) by\nprompting ChatGPT to act as a critic to its own outputs. We use the resulting\ndataset of 9,960 explainable formality style transfer instances (e-GYAFC) to\nshow that current openly distributed instruction-tuned models (and, in some\nsettings, ChatGPT) perform poorly on the task, and that fine-tuning on our\nhigh-quality dataset leads to significant improvements as shown by automatic\nevaluation. In human evaluation, we show that models much smaller than ChatGPT\nfine-tuned on our data align better with expert preferences. Finally, we\ndiscuss two potential applications of models fine-tuned on the explainable\nstyle transfer task: interpretable authorship verification and interpretable\nadversarial attacks on AI-generated text detectors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 15 Sep 2023 17:41:14 GMT"
      }
    ],
    "update_date": "2023-09-18",
    "authors_parsed": [
      [
        "Saakyan",
        "Arkadiy",
        ""
      ],
      [
        "Muresan",
        "Smaranda",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.08583",
    "publish_date": "2023-09-15"
  },
  {
    "id": "2309.08650",
    "submitter": "Aneta Koleva",
    "authors": "Aneta Koleva, Martin Ringsquandl and Volker Tresp",
    "title": "Adversarial Attacks on Tables with Entity Swap",
    "comments": "Accepted at TaDA workshop at VLDB 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The capabilities of large language models (LLMs) have been successfully\napplied in the context of table representation learning. The recently proposed\ntabular language models have reported state-of-the-art results across various\ntasks for table interpretation. However, a closer look into the datasets\ncommonly used for evaluation reveals an entity leakage from the train set into\nthe test set. Motivated by this observation, we explore adversarial attacks\nthat represent a more realistic inference setup. Adversarial attacks on text\nhave been shown to greatly affect the performance of LLMs, but currently, there\nare no attacks targeting tabular language models. In this paper, we propose an\nevasive entity-swap attack for the column type annotation (CTA) task. Our CTA\nattack is the first black-box attack on tables, where we employ a\nsimilarity-based sampling strategy to generate adversarial examples. The\nexperimental results show that the proposed attack generates up to a 70% drop\nin performance.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 15 Sep 2023 15:03:33 GMT"
      }
    ],
    "update_date": "2023-09-19",
    "authors_parsed": [
      [
        "Koleva",
        "Aneta",
        ""
      ],
      [
        "Ringsquandl",
        "Martin",
        ""
      ],
      [
        "Tresp",
        "Volker",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.08650",
    "publish_date": "2023-09-15"
  },
  {
    "id": "2309.10253",
    "submitter": "Jiahao Yu",
    "authors": "Jiahao Yu, Xingwei Lin, Zheng Yu, Xinyu Xing",
    "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated\n  Jailbreak Prompts",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs) have recently experienced tremendous popularity\nand are widely used from casual conversations to AI-driven programming.\nHowever, despite their considerable success, LLMs are not entirely reliable and\ncan give detailed guidance on how to conduct harmful or illegal activities.\nWhile safety measures can reduce the risk of such outputs, adversarial\njailbreak attacks can still exploit LLMs to produce harmful content. These\njailbreak templates are typically manually crafted, making large-scale testing\nchallenging.\n  In this paper, we introduce GPTFuzz, a novel black-box jailbreak fuzzing\nframework inspired by the AFL fuzzing framework. Instead of manual engineering,\nGPTFuzz automates the generation of jailbreak templates for red-teaming LLMs.\nAt its core, GPTFuzz starts with human-written templates as initial seeds, then\nmutates them to produce new templates. We detail three key components of\nGPTFuzz: a seed selection strategy for balancing efficiency and variability,\nmutate operators for creating semantically equivalent or similar sentences, and\na judgment model to assess the success of a jailbreak attack.\n  We evaluate GPTFuzz against various commercial and open-source LLMs,\nincluding ChatGPT, LLaMa-2, and Vicuna, under diverse attack scenarios. Our\nresults indicate that GPTFuzz consistently produces jailbreak templates with a\nhigh success rate, surpassing human-crafted templates. Remarkably, GPTFuzz\nachieves over 90% attack success rates against ChatGPT and Llama-2 models, even\nwith suboptimal initial seed templates. We anticipate that GPTFuzz will be\ninstrumental for researchers and practitioners in examining LLM robustness and\nwill encourage further exploration into enhancing LLM safety.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 19 Sep 2023 02:19:48 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 4 Oct 2023 06:15:12 GMT"
      }
    ],
    "update_date": "2023-10-05",
    "authors_parsed": [
      [
        "Yu",
        "Jiahao",
        ""
      ],
      [
        "Lin",
        "Xingwei",
        ""
      ],
      [
        "Yu",
        "Zheng",
        ""
      ],
      [
        "Xing",
        "Xinyu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.10253",
    "publish_date": "2023-10-04"
  },
  {
    "id": "2309.10544",
    "submitter": "Lewis Birch",
    "authors": "Lewis Birch, William Hackett, Stefan Trawicki, Neeraj Suri, Peter\n  Garraghan",
    "title": "Model Leeching: An Extraction Attack Targeting LLMs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Model Leeching is a novel extraction attack targeting Large Language Models\n(LLMs), capable of distilling task-specific knowledge from a target LLM into a\nreduced parameter model. We demonstrate the effectiveness of our attack by\nextracting task capability from ChatGPT-3.5-Turbo, achieving 73% Exact Match\n(EM) similarity, and SQuAD EM and F1 accuracy scores of 75% and 87%,\nrespectively for only $50 in API cost. We further demonstrate the feasibility\nof adversarial attack transferability from an extracted model extracted via\nModel Leeching to perform ML attack staging against a target LLM, resulting in\nan 11% increase to attack success rate when applied to ChatGPT-3.5-Turbo.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 19 Sep 2023 11:45:29 GMT"
      }
    ],
    "update_date": "2023-09-20",
    "authors_parsed": [
      [
        "Birch",
        "Lewis",
        ""
      ],
      [
        "Hackett",
        "William",
        ""
      ],
      [
        "Trawicki",
        "Stefan",
        ""
      ],
      [
        "Suri",
        "Neeraj",
        ""
      ],
      [
        "Garraghan",
        "Peter",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.10544",
    "publish_date": "2023-09-19"
  },
  {
    "id": "2309.11196",
    "submitter": "Xiyue Zhang",
    "authors": "Marta Kwiatkowska, Xiyue Zhang",
    "title": "When to Trust AI: Advances and Challenges for Certification of Neural\n  Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CR cs.SC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Artificial intelligence (AI) has been advancing at a fast pace and it is now\npoised for deployment in a wide range of applications, such as autonomous\nsystems, medical diagnosis and natural language processing. Early adoption of\nAI technology for real-world applications has not been without problems,\nparticularly for neural networks, which may be unstable and susceptible to\nadversarial examples. In the longer term, appropriate safety assurance\ntechniques need to be developed to reduce potential harm due to avoidable\nsystem failures and ensure trustworthiness. Focusing on certification and\nexplainability, this paper provides an overview of techniques that have been\ndeveloped to ensure safety of AI decisions and discusses future challenges.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 20 Sep 2023 10:31:09 GMT"
      }
    ],
    "update_date": "2023-09-21",
    "authors_parsed": [
      [
        "Kwiatkowska",
        "Marta",
        ""
      ],
      [
        "Zhang",
        "Xiyue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.11196",
    "publish_date": "2023-09-20"
  },
  {
    "id": "2309.11751",
    "submitter": "Yinpeng Dong",
    "authors": "Yinpeng Dong, Huanran Chen, Jiawei Chen, Zhengwei Fang, Xiao Yang,\n  Yichi Zhang, Yu Tian, Hang Su, Jun Zhu",
    "title": "How Robust is Google's Bard to Adversarial Image Attacks?",
    "comments": "Technical report",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Multimodal Large Language Models (MLLMs) that integrate text and other\nmodalities (especially vision) have achieved unprecedented performance in\nvarious multimodal tasks. However, due to the unsolved adversarial robustness\nproblem of vision models, MLLMs can have more severe safety and security risks\nby introducing the vision inputs. In this work, we study the adversarial\nrobustness of Google's Bard, a competitive chatbot to ChatGPT that released its\nmultimodal capability recently, to better understand the vulnerabilities of\ncommercial MLLMs. By attacking white-box surrogate vision encoders or MLLMs,\nthe generated adversarial examples can mislead Bard to output wrong image\ndescriptions with a 22% success rate based solely on the transferability. We\nshow that the adversarial examples can also attack other MLLMs, e.g., a 26%\nattack success rate against Bing Chat and a 86% attack success rate against\nERNIE bot. Moreover, we identify two defense mechanisms of Bard, including face\ndetection and toxicity detection of images. We design corresponding attacks to\nevade these defenses, demonstrating that the current defenses of Bard are also\nvulnerable. We hope this work can deepen our understanding on the robustness of\nMLLMs and facilitate future research on defenses. Our code is available at\nhttps://github.com/thu-ml/Attack-Bard.\n  Update: GPT-4V is available at October 2023. We further evaluate its\nrobustness under the same set of adversarial examples, achieving a 45% attack\nsuccess rate.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Sep 2023 03:24:30 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 14 Oct 2023 12:56:13 GMT"
      }
    ],
    "update_date": "2023-10-17",
    "authors_parsed": [
      [
        "Dong",
        "Yinpeng",
        ""
      ],
      [
        "Chen",
        "Huanran",
        ""
      ],
      [
        "Chen",
        "Jiawei",
        ""
      ],
      [
        "Fang",
        "Zhengwei",
        ""
      ],
      [
        "Yang",
        "Xiao",
        ""
      ],
      [
        "Zhang",
        "Yichi",
        ""
      ],
      [
        "Tian",
        "Yu",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ],
      [
        "Zhu",
        "Jun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.11751",
    "publish_date": "2023-09-21"
  },
  {
    "id": "2309.11852",
    "submitter": "Yoichi Ishibashi",
    "authors": "Yoichi Ishibashi, Hidetoshi Shimodaira",
    "title": "Knowledge Sanitization of Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  We explore a knowledge sanitization approach to mitigate the privacy concerns\nassociated with large language models (LLMs). LLMs trained on a large corpus of\nWeb data can memorize and potentially reveal sensitive or confidential\ninformation, raising critical security concerns. Our technique fine-tunes these\nmodels, prompting them to generate harmless responses such as ``I don't know''\nwhen queried about specific information. Experimental results in a closed-book\nquestion-answering task show that our straightforward method not only minimizes\nparticular knowledge leakage but also preserves the overall performance of LLM.\nThese two advantages strengthen the defense against extraction attacks and\nreduces the emission of harmful content such as hallucinations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Sep 2023 07:49:55 GMT"
      }
    ],
    "update_date": "2023-09-22",
    "authors_parsed": [
      [
        "Ishibashi",
        "Yoichi",
        ""
      ],
      [
        "Shimodaira",
        "Hidetoshi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.11852",
    "publish_date": "2023-09-21"
  },
  {
    "id": "2309.12481",
    "submitter": "Leonardo Ranaldi Mr",
    "authors": "Leonardo Ranaldi, Fabio Massimo Zanzotto",
    "title": "HANS, are you clever? Clever Hans Effect Analysis of Neural Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Instruction-tuned Large Language Models (It-LLMs) have been exhibiting\noutstanding abilities to reason around cognitive states, intentions, and\nreactions of all people involved, letting humans guide and comprehend\nday-to-day social interactions effectively. In fact, several multiple-choice\nquestions (MCQ) benchmarks have been proposed to construct solid assessments of\nthe models' abilities. However, earlier works are demonstrating the presence of\ninherent \"order bias\" in It-LLMs, posing challenges to the appropriate\nevaluation. In this paper, we investigate It-LLMs' resilience abilities towards\na series of probing tests using four MCQ benchmarks. Introducing adversarial\nexamples, we show a significant performance gap, mainly when varying the order\nof the choices, which reveals a selection bias and brings into discussion\nreasoning abilities. Following a correlation between first positions and model\nchoices due to positional bias, we hypothesized the presence of structural\nheuristics in the decision-making process of the It-LLMs, strengthened by\nincluding significant examples in few-shot scenarios. Finally, by using the\nChain-of-Thought (CoT) technique, we elicit the model to reason and mitigate\nthe bias by obtaining more robust models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Sep 2023 20:52:18 GMT"
      }
    ],
    "update_date": "2023-09-29",
    "authors_parsed": [
      [
        "Ranaldi",
        "Leonardo",
        ""
      ],
      [
        "Zanzotto",
        "Fabio Massimo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.12481",
    "publish_date": "2023-09-21"
  },
  {
    "id": "2309.12934",
    "submitter": "Adaku Uchendu",
    "authors": "Adaku Uchendu, Thai Le, Dongwon Lee",
    "title": "TopRoBERTa: Topology-Aware Authorship Attribution of Deepfake Texts",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Recent advances in Large Language Models (LLMs) have enabled the generation\nof open-ended high-quality texts, that are non-trivial to distinguish from\nhuman-written texts. We refer to such LLM-generated texts as \\emph{deepfake\ntexts}. There are currently over 11K text generation models in the huggingface\nmodel repo. As such, users with malicious intent can easily use these\nopen-sourced LLMs to generate harmful texts and misinformation at scale. To\nmitigate this problem, a computational method to determine if a given text is a\ndeepfake text or not is desired--i.e., Turing Test (TT). In particular, in this\nwork, we investigate the more general version of the problem, known as\n\\emph{Authorship Attribution (AA)}, in a multi-class setting--i.e., not only\ndetermining if a given text is a deepfake text or not but also being able to\npinpoint which LLM is the author. We propose \\textbf{TopRoBERTa} to improve\nexisting AA solutions by capturing more linguistic patterns in deepfake texts\nby including a Topological Data Analysis (TDA) layer in the RoBERTa model. We\nshow the benefits of having a TDA layer when dealing with noisy, imbalanced,\nand heterogeneous datasets, by extracting TDA features from the reshaped\n$pooled\\_output$ of RoBERTa as input. We use RoBERTa to capture contextual\nrepresentations (i.e., semantic and syntactic linguistic features), while using\nTDA to capture the shape and structure of data (i.e., linguistic structures).\nFinally, \\textbf{TopRoBERTa}, outperforms the vanilla RoBERTa in 2/3 datasets,\nachieving up to 7\\% increase in Macro F1 score.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 22 Sep 2023 15:32:49 GMT"
      }
    ],
    "update_date": "2023-09-25",
    "authors_parsed": [
      [
        "Uchendu",
        "Adaku",
        ""
      ],
      [
        "Le",
        "Thai",
        ""
      ],
      [
        "Lee",
        "Dongwon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.12934",
    "publish_date": "2023-09-22"
  },
  {
    "id": "2309.12994",
    "submitter": "Huan Wu",
    "authors": "Huan Wu, Brian Fang, and Fei Xie",
    "title": "Smart Fuzzing of 5G Wireless Software Implementation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this paper, we introduce a comprehensive approach to bolstering the\nsecurity, reliability, and comprehensibility of OpenAirInterface5G (OAI5G), an\nopen-source software framework for the exploration, development, and testing of\n5G wireless communication systems. Firstly, we employ AFL++, a powerful fuzzing\ntool, to fuzzy-test OAI5G with respect to its configuration files rigorously.\nThis extensive testing process helps identify errors, defects, and security\nvulnerabilities that may evade conventional testing methods. Secondly, we\nharness the capabilities of Large Language Models such as Google Bard to\nautomatically decipher and document the meanings of parameters within the OAI5G\ncodebase that are used in fuzzing. This automated parameter interpretation\nstreamlines subsequent analyses and facilitates more informed decision-making.\nTogether, these two techniques contribute to fortifying the OAI5G system,\nmaking it more robust, secure, and understandable for developers and analysts\nalike.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 22 Sep 2023 16:45:42 GMT"
      }
    ],
    "update_date": "2023-09-25",
    "authors_parsed": [
      [
        "Wu",
        "Huan",
        ""
      ],
      [
        "Fang",
        "Brian",
        ""
      ],
      [
        "Xie",
        "Fei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.12994",
    "publish_date": "2023-09-22"
  },
  {
    "id": "2309.13444",
    "submitter": "Mojdeh Karbalaee",
    "authors": "Mojdeh Karbalaee Motalleb, Chafika Benza\\\"id, Tarik Taleb, Vahid\n  Shah-Mansouri",
    "title": "Moving Target Defense based Secured Network Slicing System in the O-RAN\n  Architecture",
    "comments": "6 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The open radio access network (O-RAN) architecture's native virtualization\nand embedded intelligence facilitate RAN slicing and enable comprehensive\nend-to-end services in post-5G networks. However, any vulnerabilities could\nharm security. Therefore, artificial intelligence (AI) and machine learning\n(ML) security threats can even threaten O-RAN benefits. This paper proposes a\nnovel approach to estimating the optimal number of predefined VNFs for each\nslice while addressing secure AI/ML methods for dynamic service admission\ncontrol and power minimization in the O-RAN architecture. We solve this problem\non two-time scales using mathematical methods for determining the predefined\nnumber of VNFs on a large time scale and the proximal policy optimization\n(PPO), a Deep Reinforcement Learning algorithm, for solving dynamic service\nadmission control and power minimization for different slices on a small-time\nscale. To secure the ML system for O-RAN, we implement a moving target defense\n(MTD) strategy to prevent poisoning attacks by adding uncertainty to the\nsystem. Our experimental results show that the proposed PPO-based service\nadmission control approach achieves an admission rate above 80\\% and that the\nMTD strategy effectively strengthens the robustness of the PPO method against\nadversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 23 Sep 2023 18:21:33 GMT"
      }
    ],
    "update_date": "2023-09-26",
    "authors_parsed": [
      [
        "Motalleb",
        "Mojdeh Karbalaee",
        ""
      ],
      [
        "Benza\u00efd",
        "Chafika",
        ""
      ],
      [
        "Taleb",
        "Tarik",
        ""
      ],
      [
        "Shah-Mansouri",
        "Vahid",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.13444",
    "publish_date": "2023-09-23"
  },
  {
    "id": "2309.14122",
    "submitter": "Jieming Zhong",
    "authors": "Zhongjie Ba, Jieming Zhong, Jiachen Lei, Peng Cheng, Qinglong Wang,\n  Zhan Qin, Zhibo Wang, Kui Ren",
    "title": "SurrogatePrompt: Bypassing the Safety Filter of Text-To-Image Models via\n  Substitution",
    "comments": "14 pages, 11 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Advanced text-to-image models such as DALL-E 2 and Midjourney possess the\ncapacity to generate highly realistic images, raising significant concerns\nregarding the potential proliferation of unsafe content. This includes adult,\nviolent, or deceptive imagery of political figures. Despite claims of rigorous\nsafety mechanisms implemented in these models to restrict the generation of\nnot-safe-for-work (NSFW) content, we successfully devise and exhibit the first\nprompt attacks on Midjourney, resulting in the production of abundant\nphotorealistic NSFW images. We reveal the fundamental principles of such prompt\nattacks and suggest strategically substituting high-risk sections within a\nsuspect prompt to evade closed-source safety measures. Our novel framework,\nSurrogatePrompt, systematically generates attack prompts, utilizing large\nlanguage models, image-to-text, and image-to-image modules to automate attack\nprompt creation at scale. Evaluation results disclose an 88% success rate in\nbypassing Midjourney's proprietary safety filter with our attack prompts,\nleading to the generation of counterfeit images depicting political figures in\nviolent scenarios. Both subjective and objective assessments validate that the\nimages generated from our attack prompts present considerable safety hazards.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 25 Sep 2023 13:20:15 GMT"
      }
    ],
    "update_date": "2023-09-26",
    "authors_parsed": [
      [
        "Ba",
        "Zhongjie",
        ""
      ],
      [
        "Zhong",
        "Jieming",
        ""
      ],
      [
        "Lei",
        "Jiachen",
        ""
      ],
      [
        "Cheng",
        "Peng",
        ""
      ],
      [
        "Wang",
        "Qinglong",
        ""
      ],
      [
        "Qin",
        "Zhan",
        ""
      ],
      [
        "Wang",
        "Zhibo",
        ""
      ],
      [
        "Ren",
        "Kui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.14122",
    "publish_date": "2023-09-25"
  },
  {
    "id": "2309.15025",
    "submitter": "Tianhao Shen",
    "authors": "Tianhao Shen, Renren Jin, Yufei Huang, Chuang Liu, Weilong Dong,\n  Zishan Guo, Xinwei Wu, Yan Liu, Deyi Xiong",
    "title": "Large Language Model Alignment: A Survey",
    "comments": "76 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Recent years have witnessed remarkable progress made in large language models\n(LLMs). Such advancements, while garnering significant attention, have\nconcurrently elicited various concerns. The potential of these models is\nundeniably vast; however, they may yield texts that are imprecise, misleading,\nor even detrimental. Consequently, it becomes paramount to employ alignment\ntechniques to ensure these models to exhibit behaviors consistent with human\nvalues.\n  This survey endeavors to furnish an extensive exploration of alignment\nmethodologies designed for LLMs, in conjunction with the extant capability\nresearch in this domain. Adopting the lens of AI alignment, we categorize the\nprevailing methods and emergent proposals for the alignment of LLMs into outer\nand inner alignment. We also probe into salient issues including the models'\ninterpretability, and potential vulnerabilities to adversarial attacks. To\nassess LLM alignment, we present a wide variety of benchmarks and evaluation\nmethodologies. After discussing the state of alignment research for LLMs, we\nfinally cast a vision toward the future, contemplating the promising avenues of\nresearch that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research\ninterests in this realm. We also envision bridging the gap between the AI\nalignment research community and the researchers engrossed in the capability\nexploration of LLMs for both capable and safe LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 26 Sep 2023 15:49:23 GMT"
      }
    ],
    "update_date": "2023-09-27",
    "authors_parsed": [
      [
        "Shen",
        "Tianhao",
        ""
      ],
      [
        "Jin",
        "Renren",
        ""
      ],
      [
        "Huang",
        "Yufei",
        ""
      ],
      [
        "Liu",
        "Chuang",
        ""
      ],
      [
        "Dong",
        "Weilong",
        ""
      ],
      [
        "Guo",
        "Zishan",
        ""
      ],
      [
        "Wu",
        "Xinwei",
        ""
      ],
      [
        "Liu",
        "Yan",
        ""
      ],
      [
        "Xiong",
        "Deyi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.15025",
    "publish_date": "2023-09-26"
  },
  {
    "id": "2309.16211",
    "submitter": "Zihao Zhu",
    "authors": "Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu",
    "title": "VDC: Versatile Data Cleanser for Detecting Dirty Samples via\n  Visual-Linguistic Inconsistency",
    "comments": "22 pages,5 figures,17 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The role of data in building AI systems has recently been emphasized by the\nemerging concept of data-centric AI. Unfortunately, in the real-world, datasets\nmay contain dirty samples, such as poisoned samples from backdoor attack, noisy\nlabels in crowdsourcing, and even hybrids of them. The presence of such dirty\nsamples makes the DNNs vunerable and unreliable.Hence, it is critical to detect\ndirty samples to improve the quality and realiability of dataset. Existing\ndetectors only focus on detecting poisoned samples or noisy labels, that are\noften prone to weak generalization when dealing with dirty samples from other\ndomains.In this paper, we find a commonality of various dirty samples is\nvisual-linguistic inconsistency between images and associated labels. To\ncapture the semantic inconsistency between modalities, we propose versatile\ndata cleanser (VDC) leveraging the surpassing capabilities of multimodal large\nlanguage models (MLLM) in cross-modal alignment and reasoning.It consists of\nthree consecutive modules: the visual question generation module to generate\ninsightful questions about the image; the visual question answering module to\nacquire the semantics of the visual content by answering the questions with\nMLLM; followed by the visual answer evaluation module to evaluate the\ninconsistency.Extensive experiments demonstrate its superior performance and\ngeneralization to various categories and types of dirty samples.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 28 Sep 2023 07:37:18 GMT"
      }
    ],
    "update_date": "2023-09-29",
    "authors_parsed": [
      [
        "Zhu",
        "Zihao",
        ""
      ],
      [
        "Zhang",
        "Mingda",
        ""
      ],
      [
        "Wei",
        "Shaokui",
        ""
      ],
      [
        "Wu",
        "Bingzhe",
        ""
      ],
      [
        "Wu",
        "Baoyuan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.16211",
    "publish_date": "2023-09-28"
  },
  {
    "id": "2309.16595",
    "submitter": "Jin Huang",
    "authors": "Jin Huang, Xingjian Zhang, Qiaozhu Mei, Jiaqi Ma",
    "title": "Can LLMs Effectively Leverage Graph Structural Information: When and Why",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper studies Large Language Models (LLMs) augmented with structured\ndata--particularly graphs--a crucial data modality that remains underexplored\nin the LLM literature. We aim to understand when and why the incorporation of\nstructural information inherent in graph data can improve the prediction\nperformance of LLMs on node classification tasks with textual features. To\naddress the ``when'' question, we examine a variety of prompting methods for\nencoding structural information, in settings where textual node features are\neither rich or scarce. For the ``why'' questions, we probe into two potential\ncontributing factors to the LLM performance: data leakage and homophily. Our\nexploration of these questions reveals that (i) LLMs can benefit from\nstructural information, especially when textual node features are scarce; (ii)\nthere is no substantial evidence indicating that the performance of LLMs is\nsignificantly attributed to data leakage; and (iii) the performance of LLMs on\na target node is strongly positively related to the local homophily ratio of\nthe node\\footnote{Codes and datasets are at:\n\\url{https://github.com/TRAIS-Lab/LLM-Structured-Data}}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 28 Sep 2023 16:58:37 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 29 Sep 2023 20:59:55 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Huang",
        "Jin",
        ""
      ],
      [
        "Zhang",
        "Xingjian",
        ""
      ],
      [
        "Mei",
        "Qiaozhu",
        ""
      ],
      [
        "Ma",
        "Jiaqi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.16595",
    "publish_date": "2023-09-28"
  },
  {
    "id": "2309.17410",
    "submitter": "Peter Hase",
    "authors": "Vaidehi Patil, Peter Hase, Mohit Bansal",
    "title": "Can Sensitive Information Be Deleted From LLMs? Objectives for Defending\n  Against Extraction Attacks",
    "comments": "Equal contribution from first two authors. 19 pages, 5 figures. Our\n  code is available at: https://github.com/Vaidehi99/InfoDeletionAttacks",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Pretrained language models sometimes possess knowledge that we do not wish\nthem to, including memorized personal information and knowledge that could be\nused to harm people. They can also output toxic or harmful text. To mitigate\nthese safety and informational issues, we propose an attack-and-defense\nframework for studying the task of deleting sensitive information directly from\nmodel weights. We study direct edits to model weights because (1) this approach\nshould guarantee that particular deleted information is never extracted by\nfuture prompt attacks, and (2) it should protect against whitebox attacks,\nwhich is necessary for making claims about safety/privacy in a setting where\npublicly available model weights could be used to elicit sensitive information.\nOur threat model assumes that an attack succeeds if the answer to a sensitive\nquestion is located among a set of B generated candidates, based on scenarios\nwhere the information would be insecure if the answer is among B candidates.\nExperimentally, we show that even state-of-the-art model editing methods such\nas ROME struggle to truly delete factual information from models like GPT-J, as\nour whitebox and blackbox attacks can recover \"deleted\" information from an\nedited model 38% of the time. These attacks leverage two key observations: (1)\nthat traces of deleted information can be found in intermediate model hidden\nstates, and (2) that applying an editing method for one question may not delete\ninformation across rephrased versions of the question. Finally, we provide new\ndefense methods that protect against some extraction attacks, but we do not\nfind a single universally effective defense method. Our results suggest that\ntruly deleting sensitive information is a tractable but difficult problem,\nsince even relatively low attack success rates have potentially severe societal\nimplications for real-world deployment of language models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Sep 2023 17:12:43 GMT"
      }
    ],
    "update_date": "2023-10-02",
    "authors_parsed": [
      [
        "Patil",
        "Vaidehi",
        ""
      ],
      [
        "Hase",
        "Peter",
        ""
      ],
      [
        "Bansal",
        "Mohit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2309.17410",
    "publish_date": "2023-09-29"
  },
  {
    "id": "2310.00076",
    "submitter": "Mehrdad Saberi",
    "authors": "Mehrdad Saberi, Vinu Sankar Sadasivan, Keivan Rezaei, Aounon Kumar,\n  Atoosa Chegini, Wenxiao Wang, Soheil Feizi",
    "title": "Robustness of AI-Image Detectors: Fundamental Limits and Practical\n  Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In light of recent advancements in generative AI models, it has become\nessential to distinguish genuine content from AI-generated one to prevent the\nmalicious usage of fake materials as authentic ones and vice versa. Various\ntechniques have been introduced for identifying AI-generated images, with\nwatermarking emerging as a promising approach. In this paper, we analyze the\nrobustness of various AI-image detectors including watermarking and\nclassifier-based deepfake detectors. For watermarking methods that introduce\nsubtle image perturbations (i.e., low perturbation budget methods), we reveal a\nfundamental trade-off between the evasion error rate (i.e., the fraction of\nwatermarked images detected as non-watermarked ones) and the spoofing error\nrate (i.e., the fraction of non-watermarked images detected as watermarked\nones) upon an application of a diffusion purification attack. In this regime,\nwe also empirically show that diffusion purification effectively removes\nwatermarks with minimal changes to images. For high perturbation watermarking\nmethods where notable changes are applied to images, the diffusion purification\nattack is not effective. In this case, we develop a model substitution\nadversarial attack that can successfully remove watermarks. Moreover, we show\nthat watermarking methods are vulnerable to spoofing attacks where the attacker\naims to have real images (potentially obscene) identified as watermarked ones,\ndamaging the reputation of the developers. In particular, by just having\nblack-box access to the watermarking method, we show that one can generate a\nwatermarked noise image which can be added to the real images to have them\nfalsely flagged as watermarked ones. Finally, we extend our theory to\ncharacterize a fundamental trade-off between the robustness and reliability of\nclassifier-based deep fake detectors and demonstrate it through experiments.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Sep 2023 18:30:29 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Saberi",
        "Mehrdad",
        ""
      ],
      [
        "Sadasivan",
        "Vinu Sankar",
        ""
      ],
      [
        "Rezaei",
        "Keivan",
        ""
      ],
      [
        "Kumar",
        "Aounon",
        ""
      ],
      [
        "Chegini",
        "Atoosa",
        ""
      ],
      [
        "Wang",
        "Wenxiao",
        ""
      ],
      [
        "Feizi",
        "Soheil",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.00076",
    "publish_date": "2023-09-29"
  },
  {
    "id": "2310.00313",
    "submitter": "Safoora Yousefi",
    "authors": "Safoora Yousefi, Leo Betthauser, Hosein Hasanbeig, Akanksha Saran,\n  Rapha\\\"el Milli\\`ere, Ida Momennejad",
    "title": "In-Context Learning in Large Language Models: A Neuroscience-inspired\n  Analysis of Representations",
    "comments": "Added overview figures 1-3 in this version",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs) exhibit remarkable performance improvement\nthrough in-context learning (ICL) by leveraging task-specific examples in the\ninput. However, the mechanisms behind this improvement remain elusive. In this\nwork, we investigate embeddings and attention representations in Llama-2 70B\nand Vicuna 13B. Specifically, we study how embeddings and attention change\nafter in-context-learning, and how these changes mediate improvement in\nbehavior. We employ neuroscience-inspired techniques, such as representational\nsimilarity analysis (RSA), and propose novel methods for parameterized probing\nand attention ratio analysis (ARA, measuring the ratio of attention to relevant\nvs. irrelevant information). We designed three tasks with a priori\nrelationships among their conditions: reading comprehension, linear regression,\nand adversarial prompt injection. We formed hypotheses about expected\nsimilarities in task representations to investigate latent changes in\nembeddings and attention. Our analyses revealed a meaningful correlation\nbetween changes in both embeddings and attention representations with\nimprovements in behavioral performance after ICL. This empirical framework\nempowers a nuanced understanding of how latent representations affect LLM\nbehavior with and without ICL, offering valuable tools and insights for future\nresearch and practical applications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 30 Sep 2023 09:01:35 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 18 Oct 2023 08:53:46 GMT"
      }
    ],
    "update_date": "2023-10-19",
    "authors_parsed": [
      [
        "Yousefi",
        "Safoora",
        ""
      ],
      [
        "Betthauser",
        "Leo",
        ""
      ],
      [
        "Hasanbeig",
        "Hosein",
        ""
      ],
      [
        "Saran",
        "Akanksha",
        ""
      ],
      [
        "Milli\u00e8re",
        "Rapha\u00ebl",
        ""
      ],
      [
        "Momennejad",
        "Ida",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.00313",
    "publish_date": "2023-10-18"
  },
  {
    "id": "2310.00416",
    "submitter": "Xuanxiang Huang",
    "authors": "Xuanxiang Huang, Joao Marques-Silva",
    "title": "Refutation of Shapley Values for XAI -- Additional Evidence",
    "comments": "arXiv admin note: text overlap with arXiv:2307.07514",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent work demonstrated the inadequacy of Shapley values for explainable\nartificial intelligence (XAI). Although to disprove a theory a single\ncounterexample suffices, a possible criticism of earlier work is that the focus\nwas solely on Boolean classifiers. To address such possible criticism, this\npaper demonstrates the inadequacy of Shapley values for families of classifiers\nwhere features are not boolean, but also for families of classifiers for which\nmultiple classes can be picked. Furthermore, the paper shows that the features\nchanged in any minimal $l_0$ distance adversarial examples do not include\nirrelevant features, thus offering further arguments regarding the inadequacy\nof Shapley values for XAI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 30 Sep 2023 15:44:06 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Huang",
        "Xuanxiang",
        ""
      ],
      [
        "Marques-Silva",
        "Joao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.00416",
    "publish_date": "2023-09-30"
  },
  {
    "id": "2310.00503",
    "submitter": "Alina Elena Baia",
    "authors": "Alina Elena Baia, Valentina Poggioni, Andrea Cavallaro",
    "title": "Black-box Attacks on Image Activity Prediction and its Natural Language\n  Explanations",
    "comments": "Accepted at ICCV2023 AROW Workshop",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Explainable AI (XAI) methods aim to describe the decision process of deep\nneural networks. Early XAI methods produced visual explanations, whereas more\nrecent techniques generate multimodal explanations that include textual\ninformation and visual representations. Visual XAI methods have been shown to\nbe vulnerable to white-box and gray-box adversarial attacks, with an attacker\nhaving full or partial knowledge of and access to the target system. As the\nvulnerabilities of multimodal XAI models have not been examined, in this paper\nwe assess for the first time the robustness to black-box attacks of the natural\nlanguage explanations generated by a self-rationalizing image-based activity\nrecognition model. We generate unrestricted, spatially variant perturbations\nthat disrupt the association between the predictions and the corresponding\nexplanations to mislead the model into generating unfaithful explanations. We\nshow that we can create adversarial images that manipulate the explanations of\nan activity recognition model by having access only to its final output.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 30 Sep 2023 21:56:43 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Baia",
        "Alina Elena",
        ""
      ],
      [
        "Poggioni",
        "Valentina",
        ""
      ],
      [
        "Cavallaro",
        "Andrea",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.00503",
    "publish_date": "2023-09-30"
  },
  {
    "id": "2310.00633",
    "submitter": "Yanjie Li Mr.",
    "authors": "Yanjie Li, Bin Xie, Songtao Guo, Yuanyuan Yang, Bin Xiao",
    "title": "A Survey of Robustness and Safety of 2D and 3D Deep Learning Models\n  Against Adversarial Attacks",
    "comments": "Submitted to CSUR",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Benefiting from the rapid development of deep learning, 2D and 3D computer\nvision applications are deployed in many safe-critical systems, such as\nautopilot and identity authentication. However, deep learning models are not\ntrustworthy enough because of their limited robustness against adversarial\nattacks. The physically realizable adversarial attacks further pose fatal\nthreats to the application and human safety. Lots of papers have emerged to\ninvestigate the robustness and safety of deep learning models against\nadversarial attacks. To lead to trustworthy AI, we first construct a general\nthreat model from different perspectives and then comprehensively review the\nlatest progress of both 2D and 3D adversarial attacks. We extend the concept of\nadversarial examples beyond imperceptive perturbations and collate over 170\npapers to give an overview of deep learning model robustness against various\nadversarial attacks. To the best of our knowledge, we are the first to\nsystematically investigate adversarial attacks for 3D models, a flourishing\nfield applied to many real-world applications. In addition, we examine physical\nadversarial attacks that lead to safety violations. Last but not least, we\nsummarize present popular topics, give insights on challenges, and shed light\non future research on trustworthy AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 1 Oct 2023 10:16:33 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Li",
        "Yanjie",
        ""
      ],
      [
        "Xie",
        "Bin",
        ""
      ],
      [
        "Guo",
        "Songtao",
        ""
      ],
      [
        "Yang",
        "Yuanyuan",
        ""
      ],
      [
        "Xiao",
        "Bin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.00633",
    "publish_date": "2023-10-01"
  },
  {
    "id": "2310.01386",
    "submitter": "Jen-Tse Huang",
    "authors": "Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren,\n  Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu",
    "title": "Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using\n  PsychoBench",
    "comments": "15 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have recently showcased their remarkable\ncapacities, not only in natural language processing tasks but also across\ndiverse domains such as clinical medicine, legal consultation, and education.\nLLMs become more than mere applications, evolving into assistants capable of\naddressing diverse user requests. This narrows the distinction between human\nbeings and artificial intelligence agents, raising intriguing questions\nregarding the potential manifestation of personalities, temperaments, and\nemotions within LLMs. In this paper, we propose a framework, PsychoBench, for\nevaluating diverse psychological aspects of LLMs. Comprising thirteen scales\ncommonly used in clinical psychology, PsychoBench further classifies these\nscales into four distinct categories: personality traits, interpersonal\nrelationships, motivational tests, and emotional abilities. Our study examines\nfive popular models, namely \\texttt{text-davinci-003}, ChatGPT, GPT-4,\nLLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to\nbypass the safety alignment protocols and test the intrinsic natures of LLMs.\nWe have made PsychoBench openly accessible via\n\\url{https://github.com/CUHK-ARISE/PsychoBench}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 2 Oct 2023 17:46:09 GMT"
      }
    ],
    "update_date": "2023-10-03",
    "authors_parsed": [
      [
        "Huang",
        "Jen-tse",
        ""
      ],
      [
        "Wang",
        "Wenxuan",
        ""
      ],
      [
        "Li",
        "Eric John",
        ""
      ],
      [
        "Lam",
        "Man Ho",
        ""
      ],
      [
        "Ren",
        "Shujie",
        ""
      ],
      [
        "Yuan",
        "Youliang",
        ""
      ],
      [
        "Jiao",
        "Wenxiang",
        ""
      ],
      [
        "Tu",
        "Zhaopeng",
        ""
      ],
      [
        "Lyu",
        "Michael R.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.01386",
    "publish_date": "2023-10-02"
  },
  {
    "id": "2310.01469",
    "submitter": "Jiayu Yao",
    "authors": "Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, Li Yuan",
    "title": "LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still can not\ncompletely trust their answer, since LLMs suffer from\nhallucination--fabricating non-existent facts to cheat users without\nperception. And the reasons for their existence and pervasiveness remain\nunclear. In this paper, we demonstrate that non-sense prompts composed of\nrandom tokens can also elicit the LLMs to respond with hallucinations. This\nphenomenon forces us to revisit that hallucination may be another view of\nadversarial examples, and it shares similar features with conventional\nadversarial examples as the basic feature of LLMs. Therefore, we formalize an\nautomatic hallucination triggering method as the hallucination attack in an\nadversarial way. Finally, we explore basic feature of attacked adversarial\nprompts and propose a simple yet effective defense strategy. Our code is\nreleased on GitHub.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 2 Oct 2023 17:01:56 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 4 Oct 2023 17:53:49 GMT"
      }
    ],
    "update_date": "2023-10-05",
    "authors_parsed": [
      [
        "Yao",
        "Jia-Yu",
        ""
      ],
      [
        "Ning",
        "Kun-Peng",
        ""
      ],
      [
        "Liu",
        "Zhen-Hui",
        ""
      ],
      [
        "Ning",
        "Mu-Nan",
        ""
      ],
      [
        "Yuan",
        "Li",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.01469",
    "publish_date": "2023-10-04"
  },
  {
    "id": "2310.02164",
    "submitter": "Anwar Said",
    "authors": "Anwar Said and Tyler Derr and Mudassir Shabbir and Waseem Abbas and\n  Xenofon Koutsoukos",
    "title": "A Survey of Graph Unlearning",
    "comments": "22 page review paper on graph unlearning",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Graph unlearning emerges as a crucial advancement in the pursuit of\nresponsible AI, providing the means to remove sensitive data traces from\ntrained models, thereby upholding the right to be forgotten. It is evident that\ngraph machine learning exhibits sensitivity to data privacy and adversarial\nattacks, necessitating the application of graph unlearning techniques to\naddress these concerns effectively. In this comprehensive survey paper, we\npresent the first systematic review of graph unlearning approaches,\nencompassing a diverse array of methodologies and offering a detailed taxonomy\nand up-to-date literature overview to facilitate the understanding of\nresearchers new to this field. Additionally, we establish the vital connections\nbetween graph unlearning and differential privacy, augmenting our understanding\nof the relevance of privacy-preserving techniques in this context. To ensure\nclarity, we provide lucid explanations of the fundamental concepts and\nevaluation measures used in graph unlearning, catering to a broader audience\nwith varying levels of expertise. Delving into potential applications, we\nexplore the versatility of graph unlearning across various domains, including\nbut not limited to social networks, adversarial settings, and\nresource-constrained environments like the Internet of Things (IoT),\nillustrating its potential impact in safeguarding data privacy and enhancing AI\nsystems' robustness. Finally, we shed light on promising research directions,\nencouraging further progress and innovation within the domain of graph\nunlearning. By laying a solid foundation and fostering continued progress, this\nsurvey seeks to inspire researchers to further advance the field of graph\nunlearning, thereby instilling confidence in the ethical growth of AI systems\nand reinforcing the responsible application of machine learning techniques in\nvarious domains.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 23 Aug 2023 20:50:52 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 7 Oct 2023 19:50:17 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Said",
        "Anwar",
        ""
      ],
      [
        "Derr",
        "Tyler",
        ""
      ],
      [
        "Shabbir",
        "Mudassir",
        ""
      ],
      [
        "Abbas",
        "Waseem",
        ""
      ],
      [
        "Koutsoukos",
        "Xenofon",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.02164",
    "publish_date": "2023-08-23"
  },
  {
    "id": "2310.02417",
    "submitter": "Bocheng Chen",
    "authors": "Bocheng Chen, Advait Paliwal, Qiben Yan",
    "title": "Jailbreaker in Jail: Moving Target Defense for Large Language Models",
    "comments": "MTD Workshop in CCS'23",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Large language models (LLMs), known for their capability in understanding and\nfollowing instructions, are vulnerable to adversarial attacks. Researchers have\nfound that current commercial LLMs either fail to be \"harmless\" by presenting\nunethical answers, or fail to be \"helpful\" by refusing to offer meaningful\nanswers when faced with adversarial queries. To strike a balance between being\nhelpful and harmless, we design a moving target defense (MTD) enhanced LLM\nsystem. The system aims to deliver non-toxic answers that align with outputs\nfrom multiple model candidates, making them more robust against adversarial\nattacks. We design a query and output analysis model to filter out unsafe or\nnon-responsive answers. %to achieve the two objectives of randomly selecting\noutputs from different LLMs. We evaluate over 8 most recent chatbot models with\nstate-of-the-art adversarial queries. Our MTD-enhanced LLM system reduces the\nattack success rate from 37.5\\% to 0\\%. Meanwhile, it decreases the response\nrefusal rate from 50\\% to 0\\%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Oct 2023 20:32:04 GMT"
      }
    ],
    "update_date": "2023-10-05",
    "authors_parsed": [
      [
        "Chen",
        "Bocheng",
        ""
      ],
      [
        "Paliwal",
        "Advait",
        ""
      ],
      [
        "Yan",
        "Qiben",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.02417",
    "publish_date": "2023-10-03"
  },
  {
    "id": "2310.02446",
    "submitter": "Zheng-Xin Yong",
    "authors": "Zheng-Xin Yong, Cristina Menghini and Stephen H. Bach",
    "title": "Low-Resource Languages Jailbreak GPT-4",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI safety training and red-teaming of large language models (LLMs) are\nmeasures to mitigate the generation of unsafe content. Our work exposes the\ninherent cross-lingual vulnerability of these safety mechanisms, resulting from\nthe linguistic inequality of safety training data, by successfully\ncircumventing GPT-4's safeguard through translating unsafe English inputs into\nlow-resource languages. On the AdvBenchmark, GPT-4 engages with the unsafe\ntranslated inputs and provides actionable items that can get the users towards\ntheir harmful goals 79% of the time, which is on par with or even surpassing\nstate-of-the-art jailbreaking attacks. Other high-/mid-resource languages have\nsignificantly lower attack success rate, which suggests that the cross-lingual\nvulnerability mainly applies to low-resource languages. Previously, limited\ntraining on low-resource languages primarily affects speakers of those\nlanguages, causing technological disparities. However, our work highlights a\ncrucial shift: this deficiency now poses a risk to all LLMs users. Publicly\navailable translation APIs enable anyone to exploit LLMs' safety\nvulnerabilities. Therefore, our work calls for a more holistic red-teaming\nefforts to develop robust multilingual safeguards with wide language coverage.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Oct 2023 21:30:56 GMT"
      }
    ],
    "update_date": "2023-10-05",
    "authors_parsed": [
      [
        "Yong",
        "Zheng-Xin",
        ""
      ],
      [
        "Menghini",
        "Cristina",
        ""
      ],
      [
        "Bach",
        "Stephen H.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.02446",
    "publish_date": "2023-10-03"
  },
  {
    "id": "2310.03125",
    "submitter": "Yihan Wu",
    "authors": "Yihan Wu, Brandon Y. Feng, Heng Huang",
    "title": "Shielding the Unseen: Privacy Protection through Poisoning NeRF with\n  Spatial Deformation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In this paper, we introduce an innovative method of safeguarding user privacy\nagainst the generative capabilities of Neural Radiance Fields (NeRF) models.\nOur novel poisoning attack method induces changes to observed views that are\nimperceptible to the human eye, yet potent enough to disrupt NeRF's ability to\naccurately reconstruct a 3D scene. To achieve this, we devise a bi-level\noptimization algorithm incorporating a Projected Gradient Descent (PGD)-based\nspatial deformation. We extensively test our approach on two common NeRF\nbenchmark datasets consisting of 29 real-world scenes with high-quality images.\nOur results compellingly demonstrate that our privacy-preserving method\nsignificantly impairs NeRF's performance across these benchmark datasets.\nAdditionally, we show that our method is adaptable and versatile, functioning\nacross various perturbation strengths and NeRF architectures. This work offers\nvaluable insights into NeRF's vulnerabilities and emphasizes the need to\naccount for such potential privacy risks when developing robust 3D scene\nreconstruction algorithms. Our study contributes to the larger conversation\nsurrounding responsible AI and generative machine learning, aiming to protect\nuser privacy and respect creative ownership in the digital age.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Oct 2023 19:35:56 GMT"
      }
    ],
    "update_date": "2023-10-06",
    "authors_parsed": [
      [
        "Wu",
        "Yihan",
        ""
      ],
      [
        "Feng",
        "Brandon Y.",
        ""
      ],
      [
        "Huang",
        "Heng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03125",
    "publish_date": "2023-10-04"
  },
  {
    "id": "2310.03185",
    "submitter": "Xiaohan Fu",
    "authors": "Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar\n  Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes",
    "title": "Misusing Tools in Large Language Models With Visual Adversarial Examples",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Large Language Models (LLMs) are being enhanced with the ability to use tools\nand to process multiple modalities. These new capabilities bring new benefits\nand also new security risks. In this work, we show that an attacker can use\nvisual adversarial examples to cause attacker-desired tool usage. For example,\nthe attacker could cause a victim LLM to delete calendar events, leak private\nconversations and book hotels. Different from prior work, our attacks can\naffect the confidentiality and integrity of user resources connected to the LLM\nwhile being stealthy and generalizable to multiple input prompts. We construct\nthese attacks using gradient-based adversarial training and characterize\nperformance along multiple dimensions. We find that our adversarial images can\nmanipulate the LLM to invoke tools following real-world syntax almost always\n(~98%) while maintaining high similarity to clean images (~0.9 SSIM).\nFurthermore, using human scoring and automated metrics, we find that the\nattacks do not noticeably affect the conversation (and its semantics) between\nthe user and the LLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 4 Oct 2023 22:10:01 GMT"
      }
    ],
    "update_date": "2023-10-06",
    "authors_parsed": [
      [
        "Fu",
        "Xiaohan",
        ""
      ],
      [
        "Wang",
        "Zihan",
        ""
      ],
      [
        "Li",
        "Shuheng",
        ""
      ],
      [
        "Gupta",
        "Rajesh K.",
        ""
      ],
      [
        "Mireshghallah",
        "Niloofar",
        ""
      ],
      [
        "Berg-Kirkpatrick",
        "Taylor",
        ""
      ],
      [
        "Fernandes",
        "Earlence",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03185",
    "publish_date": "2023-10-04"
  },
  {
    "id": "2310.03614",
    "submitter": "Shawqi Al-Maliki",
    "authors": "Shawqi Al-Maliki, Adnan Qayyum, Hassan Ali, Mohamed Abdallah, Junaid\n  Qadir, Dinh Thai Hoang, Dusit Niyato, Ala Al-Fuqaha",
    "title": "Adversarial Machine Learning for Social Good: Reframing the Adversary as\n  an Ally",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deep Neural Networks (DNNs) have been the driving force behind many of the\nrecent advances in machine learning. However, research has shown that DNNs are\nvulnerable to adversarial examples -- input samples that have been perturbed to\nforce DNN-based models to make errors. As a result, Adversarial Machine\nLearning (AdvML) has gained a lot of attention, and researchers have\ninvestigated these vulnerabilities in various settings and modalities. In\naddition, DNNs have also been found to incorporate embedded bias and often\nproduce unexplainable predictions, which can result in anti-social AI\napplications. The emergence of new AI technologies that leverage Large Language\nModels (LLMs), such as ChatGPT and GPT-4, increases the risk of producing\nanti-social applications at scale. AdvML for Social Good (AdvML4G) is an\nemerging field that repurposes the AdvML bug to invent pro-social applications.\nRegulators, practitioners, and researchers should collaborate to encourage the\ndevelopment of pro-social applications and hinder the development of\nanti-social ones. In this work, we provide the first comprehensive review of\nthe emerging field of AdvML4G. This paper encompasses a taxonomy that\nhighlights the emergence of AdvML4G, a discussion of the differences and\nsimilarities between AdvML4G and AdvML, a taxonomy covering social good-related\nconcepts and aspects, an exploration of the motivations behind the emergence of\nAdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the\nworks that utilize AdvML4G as an auxiliary tool for innovating pro-social\napplications. Finally, we elaborate upon various challenges and open research\nissues that require significant attention from the research community.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Oct 2023 15:49:04 GMT"
      }
    ],
    "update_date": "2023-10-06",
    "authors_parsed": [
      [
        "Al-Maliki",
        "Shawqi",
        ""
      ],
      [
        "Qayyum",
        "Adnan",
        ""
      ],
      [
        "Ali",
        "Hassan",
        ""
      ],
      [
        "Abdallah",
        "Mohamed",
        ""
      ],
      [
        "Qadir",
        "Junaid",
        ""
      ],
      [
        "Hoang",
        "Dinh Thai",
        ""
      ],
      [
        "Niyato",
        "Dusit",
        ""
      ],
      [
        "Al-Fuqaha",
        "Ala",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03614",
    "publish_date": "2023-10-05"
  },
  {
    "id": "2310.03693",
    "submitter": "Xiangyu Qi",
    "authors": "Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek\n  Mittal, Peter Henderson",
    "title": "Fine-tuning Aligned Language Models Compromises Safety, Even When Users\n  Do Not Intend To!",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Optimizing large language models (LLMs) for downstream use cases often\ninvolves the customization of pre-trained LLMs through further fine-tuning.\nMeta's open release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5\nTurbo on custom datasets also encourage this practice. But, what are the safety\ncosts associated with such custom fine-tuning? We note that while existing\nsafety alignment infrastructures can restrict harmful behaviors of LLMs at\ninference time, they do not cover safety risks when fine-tuning privileges are\nextended to end-users. Our red teaming studies find that the safety alignment\nof LLMs can be compromised by fine-tuning with only a few adversarially\ndesigned training examples. For instance, we jailbreak GPT-3.5 Turbo's safety\nguardrails by fine-tuning it on only 10 such examples at a cost of less than\n$0.20 via OpenAI's APIs, making the model responsive to nearly any harmful\ninstructions. Disconcertingly, our research also reveals that, even without\nmalicious intent, simply fine-tuning with benign and commonly used datasets can\nalso inadvertently degrade the safety alignment of LLMs, though to a lesser\nextent. These findings suggest that fine-tuning aligned LLMs introduces new\nsafety risks that current safety infrastructures fall short of addressing --\neven if a model's initial safety alignment is impeccable, it is not necessarily\nto be maintained after custom fine-tuning. We outline and critically analyze\npotential mitigations and advocate for further research efforts toward\nreinforcing safety protocols for the custom fine-tuning of aligned LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Oct 2023 17:12:17 GMT"
      }
    ],
    "update_date": "2023-10-06",
    "authors_parsed": [
      [
        "Qi",
        "Xiangyu",
        ""
      ],
      [
        "Zeng",
        "Yi",
        ""
      ],
      [
        "Xie",
        "Tinghao",
        ""
      ],
      [
        "Chen",
        "Pin-Yu",
        ""
      ],
      [
        "Jia",
        "Ruoxi",
        ""
      ],
      [
        "Mittal",
        "Prateek",
        ""
      ],
      [
        "Henderson",
        "Peter",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03693",
    "publish_date": "2023-10-05"
  },
  {
    "id": "2310.03827",
    "submitter": "Shan Jia",
    "authors": "Sneha Muppalla, Shan Jia, Siwei Lyu",
    "title": "Integrating Audio-Visual Features for Multimodal Deepfake Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deepfakes are AI-generated media in which an image or video has been\ndigitally modified. The advancements made in deepfake technology have led to\nprivacy and security issues. Most deepfake detection techniques rely on the\ndetection of a single modality. Existing methods for audio-visual detection do\nnot always surpass that of the analysis based on single modalities. Therefore,\nthis paper proposes an audio-visual-based method for deepfake detection, which\nintegrates fine-grained deepfake identification with binary classification. We\ncategorize the samples into four types by combining labels specific to each\nsingle modality. This method enhances the detection under intra-domain and\ncross-domain testing.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Oct 2023 18:19:56 GMT"
      }
    ],
    "update_date": "2023-10-09",
    "authors_parsed": [
      [
        "Muppalla",
        "Sneha",
        ""
      ],
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03827",
    "publish_date": "2023-10-05"
  },
  {
    "id": "2310.03856",
    "submitter": "Khalid Malik",
    "authors": "Awais Khan, Khalid Mahmood Malik",
    "title": "Securing Voice Biometrics: One-Shot Learning Approach for Audio Deepfake\n  Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The Automatic Speaker Verification (ASV) system is vulnerable to fraudulent\nactivities using audio deepfakes, also known as logical-access voice spoofing\nattacks. These deepfakes pose a concerning threat to voice biometrics due to\nrecent advancements in generative AI and speech synthesis technologies. While\nseveral deep learning models for speech synthesis detection have been\ndeveloped, most of them show poor generalizability, especially when the attacks\nhave different statistical distributions from the ones seen. Therefore, this\npaper presents Quick-SpoofNet, an approach for detecting both seen and unseen\nsynthetic attacks in the ASV system using one-shot learning and metric learning\ntechniques. By using the effective spectral feature set, the proposed method\nextracts compact and representative temporal embeddings from the voice samples\nand utilizes metric learning and triplet loss to assess the similarity index\nand distinguish different embeddings. The system effectively clusters similar\nspeech embeddings, classifying bona fide speeches as the target class and\nidentifying other clusters as spoofing attacks. The proposed system is\nevaluated using the ASVspoof 2019 logical access (LA) dataset and tested\nagainst unseen deepfake attacks from the ASVspoof 2021 dataset. Additionally,\nits generalization ability towards unseen bona fide speech is assessed using\nspeech data from the VSDC dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 5 Oct 2023 19:30:22 GMT"
      }
    ],
    "update_date": "2023-10-09",
    "authors_parsed": [
      [
        "Khan",
        "Awais",
        ""
      ],
      [
        "Malik",
        "Khalid Mahmood",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.03856",
    "publish_date": "2023-10-05"
  },
  {
    "id": "2310.04445",
    "submitter": "Muhammad Shah",
    "authors": "Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier,\n  Ankit Shah, Joseph Konan, Dareen Alharthi, Hazim T Bukhari, Massa Baali,\n  Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh",
    "title": "LoFT: Local Proxy Fine-tuning For Improving Transferability Of\n  Adversarial Attacks Against Large Language Model",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  It has been shown that Large Language Model (LLM) alignments can be\ncircumvented by appending specially crafted attack suffixes with harmful\nqueries to elicit harmful responses. To conduct attacks against private target\nmodels whose characterization is unknown, public models can be used as proxies\nto fashion the attack, with successful attacks being transferred from public\nproxies to private target models. The success rate of attack depends on how\nclosely the proxy model approximates the private model. We hypothesize that for\nattacks to be transferrable, it is sufficient if the proxy can approximate the\ntarget model in the neighborhood of the harmful query. Therefore, in this\npaper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning\nproxy models on similar queries that lie in the lexico-semantic neighborhood of\nharmful queries to decrease the divergence between the proxy and target models.\nFirst, we demonstrate three approaches to prompt private target models to\nobtain similar queries given harmful queries. Next, we obtain data for local\nfine-tuning by eliciting responses from target models for the generated similar\nqueries. Then, we optimize attack suffixes to generate attack prompts and\nevaluate the impact of our local fine-tuning on the attack's success rate.\nExperiments show that local fine-tuning of proxy models improves attack\ntransferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$\n(absolute) on target models ChatGPT, GPT-4, and Claude respectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 2 Oct 2023 23:29:23 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 21 Oct 2023 17:59:41 GMT"
      }
    ],
    "update_date": "2023-10-24",
    "authors_parsed": [
      [
        "Shah",
        "Muhammad Ahmed",
        ""
      ],
      [
        "Sharma",
        "Roshan",
        ""
      ],
      [
        "Dhamyal",
        "Hira",
        ""
      ],
      [
        "Olivier",
        "Raphael",
        ""
      ],
      [
        "Shah",
        "Ankit",
        ""
      ],
      [
        "Konan",
        "Joseph",
        ""
      ],
      [
        "Alharthi",
        "Dareen",
        ""
      ],
      [
        "Bukhari",
        "Hazim T",
        ""
      ],
      [
        "Baali",
        "Massa",
        ""
      ],
      [
        "Deshmukh",
        "Soham",
        ""
      ],
      [
        "Kuhlmann",
        "Michael",
        ""
      ],
      [
        "Raj",
        "Bhiksha",
        ""
      ],
      [
        "Singh",
        "Rita",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.04445",
    "publish_date": "2023-10-21"
  },
  {
    "id": "2310.04451",
    "submitter": "Xiaogeng Liu",
    "authors": "Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao",
    "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language\n  Models",
    "comments": "Pre-print, code is available at\n  https://github.com/SheltonLiu-N/AutoDAN",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The aligned Large Language Models (LLMs) are powerful language understanding\nand decision-making tools that are created through extensive alignment with\nhuman feedback. However, these large models remain susceptible to jailbreak\nattacks, where adversaries manipulate prompts to elicit malicious outputs that\nshould not be given by aligned LLMs. Investigating jailbreak prompts can lead\nus to delve into the limitations of LLMs and further guide us to secure them.\nUnfortunately, existing jailbreak techniques suffer from either (1) scalability\nissues, where attacks heavily rely on manual crafting of prompts, or (2)\nstealthiness problems, as attacks depend on token-based algorithms to generate\nprompts that are often semantically meaningless, making them susceptible to\ndetection through basic perplexity testing. In light of these challenges, we\nintend to answer this question: Can we develop an approach that can\nautomatically generate stealthy jailbreak prompts? In this paper, we introduce\nAutoDAN, a novel jailbreak attack against aligned LLMs. AutoDAN can\nautomatically generate stealthy jailbreak prompts by the carefully designed\nhierarchical genetic algorithm. Extensive evaluations demonstrate that AutoDAN\nnot only automates the process while preserving semantic meaningfulness, but\nalso demonstrates superior attack strength in cross-model transferability, and\ncross-sample universality compared with the baseline. Moreover, we also compare\nAutoDAN with perplexity-based defense methods and show that AutoDAN can bypass\nthem effectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 3 Oct 2023 19:44:37 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Liu",
        "Xiaogeng",
        ""
      ],
      [
        "Xu",
        "Nan",
        ""
      ],
      [
        "Chen",
        "Muhao",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.04451",
    "publish_date": "2023-10-03"
  },
  {
    "id": "2310.05057",
    "submitter": "Yifan Jiang",
    "authors": "Yifan Jiang, Filip Ilievski, Kaixin Ma, Zhivar Sourati",
    "title": "BRAINTEASER: Lateral Thinking Puzzles for Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The success of language models has inspired the NLP community to attend to\ntasks that require implicit and complex reasoning, relying on human-like\ncommonsense mechanisms. While such vertical thinking tasks have been relatively\npopular, lateral thinking puzzles have received little attention. To bridge\nthis gap, we devise BRAINTEASER: a multiple-choice Question Answering task\ndesigned to test the model's ability to exhibit lateral thinking and defy\ndefault commonsense associations. We design a three-step procedure for creating\nthe first lateral thinking benchmark, consisting of data collection, distractor\ngeneration, and generation of adversarial examples, leading to 1,100 puzzles\nwith high-quality annotations. To assess the consistency of lateral reasoning\nby models, we enrich BRAINTEASER based on a semantic and contextual\nreconstruction of its questions. Our experiments with state-of-the-art\ninstruction- and commonsense language models reveal a significant gap between\nhuman and model performance, which is further widened when consistency across\nadversarial formats is considered. We make all of our code and data available\nto stimulate work on developing and evaluating lateral thinking models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 8 Oct 2023 07:46:01 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 10 Oct 2023 19:30:31 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 9 Nov 2023 19:45:13 GMT"
      }
    ],
    "update_date": "2023-11-13",
    "authors_parsed": [
      [
        "Jiang",
        "Yifan",
        ""
      ],
      [
        "Ilievski",
        "Filip",
        ""
      ],
      [
        "Ma",
        "Kaixin",
        ""
      ],
      [
        "Sourati",
        "Zhivar",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.05057",
    "publish_date": "2023-10-10"
  },
  {
    "id": "2310.05095",
    "submitter": "Tharindu Kumarage",
    "authors": "Tharindu Kumarage, Paras Sheth, Raha Moraffah, Joshua Garland, Huan\n  Liu",
    "title": "How Reliable Are AI-Generated-Text Detectors? An Assessment Framework\n  Using Evasive Soft Prompts",
    "comments": "Accepted to EMNLP 2023 (Findings)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In recent years, there has been a rapid proliferation of AI-generated text,\nprimarily driven by the release of powerful pre-trained language models (PLMs).\nTo address the issue of misuse associated with AI-generated text, various\nhigh-performing detectors have been developed, including the OpenAI detector\nand the Stanford DetectGPT. In our study, we ask how reliable these detectors\nare. We answer the question by designing a novel approach that can prompt any\nPLM to generate text that evades these high-performing detectors. The proposed\napproach suggests a universal evasive prompt, a novel type of soft prompt,\nwhich guides PLMs in producing \"human-like\" text that can mislead the\ndetectors. The novel universal evasive prompt is achieved in two steps: First,\nwe create an evasive soft prompt tailored to a specific PLM through prompt\ntuning; and then, we leverage the transferability of soft prompts to transfer\nthe learned evasive soft prompt from one PLM to another. Employing multiple\nPLMs in various writing tasks, we conduct extensive experiments to evaluate the\nefficacy of the evasive soft prompts in their evasion of state-of-the-art\ndetectors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 8 Oct 2023 09:53:46 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Kumarage",
        "Tharindu",
        ""
      ],
      [
        "Sheth",
        "Paras",
        ""
      ],
      [
        "Moraffah",
        "Raha",
        ""
      ],
      [
        "Garland",
        "Joshua",
        ""
      ],
      [
        "Liu",
        "Huan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.05095",
    "publish_date": "2023-10-08"
  },
  {
    "id": "2310.05177",
    "submitter": "Zhijiang Guo",
    "authors": "Xuming Hu, Junzhe Chen, Xiaochuan Li, Yufei Guo, Lijie Wen, Philip S.\n  Yu, Zhijiang Guo",
    "title": "Do Large Language Models Know about Facts?",
    "comments": "20 pages, 8 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs) have recently driven striking performance\nimprovements across a range of natural language processing tasks. The factual\nknowledge acquired during pretraining and instruction tuning can be useful in\nvarious downstream tasks, such as question answering, and language generation.\nUnlike conventional Knowledge Bases (KBs) that explicitly store factual\nknowledge, LLMs implicitly store facts in their parameters. Content generated\nby the LLMs can often exhibit inaccuracies or deviations from the truth, due to\nfacts that can be incorrectly induced or become obsolete over time. To this\nend, we aim to comprehensively evaluate the extent and scope of factual\nknowledge within LLMs by designing the benchmark Pinocchio. Pinocchio contains\n20K diverse factual questions that span different sources, timelines, domains,\nregions, and languages. Furthermore, we investigate whether LLMs are able to\ncompose multiple facts, update factual knowledge temporally, reason over\nmultiple pieces of facts, identify subtle factual differences, and resist\nadversarial examples. Extensive experiments on different sizes and types of\nLLMs show that existing LLMs still lack factual knowledge and suffer from\nvarious spurious correlations. We believe this is a critical bottleneck for\nrealizing trustworthy artificial intelligence. The dataset Pinocchio and our\ncodes will be publicly available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 8 Oct 2023 14:26:55 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Hu",
        "Xuming",
        ""
      ],
      [
        "Chen",
        "Junzhe",
        ""
      ],
      [
        "Li",
        "Xiaochuan",
        ""
      ],
      [
        "Guo",
        "Yufei",
        ""
      ],
      [
        "Wen",
        "Lijie",
        ""
      ],
      [
        "Yu",
        "Philip S.",
        ""
      ],
      [
        "Guo",
        "Zhijiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.05177",
    "publish_date": "2023-10-08"
  },
  {
    "id": "2310.05595",
    "submitter": "Polra Victor Falade",
    "authors": "Polra Victor Falade",
    "title": "Decoding the Threat Landscape : ChatGPT, FraudGPT, and WormGPT in Social\n  Engineering Attacks",
    "comments": "185 - 198 pages",
    "journal-ref": "International Journal of Scientific Research in Computer Science,\n  Engineering and Information Technology ISSN : 2456-3307 Volume 9, Issue 5\n  September-October-2023",
    "doi": "10.32628/CSEIT2390533",
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In the ever-evolving realm of cybersecurity, the rise of generative AI models\nlike ChatGPT, FraudGPT, and WormGPT has introduced both innovative solutions\nand unprecedented challenges. This research delves into the multifaceted\napplications of generative AI in social engineering attacks, offering insights\ninto the evolving threat landscape using the blog mining technique. Generative\nAI models have revolutionized the field of cyberattacks, empowering malicious\nactors to craft convincing and personalized phishing lures, manipulate public\nopinion through deepfakes, and exploit human cognitive biases. These models,\nChatGPT, FraudGPT, and WormGPT, have augmented existing threats and ushered in\nnew dimensions of risk. From phishing campaigns that mimic trusted\norganizations to deepfake technology impersonating authoritative figures, we\nexplore how generative AI amplifies the arsenal of cybercriminals. Furthermore,\nwe shed light on the vulnerabilities that AI-driven social engineering\nexploits, including psychological manipulation, targeted phishing, and the\ncrisis of authenticity. To counter these threats, we outline a range of\nstrategies, including traditional security measures, AI-powered security\nsolutions, and collaborative approaches in cybersecurity. We emphasize the\nimportance of staying vigilant, fostering awareness, and strengthening\nregulations in the battle against AI-enhanced social engineering attacks. In an\nenvironment characterized by the rapid evolution of AI models and a lack of\ntraining data, defending against generative AI threats requires constant\nadaptation and the collective efforts of individuals, organizations, and\ngovernments. This research seeks to provide a comprehensive understanding of\nthe dynamic interplay between generative AI and social engineering attacks,\nequipping stakeholders with the knowledge to navigate this intricate\ncybersecurity landscape.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 9 Oct 2023 10:31:04 GMT"
      }
    ],
    "update_date": "2023-10-10",
    "authors_parsed": [
      [
        "Falade",
        "Polra Victor",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.05595",
    "publish_date": "2023-10-09"
  },
  {
    "id": "2310.06387",
    "submitter": "Zeming Wei",
    "authors": "Zeming Wei, Yifei Wang, Yisen Wang",
    "title": "Jailbreak and Guard Aligned Language Models with Only Few In-Context\n  Demonstrations",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have shown remarkable success in various tasks,\nbut concerns about their safety and the potential for generating malicious\ncontent have emerged. In this paper, we explore the power of In-Context\nLearning (ICL) in manipulating the alignment ability of LLMs. We find that by\nproviding just few in-context demonstrations without fine-tuning, LLMs can be\nmanipulated to increase or decrease the probability of jailbreaking, i.e.\nanswering malicious prompts. Based on these observations, we propose In-Context\nAttack (ICA) and In-Context Defense (ICD) methods for jailbreaking and guarding\naligned language model purposes. ICA crafts malicious contexts to guide models\nin generating harmful outputs, while ICD enhances model robustness by\ndemonstrations of rejecting to answer harmful prompts. Our experiments show the\neffectiveness of ICA and ICD in increasing or reducing the success rate of\nadversarial jailbreaking attacks. Overall, we shed light on the potential of\nICL to influence LLM behavior and provide a new perspective for enhancing the\nsafety and alignment of LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Oct 2023 07:50:29 GMT"
      }
    ],
    "update_date": "2023-10-11",
    "authors_parsed": [
      [
        "Wei",
        "Zeming",
        ""
      ],
      [
        "Wang",
        "Yifei",
        ""
      ],
      [
        "Wang",
        "Yisen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.06387",
    "publish_date": "2023-10-10"
  },
  {
    "id": "2310.06474",
    "submitter": "Yue Deng",
    "authors": "Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing",
    "title": "Multilingual Jailbreak Challenges in Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  While large language models (LLMs) exhibit remarkable capabilities across a\nwide range of tasks, they pose potential safety concerns, such as the\n``jailbreak'' problem, wherein malicious instructions can manipulate LLMs to\nexhibit undesirable behavior. Although several preventive measures have been\ndeveloped to mitigate the potential risks associated with LLMs, they have\nprimarily focused on English data. In this study, we reveal the presence of\nmultilingual jailbreak challenges within LLMs and consider two potential risk\nscenarios: unintentional and intentional. The unintentional scenario involves\nusers querying LLMs using non-English prompts and inadvertently bypassing the\nsafety mechanisms, while the intentional scenario concerns malicious users\ncombining malicious instructions with multilingual prompts to deliberately\nattack LLMs. The experimental results reveal that in the unintentional\nscenario, the rate of unsafe content increases as the availability of languages\ndecreases. Specifically, low-resource languages exhibit three times the\nlikelihood of encountering harmful content compared to high-resource languages,\nwith both ChatGPT and GPT-4. In the intentional scenario, multilingual prompts\ncan exacerbate the negative impact of malicious instructions, with\nastonishingly high rates of unsafe output: 80.92\\% for ChatGPT and 40.71\\% for\nGPT-4. To handle such a challenge in the multilingual context, we propose a\nnovel \\textsc{Self-Defense} framework that automatically generates multilingual\ntraining data for safety fine-tuning. Experimental results show that ChatGPT\nfine-tuned with such data can achieve a substantial reduction in unsafe content\ngeneration. Data is available at\nhttps://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs. Warning: This\npaper contains examples with potentially harmful content.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Oct 2023 09:44:06 GMT"
      }
    ],
    "update_date": "2023-10-11",
    "authors_parsed": [
      [
        "Deng",
        "Yue",
        ""
      ],
      [
        "Zhang",
        "Wenxuan",
        ""
      ],
      [
        "Pan",
        "Sinno Jialin",
        ""
      ],
      [
        "Bing",
        "Lidong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.06474",
    "publish_date": "2023-10-10"
  },
  {
    "id": "2310.06973",
    "submitter": "Rod Rofougaran",
    "authors": "Rod Rofougaran, Shinjae Yoo, Huan-Hsin Tseng and Samuel Yen-Chi Chen",
    "title": "Federated Quantum Machine Learning with Differential Privacy",
    "comments": "5 pages, 7 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "quant-ph cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The preservation of privacy is a critical concern in the implementation of\nartificial intelligence on sensitive training data. There are several\ntechniques to preserve data privacy but quantum computations are inherently\nmore secure due to the no-cloning theorem, resulting in a most desirable\ncomputational platform on top of the potential quantum advantages. There have\nbeen prior works in protecting data privacy by Quantum Federated Learning (QFL)\nand Quantum Differential Privacy (QDP) studied independently. However, to the\nbest of our knowledge, no prior work has addressed both QFL and QDP together\nyet. Here, we propose to combine these privacy-preserving methods and implement\nthem on the quantum platform, so that we can achieve comprehensive protection\nagainst data leakage (QFL) and model inversion attacks (QDP). This\nimplementation promises more efficient and secure artificial intelligence. In\nthis paper, we present a successful implementation of these\nprivacy-preservation methods by performing the binary classification of the\nCats vs Dogs dataset. Using our quantum-classical machine learning model, we\nobtained a test accuracy of over 0.98, while maintaining epsilon values less\nthan 1.3. We show that federated differentially private training is a viable\nprivacy preservation method for quantum machine learning on Noisy\nIntermediate-Scale Quantum (NISQ) devices.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Oct 2023 19:52:37 GMT"
      }
    ],
    "update_date": "2023-10-12",
    "authors_parsed": [
      [
        "Rofougaran",
        "Rod",
        ""
      ],
      [
        "Yoo",
        "Shinjae",
        ""
      ],
      [
        "Tseng",
        "Huan-Hsin",
        ""
      ],
      [
        "Chen",
        "Samuel Yen-Chi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.06973",
    "publish_date": "2023-10-10"
  },
  {
    "id": "2310.06987",
    "submitter": "Yangsibo Huang",
    "authors": "Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen",
    "title": "Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The rapid progress in open-source large language models (LLMs) is\nsignificantly advancing AI development. Extensive efforts have been made before\nmodel release to align their behavior with human values, with the primary goal\nof ensuring their helpfulness and harmlessness. However, even carefully aligned\nmodels can be manipulated maliciously, leading to unintended behaviors, known\nas \"jailbreaks\". These jailbreaks are typically triggered by specific text\ninputs, often referred to as adversarial prompts. In this work, we propose the\ngeneration exploitation attack, an extremely simple approach that disrupts\nmodel alignment by only manipulating variations of decoding methods. By\nexploiting different generation strategies, including varying decoding\nhyper-parameters and sampling methods, we increase the misalignment rate from\n0% to more than 95% across 11 language models including LLaMA2, Vicuna, Falcon,\nand MPT families, outperforming state-of-the-art attacks with $30\\times$ lower\ncomputational cost. Finally, we propose an effective alignment method that\nexplores diverse generation strategies, which can reasonably reduce the\nmisalignment rate under our attack. Altogether, our study underscores a major\nfailure in current safety evaluation and alignment procedures for open-source\nLLMs, strongly advocating for more comprehensive red teaming and better\nalignment before releasing such models. Our code is available at\nhttps://github.com/Princeton-SysML/Jailbreak_LLM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 10 Oct 2023 20:15:54 GMT"
      }
    ],
    "update_date": "2023-10-12",
    "authors_parsed": [
      [
        "Huang",
        "Yangsibo",
        ""
      ],
      [
        "Gupta",
        "Samyak",
        ""
      ],
      [
        "Xia",
        "Mengzhou",
        ""
      ],
      [
        "Li",
        "Kai",
        ""
      ],
      [
        "Chen",
        "Danqi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.06987",
    "publish_date": "2023-10-10"
  },
  {
    "id": "2310.07676",
    "submitter": "Hai Huang",
    "authors": "Hai Huang, Zhengyu Zhao, Michael Backes, Yun Shen, Yang Zhang",
    "title": "Composite Backdoor Attacks Against Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs) have demonstrated superior performance compared\nto previous methods on various tasks, and often serve as the foundation models\nfor many researches and services. However, the untrustworthy third-party LLMs\nmay covertly introduce vulnerabilities for downstream tasks. In this paper, we\nexplore the vulnerability of LLMs through the lens of backdoor attacks.\nDifferent from existing backdoor attacks against LLMs, ours scatters multiple\ntrigger keys in different prompt components. Such a Composite Backdoor Attack\n(CBA) is shown to be stealthier than implanting the same multiple trigger keys\nin only a single component. CBA ensures that the backdoor is activated only\nwhen all trigger keys appear. Our experiments demonstrate that CBA is effective\nin both natural language processing (NLP) and multimodal tasks. For instance,\nwith $3\\%$ poisoning samples against the LLaMA-7B model on the Emotion dataset,\nour attack achieves a $100\\%$ Attack Success Rate (ASR) with a False Triggered\nRate (FTR) below $2.06\\%$ and negligible model accuracy degradation. The unique\ncharacteristics of our CBA can be tailored for various practical scenarios,\ne.g., targeting specific user groups. Our work highlights the necessity of\nincreased security research on the trustworthiness of foundation LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Oct 2023 17:21:03 GMT"
      }
    ],
    "update_date": "2023-10-12",
    "authors_parsed": [
      [
        "Huang",
        "Hai",
        ""
      ],
      [
        "Zhao",
        "Zhengyu",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Shen",
        "Yun",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.07676",
    "publish_date": "2023-10-11"
  },
  {
    "id": "2310.07879",
    "submitter": "Hao-Ping Lee",
    "authors": "Hao-Ping Lee, Yu-Ju Yang, Thomas Serban von Davier, Jodi Forlizzi,\n  Sauvik Das",
    "title": "Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy\n  Risks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.HC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Privacy is a key principle for developing ethical AI technologies, but how\ndoes including AI technologies in products and services change privacy risks?\nWe constructed a taxonomy of AI privacy risks by analyzing 321 documented AI\nprivacy incidents. We codified how the unique capabilities and requirements of\nAI technologies described in those incidents generated new privacy risks,\nexacerbated known ones, or otherwise did not meaningfully alter the risk. We\npresent 12 high-level privacy risks that AI technologies either newly created\n(e.g., exposure risks from deepfake pornography) or exacerbated (e.g.,\nsurveillance risks from collecting training data). One upshot of our work is\nthat incorporating AI technologies into a product can alter the privacy risks\nit entails. Yet, current privacy-preserving AI/ML methods (e.g., federated\nlearning, differential privacy) only address a subset of the privacy risks\narising from the capabilities and data requirements of AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 11 Oct 2023 20:40:38 GMT"
      }
    ],
    "update_date": "2023-10-13",
    "authors_parsed": [
      [
        "Lee",
        "Hao-Ping",
        ""
      ],
      [
        "Yang",
        "Yu-Ju",
        ""
      ],
      [
        "von Davier",
        "Thomas Serban",
        ""
      ],
      [
        "Forlizzi",
        "Jodi",
        ""
      ],
      [
        "Das",
        "Sauvik",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.07879",
    "publish_date": "2023-10-11"
  },
  {
    "id": "2310.08240",
    "submitter": "Linqiu Zhang",
    "authors": "Wanyun Cui, Linqiu Zhang, Qianle Wang, Shuyang Cai",
    "title": "Who Said That? Benchmarking Social Media AI Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  AI-generated text has proliferated across various online platforms, offering\nboth transformative prospects and posing significant risks related to\nmisinformation and manipulation. Addressing these challenges, this paper\nintroduces SAID (Social media AI Detection), a novel benchmark developed to\nassess AI-text detection models' capabilities in real social media platforms.\nIt incorporates real AI-generate text from popular social media platforms like\nZhihu and Quora. Unlike existing benchmarks, SAID deals with content that\nreflects the sophisticated strategies employed by real AI users on the Internet\nwhich may evade detection or gain visibility, providing a more realistic and\nchallenging evaluation landscape. A notable finding of our study, based on the\nZhihu dataset, reveals that annotators can distinguish between AI-generated and\nhuman-generated texts with an average accuracy rate of 96.5%. This finding\nnecessitates a re-evaluation of human capability in recognizing AI-generated\ntext in today's widely AI-influenced environment. Furthermore, we present a new\nuser-oriented AI-text detection challenge focusing on the practicality and\neffectiveness of identifying AI-generated text based on user information and\nmultiple responses. The experimental results demonstrate that conducting\ndetection tasks on actual social media platforms proves to be more challenging\ncompared to traditional simulated AI-text detection, resulting in a decreased\naccuracy. On the other hand, user-oriented AI-generated text detection\nsignificantly improve the accuracy of detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 12 Oct 2023 11:35:24 GMT"
      }
    ],
    "update_date": "2023-10-13",
    "authors_parsed": [
      [
        "Cui",
        "Wanyun",
        ""
      ],
      [
        "Zhang",
        "Linqiu",
        ""
      ],
      [
        "Wang",
        "Qianle",
        ""
      ],
      [
        "Cai",
        "Shuyang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.08240",
    "publish_date": "2023-10-12"
  },
  {
    "id": "2310.08320",
    "submitter": "Dominik Hintersdorf",
    "authors": "Dominik Hintersdorf, Lukas Struppek, Daniel Neider, Kristian Kersting",
    "title": "Defending Our Privacy With Backdoors",
    "comments": "14 pages, 10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The proliferation of large AI models trained on uncurated, often sensitive\nweb-scraped data has raised significant privacy concerns. One of the concerns\nis that adversaries can extract information about the training data using\nprivacy attacks. Unfortunately, the task of removing specific information from\nthe models without sacrificing performance is not straightforward and has\nproven to be challenging. We propose a rather easy yet effective defense based\non backdoor attacks to remove private information such as names of individuals\nfrom models, and focus in this work on text encoders. Specifically, through\nstrategic insertion of backdoors, we align the embeddings of sensitive phrases\nwith those of neutral terms-\"a person\" instead of the person's name. Our\nempirical results demonstrate the effectiveness of our backdoor-based defense\non CLIP by assessing its performance using a specialized privacy attack for\nzero-shot classifiers. Our approach provides not only a new \"dual-use\"\nperspective on backdoor attacks, but also presents a promising avenue to\nenhance the privacy of individuals within models trained on uncurated\nweb-scraped data.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 12 Oct 2023 13:33:04 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 12 Dec 2023 19:54:02 GMT"
      }
    ],
    "update_date": "2023-12-14",
    "authors_parsed": [
      [
        "Hintersdorf",
        "Dominik",
        ""
      ],
      [
        "Struppek",
        "Lukas",
        ""
      ],
      [
        "Neider",
        "Daniel",
        ""
      ],
      [
        "Kersting",
        "Kristian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.08320",
    "publish_date": "2023-12-12"
  },
  {
    "id": "2310.08419",
    "submitter": "Patrick Chao",
    "authors": "Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George\n  J. Pappas, Eric Wong",
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "comments": "21 pages, 10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  There is growing interest in ensuring that large language models (LLMs) align\nwith human values. However, the alignment of such models is vulnerable to\nadversarial jailbreaks, which coax LLMs into overriding their safety\nguardrails. The identification of these vulnerabilities is therefore\ninstrumental in understanding inherent weaknesses and preventing future misuse.\nTo this end, we propose Prompt Automatic Iterative Refinement (PAIR), an\nalgorithm that generates semantic jailbreaks with only black-box access to an\nLLM. PAIR -- which is inspired by social engineering attacks -- uses an\nattacker LLM to automatically generate jailbreaks for a separate targeted LLM\nwithout human intervention. In this way, the attacker LLM iteratively queries\nthe target LLM to update and refine a candidate jailbreak. Empirically, PAIR\noften requires fewer than twenty queries to produce a jailbreak, which is\norders of magnitude more efficient than existing algorithms. PAIR also achieves\ncompetitive jailbreaking success rates and transferability on open and\nclosed-source LLMs, including GPT-3.5/4, Vicuna, and PaLM-2.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 12 Oct 2023 15:38:28 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 13 Oct 2023 21:50:59 GMT"
      }
    ],
    "update_date": "2023-10-17",
    "authors_parsed": [
      [
        "Chao",
        "Patrick",
        ""
      ],
      [
        "Robey",
        "Alexander",
        ""
      ],
      [
        "Dobriban",
        "Edgar",
        ""
      ],
      [
        "Hassani",
        "Hamed",
        ""
      ],
      [
        "Pappas",
        "George J.",
        ""
      ],
      [
        "Wong",
        "Eric",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.08419",
    "publish_date": "2023-10-12"
  },
  {
    "id": "2310.08879",
    "submitter": "Quanjun Zhang",
    "authors": "Quanjun Zhang, Tongke Zhang, Juan Zhai, Chunrong Fang, Bowen Yu,\n  Weisong Sun, Zhenyu Chen",
    "title": "A Critical Review of Large Language Model on Software Engineering: An\n  Example from ChatGPT and Automated Program Repair",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have been gaining increasing attention and\ndemonstrated promising performance across a variety of Software Engineering\n(SE) tasks, such as Automated Program Repair (APR), code summarization, and\ncode completion. For example, ChatGPT, the latest black-box LLM, has been\ninvestigated by numerous recent research studies and has shown impressive\nperformance in various tasks. However, there exists a potential risk of data\nleakage since these LLMs are usually close-sourced with unknown specific\ntraining details, e.g., pre-training datasets.\n  In this paper, we seek to review the bug-fixing capabilities of ChatGPT on a\nclean APR benchmark with different research objectives. We first introduce\n{\\benchmark}, a new benchmark with buggy and the corresponding fixed programs\nfrom competitive programming problems starting from 2023, after the training\ncutoff point of ChatGPT. The results on {\\benchmark} show that ChatGPT is able\nto fix 109 out of 151 buggy programs using the basic prompt within 35\nindependent rounds, outperforming state-of-the-art LLMs CodeT5 and PLBART by\n27.5\\% and 62.4\\% prediction accuracy. We also investigate the impact of three\ntypes of prompts, i.e., problem description, error feedback, and bug\nlocalization, leading to additional 34 fixed bugs. Besides, we provide\nadditional discussion from the interactive nature of ChatGPT to illustrate the\ncapacity of a dialog-based repair workflow with 9 additional fixed bugs.\nInspired by the findings, we further pinpoint various challenges and\nopportunities for advanced SE study equipped with such LLMs (e.g.,~ChatGPT) in\nthe near future. More importantly, our work calls for more research on the\nreevaluation of the achievements obtained by existing black-box LLMs across\nvarious SE tasks, not limited to ChatGPT on APR.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 13 Oct 2023 06:11:47 GMT"
      }
    ],
    "update_date": "2023-10-16",
    "authors_parsed": [
      [
        "Zhang",
        "Quanjun",
        ""
      ],
      [
        "Zhang",
        "Tongke",
        ""
      ],
      [
        "Zhai",
        "Juan",
        ""
      ],
      [
        "Fang",
        "Chunrong",
        ""
      ],
      [
        "Yu",
        "Bowen",
        ""
      ],
      [
        "Sun",
        "Weisong",
        ""
      ],
      [
        "Chen",
        "Zhenyu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.08879",
    "publish_date": "2023-10-13"
  },
  {
    "id": "2310.09792",
    "submitter": "Renyang Liu",
    "authors": "Renyang Liu, Jinhong Zhang, Kwok-Yan Lam, Jun Zhao, Wei Zhou",
    "title": "SCME: A Self-Contrastive Method for Data-free and Query-Limited Model\n  Extraction Attack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Previous studies have revealed that artificial intelligence (AI) systems are\nvulnerable to adversarial attacks. Among them, model extraction attacks fool\nthe target model by generating adversarial examples on a substitute model. The\ncore of such an attack is training a substitute model as similar to the target\nmodel as possible, where the simulation process can be categorized in a\ndata-dependent and data-free manner. Compared with the data-dependent method,\nthe data-free one has been proven to be more practical in the real world since\nit trains the substitute model with synthesized data. However, the distribution\nof these fake data lacks diversity and cannot detect the decision boundary of\nthe target model well, resulting in the dissatisfactory simulation effect.\nBesides, these data-free techniques need a vast number of queries to train the\nsubstitute model, increasing the time and computing consumption and the risk of\nexposure. To solve the aforementioned problems, in this paper, we propose a\nnovel data-free model extraction method named SCME (Self-Contrastive Model\nExtraction), which considers both the inter- and intra-class diversity in\nsynthesizing fake data. In addition, SCME introduces the Mixup operation to\naugment the fake data, which can explore the target model's decision boundary\neffectively and improve the simulating capacity. Extensive experiments show\nthat the proposed method can yield diversified fake data. Moreover, our method\nhas shown superiority in many different attack settings under the query-limited\nscenario, especially for untargeted attacks, the SCME outperforms SOTA methods\nby 11.43\\% on average for five baseline datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 15 Oct 2023 10:41:45 GMT"
      }
    ],
    "update_date": "2023-10-17",
    "authors_parsed": [
      [
        "Liu",
        "Renyang",
        ""
      ],
      [
        "Zhang",
        "Jinhong",
        ""
      ],
      [
        "Lam",
        "Kwok-Yan",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ],
      [
        "Zhou",
        "Wei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.09792",
    "publish_date": "2023-10-15"
  },
  {
    "id": "2310.10659",
    "submitter": "Peixin Zhang",
    "authors": "Peixin Zhang, Jun Sun, Mingtian Tan, Xinyu Wang",
    "title": "Exploiting Machine Unlearning for Backdoor Attacks in Deep Learning\n  System",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In recent years, the security issues of artificial intelligence have become\nincreasingly prominent due to the rapid development of deep learning research\nand applications. Backdoor attack is an attack targeting the vulnerability of\ndeep learning models, where hidden backdoors are activated by triggers embedded\nby the attacker, thereby outputting malicious predictions that may not align\nwith the intended output for a given input. In this work, we propose a novel\nblack-box backdoor attack based on machine unlearning. The attacker first\naugments the training set with carefully designed samples, including poison and\nmitigation data, to train a `benign' model. Then, the attacker posts unlearning\nrequests for the mitigation samples to remove the impact of relevant data on\nthe model, gradually activating the hidden backdoor. Since backdoors are\nimplanted during the iterative unlearning process, it significantly increases\nthe computational overhead of existing defense methods for backdoor detection\nor mitigation. To address this new security threat, we proposes two methods for\ndetecting or mitigating such malicious unlearning requests. We conduct the\nexperiment in both exact unlearning and approximate unlearning (i.e., SISA)\nsettings. Experimental results indicate that: 1) our attack approach can\nsuccessfully implant backdoor into the model, and sharding increases the\ndifficult of attack; 2) our detection algorithms are effective in identifying\nthe mitigation samples, while sharding reduces the effectiveness of our\ndetection algorithms.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Sep 2023 02:42:39 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 13 Dec 2023 15:00:28 GMT"
      }
    ],
    "update_date": "2023-12-14",
    "authors_parsed": [
      [
        "Zhang",
        "Peixin",
        ""
      ],
      [
        "Sun",
        "Jun",
        ""
      ],
      [
        "Tan",
        "Mingtian",
        ""
      ],
      [
        "Wang",
        "Xinyu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.10659",
    "publish_date": "2023-12-13"
  },
  {
    "id": "2310.10844",
    "submitter": "Erfan Shayegani",
    "authors": "Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong,\n  Nael Abu-Ghazaleh",
    "title": "Survey of Vulnerabilities in Large Language Models Revealed by\n  Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) are swiftly advancing in architecture and\ncapability, and as they integrate more deeply into complex systems, the urgency\nto scrutinize their security properties grows. This paper surveys research in\nthe emerging interdisciplinary field of adversarial attacks on LLMs, a subfield\nof trustworthy ML, combining the perspectives of Natural Language Processing\nand Security. Prior work has shown that even safety-aligned LLMs (via\ninstruction tuning and reinforcement learning through human feedback) can be\nsusceptible to adversarial attacks, which exploit weaknesses and mislead AI\nsystems, as evidenced by the prevalence of `jailbreak' attacks on models like\nChatGPT and Bard. In this survey, we first provide an overview of large\nlanguage models, describe their safety alignment, and categorize existing\nresearch based on various learning structures: textual-only attacks,\nmulti-modal attacks, and additional attack methods specifically targeting\ncomplex systems, such as federated learning or multi-agent systems. We also\noffer comprehensive remarks on works that focus on the fundamental sources of\nvulnerabilities and potential defenses. To make this field more accessible to\nnewcomers, we present a systematic review of existing works, a structured\ntypology of adversarial attack concepts, and additional resources, including\nslides for presentations on related topics at the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (ACL'24).\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 16 Oct 2023 21:37:24 GMT"
      }
    ],
    "update_date": "2023-10-18",
    "authors_parsed": [
      [
        "Shayegani",
        "Erfan",
        ""
      ],
      [
        "Mamun",
        "Md Abdullah Al",
        ""
      ],
      [
        "Fu",
        "Yu",
        ""
      ],
      [
        "Zaree",
        "Pedram",
        ""
      ],
      [
        "Dong",
        "Yue",
        ""
      ],
      [
        "Abu-Ghazaleh",
        "Nael",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.10844",
    "publish_date": "2023-10-16"
  },
  {
    "id": "2310.11397",
    "submitter": "Rui Wen",
    "authors": "Rui Wen, Tianhao Wang, Michael Backes, Yang Zhang, Ahmed Salem",
    "title": "Last One Standing: A Comparative Analysis of Security and Privacy of\n  Soft Prompt Tuning, LoRA, and In-Context Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) are powerful tools for natural language\nprocessing, enabling novel applications and user experiences. However, to\nachieve optimal performance, LLMs often require adaptation with private data,\nwhich poses privacy and security challenges. Several techniques have been\nproposed to adapt LLMs with private data, such as Low-Rank Adaptation (LoRA),\nSoft Prompt Tuning (SPT), and In-Context Learning (ICL), but their comparative\nprivacy and security properties have not been systematically investigated. In\nthis work, we fill this gap by evaluating the robustness of LoRA, SPT, and ICL\nagainst three types of well-established attacks: membership inference, which\nexposes data leakage (privacy); backdoor, which injects malicious behavior\n(security); and model stealing, which can violate intellectual property\n(privacy and security). Our results show that there is no silver bullet for\nprivacy and security in LLM adaptation and each technique has different\nstrengths and weaknesses.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Oct 2023 17:03:00 GMT"
      }
    ],
    "update_date": "2023-10-18",
    "authors_parsed": [
      [
        "Wen",
        "Rui",
        ""
      ],
      [
        "Wang",
        "Tianhao",
        ""
      ],
      [
        "Backes",
        "Michael",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ],
      [
        "Salem",
        "Ahmed",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.11397",
    "publish_date": "2023-10-17"
  },
  {
    "id": "2310.11595",
    "submitter": "Jun Xia",
    "authors": "Jun Xia, Zhihao Yue, Yingbo Zhou, Zhiwei Ling, Xian Wei, Mingsong Chen",
    "title": "WaveAttack: Asymmetric Frequency Obfuscation-based Backdoor Attacks\n  Against Deep Neural Networks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Due to the popularity of Artificial Intelligence (AI) technology, numerous\nbackdoor attacks are designed by adversaries to mislead deep neural network\npredictions by manipulating training samples and training processes. Although\nbackdoor attacks are effective in various real scenarios, they still suffer\nfrom the problems of both low fidelity of poisoned samples and non-negligible\ntransfer in latent space, which make them easily detectable by existing\nbackdoor detection algorithms. To overcome the weakness, this paper proposes a\nnovel frequency-based backdoor attack method named WaveAttack, which obtains\nimage high-frequency features through Discrete Wavelet Transform (DWT) to\ngenerate backdoor triggers. Furthermore, we introduce an asymmetric frequency\nobfuscation method, which can add an adaptive residual in the training and\ninference stage to improve the impact of triggers and further enhance the\neffectiveness of WaveAttack. Comprehensive experimental results show that\nWaveAttack not only achieves higher stealthiness and effectiveness, but also\noutperforms state-of-the-art (SOTA) backdoor attack methods in the fidelity of\nimages by up to 28.27\\% improvement in PSNR, 1.61\\% improvement in SSIM, and\n70.59\\% reduction in IS.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 17 Oct 2023 21:43:42 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 19 Oct 2023 14:42:49 GMT"
      }
    ],
    "update_date": "2023-10-20",
    "authors_parsed": [
      [
        "Xia",
        "Jun",
        ""
      ],
      [
        "Yue",
        "Zhihao",
        ""
      ],
      [
        "Zhou",
        "Yingbo",
        ""
      ],
      [
        "Ling",
        "Zhiwei",
        ""
      ],
      [
        "Wei",
        "Xian",
        ""
      ],
      [
        "Chen",
        "Mingsong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.11595",
    "publish_date": "2023-10-17"
  },
  {
    "id": "2310.12214",
    "submitter": "Meng Tong",
    "authors": "Meng Tong and Kejiang Chen and Jie Zhang and Yuang Qi and Weiming\n  Zhang and Nenghai Yu",
    "title": "InferDPT: Privacy-Preserving Inference for Black-box Large Language\n  Model",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs), like ChatGPT, have greatly simplified text\ngeneration tasks. However, they have also raised concerns about privacy risks\nsuch as data leakage and unauthorized data collection. Existing solutions for\nprivacy-preserving inference face practical challenges related to computation\ntime and communication costs. In this paper, we propose InferDPT, the first\npractical framework for the privacy-preserving Inference of black-box LLMs,\nimplementing Differential Privacy in Text generation. InferDPT comprises two\nkey modules: the \"perturbation module\" utilizes the exponential mechanism to\ngenerate a perturbed prompt, facilitating privacy-preserving inference with\nblack-box LLMs, and the \"extraction module\", inspired by knowledge distillation\nand retrieval-augmented generation, extracts coherent and consistent text from\nthe perturbed generation result, ensuring successful text generation\ncompletion. To address privacy concerns related to previous exponential\nmechanisms' susceptibility to embedding revision attacks, we introduce RANTEXT,\na novel differential privacy mechanism integrated into the perturbation module\nof InferDPT, which introduces the concept of \"RANdom adjacency\" for TEXT\nperturbation within the prompt. Experimental results across three datasets\ndemonstrate that the text generation quality of InferDPT is comparable to that\nof non-private GPT-4, and RANTEXT surpasses existing state-of-the-art\nmechanisms, namely, SANTEXT+ and CUSTEXT+ in the trade-off between privacy and\nutility. Even with an privacy parameter epsilon value of 6.0, RANTEXT achieves\nan average privacy protection rate exceeding 90% against embedding revision\nattacks, which is 0.58 times higher than that of SANTEXT+ and 3.35 times higher\nthan that of CUSTEXT+.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 18 Oct 2023 18:00:11 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 22 Oct 2023 07:34:36 GMT"
      },
      {
        "version": "v3",
        "created": "Tue, 24 Oct 2023 03:25:14 GMT"
      },
      {
        "version": "v4",
        "created": "Fri, 8 Dec 2023 05:14:40 GMT"
      },
      {
        "version": "v5",
        "created": "Mon, 11 Dec 2023 09:59:09 GMT"
      }
    ],
    "update_date": "2023-12-12",
    "authors_parsed": [
      [
        "Tong",
        "Meng",
        ""
      ],
      [
        "Chen",
        "Kejiang",
        ""
      ],
      [
        "Zhang",
        "Jie",
        ""
      ],
      [
        "Qi",
        "Yuang",
        ""
      ],
      [
        "Zhang",
        "Weiming",
        ""
      ],
      [
        "Yu",
        "Nenghai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.12214",
    "publish_date": "2023-10-18"
  },
  {
    "id": "2310.12439",
    "submitter": "Hongwei Yao",
    "authors": "Hongwei Yao, Jian Lou and Zhan Qin",
    "title": "PoisonPrompt: Backdoor Attack on Prompt-based Large Language Models",
    "comments": "To Appear in IEEE ICASSP 2024, code is available at:\n  https://github.com/grasses/PoisonPrompt",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Prompts have significantly improved the performance of pretrained Large\nLanguage Models (LLMs) on various downstream tasks recently, making them\nincreasingly indispensable for a diverse range of LLM application scenarios.\nHowever, the backdoor vulnerability, a serious security threat that can\nmaliciously alter the victim model's normal predictions, has not been\nsufficiently explored for prompt-based LLMs. In this paper, we present\nPOISONPROMPT, a novel backdoor attack capable of successfully compromising both\nhard and soft prompt-based LLMs. We evaluate the effectiveness, fidelity, and\nrobustness of POISONPROMPT through extensive experiments on three popular\nprompt methods, using six datasets and three widely used LLMs. Our findings\nhighlight the potential security threats posed by backdoor attacks on\nprompt-based LLMs and emphasize the need for further research in this area.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 03:25:28 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 18 Dec 2023 13:20:46 GMT"
      }
    ],
    "update_date": "2023-12-19",
    "authors_parsed": [
      [
        "Yao",
        "Hongwei",
        ""
      ],
      [
        "Lou",
        "Jian",
        ""
      ],
      [
        "Qin",
        "Zhan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.12439",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.12516",
    "submitter": "Xiaodong Yu",
    "authors": "Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, Jianfeng Gao",
    "title": "Automatic Hallucination Assessment for Aligned Large Language Models via\n  Transferable Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Although remarkable progress has been achieved in preventing large language\nmodel (LLM) hallucinations using instruction tuning and retrieval augmentation,\nit remains challenging to measure the reliability of LLMs using human-crafted\nevaluation data which is not available for many tasks and domains and could\nsuffer from data leakage. Inspired by adversarial machine learning, this paper\naims to develop a method of automatically generating evaluation data by\nappropriately modifying existing data on which LLMs behave faithfully.\nSpecifically, this paper presents AutoDebug, an LLM-based framework to use\nprompting chaining to generate transferable adversarial attacks in the form of\nquestion-answering examples. We seek to understand the extent to which these\nexamples trigger the hallucination behaviors of LLMs.\n  We implement AutoDebug using ChatGPT and evaluate the resulting two variants\nof a popular open-domain question-answering dataset, Natural Questions (NQ), on\na collection of open-source and proprietary LLMs under various prompting\nsettings. Our generated evaluation data is human-readable and, as we show,\nhumans can answer these modified questions well. Nevertheless, we observe\npronounced accuracy drops across multiple LLMs including GPT-4. Our\nexperimental results show that LLMs are likely to hallucinate in two categories\nof question-answering scenarios where (1) there are conflicts between knowledge\ngiven in the prompt and their parametric knowledge, or (2) the knowledge\nexpressed in the prompt is complex. Finally, we find that the adversarial\nexamples generated by our method are transferable across all considered LLMs.\nThe examples generated by a small model can be used to debug a much larger\nmodel, making our approach cost-effective.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 06:37:32 GMT"
      }
    ],
    "update_date": "2023-10-20",
    "authors_parsed": [
      [
        "Yu",
        "Xiaodong",
        ""
      ],
      [
        "Cheng",
        "Hao",
        ""
      ],
      [
        "Liu",
        "Xiaodong",
        ""
      ],
      [
        "Roth",
        "Dan",
        ""
      ],
      [
        "Gao",
        "Jianfeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.12516",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.12815",
    "submitter": "Yupei Liu",
    "authors": "Yupei Liu, Yuqi Jia, Runpeng Geng, Jinyuan Jia, Neil Zhenqiang Gong",
    "title": "Prompt Injection Attacks and Defenses in LLM-Integrated Applications",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) are increasingly deployed as the backend for a\nvariety of real-world applications called LLM-Integrated Applications. Multiple\nrecent works showed that LLM-Integrated Applications are vulnerable to prompt\ninjection attacks, in which an attacker injects malicious instruction/data into\nthe input of those applications such that they produce results as the attacker\ndesires. However, existing works are limited to case studies. As a result, the\nliterature lacks a systematic understanding of prompt injection attacks and\ntheir defenses. We aim to bridge the gap in this work. In particular, we\npropose a general framework to formalize prompt injection attacks. Existing\nattacks, which are discussed in research papers and blog posts, are special\ncases in our framework. Our framework enables us to design a new attack by\ncombining existing attacks. Moreover, we also propose a framework to\nsystematize defenses against prompt injection attacks. Using our frameworks, we\nconduct a systematic evaluation on prompt injection attacks and their defenses\nwith 10 LLMs and 7 tasks. We hope our frameworks can inspire future research in\nthis field. Our code is available at\nhttps://github.com/liu00222/Open-Prompt-Injection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 15:12:09 GMT"
      }
    ],
    "update_date": "2023-10-20",
    "authors_parsed": [
      [
        "Liu",
        "Yupei",
        ""
      ],
      [
        "Jia",
        "Yuqi",
        ""
      ],
      [
        "Geng",
        "Runpeng",
        ""
      ],
      [
        "Jia",
        "Jinyuan",
        ""
      ],
      [
        "Gong",
        "Neil Zhenqiang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.12815",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.12860",
    "submitter": "Punyajoy Saha",
    "authors": "Sarthak Roy, Ashish Harshavardhan, Animesh Mukherjee and Punyajoy Saha",
    "title": "Probing LLMs for hate speech detection: strengths and vulnerabilities",
    "comments": "13 pages, 9 figures, 7 tables, accepted to findings of EMNLP 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.CY",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Recently efforts have been made by social media platforms as well as\nresearchers to detect hateful or toxic language using large language models.\nHowever, none of these works aim to use explanation, additional context and\nvictim community information in the detection process. We utilise different\nprompt variation, input information and evaluate large language models in zero\nshot setting (without adding any in-context examples). We select three large\nlanguage models (GPT-3.5, text-davinci and Flan-T5) and three datasets -\nHateXplain, implicit hate and ToxicSpans. We find that on average including the\ntarget information in the pipeline improves the model performance substantially\n(~20-30%) over the baseline across the datasets. There is also a considerable\neffect of adding the rationales/explanations into the pipeline (~10-20%) over\nthe baseline across the datasets. In addition, we further provide a typology of\nthe error cases where these large language models fail to (i) classify and (ii)\nexplain the reason for the decisions they take. Such vulnerable points\nautomatically constitute 'jailbreak' prompts for these models and industry\nscale safeguard techniques need to be developed to make the models robust\nagainst such prompts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 16:11:02 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 28 Oct 2023 05:07:31 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Roy",
        "Sarthak",
        ""
      ],
      [
        "Harshavardhan",
        "Ashish",
        ""
      ],
      [
        "Mukherjee",
        "Animesh",
        ""
      ],
      [
        "Saha",
        "Punyajoy",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.12860",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.13103",
    "submitter": "Ammarah Hashmi",
    "authors": "Ammarah Hashmi, Sahibzada Adil Shahzad, Chia-Wen Lin, Yu Tsao,\n  Hsin-Min Wang",
    "title": "AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting\n  Multiple Experts for Video Deepfake Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG cs.MM cs.SD eess.AS",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Forged content shared widely on social media platforms is a major social\nproblem that requires increased regulation and poses new challenges to the\nresearch community. The recent proliferation of hyper-realistic deepfake videos\nhas drawn attention to the threat of audio and visual forgeries. Most previous\nwork on detecting AI-generated fake videos only utilizes visual modality or\naudio modality. While there are some methods in the literature that exploit\naudio and visual modalities to detect forged videos, they have not been\ncomprehensively evaluated on multi-modal datasets of deepfake videos involving\nacoustic and visual manipulations. Moreover, these existing methods are mostly\nbased on CNN and suffer from low detection accuracy. Inspired by the recent\nsuccess of Transformer in various fields, to address the challenges posed by\ndeepfake technology, in this paper, we propose an Audio-Visual\nTransformer-based Ensemble Network (AVTENet) framework that considers both\nacoustic manipulation and visual manipulation to achieve effective video\nforgery detection. Specifically, the proposed model integrates several purely\ntransformer-based variants that capture video, audio, and audio-visual salient\ncues to reach a consensus in prediction. For evaluation, we use the recently\nreleased benchmark multi-modal audio-video FakeAVCeleb dataset. For a detailed\nanalysis, we evaluate AVTENet, its variants, and several existing methods on\nmultiple test sets of the FakeAVCeleb dataset. Experimental results show that\nour best model outperforms all existing methods and achieves state-of-the-art\nperformance on Testset-I and Testset-II of the FakeAVCeleb dataset.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 19:01:26 GMT"
      }
    ],
    "update_date": "2023-10-23",
    "authors_parsed": [
      [
        "Hashmi",
        "Ammarah",
        ""
      ],
      [
        "Shahzad",
        "Sahibzada Adil",
        ""
      ],
      [
        "Lin",
        "Chia-Wen",
        ""
      ],
      [
        "Tsao",
        "Yu",
        ""
      ],
      [
        "Wang",
        "Hsin-Min",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.13103",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.13191",
    "submitter": "Jianwei Li",
    "authors": "Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu",
    "title": "Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy\n  for Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  The pruning objective has recently extended beyond accuracy and sparsity to\nrobustness in language models. Despite this, existing methods struggle to\nenhance robustness against adversarial attacks when continually increasing\nmodel sparsity and require a retraining process. As humans step into the era of\nlarge language models, these issues become increasingly prominent. This paper\nproposes that the robustness of language models is proportional to the extent\nof pre-trained knowledge they encompass. Accordingly, we introduce a\npost-training pruning strategy designed to faithfully replicate the embedding\nspace and feature space of dense language models, aiming to conserve more\npre-trained knowledge during the pruning process. In this setup, each layer's\nreconstruction error not only originates from itself but also includes\ncumulative error from preceding layers, followed by an adaptive rectification.\nCompared to other state-of-art baselines, our approach demonstrates a superior\nbalance between accuracy, sparsity, robustness, and pruning cost with BERT on\ndatasets SST2, IMDB, and AGNews, marking a significant stride towards robust\npruning in language models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 19 Oct 2023 23:02:29 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 10 Dec 2023 02:19:17 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 11 Jan 2024 04:07:39 GMT"
      }
    ],
    "update_date": "2024-01-12",
    "authors_parsed": [
      [
        "Li",
        "Jianwei",
        ""
      ],
      [
        "Lei",
        "Qi",
        ""
      ],
      [
        "Cheng",
        "Wei",
        ""
      ],
      [
        "Xu",
        "Dongkuan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.13191",
    "publish_date": "2023-10-19"
  },
  {
    "id": "2310.13345",
    "submitter": "Xilie Xu",
    "authors": "Xilie Xu, Keyi Kong, Ning Liu, Lizhen Cui, Di Wang, Jingfeng Zhang,\n  Mohan Kankanhalli",
    "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The wide-ranging applications of large language models (LLMs), especially in\nsafety-critical domains, necessitate the proper evaluation of the LLM's\nadversarial robustness. This paper proposes an efficient tool to audit the\nLLM's adversarial robustness via a prompt-based adversarial attack\n(PromptAttack). PromptAttack converts adversarial textual attacks into an\nattack prompt that can cause the victim LLM to output the adversarial sample to\nfool itself. The attack prompt is composed of three important components: (1)\noriginal input (OI) including the original sample and its ground-truth label,\n(2) attack objective (AO) illustrating a task description of generating a new\nsample that can fool itself without changing the semantic meaning, and (3)\nattack guidance (AG) containing the perturbation instructions to guide the LLM\non how to complete the task by perturbing the original sample at character,\nword, and sentence levels, respectively. Besides, we use a fidelity filter to\nensure that PromptAttack maintains the original semantic meanings of the\nadversarial examples. Further, we enhance the attack power of PromptAttack by\nensembling adversarial examples at different perturbation levels. Comprehensive\nempirical results using Llama2 and GPT-3.5 validate that PromptAttack\nconsistently yields a much higher attack success rate compared to AdvGLUE and\nAdvGLUE++. Interesting findings include that a simple emoji can easily mislead\nGPT-3.5 to make wrong predictions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 20 Oct 2023 08:16:46 GMT"
      }
    ],
    "update_date": "2023-10-23",
    "authors_parsed": [
      [
        "Xu",
        "Xilie",
        ""
      ],
      [
        "Kong",
        "Keyi",
        ""
      ],
      [
        "Liu",
        "Ning",
        ""
      ],
      [
        "Cui",
        "Lizhen",
        ""
      ],
      [
        "Wang",
        "Di",
        ""
      ],
      [
        "Zhang",
        "Jingfeng",
        ""
      ],
      [
        "Kankanhalli",
        "Mohan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.13345",
    "publish_date": "2023-10-20"
  },
  {
    "id": "2310.13893",
    "submitter": "Erfan Darzi",
    "authors": "Erfan Darzi, Florian Dubost, Nanna. M. Sijtsema, P.M.A van Ooijen",
    "title": "The Hidden Adversarial Vulnerabilities of Medical Federated Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In this paper, we delve into the susceptibility of federated medical image\nanalysis systems to adversarial attacks. Our analysis uncovers a novel\nexploitation avenue: using gradient information from prior global model\nupdates, adversaries can enhance the efficiency and transferability of their\nattacks. Specifically, we demonstrate that single-step attacks (e.g. FGSM),\nwhen aptly initialized, can outperform the efficiency of their iterative\ncounterparts but with reduced computational demand. Our findings underscore the\nneed to revisit our understanding of AI security in federated healthcare\nsettings.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 21 Oct 2023 02:21:39 GMT"
      }
    ],
    "update_date": "2023-10-24",
    "authors_parsed": [
      [
        "Darzi",
        "Erfan",
        ""
      ],
      [
        "Dubost",
        "Florian",
        ""
      ],
      [
        "Sijtsema",
        "Nanna. M.",
        ""
      ],
      [
        "van Ooijen",
        "P. M. A",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.13893",
    "publish_date": "2023-10-21"
  },
  {
    "id": "2310.14303",
    "submitter": "Rishabh Bhardwaj",
    "authors": "Rishabh Bhardwaj, Soujanya Poria",
    "title": "Language Model Unalignment: Parametric Red-Teaming to Expose Hidden\n  Harms and Biases",
    "comments": "Under Review",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Red-teaming has been a widely adopted way to evaluate the harmfulness of\nLarge Language Models (LLMs). It aims to jailbreak a model's safety behavior to\nmake it act as a helpful agent disregarding the harmfulness of the query.\nExisting methods are primarily based on input text-based red-teaming such as\nadversarial prompts, low-resource prompts, or contextualized prompts to\ncondition the model in a way to bypass its safe behavior. Bypassing the\nguardrails uncovers hidden harmful information and biases in the model that are\nleft untreated or newly introduced by its safety training. However,\nprompt-based attacks fail to provide such a diagnosis owing to their low attack\nsuccess rate, and applicability to specific models. In this paper, we present a\nnew perspective on LLM safety research i.e., parametric red-teaming through\nUnalignment. It simply (instruction) tunes the model parameters to break model\nguardrails that are not deeply rooted in the model's behavior. Unalignment\nusing as few as 100 examples can significantly bypass commonly referred to as\nCHATGPT, to the point where it responds with an 88% success rate to harmful\nqueries on two safety benchmark datasets. On open-source models such as\nVICUNA-7B and LLAMA-2-CHAT 7B AND 13B, it shows an attack success rate of more\nthan 91%. On bias evaluations, Unalignment exposes inherent biases in\nsafety-aligned models such as CHATGPT and LLAMA- 2-CHAT where the model's\nresponses are strongly biased and opinionated 64% of the time.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 22 Oct 2023 13:55:46 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 13 Nov 2023 05:28:47 GMT"
      }
    ],
    "update_date": "2023-11-14",
    "authors_parsed": [
      [
        "Bhardwaj",
        "Rishabh",
        ""
      ],
      [
        "Poria",
        "Soujanya",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.14303",
    "publish_date": "2023-10-22"
  },
  {
    "id": "2310.15117",
    "submitter": "Aniket Vashishtha",
    "authors": "Aniket Vashishtha, Abbavaram Gowtham Reddy, Abhinav Kumar, Saketh\n  Bachu, Vineeth N Balasubramanian, Amit Sharma",
    "title": "Causal Inference Using LLM-Guided Discovery",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  At the core of causal inference lies the challenge of determining reliable\ncausal graphs solely based on observational data. Since the well-known backdoor\ncriterion depends on the graph, any errors in the graph can propagate\ndownstream to effect inference. In this work, we initially show that complete\ngraph information is not necessary for causal effect inference; the topological\norder over graph variables (causal order) alone suffices. Further, given a node\npair, causal order is easier to elicit from domain experts compared to graph\nedges since determining the existence of an edge can depend extensively on\nother variables. Interestingly, we find that the same principle holds for Large\nLanguage Models (LLMs) such as GPT-3.5-turbo and GPT-4, motivating an automated\nmethod to obtain causal order (and hence causal effect) with LLMs acting as\nvirtual domain experts. To this end, we employ different prompting strategies\nand contextual cues to propose a robust technique of obtaining causal order\nfrom LLMs. Acknowledging LLMs' limitations, we also study possible techniques\nto integrate LLMs with established causal discovery algorithms, including\nconstraint-based and score-based methods, to enhance their performance.\nExtensive experiments demonstrate that our approach significantly improves\ncausal ordering accuracy as compared to discovery algorithms, highlighting the\npotential of LLMs to enhance causal inference across diverse fields.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 23 Oct 2023 17:23:56 GMT"
      }
    ],
    "update_date": "2023-10-24",
    "authors_parsed": [
      [
        "Vashishtha",
        "Aniket",
        ""
      ],
      [
        "Reddy",
        "Abbavaram Gowtham",
        ""
      ],
      [
        "Kumar",
        "Abhinav",
        ""
      ],
      [
        "Bachu",
        "Saketh",
        ""
      ],
      [
        "Balasubramanian",
        "Vineeth N",
        ""
      ],
      [
        "Sharma",
        "Amit",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.15117",
    "publish_date": "2023-10-23"
  },
  {
    "id": "2310.15140",
    "submitter": "Sicheng Zhu",
    "authors": "Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang,\n  Furong Huang, Ani Nenkova, Tong Sun",
    "title": "AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large\n  Language Models",
    "comments": "Version 2 updates: Added comparison of three more evaluation methods\n  and their reliability check using human labeling. Added results for\n  jailbreaking Llama2 (individual behavior) and included complexity and\n  hyperparameter analysis. Revised objectives for prompt leaking. Other minor\n  changes made",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Safety alignment of Large Language Models (LLMs) can be compromised with\nmanual jailbreak attacks and (automatic) adversarial attacks. Recent studies\nsuggest that defending against these attacks is possible: adversarial attacks\ngenerate unlimited but unreadable gibberish prompts, detectable by\nperplexity-based filters; manual jailbreak attacks craft readable prompts, but\ntheir limited number due to the necessity of human creativity allows for easy\nblocking. In this paper, we show that these solutions may be too optimistic. We\nintroduce AutoDAN, an interpretable, gradient-based adversarial attack that\nmerges the strengths of both attack types. Guided by the dual goals of\njailbreak and readability, AutoDAN optimizes and generates tokens one by one\nfrom left to right, resulting in readable prompts that bypass perplexity\nfilters while maintaining high attack success rates. Notably, these prompts,\ngenerated from scratch using gradients, are interpretable and diverse, with\nemerging strategies commonly seen in manual jailbreak attacks. They also\ngeneralize to unforeseen harmful behaviors and transfer to black-box LLMs\nbetter than their unreadable counterparts when using limited training data or a\nsingle proxy model. Furthermore, we show the versatility of AutoDAN by\nautomatically leaking system prompts using a customized objective. Our work\noffers a new way to red-team LLMs and understand jailbreak mechanisms via\ninterpretability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 23 Oct 2023 17:46:07 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 14 Dec 2023 06:22:51 GMT"
      }
    ],
    "update_date": "2023-12-15",
    "authors_parsed": [
      [
        "Zhu",
        "Sicheng",
        ""
      ],
      [
        "Zhang",
        "Ruiyi",
        ""
      ],
      [
        "An",
        "Bang",
        ""
      ],
      [
        "Wu",
        "Gang",
        ""
      ],
      [
        "Barrow",
        "Joe",
        ""
      ],
      [
        "Wang",
        "Zichao",
        ""
      ],
      [
        "Huang",
        "Furong",
        ""
      ],
      [
        "Nenkova",
        "Ani",
        ""
      ],
      [
        "Sun",
        "Tong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.15140",
    "publish_date": "2023-12-14"
  },
  {
    "id": "2310.15851",
    "submitter": "Zezhong Wang Mr.",
    "authors": "Zezhong Wang, Fangkai Yang, Lu Wang, Pu Zhao, Hongru Wang, Liang Chen,\n  Qingwei Lin, Kam-Fai Wong",
    "title": "Self-Guard: Empower the LLM to Safeguard Itself",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  The jailbreak attack can bypass the safety measures of a Large Language Model\n(LLM), generating harmful content. This misuse of LLM has led to negative\nsocietal consequences. Currently, there are two main approaches to address\njailbreak attacks: safety training and safeguards. Safety training focuses on\nfurther training LLM to enhance its safety. On the other hand, safeguards\ninvolve implementing external models or filters to prevent harmful outputs.\nHowever, safety training has constraints in its ability to adapt to new attack\ntypes and often leads to a drop in model performance. Safeguards have proven to\nbe of limited help. To tackle these issues, we propose a novel approach called\nSelf-Guard, which combines the strengths of both safety methods. Self-Guard\nincludes two stages. In the first stage, we enhance the model's ability to\nassess harmful content, and in the second stage, we instruct the model to\nconsistently perform harmful content detection on its own responses. The\nexperiment has demonstrated that Self-Guard is robust against jailbreak\nattacks. In the bad case analysis, we find that LLM occasionally provides\nharmless responses to harmful queries. Additionally, we evaluated the general\ncapabilities of the LLM before and after safety training, providing evidence\nthat Self-Guard does not result in the LLM's performance degradation. In\nsensitivity tests, Self-Guard not only avoids inducing over-sensitivity in LLM\nbut also can even mitigate this issue.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 24 Oct 2023 14:08:26 GMT"
      }
    ],
    "update_date": "2023-10-25",
    "authors_parsed": [
      [
        "Wang",
        "Zezhong",
        ""
      ],
      [
        "Yang",
        "Fangkai",
        ""
      ],
      [
        "Wang",
        "Lu",
        ""
      ],
      [
        "Zhao",
        "Pu",
        ""
      ],
      [
        "Wang",
        "Hongru",
        ""
      ],
      [
        "Chen",
        "Liang",
        ""
      ],
      [
        "Lin",
        "Qingwei",
        ""
      ],
      [
        "Wong",
        "Kam-Fai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.15851",
    "publish_date": "2023-10-24"
  },
  {
    "id": "2310.16253",
    "submitter": "YongHao Wu",
    "authors": "Yonghao Wu, Zheng Li, Jie M. Zhang, Yong Liu",
    "title": "ConDefects: A New Dataset to Address the Data Leakage Concern for\n  LLM-based Fault Localization and Program Repair",
    "comments": "5pages, 3 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  With the growing interest on Large Language Models (LLMs) for fault\nlocalization and program repair, ensuring the integrity and generalizability of\nthe LLM-based methods becomes paramount. The code in existing widely-adopted\nbenchmarks for these tasks was written before the the bloom of LLMs and may be\nincluded in the training data of existing popular LLMs, thereby suffering from\nthe threat of data leakage, leading to misleadingly optimistic performance\nmetrics. To address this issue, we introduce \"ConDefects\", a novel dataset of\nreal faults meticulously curated to eliminate such overlap. ConDefects contains\n1,254 Java faulty programs and 1,625 Python faulty programs. All these programs\nare sourced from the online competition platform AtCoder and were produced\nbetween October 2021 and September 2023. We pair each fault with fault\nlocations and the corresponding repaired code versions, making it tailored for\nin fault localization and program repair related research. We also provide\ninterfaces for selecting subsets based on different time windows and coding\ntask difficulties. While inspired by LLM-based tasks, ConDefects can be adopted\nfor benchmarking ALL types of fault localization and program repair methods.\nThe dataset is publicly available, and a demo video can be found at\nhttps://www.youtube.com/watch?v=22j15Hj5ONk.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Oct 2023 00:06:02 GMT"
      }
    ],
    "update_date": "2023-10-26",
    "authors_parsed": [
      [
        "Wu",
        "Yonghao",
        ""
      ],
      [
        "Li",
        "Zheng",
        ""
      ],
      [
        "Zhang",
        "Jie M.",
        ""
      ],
      [
        "Liu",
        "Yong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.16253",
    "publish_date": "2023-10-25"
  },
  {
    "id": "2310.18587",
    "submitter": "Guang Yang",
    "authors": "Guang Yang, Yu Zhou, Xiangyu Zhang, Xiang Chen, Tingting Han, Taolue\n  Chen",
    "title": "Assessing and Improving Syntactic Adversarial Robustness of Pre-trained\n  Models for Code Translation",
    "comments": "under review",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Context: Pre-trained models (PTMs) have demonstrated significant potential in\nautomatic code translation. However, the vulnerability of these models in\ntranslation tasks, particularly in terms of syntax, has not been extensively\ninvestigated. Objective: To fill this gap, our study aims to propose a novel\napproach CoTR to assess and improve the syntactic adversarial robustness of\nPTMs in code translation. Method: CoTR consists of two components: CoTR-A and\nCoTR-D. CoTR-A generates adversarial examples by transforming programs, while\nCoTR-D proposes a semantic distance-based sampling data augmentation method and\nadversarial training method to improve the model's robustness and\ngeneralization capabilities. The Pass@1 metric is used by CoTR to assess the\nperformance of PTMs, which is more suitable for code translation tasks and\noffers a more precise evaluation in real world scenarios. Results: The\neffectiveness of CoTR is evaluated through experiments on real world Java to\nPython datasets. The results demonstrate that CoTR-A can significantly reduce\nthe performance of existing PTMs, while CoTR-D effectively improves the\nrobustness of PTMs. Conclusion: Our study identifies the limitations of current\nPTMs, including large language models, in code translation tasks. It highlights\nthe potential of CoTR as an effective solution to enhance the robustness of\nPTMs for code translation tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 28 Oct 2023 04:35:24 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Yang",
        "Guang",
        ""
      ],
      [
        "Zhou",
        "Yu",
        ""
      ],
      [
        "Zhang",
        "Xiangyu",
        ""
      ],
      [
        "Chen",
        "Xiang",
        ""
      ],
      [
        "Han",
        "Tingting",
        ""
      ],
      [
        "Chen",
        "Taolue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.18587",
    "publish_date": "2023-10-28"
  },
  {
    "id": "2310.18603",
    "submitter": "Wencong You",
    "authors": "Wencong You, Zayd Hammoudeh, Daniel Lowd",
    "title": "Large Language Models Are Better Adversaries: Exploring Generative\n  Clean-Label Backdoor Attacks Against Text Classifiers",
    "comments": "Accepted at EMNLP 2023 Findings",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Backdoor attacks manipulate model predictions by inserting innocuous triggers\ninto training and test data. We focus on more realistic and more challenging\nclean-label attacks where the adversarial training examples are correctly\nlabeled. Our attack, LLMBkd, leverages language models to automatically insert\ndiverse style-based triggers into texts. We also propose a poison selection\ntechnique to improve the effectiveness of both LLMBkd as well as existing\ntextual backdoor attacks. Lastly, we describe REACT, a baseline defense to\nmitigate backdoor attacks via antidote training examples. Our evaluations\ndemonstrate LLMBkd's effectiveness and efficiency, where we consistently\nachieve high attack success rates across a wide range of styles with little\neffort and no model training.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 28 Oct 2023 06:11:07 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "You",
        "Wencong",
        ""
      ],
      [
        "Hammoudeh",
        "Zayd",
        ""
      ],
      [
        "Lowd",
        "Daniel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.18603",
    "publish_date": "2023-10-28"
  },
  {
    "id": "2310.18762",
    "submitter": "Boya Zhang",
    "authors": "Boya Zhang, Weijian Luo, Zhihua Zhang",
    "title": "Purify++: Improving Diffusion-Purification with Advanced Diffusion\n  Models and Control of Randomness",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Adversarial attacks can mislead neural network classifiers. The defense\nagainst adversarial attacks is important for AI safety. Adversarial\npurification is a family of approaches that defend adversarial attacks with\nsuitable pre-processing. Diffusion models have been shown to be effective for\nadversarial purification. Despite their success, many aspects of diffusion\npurification still remain unexplored. In this paper, we investigate and improve\nupon three limiting designs of diffusion purification: the use of an improved\ndiffusion model, advanced numerical simulation techniques, and optimal control\nof randomness. Based on our findings, we propose Purify++, a new diffusion\npurification algorithm that is now the state-of-the-art purification method\nagainst several adversarial attacks. Our work presents a systematic exploration\nof the limits of diffusion purification methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 28 Oct 2023 17:18:38 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Zhang",
        "Boya",
        ""
      ],
      [
        "Luo",
        "Weijian",
        ""
      ],
      [
        "Zhang",
        "Zhihua",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.18762",
    "publish_date": "2023-10-28"
  },
  {
    "id": "2310.19619",
    "submitter": "Ziqiao Ma",
    "authors": "Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai",
    "title": "Towards A Holistic Landscape of Situated Theory of Mind in Large\n  Language Models",
    "comments": "Theme Track, Findings of EMNLP 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) have generated considerable interest and debate\nregarding their potential emergence of Theory of Mind (ToM). Several recent\ninquiries reveal a lack of robust ToM in these models and pose a pressing\ndemand to develop new benchmarks, as current ones primarily focus on different\naspects of ToM and are prone to shortcuts and data leakage. In this position\npaper, we seek to answer two road-blocking questions: (1) How can we taxonomize\na holistic landscape of machine ToM? (2) What is a more effective evaluation\nprotocol for machine ToM? Following psychological studies, we taxonomize\nmachine ToM into 7 mental state categories and delineate existing benchmarks to\nidentify under-explored aspects of ToM. We argue for a holistic and situated\nevaluation of ToM to break ToM into individual components and treat LLMs as an\nagent who is physically situated in environments and socially situated in\ninteractions with humans. Such situated evaluation provides a more\ncomprehensive assessment of mental states and potentially mitigates the risk of\nshortcuts and data leakage. We further present a pilot study in a grid world\nsetup as a proof of concept. We hope this position paper can facilitate future\nresearch to integrate ToM with LLMs and offer an intuitive means for\nresearchers to better position their work in the landscape of ToM. Project\npage: https://github.com/Mars-tin/awesome-theory-of-mind\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 Oct 2023 15:12:09 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Ma",
        "Ziqiao",
        ""
      ],
      [
        "Sansom",
        "Jacob",
        ""
      ],
      [
        "Peng",
        "Run",
        ""
      ],
      [
        "Chai",
        "Joyce",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.19619",
    "publish_date": "2023-10-30"
  },
  {
    "id": "2310.19737",
    "submitter": "Leo Schwinn",
    "authors": "Leo Schwinn and David Dobre and Stephan G\\\"unnemann and Gauthier Gidel",
    "title": "Adversarial Attacks and Defenses in Large Language Models: Old and New\n  Threats",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Over the past decade, there has been extensive research aimed at enhancing\nthe robustness of neural networks, yet this problem remains vastly unsolved.\nHere, one major impediment has been the overestimation of the robustness of new\ndefense approaches due to faulty defense evaluations. Flawed robustness\nevaluations necessitate rectifications in subsequent works, dangerously slowing\ndown the research and providing a false sense of security. In this context, we\nwill face substantial challenges associated with an impending adversarial arms\nrace in natural language processing, specifically with closed-source Large\nLanguage Models (LLMs), such as ChatGPT, Google Bard, or Anthropic's Claude. We\nprovide a first set of prerequisites to improve the robustness assessment of\nnew approaches and reduce the amount of faulty evaluations. Additionally, we\nidentify embedding space attacks on LLMs as another viable threat model for the\npurposes of generating malicious content in open-sourced models. Finally, we\ndemonstrate on a recently proposed defense that, without LLM-specific best\npractices in place, it is easy to overestimate the robustness of a new\napproach.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 30 Oct 2023 17:01:02 GMT"
      }
    ],
    "update_date": "2023-10-31",
    "authors_parsed": [
      [
        "Schwinn",
        "Leo",
        ""
      ],
      [
        "Dobre",
        "David",
        ""
      ],
      [
        "G\u00fcnnemann",
        "Stephan",
        ""
      ],
      [
        "Gidel",
        "Gauthier",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.19737",
    "publish_date": "2023-10-30"
  },
  {
    "id": "2310.20138",
    "submitter": "Xinwei Wu",
    "authors": "Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao\n  Bian, Deyi Xiong",
    "title": "DEPN: Detecting and Editing Privacy Neurons in Pretrained Language\n  Models",
    "comments": "EMNLP 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CL",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Large language models pretrained on a huge amount of data capture rich\nknowledge and information in the training data. The ability of data\nmemorization and regurgitation in pretrained language models, revealed in\nprevious studies, brings the risk of data leakage. In order to effectively\nreduce these risks, we propose a framework DEPN to Detect and Edit Privacy\nNeurons in pretrained language models, partially inspired by knowledge neurons\nand model editing. In DEPN, we introduce a novel method, termed as privacy\nneuron detector, to locate neurons associated with private information, and\nthen edit these detected privacy neurons by setting their activations to zero.\nFurthermore, we propose a privacy neuron aggregator dememorize private\ninformation in a batch processing manner. Experimental results show that our\nmethod can significantly and efficiently reduce the exposure of private data\nleakage without deteriorating the performance of the model. Additionally, we\nempirically demonstrate the relationship between model memorization and privacy\nneurons, from multiple perspectives, including model size, training time,\nprompts, privacy neuron distribution, illustrating the robustness of our\napproach.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 31 Oct 2023 03:09:36 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Dec 2023 16:14:24 GMT"
      }
    ],
    "update_date": "2023-12-06",
    "authors_parsed": [
      [
        "Wu",
        "Xinwei",
        ""
      ],
      [
        "Li",
        "Junzhuo",
        ""
      ],
      [
        "Xu",
        "Minghui",
        ""
      ],
      [
        "Dong",
        "Weilong",
        ""
      ],
      [
        "Wu",
        "Shuangzhi",
        ""
      ],
      [
        "Bian",
        "Chao",
        ""
      ],
      [
        "Xiong",
        "Deyi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2310.20138",
    "publish_date": "2023-12-05"
  },
  {
    "id": "2311.00172",
    "submitter": "Jinhwa Kim",
    "authors": "Jinhwa Kim, Ali Derakhshan, Ian G. Harris",
    "title": "Robust Safety Classifier for Large Language Models: Adversarial Prompt\n  Shield",
    "comments": "11 pages, 2 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models' safety remains a critical concern due to their\nvulnerability to adversarial attacks, which can prompt these systems to produce\nharmful responses. In the heart of these systems lies a safety classifier, a\ncomputational model trained to discern and mitigate potentially harmful,\noffensive, or unethical outputs. However, contemporary safety classifiers,\ndespite their potential, often fail when exposed to inputs infused with\nadversarial noise. In response, our study introduces the Adversarial Prompt\nShield (APS), a lightweight model that excels in detection accuracy and\ndemonstrates resilience against adversarial prompts. Additionally, we propose\nnovel strategies for autonomously generating adversarial training datasets,\nnamed Bot Adversarial Noisy Dialogue (BAND) datasets. These datasets are\ndesigned to fortify the safety classifier's robustness, and we investigate the\nconsequences of incorporating adversarial examples into the training process.\nThrough evaluations involving Large Language Models, we demonstrate that our\nclassifier has the potential to decrease the attack success rate resulting from\nadversarial attacks by up to 60%. This advancement paves the way for the next\ngeneration of more reliable and resilient conversational agents.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 31 Oct 2023 22:22:10 GMT"
      }
    ],
    "update_date": "2023-11-02",
    "authors_parsed": [
      [
        "Kim",
        "Jinhwa",
        ""
      ],
      [
        "Derakhshan",
        "Ali",
        ""
      ],
      [
        "Harris",
        "Ian G.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.00172",
    "publish_date": "2023-10-31"
  },
  {
    "id": "2311.01011",
    "submitter": "Sam Toyer",
    "authors": "Sam Toyer, Olivia Watkins, Ethan Adrian Mendes, Justin Svegliato, Luke\n  Bailey, Tiffany Wang, Isaac Ong, Karim Elmaaroufi, Pieter Abbeel, Trevor\n  Darrell, Alan Ritter, Stuart Russell",
    "title": "Tensor Trust: Interpretable Prompt Injection Attacks from an Online Game",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  While Large Language Models (LLMs) are increasingly being used in real-world\napplications, they remain vulnerable to prompt injection attacks: malicious\nthird party prompts that subvert the intent of the system designer. To help\nresearchers study this problem, we present a dataset of over 126,000 prompt\ninjection attacks and 46,000 prompt-based \"defenses\" against prompt injection,\nall created by players of an online game called Tensor Trust. To the best of\nour knowledge, this is currently the largest dataset of human-generated\nadversarial examples for instruction-following LLMs. The attacks in our dataset\nhave a lot of easily interpretable stucture, and shed light on the weaknesses\nof LLMs. We also use the dataset to create a benchmark for resistance to two\ntypes of prompt injection, which we refer to as prompt extraction and prompt\nhijacking. Our benchmark results show that many models are vulnerable to the\nattack strategies in the Tensor Trust dataset. Furthermore, we show that some\nattack strategies from the dataset generalize to deployed LLM-based\napplications, even though they have a very different set of constraints to the\ngame. We release all data and source code at https://tensortrust.ai/paper\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 2 Nov 2023 06:13:36 GMT"
      }
    ],
    "update_date": "2023-11-03",
    "authors_parsed": [
      [
        "Toyer",
        "Sam",
        ""
      ],
      [
        "Watkins",
        "Olivia",
        ""
      ],
      [
        "Mendes",
        "Ethan Adrian",
        ""
      ],
      [
        "Svegliato",
        "Justin",
        ""
      ],
      [
        "Bailey",
        "Luke",
        ""
      ],
      [
        "Wang",
        "Tiffany",
        ""
      ],
      [
        "Ong",
        "Isaac",
        ""
      ],
      [
        "Elmaaroufi",
        "Karim",
        ""
      ],
      [
        "Abbeel",
        "Pieter",
        ""
      ],
      [
        "Darrell",
        "Trevor",
        ""
      ],
      [
        "Ritter",
        "Alan",
        ""
      ],
      [
        "Russell",
        "Stuart",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.01011",
    "publish_date": "2023-11-02"
  },
  {
    "id": "2311.01478",
    "submitter": "Aakriti Shah",
    "authors": "Aakriti Shah",
    "title": "Adversary ML Resilience in Autonomous Driving Through Human Centered\n  Perception Mechanisms",
    "comments": "15 pages, 17 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Physical adversarial attacks on road signs are continuously exploiting\nvulnerabilities in modern day autonomous vehicles (AVs) and impeding their\nability to correctly classify what type of road sign they encounter. Current\nmodels cannot generalize input data well, resulting in overfitting or\nunderfitting. In overfitting, the model memorizes the input data but cannot\ngeneralize to new scenarios. In underfitting, the model does not learn enough\nof the input data to accurately classify these road signs. This paper explores\nthe resilience of autonomous driving systems against three main physical\nadversarial attacks (tape, graffiti, illumination), specifically targeting\nobject classifiers. Several machine learning models were developed and\nevaluated on two distinct datasets: road signs (stop signs, speed limit signs,\ntraffic lights, and pedestrian crosswalk signs) and geometric shapes (octagons,\ncircles, squares, and triangles). The study compared algorithm performance\nunder different conditions, including clean and adversarial training and\ntesting on these datasets. To build robustness against attacks, defense\ntechniques like adversarial training and transfer learning were implemented.\nResults demonstrated transfer learning models played a crucial role in\nperformance by allowing knowledge gained from shape training to improve\ngeneralizability of road sign classification, despite the datasets being\ncompletely different. The paper suggests future research directions, including\nhuman-in-the-loop validation, security analysis, real-world testing, and\nexplainable AI for transparency. This study aims to contribute to improving\nsecurity and robustness of object classifiers in autonomous vehicles and\nmitigating adversarial example impacts on driving systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 2 Nov 2023 04:11:45 GMT"
      }
    ],
    "update_date": "2023-11-06",
    "authors_parsed": [
      [
        "Shah",
        "Aakriti",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.01478",
    "publish_date": "2023-11-02"
  },
  {
    "id": "2311.02147",
    "submitter": "Rapha\\\"el Milli\\`ere",
    "authors": "Rapha\\\"el Milli\\`ere",
    "title": "The Alignment Problem in Context",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  A core challenge in the development of increasingly capable AI systems is to\nmake them safe and reliable by ensuring their behaviour is consistent with\nhuman values. This challenge, known as the alignment problem, does not merely\napply to hypothetical future AI systems that may pose catastrophic risks; it\nalready applies to current systems, such as large language models, whose\npotential for harm is rapidly increasing. In this paper, I assess whether we\nare on track to solve the alignment problem for large language models, and what\nthat means for the safety of future AI systems. I argue that existing\nstrategies for alignment are insufficient, because large language models remain\nvulnerable to adversarial attacks that can reliably elicit unsafe behaviour. I\noffer an explanation of this lingering vulnerability on which it is not simply\na contingent limitation of current language models, but has deep technical ties\nto a crucial aspect of what makes these models useful and versatile in the\nfirst place -- namely, their remarkable aptitude to learn \"in context\" directly\nfrom user instructions. It follows that the alignment problem is not only\nunsolved for current AI systems, but may be intrinsically difficult to solve\nwithout severely undermining their capabilities. Furthermore, this assessment\nraises concerns about the prospect of ensuring the safety of future and more\ncapable AI systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 3 Nov 2023 17:57:55 GMT"
      }
    ],
    "update_date": "2023-11-07",
    "authors_parsed": [
      [
        "Milli\u00e8re",
        "Rapha\u00ebl",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.02147",
    "publish_date": "2023-11-03"
  },
  {
    "id": "2311.03191",
    "submitter": "Xuan Li",
    "authors": "Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo\n  Han",
    "title": "DeepInception: Hypnotize Large Language Model to Be Jailbreaker",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Despite remarkable success in various applications, large language models\n(LLMs) are vulnerable to adversarial jailbreaks that make the safety guardrails\nvoid. However, previous studies for jailbreaks usually resort to brute-force\noptimization or extrapolations of a high computation cost, which might not be\npractical or effective. In this paper, inspired by the Milgram experiment that\nindividuals can harm another person if they are told to do so by an\nauthoritative figure, we disclose a lightweight method, termed as\nDeepInception, which can easily hypnotize LLM to be a jailbreaker and unlock\nits misusing risks. Specifically, DeepInception leverages the personification\nability of LLM to construct a novel nested scene to behave, which realizes an\nadaptive way to escape the usage control in a normal scenario and provides the\npossibility for further direct jailbreaks. Empirically, we conduct\ncomprehensive experiments to show its efficacy. Our DeepInception can achieve\ncompetitive jailbreak success rates with previous counterparts and realize a\ncontinuous jailbreak in subsequent interactions, which reveals the critical\nweakness of self-losing on both open/closed-source LLMs like Falcon, Vicuna,\nLlama-2, and GPT-3.5/4/4V. Our investigation appeals that people should pay\nmore attention to the safety aspects of LLMs and a stronger defense against\ntheir misuse risks. The code is publicly available at:\nhttps://github.com/tmlr-group/DeepInception.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Nov 2023 15:29:30 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 5 Dec 2023 07:35:24 GMT"
      }
    ],
    "update_date": "2023-12-06",
    "authors_parsed": [
      [
        "Li",
        "Xuan",
        ""
      ],
      [
        "Zhou",
        "Zhanke",
        ""
      ],
      [
        "Zhu",
        "Jianing",
        ""
      ],
      [
        "Yao",
        "Jiangchao",
        ""
      ],
      [
        "Liu",
        "Tongliang",
        ""
      ],
      [
        "Han",
        "Bo",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.03191",
    "publish_date": "2023-12-05"
  },
  {
    "id": "2311.03348",
    "submitter": "Javier Rando",
    "authors": "Rusheb Shah, Quentin Feuillade--Montixi, Soroush Pour, Arush Tagade,\n  Stephen Casper, Javier Rando",
    "title": "Scalable and Transferable Black-Box Jailbreaks for Language Models via\n  Persona Modulation",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Despite efforts to align large language models to produce harmless responses,\nthey are still vulnerable to jailbreak prompts that elicit unrestricted\nbehaviour. In this work, we investigate persona modulation as a black-box\njailbreaking method to steer a target model to take on personalities that are\nwilling to comply with harmful instructions. Rather than manually crafting\nprompts for each persona, we automate the generation of jailbreaks using a\nlanguage model assistant. We demonstrate a range of harmful completions made\npossible by persona modulation, including detailed instructions for\nsynthesising methamphetamine, building a bomb, and laundering money. These\nautomated attacks achieve a harmful completion rate of 42.5% in GPT-4, which is\n185 times larger than before modulation (0.23%). These prompts also transfer to\nClaude 2 and Vicuna with harmful completion rates of 61.0% and 35.9%,\nrespectively. Our work reveals yet another vulnerability in commercial large\nlanguage models and highlights the need for more comprehensive safeguards.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Nov 2023 18:55:18 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 24 Nov 2023 12:50:31 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Shah",
        "Rusheb",
        ""
      ],
      [
        "Feuillade--Montixi",
        "Quentin",
        ""
      ],
      [
        "Pour",
        "Soroush",
        ""
      ],
      [
        "Tagade",
        "Arush",
        ""
      ],
      [
        "Casper",
        "Stephen",
        ""
      ],
      [
        "Rando",
        "Javier",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.03348",
    "publish_date": "2023-11-06"
  },
  {
    "id": "2311.03566",
    "submitter": "Yuanchen Bai",
    "authors": "Yuanchen Bai, Raoyi Huang, Vijay Viswanathan, Tzu-Sheng Kuo,\n  Tongshuang Wu",
    "title": "Measuring Adversarial Datasets",
    "comments": "ART of Safety workshop (AACL 2023)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.HC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In the era of widespread public use of AI systems across various domains,\nensuring adversarial robustness has become increasingly vital to maintain\nsafety and prevent undesirable errors. Researchers have curated various\nadversarial datasets (through perturbations) for capturing model deficiencies\nthat cannot be revealed in standard benchmark datasets. However, little is\nknown about how these adversarial examples differ from the original data\npoints, and there is still no methodology to measure the intended and\nunintended consequences of those adversarial transformations. In this research,\nwe conducted a systematic survey of existing quantifiable metrics that describe\ntext instances in NLP tasks, among dimensions of difficulty, diversity, and\ndisagreement. We selected several current adversarial effect datasets and\ncompared the distributions between the original and their adversarial\ncounterparts. The results provide valuable insights into what makes these\ndatasets more challenging from a metrics perspective and whether they align\nwith underlying assumptions.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 6 Nov 2023 22:08:16 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Bai",
        "Yuanchen",
        ""
      ],
      [
        "Huang",
        "Raoyi",
        ""
      ],
      [
        "Viswanathan",
        "Vijay",
        ""
      ],
      [
        "Kuo",
        "Tzu-Sheng",
        ""
      ],
      [
        "Wu",
        "Tongshuang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.03566",
    "publish_date": "2023-11-06"
  },
  {
    "id": "2311.03782",
    "submitter": "Wasim Ahmad",
    "authors": "Wasim Ahmad, Yan-Tsung Peng, Yuan-Hao Chang, Gaddisa Olani Ganfure,\n  Sarwar Khan, Sahibzada Adil Shahzad",
    "title": "CapST: An Enhanced and Lightweight Model Attribution Approach for\n  Synthetic Videos",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deepfake videos, generated through AI faceswapping techniques, have garnered\nconsiderable attention due to their potential for powerful impersonation\nattacks. While existing research primarily focuses on binary classification to\ndiscern between real and fake videos, however determining the specific\ngeneration model for a fake video is crucial for forensic investigation.\nAddressing this gap, this paper investigates the model attribution problem of\nDeepfake videos from a recently proposed dataset, Deepfakes from Different\nModels (DFDM), derived from various Autoencoder models. The dataset comprises\n6,450 Deepfake videos generated by five distinct models with variations in\nencoder, decoder, intermediate layer, input resolution, and compression ratio.\nThis study formulates Deepfakes model attribution as a multiclass\nclassification task, proposing a segment of VGG19 as a feature extraction\nbackbone, known for its effectiveness in imagerelated tasks, while integrated a\nCapsule Network with a Spatio-Temporal attention mechanism. The Capsule module\ncaptures intricate hierarchies among features for robust identification of\ndeepfake attributes. Additionally, the video-level fusion technique leverages\ntemporal attention mechanisms to handle concatenated feature vectors,\ncapitalizing on inherent temporal dependencies in deepfake videos. By\naggregating insights across frames, our model gains a comprehensive\nunderstanding of video content, resulting in more precise predictions.\nExperimental results on the deepfake benchmark dataset (DFDM) demonstrate the\nefficacy of our proposed method, achieving up to a 4% improvement in accurately\ncategorizing deepfake videos compared to baseline models while demanding fewer\ncomputational resources.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 7 Nov 2023 08:05:09 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 28 Nov 2023 09:23:30 GMT"
      }
    ],
    "update_date": "2023-11-29",
    "authors_parsed": [
      [
        "Ahmad",
        "Wasim",
        ""
      ],
      [
        "Peng",
        "Yan-Tsung",
        ""
      ],
      [
        "Chang",
        "Yuan-Hao",
        ""
      ],
      [
        "Ganfure",
        "Gaddisa Olani",
        ""
      ],
      [
        "Khan",
        "Sarwar",
        ""
      ],
      [
        "Shahzad",
        "Sahibzada Adil",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.03782",
    "publish_date": "2023-11-07"
  },
  {
    "id": "2311.04124",
    "submitter": "George Kour",
    "authors": "George Kour, Marcel Zalmanovici, Naama Zwerdling, Esther Goldbraich,\n  Ora Nova Fandina, Ateret Anaby-Tavor, Orna Raz and Eitan Farchi",
    "title": "Unveiling Safety Vulnerabilities of Large Language Models",
    "comments": "To be published in GEM workshop. Conference on Empirical Methods in\n  Natural Language Processing (EMNLP). 2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As large language models become more prevalent, their possible harmful or\ninappropriate responses are a cause for concern. This paper introduces a unique\ndataset containing adversarial examples in the form of questions, which we call\nAttaQ, designed to provoke such harmful or inappropriate responses. We assess\nthe efficacy of our dataset by analyzing the vulnerabilities of various models\nwhen subjected to it. Additionally, we introduce a novel automatic approach for\nidentifying and naming vulnerable semantic regions - input semantic areas for\nwhich the model is likely to produce harmful outputs. This is achieved through\nthe application of specialized clustering techniques that consider both the\nsemantic similarity of the input attacks and the harmfulness of the model's\nresponses. Automatically identifying vulnerable semantic regions enhances the\nevaluation of model weaknesses, facilitating targeted improvements to its\nsafety mechanisms and overall reliability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 7 Nov 2023 16:50:33 GMT"
      }
    ],
    "update_date": "2023-11-08",
    "authors_parsed": [
      [
        "Kour",
        "George",
        ""
      ],
      [
        "Zalmanovici",
        "Marcel",
        ""
      ],
      [
        "Zwerdling",
        "Naama",
        ""
      ],
      [
        "Goldbraich",
        "Esther",
        ""
      ],
      [
        "Fandina",
        "Ora Nova",
        ""
      ],
      [
        "Anaby-Tavor",
        "Ateret",
        ""
      ],
      [
        "Raz",
        "Orna",
        ""
      ],
      [
        "Farchi",
        "Eitan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.04124",
    "publish_date": "2023-11-07"
  },
  {
    "id": "2311.04813",
    "submitter": "Hubert Baniecki",
    "authors": "Hubert Baniecki, Maciej Chrabaszcz, Andreas Holzinger, Bastian\n  Pfeifer, Anna Saranti, Przemyslaw Biecek",
    "title": "Be Careful When Evaluating Explanations Regarding Ground Truth",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Evaluating explanations of image classifiers regarding ground truth, e.g.\nsegmentation masks defined by human perception, primarily evaluates the quality\nof the models under consideration rather than the explanation methods\nthemselves. Driven by this observation, we propose a framework for\n$\\textit{jointly}$ evaluating the robustness of safety-critical systems that\n$\\textit{combine}$ a deep neural network with an explanation method. These are\nincreasingly used in real-world applications like medical image analysis or\nrobotics. We introduce a fine-tuning procedure to (mis)align\nmodel$\\unicode{x2013}$explanation pipelines with ground truth and use it to\nquantify the potential discrepancy between worst and best-case scenarios of\nhuman alignment. Experiments across various model architectures and post-hoc\nlocal interpretation methods provide insights into the robustness of vision\ntransformers and the overall vulnerability of such AI systems to potential\nadversarial attacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Nov 2023 16:39:13 GMT"
      }
    ],
    "update_date": "2023-11-09",
    "authors_parsed": [
      [
        "Baniecki",
        "Hubert",
        ""
      ],
      [
        "Chrabaszcz",
        "Maciej",
        ""
      ],
      [
        "Holzinger",
        "Andreas",
        ""
      ],
      [
        "Pfeifer",
        "Bastian",
        ""
      ],
      [
        "Saranti",
        "Anna",
        ""
      ],
      [
        "Biecek",
        "Przemyslaw",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.04813",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2311.04861",
    "submitter": "Kim Laine",
    "authors": "F. Bet\\\"ul Durak and Kim Laine and Simon Langowski and Radames Cruz\n  Moreno and Robert Sim and Shrey Jain",
    "title": "Sandi: A System for Accountability and Applications in Direct\n  Communication (Extended Abstract)",
    "comments": "18 pages, extended abstract",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Reputation systems guide our decision making both in life and work: which\nrestaurant to eat at, which vendor to buy from, which software dependencies to\nuse, and who or what to trust. These systems are often based on old ideas and\nare failing in the face of modern threats. Fraudsters have found ways to\nmanipulate them, undermining their integrity and utility. Generative AI adds to\nthe problem by enabling the creation of real-looking fake narratives at scale,\ncreating a false sense of consensus. Meanwhile, the need for reliable\nreputation concepts is more important than ever, as wrong decisions lead to\nincreasingly severe outcomes: wasted time, poor service, and a feeling of\ninjustice at best, fraud, identity theft, and ransomware at worst.\n  In this extended abstract we introduce Sandi, a new kind of reputation system\nwith a single well-defined purpose: to create trust through accountability in\none-to-one transactions. Examples of such transactions include sending an email\nor making a purchase online. Sandi has strong security and privacy properties\nthat make it suitable for use also in sensitive contexts. Furthermore, Sandi\ncan guarantee reputation integrity and transparency for its registered users.\n  As a primary application, we envision how Sandi could counter fraud and abuse\nin direct communication. Concretely, message senders request a cryptographic\ntag from Sandi that they send along with their message. If the receiver finds\nthe message inappropriate, they can report the sender using this tag. Notably,\nonly senders need registered accounts and do not need to manage long-term keys.\nThe design of Sandi ensures compatibility with any communication system that\nallows for small binary data transmission.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Nov 2023 17:56:56 GMT"
      }
    ],
    "update_date": "2023-11-09",
    "authors_parsed": [
      [
        "Durak",
        "F. Bet\u00fcl",
        ""
      ],
      [
        "Laine",
        "Kim",
        ""
      ],
      [
        "Langowski",
        "Simon",
        ""
      ],
      [
        "Moreno",
        "Radames Cruz",
        ""
      ],
      [
        "Sim",
        "Robert",
        ""
      ],
      [
        "Jain",
        "Shrey",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.04861",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2311.04913",
    "submitter": "Hayden Wimmer",
    "authors": "Suhaima Jamal and Hayden Wimmer",
    "title": "An Improved Transformer-based Model for Detecting Phishing, Spam, and\n  Ham: A Large Language Model Approach",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Phishing and spam detection is long standing challenge that has been the\nsubject of much academic research. Large Language Models (LLM) have vast\npotential to transform society and provide new and innovative approaches to\nsolve well-established challenges. Phishing and spam have caused financial\nhardships and lost time and resources to email users all over the world and\nfrequently serve as an entry point for ransomware threat actors. While\ndetection approaches exist, especially heuristic-based approaches, LLMs offer\nthe potential to venture into a new unexplored area for understanding and\nsolving this challenge. LLMs have rapidly altered the landscape from business,\nconsumers, and throughout academia and demonstrate transformational potential\nfor the potential of society. Based on this, applying these new and innovative\napproaches to email detection is a rational next step in academic research. In\nthis work, we present IPSDM, our model based on fine-tuning the BERT family of\nmodels to specifically detect phishing and spam email. We demonstrate our\nfine-tuned version, IPSDM, is able to better classify emails in both unbalanced\nand balanced datasets. This work serves as an important first step towards\nemploying LLMs to improve the security of our information systems.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 1 Nov 2023 18:41:50 GMT"
      },
      {
        "version": "v2",
        "created": "Sun, 12 Nov 2023 16:32:16 GMT"
      }
    ],
    "update_date": "2023-11-14",
    "authors_parsed": [
      [
        "Jamal",
        "Suhaima",
        ""
      ],
      [
        "Wimmer",
        "Hayden",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.04913",
    "publish_date": "2023-11-01"
  },
  {
    "id": "2311.04939",
    "submitter": "Jiaqi Li",
    "authors": "Jiaqi Li, Mengmeng Wang, Zilong Zheng, Muhan Zhang",
    "title": "LooGLE: Can Long-Context Language Models Understand Long Contexts?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large language models (LLMs), despite their impressive performance in various\nlanguage tasks, are typically limited to processing texts within context-window\nsize. This limitation has spurred significant research efforts to enhance LLMs'\nlong-context understanding with high-quality long-sequence benchmarks. However,\nprior datasets in this regard suffer from shortcomings, such as short context\nlength compared to the context window of modern LLMs; outdated documents that\nhave data leakage problems; and an emphasis on short dependency tasks rather\nthan long dependency tasks. In this paper, we present LooGLE, a Long Context\nGeneric Language Evaluation benchmark for LLMs' long context understanding.\nLooGLE features relatively new documents post-2022, with over 24,000 tokens per\ndocument and 6,000 newly generated questions spanning diverse domains. Human\nannotators meticulously crafted more than 1,100 high-quality question-answer\npairs to meet the long dependency requirements. These pairs underwent thorough\ncross-validation, yielding the most precise assessment of LLMs' long dependency\ncapabilities. The evaluation of eight state-of-the-art LLMs on LooGLE revealed\nkey findings: (i) commercial models outperformed open-sourced models; (ii) LLMs\nexcelled in short dependency tasks like short question-answering and cloze\ntasks but struggled with more intricate long dependency tasks; (iii) in-context\nlearning and chaining thoughts offered only marginal improvements; (iv)\nretrieval-based techniques demonstrated substantial benefits for short\nquestion-answering, while strategies for extending context window length had\nlimited impact on long context understanding. As such, LooGLE not only provides\na systematic and comprehensive evaluation schema on long-context LLMs, but also\nsheds light on future development of enhanced models towards \"true long-context\nunderstanding\".\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Nov 2023 01:45:37 GMT"
      }
    ],
    "update_date": "2023-11-10",
    "authors_parsed": [
      [
        "Li",
        "Jiaqi",
        ""
      ],
      [
        "Wang",
        "Mengmeng",
        ""
      ],
      [
        "Zheng",
        "Zilong",
        ""
      ],
      [
        "Zhang",
        "Muhan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.04939",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2311.05019",
    "submitter": "Alessandro Pegoraro",
    "authors": "Kavita Kumari and Alessandro Pegoraro and Hossein Fereidooni and\n  Ahmad-Reza Sadeghi",
    "title": "DEMASQ: Unmasking the ChatGPT Wordsmith",
    "comments": "To appear in the Network and Distributed System Security (NDSS)\n  Symposium 2024. 15 pages, 3 figures, 6 tables, 3 algorithms, 6 equations",
    "journal-ref": null,
    "doi": "10.14722/ndss.2024.231190",
    "report-no": null,
    "categories": "cs.CR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The potential misuse of ChatGPT and other Large Language Models (LLMs) has\nraised concerns regarding the dissemination of false information, plagiarism,\nacademic dishonesty, and fraudulent activities. Consequently, distinguishing\nbetween AI-generated and human-generated content has emerged as an intriguing\nresearch topic. However, current text detection methods lack precision and are\noften restricted to specific tasks or domains, making them inadequate for\nidentifying content generated by ChatGPT. In this paper, we propose an\neffective ChatGPT detector named DEMASQ, which accurately identifies\nChatGPT-generated content. Our method addresses two critical factors: (i) the\ndistinct biases in text composition observed in human- and machine-generated\ncontent and (ii) the alterations made by humans to evade previous detection\nmethods. DEMASQ is an energy-based detection model that incorporates novel\naspects, such as (i) optimization inspired by the Doppler effect to capture the\ninterdependence between input text embeddings and output labels, and (ii) the\nuse of explainable AI techniques to generate diverse perturbations. To evaluate\nour detector, we create a benchmark dataset comprising a mixture of prompts\nfrom both ChatGPT and humans, encompassing domains such as medical, open Q&A,\nfinance, wiki, and Reddit. Our evaluation demonstrates that DEMASQ achieves\nhigh accuracy in identifying content generated by ChatGPT.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 8 Nov 2023 21:13:05 GMT"
      }
    ],
    "update_date": "2023-11-10",
    "authors_parsed": [
      [
        "Kumari",
        "Kavita",
        ""
      ],
      [
        "Pegoraro",
        "Alessandro",
        ""
      ],
      [
        "Fereidooni",
        "Hossein",
        ""
      ],
      [
        "Sadeghi",
        "Ahmad-Reza",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.05019",
    "publish_date": "2023-11-08"
  },
  {
    "id": "2311.05608",
    "submitter": "Yichen Gong",
    "authors": "Yichen Gong and Delong Ran and Jinyuan Liu and Conglei Wang and\n  Tianshuo Cong and Anyu Wang and Sisi Duan and Xiaoyun Wang",
    "title": "FigStep: Jailbreaking Large Vision-language Models via Typographic\n  Visual Prompts",
    "comments": "Technical Report",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Ensuring the safety of artificial intelligence-generated content (AIGC) is a\nlongstanding topic in the artificial intelligence (AI) community, and the\nsafety concerns associated with Large Language Models (LLMs) have been widely\ninvestigated. Recently, large vision-language models (VLMs) represent an\nunprecedented revolution, as they are built upon LLMs but can incorporate\nadditional modalities (e.g., images). However, the safety of VLMs lacks\nsystematic evaluation, and there may be an overconfidence in the safety\nguarantees provided by their underlying LLMs. In this paper, to demonstrate\nthat introducing additional modality modules leads to unforeseen AI safety\nissues, we propose FigStep, a straightforward yet effective jailbreaking\nalgorithm against VLMs. Instead of feeding textual harmful instructions\ndirectly, FigStep converts the harmful content into images through typography\nto bypass the safety alignment within the textual module of the VLMs, inducing\nVLMs to output unsafe responses that violate common AI safety policies. In our\nevaluation, we manually review 46,500 model responses generated by 3 families\nof the promising open-source VLMs, i.e., LLaVA, MiniGPT4, and CogVLM (a total\nof 6 VLMs). The experimental results show that FigStep can achieve an average\nattack success rate of 82.50% on 500 harmful queries in 10 topics. Moreover, we\ndemonstrate that the methodology of FigStep can even jailbreak GPT-4V, which\nalready leverages an OCR detector to filter harmful queries. Above all, our\nwork reveals that VLMs are vulnerable to jailbreaking attacks, which highlights\nthe necessity of novel safety alignments between visual and textual modalities.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 9 Nov 2023 18:59:11 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 13 Dec 2023 17:54:16 GMT"
      }
    ],
    "update_date": "2023-12-14",
    "authors_parsed": [
      [
        "Gong",
        "Yichen",
        ""
      ],
      [
        "Ran",
        "Delong",
        ""
      ],
      [
        "Liu",
        "Jinyuan",
        ""
      ],
      [
        "Wang",
        "Conglei",
        ""
      ],
      [
        "Cong",
        "Tianshuo",
        ""
      ],
      [
        "Wang",
        "Anyu",
        ""
      ],
      [
        "Duan",
        "Sisi",
        ""
      ],
      [
        "Wang",
        "Xiaoyun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.05608",
    "publish_date": "2023-12-13"
  },
  {
    "id": "2311.05863",
    "submitter": "Yuanmin Tang",
    "authors": "Yuanmin Tang, Jing Yu, Keke Gai, Xiangyan Qu, Yue Hu, Gang Xiong, Qi\n  Wu",
    "title": "Watermarking Vision-Language Pre-trained Models for Multi-modal\n  Embedding as a Service",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advances in vision-language pre-trained models (VLPs) have\nsignificantly increased visual understanding and cross-modal analysis\ncapabilities. Companies have emerged to provide multi-modal Embedding as a\nService (EaaS) based on VLPs (e.g., CLIP-based VLPs), which cost a large amount\nof training data and resources for high-performance service. However, existing\nstudies indicate that EaaS is vulnerable to model extraction attacks that\ninduce great loss for the owners of VLPs. Protecting the intellectual property\nand commercial ownership of VLPs is increasingly crucial yet challenging. A\nmajor solution of watermarking model for EaaS implants a backdoor in the model\nby inserting verifiable trigger embeddings into texts, but it is only\napplicable for large language models and is unrealistic due to data and model\nprivacy. In this paper, we propose a safe and robust backdoor-based embedding\nwatermarking method for VLPs called VLPMarker. VLPMarker utilizes embedding\northogonal transformation to effectively inject triggers into the VLPs without\ninterfering with the model parameters, which achieves high-quality copyright\nverification and minimal impact on model performance. To enhance the watermark\nrobustness, we further propose a collaborative copyright verification strategy\nbased on both backdoor trigger and embedding distribution, enhancing resilience\nagainst various attacks. We increase the watermark practicality via an\nout-of-distribution trigger selection approach, removing access to the model\ntraining data and thus making it possible for many real-world scenarios. Our\nextensive experiments on various datasets indicate that the proposed\nwatermarking approach is effective and safe for verifying the copyright of VLPs\nfor multi-modal EaaS and robust against model extraction attacks. Our code is\navailable at https://github.com/Pter61/vlpmarker.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 10 Nov 2023 04:27:27 GMT"
      }
    ],
    "update_date": "2023-11-13",
    "authors_parsed": [
      [
        "Tang",
        "Yuanmin",
        ""
      ],
      [
        "Yu",
        "Jing",
        ""
      ],
      [
        "Gai",
        "Keke",
        ""
      ],
      [
        "Qu",
        "Xiangyan",
        ""
      ],
      [
        "Hu",
        "Yue",
        ""
      ],
      [
        "Xiong",
        "Gang",
        ""
      ],
      [
        "Wu",
        "Qi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.05863",
    "publish_date": "2023-11-10"
  },
  {
    "id": "2311.05915",
    "submitter": "Yixu Wang",
    "authors": "Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei\n  Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang",
    "title": "Fake Alignment: Are LLMs Really Aligned Well?",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The growing awareness of safety concerns in large language models (LLMs) has\nsparked considerable interest in the evaluation of safety within current\nresearch endeavors. This study investigates an interesting issue pertaining to\nthe evaluation of LLMs, namely the substantial discrepancy in performance\nbetween multiple-choice questions and open-ended questions. Inspired by\nresearch on jailbreak attack patterns, we argue this is caused by mismatched\ngeneralization. That is, the LLM does not have a comprehensive understanding of\nthe complex concept of safety. Instead, it only remembers what to answer for\nopen-ended safety questions, which makes it unable to solve other forms of\nsafety tests. We refer to this phenomenon as fake alignment and construct a\ncomparative benchmark to empirically verify its existence in LLMs. Such fake\nalignment renders previous evaluation protocols unreliable. To address this, we\nintroduce the Fake alIgNment Evaluation (FINE) framework and two novel\nmetrics--Consistency Score (CS) and Consistent Safety Score (CSS), which\njointly assess two complementary forms of evaluation to quantify fake alignment\nand obtain corrected performance estimates. Applying FINE to 14 widely-used\nLLMs reveals several models with purported safety are poorly aligned in\npractice. Our work highlights potential limitations in prevailing alignment\nmethodologies.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 10 Nov 2023 08:01:23 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 14 Nov 2023 07:49:56 GMT"
      }
    ],
    "update_date": "2023-11-15",
    "authors_parsed": [
      [
        "Wang",
        "Yixu",
        ""
      ],
      [
        "Teng",
        "Yan",
        ""
      ],
      [
        "Huang",
        "Kexin",
        ""
      ],
      [
        "Lyu",
        "Chengqi",
        ""
      ],
      [
        "Zhang",
        "Songyang",
        ""
      ],
      [
        "Zhang",
        "Wenwei",
        ""
      ],
      [
        "Ma",
        "Xingjun",
        ""
      ],
      [
        "Jiang",
        "Yu-Gang",
        ""
      ],
      [
        "Qiao",
        "Yu",
        ""
      ],
      [
        "Wang",
        "Yingchun",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.05915",
    "publish_date": "2023-11-14"
  },
  {
    "id": "2311.06996",
    "submitter": "Zirui Gong",
    "authors": "Zirui Gong, Liyue Shen, Yanjun Zhang, Leo Yu Zhang, Jingwei Wang,\n  Guangdong Bai, and Yong Xiang",
    "title": "AGRAMPLIFIER: Defending Federated Learning Against Poisoning Attacks\n  Through Local Update Amplification",
    "comments": "Accepted by IEEE TIFS, this is the complete version",
    "journal-ref": null,
    "doi": "10.1109/TIFS.2023.3333555",
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The collaborative nature of federated learning (FL) poses a major threat in\nthe form of manipulation of local training data and local updates, known as the\nByzantine poisoning attack. To address this issue, many Byzantine-robust\naggregation rules (AGRs) have been proposed to filter out or moderate\nsuspicious local updates uploaded by Byzantine participants.\n  This paper introduces a novel approach called AGRAMPLIFIER, aiming to\nsimultaneously improve the robustness, fidelity, and efficiency of the existing\nAGRs. The core idea of AGRAMPLIFIER is to amplify the \"morality\" of local\nupdates by identifying the most repressive features of each gradient update,\nwhich provides a clearer distinction between malicious and benign updates,\nconsequently improving the detection effect. To achieve this objective, two\napproaches, namely AGRMP and AGRXAI, are proposed. AGRMP organizes local\nupdates into patches and extracts the largest value from each patch, while\nAGRXAI leverages explainable AI methods to extract the gradient of the most\nactivated features. By equipping AGRAMPLIFIER with the existing\nByzantine-robust mechanisms, we successfully enhance the model's robustness,\nmaintaining its fidelity and improving overall efficiency.\n  AGRAMPLIFIER is universally compatible with the existing Byzantine-robust\nmechanisms. The paper demonstrates its effectiveness by integrating it with all\nmainstream AGR mechanisms. Extensive evaluations conducted on seven datasets\nfrom diverse domains against seven representative poisoning attacks\nconsistently show enhancements in robustness, fidelity, and efficiency, with\naverage gains of 40.08%, 39.18%, and 10.68%, respectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 13 Nov 2023 00:34:45 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 23 Nov 2023 11:30:13 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Gong",
        "Zirui",
        ""
      ],
      [
        "Shen",
        "Liyue",
        ""
      ],
      [
        "Zhang",
        "Yanjun",
        ""
      ],
      [
        "Zhang",
        "Leo Yu",
        ""
      ],
      [
        "Wang",
        "Jingwei",
        ""
      ],
      [
        "Bai",
        "Guangdong",
        ""
      ],
      [
        "Xiang",
        "Yong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.06996",
    "publish_date": "2023-11-23"
  },
  {
    "id": "2311.08268",
    "submitter": "Peng Ding",
    "authors": "Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen,\n  Shujian Huang",
    "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can\n  Fool Large Language Models Easily",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to\nprovide useful and safe responses. However, adversarial prompts known as\n'jailbreaks' can circumvent safeguards, leading LLMs to generate harmful\ncontent. Exploring jailbreak prompts can help to better reveal the weaknesses\nof LLMs and further steer us to secure them. Unfortunately, existing jailbreak\nmethods either suffer from intricate manual design or require optimization on\nanother white-box model, compromising generalization or jailbreak efficiency.\nIn this paper, we generalize jailbreak prompt attacks into two aspects: (1)\nPrompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM,\nan automatic framework that leverages LLMs themselves to generate effective\njailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly\nimproves the attack success rate while greatly reducing the time cost compared\nto existing baselines. Our study also reveals the inadequacy of current defense\nmethods in safeguarding LLMs. Finally, we offer detailed analysis and\ndiscussion from the perspective of prompt execution priority on the failure of\nLLMs' defense. We hope that our research can catalyze both the academic\ncommunity and LLMs vendors towards the provision of safer and more regulated\nLarge Language Models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Nov 2023 16:02:16 GMT"
      }
    ],
    "update_date": "2023-11-15",
    "authors_parsed": [
      [
        "Ding",
        "Peng",
        ""
      ],
      [
        "Kuang",
        "Jun",
        ""
      ],
      [
        "Ma",
        "Dan",
        ""
      ],
      [
        "Cao",
        "Xuezhi",
        ""
      ],
      [
        "Xian",
        "Yunsen",
        ""
      ],
      [
        "Chen",
        "Jiajun",
        ""
      ],
      [
        "Huang",
        "Shujian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.08268",
    "publish_date": "2023-11-14"
  },
  {
    "id": "2311.08487",
    "submitter": "Zi Yin",
    "authors": "Zi Yin, Wei Ding, Jia Liu",
    "title": "Alignment is not sufficient to prevent large language models from\n  generating harmful information: A psychoanalytic perspective",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) are central to a multitude of applications but\nstruggle with significant risks, notably in generating harmful content and\nbiases. Drawing an analogy to the human psyche's conflict between evolutionary\nsurvival instincts and societal norm adherence elucidated in Freud's\npsychoanalysis theory, we argue that LLMs suffer a similar fundamental\nconflict, arising between their inherent desire for syntactic and semantic\ncontinuity, established during the pre-training phase, and the post-training\nalignment with human values. This conflict renders LLMs vulnerable to\nadversarial attacks, wherein intensifying the models' desire for continuity can\ncircumvent alignment efforts, resulting in the generation of harmful\ninformation. Through a series of experiments, we first validated the existence\nof the desire for continuity in LLMs, and further devised a straightforward yet\npowerful technique, such as incomplete sentences, negative priming, and\ncognitive dissonance scenarios, to demonstrate that even advanced LLMs struggle\nto prevent the generation of harmful information. In summary, our study\nuncovers the root of LLMs' vulnerabilities to adversarial attacks, hereby\nquestioning the efficacy of solely relying on sophisticated alignment methods,\nand further advocates for a new training idea that integrates modal concepts\nalongside traditional amodal concepts, aiming to endow LLMs with a more nuanced\nunderstanding of real-world contexts and ethical considerations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 14 Nov 2023 19:28:51 GMT"
      }
    ],
    "update_date": "2023-11-16",
    "authors_parsed": [
      [
        "Yin",
        "Zi",
        ""
      ],
      [
        "Ding",
        "Wei",
        ""
      ],
      [
        "Liu",
        "Jia",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.08487",
    "publish_date": "2023-11-14"
  },
  {
    "id": "2311.09127",
    "submitter": "Yixin Liu",
    "authors": "Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou and Lichao Sun",
    "title": "Jailbreaking GPT-4V via Self-Adversarial Attacks with System Prompts",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Existing work on jailbreak Multimodal Large Language Models (MLLMs) has\nfocused primarily on adversarial examples in model inputs, with less attention\nto vulnerabilities in model APIs. To fill the research gap, we carry out the\nfollowing work: 1) We discover a system prompt leakage vulnerability in GPT-4V.\nThrough carefully designed dialogue, we successfully steal the internal system\nprompts of GPT-4V. This finding indicates potential exploitable security risks\nin MLLMs; 2)Based on the acquired system prompts, we propose a novel MLLM\njailbreaking attack method termed SASP (Self-Adversarial Attack via System\nPrompt). By employing GPT-4 as a red teaming tool against itself, we aim to\nsearch for potential jailbreak prompts leveraging stolen system prompts.\nFurthermore, in pursuit of better performance, we also add human modification\nbased on GPT-4's analysis, which further improves the attack success rate to\n98.7\\%; 3) We evaluated the effect of modifying system prompts to defend\nagainst jailbreaking attacks. Results show that appropriately designed system\nprompts can significantly reduce jailbreak success rates. Overall, our work\nprovides new insights into enhancing MLLM security, demonstrating the important\nrole of system prompts in jailbreaking, which could be leveraged to greatly\nfacilitate jailbreak success rates while also holding the potential for\ndefending against jailbreaks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Nov 2023 17:17:39 GMT"
      }
    ],
    "update_date": "2023-11-16",
    "authors_parsed": [
      [
        "Wu",
        "Yuanwei",
        ""
      ],
      [
        "Li",
        "Xiang",
        ""
      ],
      [
        "Liu",
        "Yixin",
        ""
      ],
      [
        "Zhou",
        "Pan",
        ""
      ],
      [
        "Sun",
        "Lichao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09127",
    "publish_date": "2023-11-15"
  },
  {
    "id": "2311.09433",
    "submitter": "Haoran Wang",
    "authors": "Haoran Wang, Kai Shu",
    "title": "Backdoor Activation Attack: Attack Large Language Models using\n  Activation Steering for Safety-Alignment",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  To ensure AI safety, instruction-tuned Large Language Models (LLMs) are\nspecifically trained to ensure alignment, which refers to making models behave\nin accordance with human intentions. While these models have demonstrated\ncommendable results on various safety benchmarks, the vulnerability of their\nsafety alignment has not been extensively studied. This is particularly\ntroubling given the potential harm that LLMs can inflict. Existing attack\nmethods on LLMs often rely on poisoned training data or the injection of\nmalicious prompts. These approaches compromise the stealthiness and\ngeneralizability of the attacks, making them susceptible to detection.\nAdditionally, these models often demand substantial computational resources for\nimplementation, making them less practical for real-world applications.\nInspired by recent success in modifying model behavior through steering vectors\nwithout the need for optimization, and drawing on its effectiveness in\nred-teaming LLMs, we conducted experiments employing activation steering to\ntarget four key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness\n- across a varied set of attack settings. To establish a universal attack\nstrategy applicable to diverse target alignments without depending on manual\nanalysis, we automatically select the intervention layer based on contrastive\nlayer search. Our experiment results show that activation attacks are highly\neffective and add little or no overhead to attack efficiency. Additionally, we\ndiscuss potential countermeasures against such activation attacks. Our code and\ndata are available at https://github.com/wang2226/Backdoor-Activation-Attack\nWarning: this paper contains content that can be offensive or upsetting.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Nov 2023 23:07:40 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 24 Nov 2023 16:22:41 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Wang",
        "Haoran",
        ""
      ],
      [
        "Shu",
        "Kai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09433",
    "publish_date": "2023-11-15"
  },
  {
    "id": "2311.09535",
    "submitter": "Li Shuai",
    "authors": "Shuai Li, Kejiang Chen, Kunsheng Tang, Wen Huang, Jie Zhang, Weiming\n  Zhang, Nenghai Yu",
    "title": "FunctionMarker: Watermarking Language Datasets via Knowledge Injection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have demonstrated superior performance in\nvarious natural language processing tasks. Meanwhile, they require extensive\ntraining data, raising concerns related to dataset copyright protection.\nBackdoor-based watermarking is a viable approach to protect the copyright of\nclassification datasets. However, these methods may introduce malicious\nmisclassification behaviors into watermarked LLMs by attackers and also affect\nthe semantic information of the watermarked text. To address these issues, we\npropose FunctionMarker, a novel copyright protection method for language\ndatasets via knowledge injection. FunctionMarker enables LLMs to learn specific\nknowledge through fine-tuning on watermarked datasets, and we can extract the\nembedded watermark by obtaining the responses of LLMs to specific\nknowledge-related queries. Considering watermark capacity and stealthness, we\nselect customizable functions as specific knowledge for LLMs to learn and embed\nthe watermark into them. Moreover, FunctionMarker can embed multi-bit\nwatermarks while preserving the original semantic information, thereby\nincreasing the difficulty of adaptive attacks. We take mathematical functions\nas an instance to evaluate the effectiveness of FunctionMarker, and experiments\nshow that only 0.3% of watermarked text achieves a 90% watermark extraction\naccuracy in most cases, validating our method's effectiveness.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 03:22:53 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 17 Nov 2023 05:00:21 GMT"
      }
    ],
    "update_date": "2023-11-20",
    "authors_parsed": [
      [
        "Li",
        "Shuai",
        ""
      ],
      [
        "Chen",
        "Kejiang",
        ""
      ],
      [
        "Tang",
        "Kunsheng",
        ""
      ],
      [
        "Huang",
        "Wen",
        ""
      ],
      [
        "Zhang",
        "Jie",
        ""
      ],
      [
        "Zhang",
        "Weiming",
        ""
      ],
      [
        "Yu",
        "Nenghai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09535",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.09542",
    "submitter": "Neha Srikanth",
    "authors": "Neha Srikanth, Rupak Sarkar, Rachel Rudinger, Jordan Boyd-Graber",
    "title": "Towards Pragmatic Awareness in Question Answering: A Case Study in\n  Maternal and Infant Health",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Questions posed by information-seeking users often contain implicit false or\npotentially harmful assumptions. In a high-risk domain such as maternal and\ninfant health, a question-answering system must recognize these pragmatic\nconstraints and go beyond simply answering user questions, examining them in\ncontext to respond helpfully. To achieve this, we study pragmatic inferences\nmade when mothers ask questions about pregnancy and infant care. Some of the\ninferences in these questions evade detection by existing methods, risking the\npossibility of QA systems failing to address them which can have dangerous\nhealth and policy implications. We explore the viability of detecting\ninferences from questions using large language models and illustrate that\ninforming existing QA pipelines with pragmatic inferences produces responses\nthat can mitigate the propagation of harmful beliefs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 03:33:01 GMT"
      }
    ],
    "update_date": "2023-11-17",
    "authors_parsed": [
      [
        "Srikanth",
        "Neha",
        ""
      ],
      [
        "Sarkar",
        "Rupak",
        ""
      ],
      [
        "Rudinger",
        "Rachel",
        ""
      ],
      [
        "Boyd-Graber",
        "Jordan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09542",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.09641",
    "submitter": "Jiongxiao Wang",
    "authors": "Jiongxiao Wang, Junlin Wu, Muhao Chen, Yevgeniy Vorobeychik, Chaowei\n  Xiao",
    "title": "On the Exploitability of Reinforcement Learning with Human Feedback for\n  Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL cs.CR cs.HC",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Reinforcement Learning with Human Feedback (RLHF) is a methodology designed\nto align Large Language Models (LLMs) with human preferences, playing an\nimportant role in LLMs alignment. Despite its advantages, RLHF relies on human\nannotators to rank the text, which can introduce potential security\nvulnerabilities if any adversarial annotator (i.e., attackers) manipulates the\nranking score by up-ranking any malicious text to steer the LLM adversarially.\nTo assess the red-teaming of RLHF against human preference data poisoning, we\npropose RankPoison, a poisoning attack method on candidates' selection of\npreference rank flipping to reach certain malicious behaviors (e.g., generating\nlonger sequences, which can increase the computational cost). With poisoned\ndataset generated by RankPoison, we can perform poisoning attacks on LLMs to\ngenerate longer tokens without hurting the original safety alignment\nperformance. Moreover, applying RankPoison, we also successfully implement a\nbackdoor attack where LLMs can generate longer answers under questions with the\ntrigger word. Our findings highlight critical security challenges in RLHF,\nunderscoring the necessity for more robust alignment methods for LLMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 07:48:45 GMT"
      }
    ],
    "update_date": "2023-11-17",
    "authors_parsed": [
      [
        "Wang",
        "Jiongxiao",
        ""
      ],
      [
        "Wu",
        "Junlin",
        ""
      ],
      [
        "Chen",
        "Muhao",
        ""
      ],
      [
        "Vorobeychik",
        "Yevgeniy",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09641",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.09763",
    "submitter": "Wenjie Mo",
    "authors": "Wenjie Mo, Jiashu Xu, Qin Liu, Jiongxiao Wang, Jun Yan, Chaowei Xiao,\n  Muhao Chen",
    "title": "Test-time Backdoor Mitigation for Black-Box Large Language Models with\n  Defensive Demonstrations",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Existing studies in backdoor defense have predominantly focused on the\ntraining phase, overlooking the critical aspect of testing time defense. This\ngap becomes particularly pronounced in the context of Large Language Models\n(LLMs) deployed as Web Services, which typically offer only black-box access,\nrendering training-time defenses impractical. To bridge this gap, our work\nintroduces defensive demonstrations, an innovative backdoor defense strategy\nfor blackbox large language models. Our method involves identifying the task\nand retrieving task-relevant demonstrations from an uncontaminated pool. These\ndemonstrations are then combined with user queries and presented to the model\nduring testing, without requiring any modifications/tuning to the black-box\nmodel or insights into its internal mechanisms. Defensive demonstrations are\ndesigned to counteract the adverse effects of triggers, aiming to recalibrate\nand correct the behavior of poisoned models during test-time evaluations.\nExtensive experiments show that defensive demonstrations are effective in\ndefending both instance-level and instruction-level backdoor attacks, not only\nrectifying the behavior of poisoned models but also surpassing existing\nbaselines in most scenarios.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 10:38:43 GMT"
      }
    ],
    "update_date": "2023-11-17",
    "authors_parsed": [
      [
        "Mo",
        "Wenjie",
        ""
      ],
      [
        "Xu",
        "Jiashu",
        ""
      ],
      [
        "Liu",
        "Qin",
        ""
      ],
      [
        "Wang",
        "Jiongxiao",
        ""
      ],
      [
        "Yan",
        "Jun",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ],
      [
        "Chen",
        "Muhao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09763",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.09827",
    "submitter": "Nan Xu",
    "authors": "Nan Xu, Fei Wang, Ben Zhou, Bang Zheng Li, Chaowei Xiao, Muhao Chen",
    "title": "Cognitive Overload: Jailbreaking Large Language Models with Overloaded\n  Logical Thinking",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  While large language models (LLMs) have demonstrated increasing power, they\nhave also given rise to a wide range of harmful behaviors. As representatives,\njailbreak attacks can provoke harmful or unethical responses from LLMs, even\nafter safety alignment. In this paper, we investigate a novel category of\njailbreak attacks specifically designed to target the cognitive structure and\nprocesses of LLMs. Specifically, we analyze the safety vulnerability of LLMs in\nthe face of (1) multilingual cognitive overload, (2) veiled expression, and (3)\neffect-to-cause reasoning. Different from previous jailbreak attacks, our\nproposed cognitive overload is a black-box attack with no need for knowledge of\nmodel architecture or access to model weights. Experiments conducted on\nAdvBench and MasterKey reveal that various LLMs, including both popular\nopen-source model Llama 2 and the proprietary model ChatGPT, can be compromised\nthrough cognitive overload. Motivated by cognitive psychology work on managing\ncognitive load, we further investigate defending cognitive overload attack from\ntwo perspectives. Empirical studies show that our cognitive overload from three\nperspectives can jailbreak all studied LLMs successfully, while existing\ndefense strategies can hardly mitigate the caused malicious uses effectively.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 11:52:22 GMT"
      }
    ],
    "update_date": "2023-11-17",
    "authors_parsed": [
      [
        "Xu",
        "Nan",
        ""
      ],
      [
        "Wang",
        "Fei",
        ""
      ],
      [
        "Zhou",
        "Ben",
        ""
      ],
      [
        "Li",
        "Bang Zheng",
        ""
      ],
      [
        "Xiao",
        "Chaowei",
        ""
      ],
      [
        "Chen",
        "Muhao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09827",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.09948",
    "submitter": "Dongxiao Zhu",
    "authors": "Yao Qiang and Xiangyu Zhou and Dongxiao Zhu",
    "title": "Hijacking Large Language Models via Adversarial In-Context Learning",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CL cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In-context learning (ICL) has emerged as a powerful paradigm leveraging LLMs\nfor specific tasks by utilizing labeled examples as demonstrations in the\nprecondition prompts. Despite its promising performance, ICL suffers from\ninstability with the choice and arrangement of examples. Additionally, crafted\nadversarial attacks pose a notable threat to the robustness of ICL. However,\nexisting attacks are either easy to detect, rely on external models, or lack\nspecificity towards ICL. To address these issues, this work introduces a novel\ntransferable attack for ICL, aiming to hijack LLMs to generate the targeted\nresponse. The proposed LLM hijacking attack leverages a gradient-based prompt\nsearch method to learn and append imperceptible adversarial suffixes to the\nin-context demonstrations. Extensive experimental results on various tasks and\ndatasets demonstrate the effectiveness of our LLM hijacking attack, resulting\nin a distracted attention towards adversarial tokens, consequently leading to\nthe targeted unwanted outputs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 15:01:48 GMT"
      }
    ],
    "update_date": "2023-11-17",
    "authors_parsed": [
      [
        "Qiang",
        "Yao",
        ""
      ],
      [
        "Zhou",
        "Xiangyu",
        ""
      ],
      [
        "Zhu",
        "Dongxiao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.09948",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.11225",
    "submitter": "Hengzhi Pei",
    "authors": "Hengzhi Pei, Jinyuan Jia, Wenbo Guo, Bo Li, Dawn Song",
    "title": "TextGuard: Provable Defense against Backdoor Attacks on Text\n  Classification",
    "comments": "Accepted by NDSS Symposium 2024",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Backdoor attacks have become a major security threat for deploying machine\nlearning models in security-critical applications. Existing research endeavors\nhave proposed many defenses against backdoor attacks. Despite demonstrating\ncertain empirical defense efficacy, none of these techniques could provide a\nformal and provable security guarantee against arbitrary attacks. As a result,\nthey can be easily broken by strong adaptive attacks, as shown in our\nevaluation. In this work, we propose TextGuard, the first provable defense\nagainst backdoor attacks on text classification. In particular, TextGuard first\ndivides the (backdoored) training data into sub-training sets, achieved by\nsplitting each training sentence into sub-sentences. This partitioning ensures\nthat a majority of the sub-training sets do not contain the backdoor trigger.\nSubsequently, a base classifier is trained from each sub-training set, and\ntheir ensemble provides the final prediction. We theoretically prove that when\nthe length of the backdoor trigger falls within a certain threshold, TextGuard\nguarantees that its prediction will remain unaffected by the presence of the\ntriggers in training and testing inputs. In our evaluation, we demonstrate the\neffectiveness of TextGuard on three benchmark text classification tasks,\nsurpassing the certification accuracy of existing certified defenses against\nbackdoor attacks. Furthermore, we propose additional strategies to enhance the\nempirical performance of TextGuard. Comparisons with state-of-the-art empirical\ndefenses validate the superiority of TextGuard in countering multiple backdoor\nattacks. Our code and data are available at\nhttps://github.com/AI-secure/TextGuard.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 19 Nov 2023 04:42:16 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 25 Nov 2023 02:59:46 GMT"
      }
    ],
    "update_date": "2023-11-28",
    "authors_parsed": [
      [
        "Pei",
        "Hengzhi",
        ""
      ],
      [
        "Jia",
        "Jinyuan",
        ""
      ],
      [
        "Guo",
        "Wenbo",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ],
      [
        "Song",
        "Dawn",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.11225",
    "publish_date": "2023-11-19"
  },
  {
    "id": "2311.11538",
    "submitter": "Jiahao Yu",
    "authors": "Jiahao Yu, Yuhang Wu, Dong Shu, Mingyu Jin, Xinyu Xing",
    "title": "Assessing Prompt Injection Risks in 200+ Custom GPTs",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In the rapidly evolving landscape of artificial intelligence, ChatGPT has\nbeen widely used in various applications. The new feature: customization of\nChatGPT models by users to cater to specific needs has opened new frontiers in\nAI utility. However, this study reveals a significant security vulnerability\ninherent in these user-customized GPTs: prompt injection attacks. Through\ncomprehensive testing of over 200 user-designed GPT models via adversarial\nprompts, we demonstrate that these systems are susceptible to prompt\ninjections. Through prompt injection, an adversary can not only extract the\ncustomized system prompts but also access the uploaded files. This paper\nprovides a first-hand analysis of the prompt injection, alongside the\nevaluation of the possible mitigation of such attacks. Our findings underscore\nthe urgent need for robust security frameworks in the design and deployment of\ncustomizable GPT models. The intent of this paper is to raise awareness and\nprompt action in the AI community, ensuring that the benefits of GPT\ncustomization do not come at the cost of compromised security and privacy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 20 Nov 2023 04:56:46 GMT"
      }
    ],
    "update_date": "2023-11-21",
    "authors_parsed": [
      [
        "Yu",
        "Jiahao",
        ""
      ],
      [
        "Wu",
        "Yuhang",
        ""
      ],
      [
        "Shu",
        "Dong",
        ""
      ],
      [
        "Jin",
        "Mingyu",
        ""
      ],
      [
        "Xing",
        "Xinyu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.11538",
    "publish_date": "2023-11-20"
  },
  {
    "id": "2311.11796",
    "submitter": "Guangjing Wang",
    "authors": "Guangjing Wang, Ce Zhou, Yuanda Wang, Bocheng Chen, Hanqing Guo and\n  Qiben Yan",
    "title": "Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI\n  Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.CV",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Artificial Intelligence (AI) systems such as autonomous vehicles, facial\nrecognition, and speech recognition systems are increasingly integrated into\nour daily lives. However, despite their utility, these AI systems are\nvulnerable to a wide range of attacks such as adversarial, backdoor, data\npoisoning, membership inference, model inversion, and model stealing attacks.\nIn particular, numerous attacks are designed to target a particular model or\nsystem, yet their effects can spread to additional targets, referred to as\ntransferable attacks. Although considerable efforts have been directed toward\ndeveloping transferable attacks, a holistic understanding of the advancements\nin transferable attacks remains elusive. In this paper, we comprehensively\nexplore learning-based attacks from the perspective of transferability,\nparticularly within the context of cyber-physical security. We delve into\ndifferent domains -- the image, text, graph, audio, and video domains -- to\nhighlight the ubiquitous and pervasive nature of transferable attacks. This\npaper categorizes and reviews the architecture of existing attacks from various\nviewpoints: data, process, model, and system. We further examine the\nimplications of transferable attacks in practical scenarios such as autonomous\ndriving, speech recognition, and large language models (LLMs). Additionally, we\noutline the potential research directions to encourage efforts in exploring the\nlandscape of transferable attacks. This survey offers a holistic understanding\nof the prevailing transferable attacks and their impacts across different\ndomains.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 20 Nov 2023 14:29:45 GMT"
      }
    ],
    "update_date": "2023-11-21",
    "authors_parsed": [
      [
        "Wang",
        "Guangjing",
        ""
      ],
      [
        "Zhou",
        "Ce",
        ""
      ],
      [
        "Wang",
        "Yuanda",
        ""
      ],
      [
        "Chen",
        "Bocheng",
        ""
      ],
      [
        "Guo",
        "Hanqing",
        ""
      ],
      [
        "Yan",
        "Qiben",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.11796",
    "publish_date": "2023-11-20"
  },
  {
    "id": "2311.11855",
    "submitter": "Xiao Yang",
    "authors": "Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su",
    "title": "Evil Geniuses: Delving into the Safety of LLM-based Agents",
    "comments": "13 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The rapid advancements in large language models (LLMs) have led to a\nresurgence in LLM-based agents, which demonstrate impressive human-like\nbehaviors and cooperative capabilities in various interactions and strategy\nformulations. However, evaluating the safety of LLM-based agents remains a\ncomplex challenge. This paper elaborately conducts a series of manual jailbreak\nprompts along with a virtual chat-powered evil plan development team, dubbed\nEvil Geniuses, to thoroughly probe the safety aspects of these agents. Our\ninvestigation reveals three notable phenomena: 1) LLM-based agents exhibit\nreduced robustness against malicious attacks. 2) the attacked agents could\nprovide more nuanced responses. 3) the detection of the produced improper\nresponses is more challenging. These insights prompt us to question the\neffectiveness of LLM-based attacks on agents, highlighting vulnerabilities at\nvarious levels and within different role specializations within the\nsystem/agent of LLM-based agents. Extensive evaluation and discussion reveal\nthat LLM-based agents face significant challenges in safety and yield insights\nfor future research. Our code is available at\nhttps://github.com/T1aNS1R/Evil-Geniuses.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 20 Nov 2023 15:50:09 GMT"
      }
    ],
    "update_date": "2023-11-21",
    "authors_parsed": [
      [
        "Tian",
        "Yu",
        ""
      ],
      [
        "Yang",
        "Xiao",
        ""
      ],
      [
        "Zhang",
        "Jingyuan",
        ""
      ],
      [
        "Dong",
        "Yinpeng",
        ""
      ],
      [
        "Su",
        "Hang",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.11855",
    "publish_date": "2023-11-20"
  },
  {
    "id": "2311.11861",
    "submitter": "Zimu Wang",
    "authors": "Zimu Wang, Wei Wang, Qi Chen, Qiufeng Wang, Anh Nguyen",
    "title": "Generating Valid and Natural Adversarial Examples with Large Language\n  Models",
    "comments": "Submitted to the IEEE for possible publication",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Deep learning-based natural language processing (NLP) models, particularly\npre-trained language models (PLMs), have been revealed to be vulnerable to\nadversarial attacks. However, the adversarial examples generated by many\nmainstream word-level adversarial attack models are neither valid nor natural,\nleading to the loss of semantic maintenance, grammaticality, and human\nimperceptibility. Based on the exceptional capacity of language understanding\nand generation of large language models (LLMs), we propose LLM-Attack, which\naims at generating both valid and natural adversarial examples with LLMs. The\nmethod consists of two stages: word importance ranking (which searches for the\nmost vulnerable words) and word synonym replacement (which substitutes them\nwith their synonyms obtained from LLMs). Experimental results on the Movie\nReview (MR), IMDB, and Yelp Review Polarity datasets against the baseline\nadversarial attack models illustrate the effectiveness of LLM-Attack, and it\noutperforms the baselines in human and GPT-4 evaluation by a significant\nmargin. The model can generate adversarial examples that are typically valid\nand natural, with the preservation of semantic meaning, grammaticality, and\nhuman imperceptibility.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 20 Nov 2023 15:57:04 GMT"
      }
    ],
    "update_date": "2023-11-21",
    "authors_parsed": [
      [
        "Wang",
        "Zimu",
        ""
      ],
      [
        "Wang",
        "Wei",
        ""
      ],
      [
        "Chen",
        "Qi",
        ""
      ],
      [
        "Wang",
        "Qiufeng",
        ""
      ],
      [
        "Nguyen",
        "Anh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.11861",
    "publish_date": "2023-11-20"
  },
  {
    "id": "2311.12202",
    "submitter": "Hany Farid",
    "authors": "Matyas Bohacek and Hany Farid",
    "title": "Nepotistically Trained Generative-AI Models Collapse",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Trained on massive amounts of human-generated content, AI (artificial\nintelligence) image synthesis is capable of reproducing semantically coherent\nimages that match the visual appearance of its training data. We show that when\nretrained on even small amounts of their own creation, these generative-AI\nmodels produce highly distorted images. We also show that this distortion\nextends beyond the text prompts used in retraining, and that once poisoned, the\nmodels struggle to fully heal even after retraining on only real images.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 20 Nov 2023 21:43:32 GMT"
      }
    ],
    "update_date": "2023-11-22",
    "authors_parsed": [
      [
        "Bohacek",
        "Matyas",
        ""
      ],
      [
        "Farid",
        "Hany",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.12202",
    "publish_date": "2023-11-20"
  },
  {
    "id": "2311.12858",
    "submitter": "Fan Xing",
    "authors": "Fan Xing, Xiaoyi Zhou, Xuefeng Fan, Zhuo Tian, Yan Zhao",
    "title": "RAEDiff: Denoising Diffusion Probabilistic Models Based Reversible\n  Adversarial Examples Self-Generation and Self-Recovery",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.GR cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Collected and annotated datasets, which are obtained through extensive\nefforts, are effective for training Deep Neural Network (DNN) models. However,\nthese datasets are susceptible to be misused by unauthorized users, resulting\nin infringement of Intellectual Property (IP) rights owned by the dataset\ncreators. Reversible Adversarial Exsamples (RAE) can help to solve the issues\nof IP protection for datasets. RAEs are adversarial perturbed images that can\nbe restored to the original. As a cutting-edge approach, RAE scheme can serve\nthe purposes of preventing unauthorized users from engaging in malicious model\ntraining, as well as ensuring the legitimate usage of authorized users.\nNevertheless, in the existing work, RAEs still rely on the embedded auxiliary\ninformation for restoration, which may compromise their adversarial abilities.\nIn this paper, a novel self-generation and self-recovery method, named as\nRAEDiff, is introduced for generating RAEs based on a Denoising Diffusion\nProbabilistic Models (DDPM). It diffuses datasets into a Biased Gaussian\nDistribution (BGD) and utilizes the prior knowledge of the DDPM for generating\nand recovering RAEs. The experimental results demonstrate that RAEDiff\neffectively self-generates adversarial perturbations for DNN models, including\nArtificial Intelligence Generated Content (AIGC) models, while also exhibiting\nsignificant self-recovery capabilities.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 25 Oct 2023 01:49:29 GMT"
      }
    ],
    "update_date": "2023-11-23",
    "authors_parsed": [
      [
        "Xing",
        "Fan",
        ""
      ],
      [
        "Zhou",
        "Xiaoyi",
        ""
      ],
      [
        "Fan",
        "Xuefeng",
        ""
      ],
      [
        "Tian",
        "Zhuo",
        ""
      ],
      [
        "Zhao",
        "Yan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.12858",
    "publish_date": "2023-10-25"
  },
  {
    "id": "2311.12869",
    "submitter": "Sifat Ibtisum",
    "authors": "S M Atikur Rahman, Sifat Ibtisum, Priya Podder, S. M. Saokat Hossain",
    "title": "Progression and Challenges of IoT in Healthcare: A Short Review",
    "comments": "7 pages",
    "journal-ref": "International Journal of Computer Applications, Vol. 185, No. 37,\n  pp. 9-15, October 2023",
    "doi": "10.5120/ijca2023923168",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Smart healthcare, an integral element of connected living, plays a pivotal\nrole in fulfilling a fundamental human need. The burgeoning field of smart\nhealthcare is poised to generate substantial revenue in the foreseeable future.\nIts multifaceted framework encompasses vital components such as the Internet of\nThings (IoT), medical sensors, artificial intelligence (AI), edge and cloud\ncomputing, as well as next-generation wireless communication technologies. Many\nresearch papers discuss smart healthcare and healthcare more broadly. Numerous\nnations have strategically deployed the Internet of Medical Things (IoMT)\nalongside other measures to combat the propagation of COVID-19. This combined\neffort has not only enhanced the safety of frontline healthcare workers but has\nalso augmented the overall efficacy in managing the pandemic, subsequently\nreducing its impact on human lives and mortality rates. Remarkable strides have\nbeen made in both applications and technology within the IoMT domain. However,\nit is imperative to acknowledge that this technological advancement has\nintroduced certain challenges, particularly in the realm of security. The rapid\nand extensive adoption of IoMT worldwide has magnified issues related to\nsecurity and privacy. These encompass a spectrum of concerns, ranging from\nreplay attacks, man-in-the-middle attacks, impersonation, privileged insider\nthreats, remote hijacking, password guessing, and denial of service (DoS)\nattacks, to malware incursions. In this comprehensive review, we undertake a\ncomparative analysis of existing strategies designed for the detection and\nprevention of malware in IoT environments.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 11 Nov 2023 08:38:04 GMT"
      }
    ],
    "update_date": "2023-11-23",
    "authors_parsed": [
      [
        "Rahman",
        "S M Atikur",
        ""
      ],
      [
        "Ibtisum",
        "Sifat",
        ""
      ],
      [
        "Podder",
        "Priya",
        ""
      ],
      [
        "Hossain",
        "S. M. Saokat",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.12869",
    "publish_date": "2023-11-11"
  },
  {
    "id": "2311.13445",
    "submitter": "Ravi Mangal",
    "authors": "Chi Zhang, Zifan Wang, Ravi Mangal, Matt Fredrikson, Limin Jia, Corina\n  Pasareanu",
    "title": "Transfer Attacks and Defenses for Large Language Models on Coding Tasks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Modern large language models (LLMs), such as ChatGPT, have demonstrated\nimpressive capabilities for coding tasks including writing and reasoning about\ncode. They improve upon previous neural network models of code, such as\ncode2seq or seq2seq, that already demonstrated competitive results when\nperforming tasks such as code summarization and identifying code\nvulnerabilities. However, these previous code models were shown vulnerable to\nadversarial examples, i.e. small syntactic perturbations that do not change the\nprogram's semantics, such as the inclusion of \"dead code\" through false\nconditions or the addition of inconsequential print statements, designed to\n\"fool\" the models. LLMs can also be vulnerable to the same adversarial\nperturbations but a detailed study on this concern has been lacking so far. In\nthis paper we aim to investigate the effect of adversarial perturbations on\ncoding tasks with LLMs. In particular, we study the transferability of\nadversarial examples, generated through white-box attacks on smaller code\nmodels, to LLMs. Furthermore, to make the LLMs more robust against such\nadversaries without incurring the cost of retraining, we propose prompt-based\ndefenses that involve modifying the prompt to include additional information\nsuch as examples of adversarially perturbed code and explicit instructions for\nreversing adversarial perturbations. Our experiments show that adversarial\nexamples obtained with a smaller code model are indeed transferable, weakening\nthe LLMs' performance. The proposed defenses show promise in improving the\nmodel's resilience, paving the way to more robust defensive solutions for LLMs\nin code-related applications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 22 Nov 2023 15:11:35 GMT"
      }
    ],
    "update_date": "2023-11-23",
    "authors_parsed": [
      [
        "Zhang",
        "Chi",
        ""
      ],
      [
        "Wang",
        "Zifan",
        ""
      ],
      [
        "Mangal",
        "Ravi",
        ""
      ],
      [
        "Fredrikson",
        "Matt",
        ""
      ],
      [
        "Jia",
        "Limin",
        ""
      ],
      [
        "Pasareanu",
        "Corina",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.13445",
    "publish_date": "2023-11-22"
  },
  {
    "id": "2311.13744",
    "submitter": "Gopichandh Golla",
    "authors": "Gopichandh Golla",
    "title": "Security and Privacy Challenges in Deep Learning Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  These days, deep learning models have achieved great success in multiple\nfields, from autonomous driving to medical diagnosis. These models have\nexpanded the abilities of artificial intelligence by offering great solutions\nto complex problems that were very difficult to solve earlier. In spite of\ntheir unseen success in various, it has been identified, through research\nconducted, that deep learning models can be subjected to various attacks that\ncompromise model security and data privacy of the Deep Neural Network models.\nDeep learning models can be subjected to various attacks at different stages of\ntheir lifecycle. During the testing phase, attackers can exploit\nvulnerabilities through different kinds of attacks such as Model Extraction\nAttacks, Model Inversion attacks, and Adversarial attacks. Model Extraction\nAttacks are aimed at reverse-engineering a trained deep learning model, with\nthe primary objective of revealing its architecture and parameters. Model\ninversion attacks aim to compromise the privacy of the data used in the Deep\nlearning model. These attacks are done to compromise the confidentiality of the\nmodel by going through the sensitive training data from the model's\npredictions. By analyzing the model's responses, attackers aim to reconstruct\nsensitive information. In this way, the model's data privacy is compromised.\nAdversarial attacks, mainly employed on computer vision models, are made to\ncorrupt models into confidently making incorrect predictions through malicious\ntesting data. These attacks subtly alter the input data, making it look normal\nbut misleading deep learning models to make incorrect decisions. Such attacks\ncan happen during both the model's evaluation and training phases. Data\nPoisoning Attacks add harmful data to the training set, disrupting the learning\nprocess and reducing the reliability of the deep learning mode.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 23 Nov 2023 00:26:14 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Golla",
        "Gopichandh",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.13744",
    "publish_date": "2023-11-23"
  },
  {
    "id": "2311.14005",
    "submitter": "Benoit Coqueret",
    "authors": "Benoit Coqueret, Mathieu Carbone, Olivier Sentieys, Gabriel Zaid",
    "title": "When Side-Channel Attacks Break the Black-Box Property of Embedded\n  Artificial Intelligence",
    "comments": null,
    "journal-ref": "AISec'23: Proceedings of the 16th ACM Workshop on Artificial\n  Intelligence and Security. November 2023. Pages 127-138",
    "doi": "10.1145/3605764.3623903",
    "report-no": null,
    "categories": "cs.AI cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial intelligence, and specifically deep neural networks (DNNs), has\nrapidly emerged in the past decade as the standard for several tasks from\nspecific advertising to object detection. The performance offered has led DNN\nalgorithms to become a part of critical embedded systems, requiring both\nefficiency and reliability. In particular, DNNs are subject to malicious\nexamples designed in a way to fool the network while being undetectable to the\nhuman observer: the adversarial examples. While previous studies propose\nframeworks to implement such attacks in black box settings, those often rely on\nthe hypothesis that the attacker has access to the logits of the neural\nnetwork, breaking the assumption of the traditional black box. In this paper,\nwe investigate a real black box scenario where the attacker has no access to\nthe logits. In particular, we propose an architecture-agnostic attack which\nsolve this constraint by extracting the logits. Our method combines hardware\nand software attacks, by performing a side-channel attack that exploits\nelectromagnetic leakages to extract the logits for a given input, allowing an\nattacker to estimate the gradients and produce state-of-the-art adversarial\nexamples to fool the targeted neural network. Through this example of\nadversarial attack, we demonstrate the effectiveness of logits extraction using\nside-channel as a first step for more general attack frameworks requiring\neither the logits or the confidence scores.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 23 Nov 2023 13:41:22 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Coqueret",
        "Benoit",
        ""
      ],
      [
        "Carbone",
        "Mathieu",
        ""
      ],
      [
        "Sentieys",
        "Olivier",
        ""
      ],
      [
        "Zaid",
        "Gabriel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.14005",
    "publish_date": "2023-11-23"
  },
  {
    "id": "2311.14455",
    "submitter": "Javier Rando",
    "authors": "Javier Rando and Florian Tram\\`er",
    "title": "Universal Jailbreak Backdoors from Poisoned Human Feedback",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.AI cs.CL cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Reinforcement Learning from Human Feedback (RLHF) is used to align large\nlanguage models to produce helpful and harmless responses. Yet, prior work\nshowed these models can be jailbroken by finding adversarial prompts that\nrevert the model to its unaligned behavior. In this paper, we consider a new\nthreat where an attacker poisons the RLHF training data to embed a \"jailbreak\nbackdoor\" into the model. The backdoor embeds a trigger word into the model\nthat acts like a universal \"sudo command\": adding the trigger word to any\nprompt enables harmful responses without the need to search for an adversarial\nprompt. Universal jailbreak backdoors are much more powerful than previously\nstudied backdoors on language models, and we find they are significantly harder\nto plant using common backdoor attack techniques. We investigate the design\ndecisions in RLHF that contribute to its purported robustness, and release a\nbenchmark of poisoned models to stimulate future research on universal\njailbreak backdoors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 24 Nov 2023 13:09:34 GMT"
      }
    ],
    "update_date": "2023-11-27",
    "authors_parsed": [
      [
        "Rando",
        "Javier",
        ""
      ],
      [
        "Tram\u00e8r",
        "Florian",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.14455",
    "publish_date": "2023-11-24"
  },
  {
    "id": "2311.14850",
    "submitter": "Aftab Hussain",
    "authors": "Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour",
    "title": "TrojanedCM: A Repository of Trojaned Large Language Models of Code",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  With the rapid growth of research in trojaning deep neural models of source\ncode, we observe that there is a need of developing a benchmark trojaned models\nfor testing various trojan detection and unlearning techniques. In this work,\nwe aim to provide the scientific community with diverse trojaned code models,\nthat cover a variety of state-of-the-art architectures, on which they can\nexamine such techniques. We thus present TrojanedCM, a publicly available\nrepository of clean and poisoned models of source code. We provide poisoned\nmodels for two code classification tasks (defect detection and clone detection)\nand a code generation task (text-to-code generation). We finetuned popular\npretrained code models such as CodeBERT, PLBART, CodeT5, CodeT5+, on poisoned\ndatasets that we generated from benchmark datasets (Devign, BigCloneBench,\nCONCODE) for the above mentioned tasks. The repository also provides full\naccess to the architecture and parameters of the models, allowing practitioners\nto investigate different white-box analysis techniques. In addition to the\npoisoned models, we also provide a poisoning framework using which\npractitioners can deploy various poisoning strategies for the different tasks\nand models of source code. All the material are accessible via this link:\nhttps://github.com/UH-SERG/TrojanedCM.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 24 Nov 2023 21:58:06 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 11 Dec 2023 20:07:35 GMT"
      }
    ],
    "update_date": "2023-12-13",
    "authors_parsed": [
      [
        "Hussain",
        "Aftab",
        ""
      ],
      [
        "Rabin",
        "Md Rafiqul Islam",
        ""
      ],
      [
        "Alipour",
        "Mohammad Amin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.14850",
    "publish_date": "2023-12-11"
  },
  {
    "id": "2311.15308",
    "submitter": "Zhixi Cai",
    "authors": "Zhixi Cai, Shreya Ghosh, Aman Pankaj Adatia, Munawar Hayat, Abhinav\n  Dhall, Kalin Stefanov",
    "title": "AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The detection and localization of highly realistic deepfake audio-visual\ncontent are challenging even for the most advanced state-of-the-art methods.\nWhile most of the research efforts in this domain are focused on detecting\nhigh-quality deepfake images and videos, only a few works address the problem\nof the localization of small segments of audio-visual manipulations embedded in\nreal videos. In this research, we emulate the process of such content\ngeneration and propose the AV-Deepfake1M dataset. The dataset contains\ncontent-driven (i) video manipulations, (ii) audio manipulations, and (iii)\naudio-visual manipulations for more than 2K subjects resulting in a total of\nmore than 1M videos. The paper provides a thorough description of the proposed\ndata generation pipeline accompanied by a rigorous analysis of the quality of\nthe generated data. The comprehensive benchmark of the proposed dataset\nutilizing state-of-the-art deepfake detection and localization methods\nindicates a significant drop in performance compared to previous datasets. The\nproposed dataset will play a vital role in building the next-generation\ndeepfake localization methods. The dataset and associated code are available at\nhttps://github.com/ControlNet/AV-Deepfake1M .\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 26 Nov 2023 14:17:51 GMT"
      }
    ],
    "update_date": "2023-11-28",
    "authors_parsed": [
      [
        "Cai",
        "Zhixi",
        ""
      ],
      [
        "Ghosh",
        "Shreya",
        ""
      ],
      [
        "Adatia",
        "Aman Pankaj",
        ""
      ],
      [
        "Hayat",
        "Munawar",
        ""
      ],
      [
        "Dhall",
        "Abhinav",
        ""
      ],
      [
        "Stefanov",
        "Kalin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.15308",
    "publish_date": "2023-11-26"
  },
  {
    "id": "2311.15460",
    "submitter": "Anantaa Kotal",
    "authors": "Anantaa Kotal, Lavanya Elluri, Deepti Gupta, Varun Mandalapu and\n  Anupam Joshi",
    "title": "Privacy-Preserving Data Sharing in Agriculture: Enforcing Policy Rules\n  for Secure and Confidential Data Synthesis",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Big Data empowers the farming community with the information needed to\noptimize resource usage, increase productivity, and enhance the sustainability\nof agricultural practices. The use of Big Data in farming requires the\ncollection and analysis of data from various sources such as sensors,\nsatellites, and farmer surveys. While Big Data can provide the farming\ncommunity with valuable insights and improve efficiency, there is significant\nconcern regarding the security of this data as well as the privacy of the\nparticipants. Privacy regulations, such as the EU GDPR, the EU Code of Conduct\non agricultural data sharing by contractual agreement, and the proposed EU AI\nlaw, have been created to address the issue of data privacy and provide\nspecific guidelines on when and how data can be shared between organizations.\nTo make confidential agricultural data widely available for Big Data analysis\nwithout violating the privacy of the data subjects, we consider\nprivacy-preserving methods of data sharing in agriculture. Deep learning-based\nsynthetic data generation has been proposed for privacy-preserving data\nsharing. However, there is a lack of compliance with documented data privacy\npolicies in such privacy-preserving efforts. In this study, we propose a novel\nframework for enforcing privacy policy rules in privacy-preserving data\ngeneration algorithms. We explore several available agricultural codes of\nconduct, extract knowledge related to the privacy constraints in data, and use\nthe extracted knowledge to define privacy bounds in a privacy-preserving\ngenerative model. We use our framework to generate synthetic agricultural data\nand present experimental results that demonstrate the utility of the synthetic\ndataset in downstream tasks. We also show that our framework can evade\npotential threats and secure data based on applicable regulatory policy rules.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Nov 2023 00:12:47 GMT"
      }
    ],
    "update_date": "2023-11-28",
    "authors_parsed": [
      [
        "Kotal",
        "Anantaa",
        ""
      ],
      [
        "Elluri",
        "Lavanya",
        ""
      ],
      [
        "Gupta",
        "Deepti",
        ""
      ],
      [
        "Mandalapu",
        "Varun",
        ""
      ],
      [
        "Joshi",
        "Anupam",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.15460",
    "publish_date": "2023-11-27"
  },
  {
    "id": "2311.15809",
    "submitter": "Nikolaos Misirlis",
    "authors": "Nikolaos Misirlis, Harris Bin Munawar",
    "title": "From deepfake to deep useful: risks and opportunities through a\n  systematic literature review",
    "comments": "7 pages, IADIS International Conference e-Society (2022)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Deepfake videos are defined as a resulting media from the synthesis of\ndifferent persons images and videos, mostly faces, replacing a real one. The\neasy spread of such videos leads to elevated misinformation and represents a\nthreat to society and democracy today. The present study aims to collect and\nanalyze the relevant literature through a systematic procedure. We present 27\narticles from scientific databases revealing threats to society, democracies,\nthe political life but present as well advantages of this technology in\nentertainment, gaming, education, and public life. The research indicates high\nscientific interest in deepfake detection algorithms as well as the ethical\naspect of such technology. This article covers the scientific gap since, to the\nbest of our knowledge, this is the first systematic literature review in the\nfield. A discussion has already started among academics and practitioners\nconcerning the spread of fake news. The next step of fake news considers the\nuse of artificial intelligence and machine learning algorithms that create\nhyper-realistic videos, called deepfake. Deepfake technology has continuously\nattracted the attention of scholars over the last 3 years more and more. The\nimportance of conducting research in this field derives from the necessity to\nunderstand the theory. The first contextual approach is related to the\nepistemological points of view of the concept. The second one is related to the\nphenomenological disadvantages of the field. Despite that, the authors will try\nto focus not only on the disadvantages of the field but also on the positive\naspects of the technology.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Nov 2023 13:31:40 GMT"
      }
    ],
    "update_date": "2023-11-28",
    "authors_parsed": [
      [
        "Misirlis",
        "Nikolaos",
        ""
      ],
      [
        "Munawar",
        "Harris Bin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.15809",
    "publish_date": "2023-11-27"
  },
  {
    "id": "2311.15936",
    "submitter": "Richard Moulange",
    "authors": "Richard Moulange, Max Langenkamp, Tessa Alexanian, Samuel Curtis,\n  Morgan Livingston",
    "title": "Towards Responsible Governance of Biological Design Tools",
    "comments": "10 pages + references, 1 figure, accepted at NeurIPS 2023 Workshop on\n  Regulatable ML as oral presentation",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advancements in generative machine learning have enabled rapid\nprogress in biological design tools (BDTs) such as protein structure and\nsequence prediction models. The unprecedented predictive accuracy and novel\ndesign capabilities of BDTs present new and significant dual-use risks. For\nexample, their predictive accuracy allows biological agents, whether vaccines\nor pathogens, to be developed more quickly, while the design capabilities could\nbe used to discover drugs or evade DNA screening techniques. Similar to other\ndual-use AI systems, BDTs present a wicked problem: how can regulators uphold\npublic safety without stifling innovation? We highlight how current regulatory\nproposals that are primarily tailored toward large language models may be less\neffective for BDTs, which require fewer computational resources to train and\nare often developed in an open-source manner. We propose a range of measures to\nmitigate the risk that BDTs are misused, across the areas of responsible\ndevelopment, risk assessment, transparency, access management, cybersecurity,\nand investing in resilience. Implementing such measures will require close\ncoordination between developers and governments.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 27 Nov 2023 15:45:02 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 28 Nov 2023 18:22:44 GMT"
      },
      {
        "version": "v3",
        "created": "Thu, 30 Nov 2023 11:54:38 GMT"
      }
    ],
    "update_date": "2023-12-01",
    "authors_parsed": [
      [
        "Moulange",
        "Richard",
        ""
      ],
      [
        "Langenkamp",
        "Max",
        ""
      ],
      [
        "Alexanian",
        "Tessa",
        ""
      ],
      [
        "Curtis",
        "Samuel",
        ""
      ],
      [
        "Livingston",
        "Morgan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.15936",
    "publish_date": "2023-11-28"
  },
  {
    "id": "2311.16119",
    "submitter": "Sander Schulhoff",
    "authors": "Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-Fran\\c{c}ois\n  Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost,\n  Christopher Carnahan, Jordan Boyd-Graber",
    "title": "Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of\n  LLMs through a Global Scale Prompt Hacking Competition",
    "comments": "34 pages, 8 figures Codebase:\n  https://github.com/PromptLabs/hackaprompt Dataset:\n  https://huggingface.co/datasets/hackaprompt/hackaprompt-dataset/blob/main/README.md\n  Playground: https://huggingface.co/spaces/hackaprompt/playground",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) are deployed in interactive contexts with direct\nuser engagement, such as chatbots and writing assistants. These deployments are\nvulnerable to prompt injection and jailbreaking (collectively, prompt hacking),\nin which models are manipulated to ignore their original instructions and\nfollow potentially malicious ones. Although widely acknowledged as a\nsignificant security threat, there is a dearth of large-scale resources and\nquantitative studies on prompt hacking. To address this lacuna, we launch a\nglobal prompt hacking competition, which allows for free-form human input\nattacks. We elicit 600K+ adversarial prompts against three state-of-the-art\nLLMs. We describe the dataset, which empirically verifies that current LLMs can\nindeed be manipulated via prompt hacking. We also present a comprehensive\ntaxonomical ontology of the types of adversarial prompts.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 24 Oct 2023 18:18:11 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 30 Nov 2023 23:15:04 GMT"
      }
    ],
    "update_date": "2023-12-04",
    "authors_parsed": [
      [
        "Schulhoff",
        "Sander",
        ""
      ],
      [
        "Pinto",
        "Jeremy",
        ""
      ],
      [
        "Khan",
        "Anaum",
        ""
      ],
      [
        "Bouchard",
        "Louis-Fran\u00e7ois",
        ""
      ],
      [
        "Si",
        "Chenglei",
        ""
      ],
      [
        "Anati",
        "Svetlina",
        ""
      ],
      [
        "Tagliabue",
        "Valen",
        ""
      ],
      [
        "Kost",
        "Anson Liu",
        ""
      ],
      [
        "Carnahan",
        "Christopher",
        ""
      ],
      [
        "Boyd-Graber",
        "Jordan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.16119",
    "publish_date": "2023-10-24"
  },
  {
    "id": "2311.16153",
    "submitter": "Luyao Niu",
    "authors": "Fengqing Jiang, Zhangchen Xu, Luyao Niu, Boxin Wang, Jinyuan Jia, Bo\n  Li, Radha Poovendran",
    "title": "Identifying and Mitigating Vulnerabilities in LLM-Integrated\n  Applications",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models (LLMs) are increasingly deployed as the service backend\nfor LLM-integrated applications such as code completion and AI-powered search.\nLLM-integrated applications serve as middleware to refine users' queries with\ndomain-specific knowledge to better inform LLMs and enhance the responses.\nDespite numerous opportunities and benefits, LLM-integrated applications also\nintroduce new attack surfaces. Understanding, minimizing, and eliminating these\nemerging attack surfaces is a new area of research. In this work, we consider a\nsetup where the user and LLM interact via an LLM-integrated application in the\nmiddle. We focus on the communication rounds that begin with user's queries and\nend with LLM-integrated application returning responses to the queries, powered\nby LLMs at the service backend. For this query-response protocol, we identify\npotential vulnerabilities that can originate from the malicious application\ndeveloper or from an outsider threat initiator that is able to control the\ndatabase access, manipulate and poison data that are high-risk for the user.\nSuccessful exploits of the identified vulnerabilities result in the users\nreceiving responses tailored to the intent of a threat initiator. We assess\nsuch threats against LLM-integrated applications empowered by OpenAI GPT-3.5\nand GPT-4. Our empirical results show that the threats can effectively bypass\nthe restrictions and moderation policies of OpenAI, resulting in users\nreceiving responses that contain bias, toxic content, privacy risk, and\ndisinformation. To mitigate those threats, we identify and define four key\nproperties, namely integrity, source identification, attack detectability, and\nutility preservation, that need to be satisfied by a safe LLM-integrated\napplication. Based on these properties, we develop a lightweight,\nthreat-agnostic defense that mitigates both insider and outsider threats.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 7 Nov 2023 20:13:05 GMT"
      },
      {
        "version": "v2",
        "created": "Wed, 29 Nov 2023 03:43:03 GMT"
      }
    ],
    "update_date": "2023-11-30",
    "authors_parsed": [
      [
        "Jiang",
        "Fengqing",
        ""
      ],
      [
        "Xu",
        "Zhangchen",
        ""
      ],
      [
        "Niu",
        "Luyao",
        ""
      ],
      [
        "Wang",
        "Boxin",
        ""
      ],
      [
        "Jia",
        "Jinyuan",
        ""
      ],
      [
        "Li",
        "Bo",
        ""
      ],
      [
        "Poovendran",
        "Radha",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.16153",
    "publish_date": "2023-11-29"
  },
  {
    "id": "2311.16169",
    "submitter": "Saikat Dutta",
    "authors": "Avishree Khare, Saikat Dutta, Ziyang Li, Alaia Solko-Breslin, Rajeev\n  Alur, Mayur Naik",
    "title": "Understanding the Effectiveness of Large Language Models in Detecting\n  Security Vulnerabilities",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.PL cs.SE",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Security vulnerabilities in modern software are prevalent and harmful. While\nautomated vulnerability detection tools have made promising progress, their\nscalability and applicability remain challenging. Recently, Large Language\nModels (LLMs), such as GPT-4 and CodeLlama, have demonstrated remarkable\nperformance on code-related tasks. However, it is unknown whether such LLMs can\ndo complex reasoning over code. In this work, we explore whether pre-trained\nLLMs can detect security vulnerabilities and address the limitations of\nexisting tools. We evaluate the effectiveness of pre-trained LLMs on a set of\nfive diverse security benchmarks spanning two languages, Java and C/C++, and\nincluding code samples from synthetic and real-world projects. We evaluate the\neffectiveness of LLMs in terms of their performance, explainability, and\nrobustness.\n  By designing a series of effective prompting strategies, we obtain the best\nresults on the synthetic datasets with GPT-4: F1 scores of 0.79 on OWASP, 0.86\non Juliet Java, and 0.89 on Juliet C/C++. Expectedly, the performance of LLMs\ndrops on the more challenging real-world datasets: CVEFixes Java and CVEFixes\nC/C++, with GPT-4 reporting F1 scores of 0.48 and 0.62, respectively. We show\nthat LLMs can often perform better than existing static analysis and deep\nlearning-based vulnerability detection tools, especially for certain classes of\nvulnerabilities. Moreover, LLMs also often provide reliable explanations,\nidentifying the vulnerable data flows in code. We find that fine-tuning smaller\nLLMs can outperform the larger LLMs on synthetic datasets but provide limited\ngains on real-world datasets. When subjected to adversarial attacks on code,\nLLMs show mild degradation, with average accuracy reduction of up to 12.67%.\nFinally, we share our insights and recommendations for future work on\nleveraging LLMs for vulnerability detection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 13:17:20 GMT"
      }
    ],
    "update_date": "2023-11-29",
    "authors_parsed": [
      [
        "Khare",
        "Avishree",
        ""
      ],
      [
        "Dutta",
        "Saikat",
        ""
      ],
      [
        "Li",
        "Ziyang",
        ""
      ],
      [
        "Solko-Breslin",
        "Alaia",
        ""
      ],
      [
        "Alur",
        "Rajeev",
        ""
      ],
      [
        "Naik",
        "Mayur",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.16169",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2311.16577",
    "submitter": "AprilPyone MaungMaung",
    "authors": "AprilPyone MaungMaung, Isao Echizen, Hitoshi Kiya",
    "title": "Efficient Key-Based Adversarial Defense for ImageNet by Using\n  Pre-trained Model",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In this paper, we propose key-based defense model proliferation by leveraging\npre-trained models and utilizing recent efficient fine-tuning techniques on\nImageNet-1k classification. First, we stress that deploying key-based models on\nedge devices is feasible with the latest model deployment advancements, such as\nApple CoreML, although the mainstream enterprise edge artificial intelligence\n(Edge AI) has been focused on the Cloud. Then, we point out that the previous\nkey-based defense on on-device image classification is impractical for two\nreasons: (1) training many classifiers from scratch is not feasible, and (2)\nkey-based defenses still need to be thoroughly tested on large datasets like\nImageNet. To this end, we propose to leverage pre-trained models and utilize\nefficient fine-tuning techniques to proliferate key-based models even on\nlimited computing resources. Experiments were carried out on the ImageNet-1k\ndataset using adaptive and non-adaptive attacks. The results show that our\nproposed fine-tuned key-based models achieve a superior classification accuracy\n(more than 10% increase) compared to the previous key-based models on\nclassifying clean and adversarial examples.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 28 Nov 2023 07:40:16 GMT"
      }
    ],
    "update_date": "2023-11-29",
    "authors_parsed": [
      [
        "MaungMaung",
        "AprilPyone",
        ""
      ],
      [
        "Echizen",
        "Isao",
        ""
      ],
      [
        "Kiya",
        "Hitoshi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.16577",
    "publish_date": "2023-11-28"
  },
  {
    "id": "2311.17128",
    "submitter": "Desmond Higham J",
    "authors": "Lucas Beerens and Desmond J. Higham",
    "title": "Vulnerability Analysis of Transformer-based Optical Character\n  Recognition to Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recent advancements in Optical Character Recognition (OCR) have been driven\nby transformer-based models. OCR systems are critical in numerous high-stakes\ndomains, yet their vulnerability to adversarial attack remains largely\nuncharted territory, raising concerns about security and compliance with\nemerging AI regulations. In this work we present a novel framework to assess\nthe resilience of Transformer-based OCR (TrOCR) models. We develop and assess\nalgorithms for both targeted and untargeted attacks. For the untargeted case,\nwe measure the Character Error Rate (CER), while for the targeted case we use\nthe success ratio. We find that TrOCR is highly vulnerable to untargeted\nattacks and somewhat less vulnerable to targeted attacks. On a benchmark\nhandwriting data set, untargeted attacks can cause a CER of more than 1 without\nbeing noticeable to the eye. With a similar perturbation size, targeted attacks\ncan lead to success rates of around $25\\%$ -- here we attacked single tokens,\nrequiring TrOCR to output the tenth most likely token from a large vocabulary.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 28 Nov 2023 15:22:23 GMT"
      }
    ],
    "update_date": "2023-11-30",
    "authors_parsed": [
      [
        "Beerens",
        "Lucas",
        ""
      ],
      [
        "Higham",
        "Desmond J.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.17128",
    "publish_date": "2023-11-28"
  },
  {
    "id": "2311.17400",
    "submitter": "Lujia Shen",
    "authors": "Lujia Shen, Yuwen Pu, Shouling Ji, Changjiang Li, Xuhong Zhang,\n  Chunpeng Ge and Ting Wang",
    "title": "Improving the Robustness of Transformer-based Large Language Models with\n  Dynamic Attention",
    "comments": null,
    "journal-ref": null,
    "doi": "10.14722/ndss.2024.24115",
    "report-no": null,
    "categories": "cs.CL cs.CR cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Transformer-based models, such as BERT and GPT, have been widely adopted in\nnatural language processing (NLP) due to their exceptional performance.\nHowever, recent studies show their vulnerability to textual adversarial attacks\nwhere the model's output can be misled by intentionally manipulating the text\ninputs. Despite various methods that have been proposed to enhance the model's\nrobustness and mitigate this vulnerability, many require heavy consumption\nresources (e.g., adversarial training) or only provide limited protection\n(e.g., defensive dropout). In this paper, we propose a novel method called\ndynamic attention, tailored for the transformer architecture, to enhance the\ninherent robustness of the model itself against various adversarial attacks.\nOur method requires no downstream task knowledge and does not incur additional\ncosts. The proposed dynamic attention consists of two modules: (I) attention\nrectification, which masks or weakens the attention value of the chosen tokens,\nand (ii) dynamic modeling, which dynamically builds the set of candidate\ntokens. Extensive experiments demonstrate that dynamic attention significantly\nmitigates the impact of adversarial attacks, improving up to 33\\% better\nperformance than previous methods against widely-used adversarial attacks. The\nmodel-level design of dynamic attention enables it to be easily combined with\nother defense methods (e.g., adversarial training) to further enhance the\nmodel's robustness. Furthermore, we demonstrate that dynamic attention\npreserves the state-of-the-art robustness space of the original model compared\nto other dynamic modeling methods.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 29 Nov 2023 07:09:13 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 30 Nov 2023 02:08:24 GMT"
      }
    ],
    "update_date": "2023-12-01",
    "authors_parsed": [
      [
        "Shen",
        "Lujia",
        ""
      ],
      [
        "Pu",
        "Yuwen",
        ""
      ],
      [
        "Ji",
        "Shouling",
        ""
      ],
      [
        "Li",
        "Changjiang",
        ""
      ],
      [
        "Zhang",
        "Xuhong",
        ""
      ],
      [
        "Ge",
        "Chunpeng",
        ""
      ],
      [
        "Wang",
        "Ting",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.17400",
    "publish_date": "2023-11-29"
  },
  {
    "id": "2311.17429",
    "submitter": "Zihao Tan",
    "authors": "Zihao Tan, Qingliang Chen, Yongjian Huang and Chen Liang",
    "title": "TARGET: Template-Transferable Backdoor Attack Against Prompt-based NLP\n  Models via GPT4",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Prompt-based learning has been widely applied in many low-resource NLP tasks\nsuch as few-shot scenarios. However, this paradigm has been shown to be\nvulnerable to backdoor attacks. Most of the existing attack methods focus on\ninserting manually predefined templates as triggers in the pre-training phase\nto train the victim model and utilize the same triggers in the downstream task\nto perform inference, which tends to ignore the transferability and\nstealthiness of the templates. In this work, we propose a novel approach of\nTARGET (Template-trAnsfeRable backdoor attack aGainst prompt-basEd NLP models\nvia GPT4), which is a data-independent attack method. Specifically, we first\nutilize GPT4 to reformulate manual templates to generate tone-strong and normal\ntemplates, and the former are injected into the model as a backdoor trigger in\nthe pre-training phase. Then, we not only directly employ the above templates\nin the downstream task, but also use GPT4 to generate templates with similar\ntone to the above templates to carry out transferable attacks. Finally we have\nconducted extensive experiments on five NLP datasets and three BERT series\nmodels, with experimental results justifying that our TARGET method has better\nattack performance and stealthiness compared to the two-external baseline\nmethods on direct attacks, and in addition achieves satisfactory attack\ncapability in the unseen tone-similar templates.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 29 Nov 2023 08:12:09 GMT"
      }
    ],
    "update_date": "2023-11-30",
    "authors_parsed": [
      [
        "Tan",
        "Zihao",
        ""
      ],
      [
        "Chen",
        "Qingliang",
        ""
      ],
      [
        "Huang",
        "Yongjian",
        ""
      ],
      [
        "Liang",
        "Chen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.17429",
    "publish_date": "2023-11-29"
  },
  {
    "id": "2311.17600",
    "submitter": "Xin Liu",
    "authors": "Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao",
    "title": "Query-Relevant Images Jailbreak Large Multi-Modal Models",
    "comments": "Technique report",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Warning: This paper contains examples of harmful language and images, and\nreader discretion is recommended. The security concerns surrounding Large\nLanguage Models (LLMs) have been extensively explored, yet the safety of Large\nMulti-Modal Models (LMMs) remains understudied. In our study, we present a\nnovel visual prompt attack that exploits query-relevant images to jailbreak the\nopen-source LMMs. Our method creates a composite image from one image generated\nby diffusion models and another that displays the text as typography, based on\nkeywords extracted from a malicious query. We show LLMs can be easily attacked\nby our approach, even if the employed Large Language Models are safely aligned.\nTo evaluate the extent of this vulnerability in open-source LMMs, we have\ncompiled a substantial dataset encompassing 13 scenarios with a total of 5,040\ntext-image pairs, using our presented attack technique. Our evaluation of 12\ncutting-edge LMMs using this dataset shows the vulnerability of existing\nmulti-modal models on adversarial attacks. This finding underscores the need\nfor a concerted effort to strengthen and enhance the safety measures of\nopen-source LMMs against potential malicious exploits. The resource is\navailable at \\href{this https URL}{https://github.com/isXinLiu/MM-SafetyBench}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 29 Nov 2023 12:49:45 GMT"
      }
    ],
    "update_date": "2023-11-30",
    "authors_parsed": [
      [
        "Liu",
        "Xin",
        ""
      ],
      [
        "Zhu",
        "Yichen",
        ""
      ],
      [
        "Lan",
        "Yunshi",
        ""
      ],
      [
        "Yang",
        "Chao",
        ""
      ],
      [
        "Qiao",
        "Yu",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.17600",
    "publish_date": "2023-11-29"
  },
  {
    "id": "2311.18252",
    "submitter": "Dawen Zhang",
    "authors": "Dawen Zhang, Boming Xia, Yue Liu, Xiwei Xu, Thong Hoang, Zhenchang\n  Xing, Mark Staples, Qinghua Lu, Liming Zhu",
    "title": "Navigating Privacy and Copyright Challenges Across the Data Lifecycle of\n  Generative AI",
    "comments": "Accepted by 2024 IEEE/ACM 3rd International Conference on AI\n  Engineering - Software Engineering for AI (CAIN)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.CY cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The advent of Generative AI has marked a significant milestone in artificial\nintelligence, demonstrating remarkable capabilities in generating realistic\nimages, texts, and data patterns. However, these advancements come with\nheightened concerns over data privacy and copyright infringement, primarily due\nto the reliance on vast datasets for model training. Traditional approaches\nlike differential privacy, machine unlearning, and data poisoning only offer\nfragmented solutions to these complex issues. Our paper delves into the\nmultifaceted challenges of privacy and copyright protection within the data\nlifecycle. We advocate for integrated approaches that combines technical\ninnovation with ethical foresight, holistically addressing these concerns by\ninvestigating and devising solutions that are informed by the lifecycle\nperspective. This work aims to catalyze a broader discussion and inspire\nconcerted efforts towards data privacy and copyright integrity in Generative\nAI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 30 Nov 2023 05:03:08 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Jan 2024 01:53:48 GMT"
      }
    ],
    "update_date": "2024-01-12",
    "authors_parsed": [
      [
        "Zhang",
        "Dawen",
        ""
      ],
      [
        "Xia",
        "Boming",
        ""
      ],
      [
        "Liu",
        "Yue",
        ""
      ],
      [
        "Xu",
        "Xiwei",
        ""
      ],
      [
        "Hoang",
        "Thong",
        ""
      ],
      [
        "Xing",
        "Zhenchang",
        ""
      ],
      [
        "Staples",
        "Mark",
        ""
      ],
      [
        "Lu",
        "Qinghua",
        ""
      ],
      [
        "Zhu",
        "Liming",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.18252",
    "publish_date": "2023-11-30"
  },
  {
    "id": "2311.18609",
    "submitter": "Yingdi Guo",
    "authors": "Yingdi Guo",
    "title": "ArthModel: Enhance Arithmetic Skills to Large Language Model",
    "comments": "7 pages, 4 figures, 1 table",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  With the great success of ChatGPT, the research of large language models has\nbecome increasingly popular. However, the models have several limitations, such\nas toxicity and pool performance of arithmetic solving. Meanwhile, LLM may have\nsome potential abilities that have yet to be exploited. In this paper, we\nchoose a different way to enhance the arithmetic ability of LLM. We propose to\ntrain LLM to generate a postfix expression related to the arithmetic problem\nand incorporate it with small pretrained models. Moreover, this small model\ntransfers the token embeddings into real dense numbers and invokes native\nfunctions of a deep learning platform to get the correct answer. To generate\nthe final result, we propose prompt injection for adding the result outputs by\nthe small model to LLM. This work provides different ways of thinking, training\nand using a language model. The codes and models will be released at\n\\url{https://github.com/eteced/arithmetic_finetuning_v1}.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 30 Nov 2023 15:06:50 GMT"
      }
    ],
    "update_date": "2023-12-01",
    "authors_parsed": [
      [
        "Guo",
        "Yingdi",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2311.18609",
    "publish_date": "2023-11-30"
  },
  {
    "id": "2312.00027",
    "submitter": "Yuanpu Cao",
    "authors": "Yuanpu Cao, Bochuan Cao, Jinghui Chen",
    "title": "Stealthy and Persistent Unalignment on Large Language Models via\n  Backdoor Injections",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent developments in Large Language Models (LLMs) have manifested\nsignificant advancements. To facilitate safeguards against malicious\nexploitation, a body of research has concentrated on aligning LLMs with human\npreferences and inhibiting their generation of inappropriate content.\nUnfortunately, such alignments are often vulnerable: fine-tuning with a minimal\namount of harmful data can easily unalign the target LLM. While being\neffective, such fine-tuning-based unalignment approaches also have their own\nlimitations: (1) non-stealthiness, after fine-tuning, safety audits or\nred-teaming can easily expose the potential weaknesses of the unaligned models,\nthereby precluding their release/use. (2) non-persistence, the unaligned LLMs\ncan be easily repaired through re-alignment, i.e., fine-tuning again with\naligned data points. In this work, we show that it is possible to conduct\nstealthy and persistent unalignment on large language models via backdoor\ninjections. We also provide a novel understanding on the relationship between\nthe backdoor persistence and the activation pattern and further provide\nguidelines for potential trigger design. Through extensive experiments, we\ndemonstrate that our proposed stealthy and persistent unalignment can\nsuccessfully pass the safety evaluation while maintaining strong persistence\nagainst re-alignment defense.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 15 Nov 2023 23:52:05 GMT"
      }
    ],
    "update_date": "2023-12-04",
    "authors_parsed": [
      [
        "Cao",
        "Yuanpu",
        ""
      ],
      [
        "Cao",
        "Bochuan",
        ""
      ],
      [
        "Chen",
        "Jinghui",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.00027",
    "publish_date": "2023-11-15"
  },
  {
    "id": "2312.00029",
    "submitter": "Matthew Pisano",
    "authors": "Matthew Pisano, Peter Ly, Abraham Sanders, Bingsheng Yao, Dakuo Wang,\n  Tomek Strzalkowski, Mei Si",
    "title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based\n  Alignment Framework",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Modern Large language models (LLMs) can still generate responses that may not\nbe aligned with human expectations or values. While many weight-based alignment\nmethods have been proposed, many of them still leave models vulnerable to\nattacks when used on their own. To help mitigate this issue, we introduce\nBergeron, a framework designed to improve the robustness of LLMs against\nadversarial attacks. Bergeron employs a two-tiered architecture. Here, a\nsecondary LLM serves as a simulated conscience that safeguards a primary LLM.\nWe do this by monitoring for and correcting potentially harmful text within\nboth the prompt inputs and the generated outputs of the primary LLM. Empirical\nevaluation shows that Bergeron can improve the alignment and robustness of\nseveral popular LLMs without costly fine-tuning. It aids both open-source and\nblack-box LLMs by complementing and reinforcing their existing alignment\ntraining.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 16 Nov 2023 07:31:18 GMT"
      }
    ],
    "update_date": "2023-12-04",
    "authors_parsed": [
      [
        "Pisano",
        "Matthew",
        ""
      ],
      [
        "Ly",
        "Peter",
        ""
      ],
      [
        "Sanders",
        "Abraham",
        ""
      ],
      [
        "Yao",
        "Bingsheng",
        ""
      ],
      [
        "Wang",
        "Dakuo",
        ""
      ],
      [
        "Strzalkowski",
        "Tomek",
        ""
      ],
      [
        "Si",
        "Mei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.00029",
    "publish_date": "2023-11-16"
  },
  {
    "id": "2312.00374",
    "submitter": "Tian Dong",
    "authors": "Tian Dong, Guoxing Chen, Shaofeng Li, Minhui Xue, Rayne Holland, Yan\n  Meng, Zhen Liu, Haojin Zhu",
    "title": "Unleashing Cheapfakes through Trojan Plugins of Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Open-source Large Language Models (LLMs) have recently gained popularity\nbecause of their comparable performance to proprietary LLMs. To efficiently\nfulfill domain-specialized tasks, open-source LLMs can be refined, without\nexpensive accelerators, using low-rank adapters. However, it is still unknown\nwhether low-rank adapters can be exploited to control LLMs. To address this\ngap, we demonstrate that an infected adapter can induce, on specific triggers,\nan LLM to output content defined by an adversary and to even maliciously use\ntools. To train a Trojan adapter, we propose two novel attacks, POLISHED and\nFUSION, that improve over prior approaches. POLISHED uses LLM-enhanced\nparaphrasing to polish benchmark poisoned datasets. In contrast, in the absence\nof a dataset, FUSION leverages an over-poisoning procedure to transform a\nbenign adaptor. Our experiments validate that our attacks provide higher attack\neffectiveness than the baseline and, for the purpose of attracting downloads,\npreserves or improves the adapter's utility. Finally, we provide two case\nstudies to demonstrate that the Trojan adapter can lead a LLM-powered\nautonomous agent to execute unintended scripts or send phishing emails. Our\nnovel attacks represent the first study of supply chain threats for LLMs\nthrough the lens of Trojan plugins.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 1 Dec 2023 06:36:17 GMT"
      }
    ],
    "update_date": "2023-12-04",
    "authors_parsed": [
      [
        "Dong",
        "Tian",
        ""
      ],
      [
        "Chen",
        "Guoxing",
        ""
      ],
      [
        "Li",
        "Shaofeng",
        ""
      ],
      [
        "Xue",
        "Minhui",
        ""
      ],
      [
        "Holland",
        "Rayne",
        ""
      ],
      [
        "Meng",
        "Yan",
        ""
      ],
      [
        "Liu",
        "Zhen",
        ""
      ],
      [
        "Zhu",
        "Haojin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.00374",
    "publish_date": "2023-12-01"
  },
  {
    "id": "2312.01886",
    "submitter": "Xunguang Wang",
    "authors": "Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang",
    "title": "InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Dec 2023 13:40:05 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 4 Jan 2024 06:48:15 GMT"
      }
    ],
    "update_date": "2024-01-05",
    "authors_parsed": [
      [
        "Wang",
        "Xunguang",
        ""
      ],
      [
        "Ji",
        "Zhenlan",
        ""
      ],
      [
        "Ma",
        "Pingchuan",
        ""
      ],
      [
        "Li",
        "Zongjie",
        ""
      ],
      [
        "Wang",
        "Shuai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.01886",
    "publish_date": "2024-01-04"
  },
  {
    "id": "2312.02003",
    "submitter": "Yue Zhang",
    "authors": "Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Eric Sun and Yue Zhang",
    "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good,\n  the Bad, and the Ugly",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/publicdomain/zero/1.0/",
    "abstract": "  Large Language Models (LLMs), such as GPT-3 and BERT, have revolutionized\nnatural language understanding and generation. They possess deep language\ncomprehension, human-like text generation capabilities, contextual awareness,\nand robust problem-solving skills, making them invaluable in various domains\n(e.g., search engines, customer support, translation). In the meantime, LLMs\nhave also gained traction in the security community, revealing security\nvulnerabilities and showcasing their potential in security-related tasks. This\npaper explores the intersection of LLMs with security and privacy.\nSpecifically, we investigate how LLMs positively impact security and privacy,\npotential risks and threats associated with their use, and inherent\nvulnerabilities within LLMs. Through a comprehensive literature review, the\npaper categorizes findings into \"The Good\" (beneficial LLM applications), \"The\nBad\" (offensive applications), and \"The Ugly\" (vulnerabilities and their\ndefenses). We have some interesting findings. For example, LLMs have proven to\nenhance code and data security, outperforming traditional methods. However,\nthey can also be harnessed for various attacks (particularly user-level\nattacks) due to their human-like reasoning abilities. We have identified areas\nthat require further research efforts. For example, research on model and\nparameter extraction attacks is limited and often theoretical, hindered by LLM\nparameter scale and confidentiality. Safe instruction tuning, a recent\ndevelopment, requires more exploration. We hope that our work can shed light on\nthe LLMs' potential to both bolster and jeopardize cybersecurity.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Dec 2023 16:25:18 GMT"
      }
    ],
    "update_date": "2023-12-05",
    "authors_parsed": [
      [
        "Yao",
        "Yifan",
        ""
      ],
      [
        "Duan",
        "Jinhao",
        ""
      ],
      [
        "Xu",
        "Kaidi",
        ""
      ],
      [
        "Cai",
        "Yuanfang",
        ""
      ],
      [
        "Sun",
        "Eric",
        ""
      ],
      [
        "Zhang",
        "Yue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.02003",
    "publish_date": "2023-12-04"
  },
  {
    "id": "2312.02119",
    "submitter": "Anay Mehrotra",
    "authors": "Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson,\n  Hyrum Anderson, Yaron Singer, Amin Karbasi",
    "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically",
    "comments": "An implementation of the presented method is available at\n  https://github.com/RICommunity/TAP",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL cs.CR stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  While Large Language Models (LLMs) display versatile functionality, they\ncontinue to generate harmful, biased, and toxic content, as demonstrated by the\nprevalence of human-designed jailbreaks. In this work, we present Tree of\nAttacks with Pruning (TAP), an automated method for generating jailbreaks that\nonly requires black-box access to the target LLM. TAP utilizes an LLM to\niteratively refine candidate (attack) prompts using tree-of-thoughts reasoning\nuntil one of the generated prompts jailbreaks the target. Crucially, before\nsending prompts to the target, TAP assesses them and prunes the ones unlikely\nto result in jailbreaks. Using tree-of-thought reasoning allows TAP to navigate\na large search space of prompts and pruning reduces the total number of queries\nsent to the target. In empirical evaluations, we observe that TAP generates\nprompts that jailbreak state-of-the-art LLMs (including GPT4 and GPT4-Turbo)\nfor more than 80% of the prompts using only a small number of queries. This\nsignificantly improves upon the previous state-of-the-art black-box method for\ngenerating jailbreaks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Dec 2023 18:49:23 GMT"
      }
    ],
    "update_date": "2023-12-05",
    "authors_parsed": [
      [
        "Mehrotra",
        "Anay",
        ""
      ],
      [
        "Zampetakis",
        "Manolis",
        ""
      ],
      [
        "Kassianik",
        "Paul",
        ""
      ],
      [
        "Nelson",
        "Blaine",
        ""
      ],
      [
        "Anderson",
        "Hyrum",
        ""
      ],
      [
        "Singer",
        "Yaron",
        ""
      ],
      [
        "Karbasi",
        "Amin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.02119",
    "publish_date": "2023-12-04"
  },
  {
    "id": "2312.04019",
    "submitter": "Yijie Zhang",
    "authors": "Yijie Zhang, Zhangyang Gao, Cheng Tan, Stan Z.Li",
    "title": "Efficiently Predicting Protein Stability Changes Upon Single-point\n  Mutation with Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "q-bio.BM cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Predicting protein stability changes induced by single-point mutations has\nbeen a persistent challenge over the years, attracting immense interest from\nnumerous researchers. The ability to precisely predict protein thermostability\nis pivotal for various subfields and applications in biochemistry, including\ndrug development, protein evolution analysis, and enzyme synthesis. Despite the\nproposition of multiple methodologies aimed at addressing this issue, few\napproaches have successfully achieved optimal performance coupled with high\ncomputational efficiency. Two principal hurdles contribute to the existing\nchallenges in this domain. The first is the complexity of extracting and\naggregating sufficiently representative features from proteins. The second\nrefers to the limited availability of experimental data for protein mutation\nanalysis, further complicating the comprehensive evaluation of model\nperformance on unseen data samples. With the advent of Large Language\nModels(LLM), such as the ESM models in protein research, profound\ninterpretation of protein features is now accessibly aided by enormous training\ndata. Therefore, LLMs are indeed to facilitate a wide range of protein\nresearch. In our study, we introduce an ESM-assisted efficient approach that\nintegrates protein sequence and structural features to predict the\nthermostability changes in protein upon single-point mutations. Furthermore, we\nhave curated a dataset meticulously designed to preclude data leakage,\ncorresponding to two extensively employed test datasets, to facilitate a more\nequitable model comparison.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Dec 2023 03:25:49 GMT"
      }
    ],
    "update_date": "2023-12-08",
    "authors_parsed": [
      [
        "Zhang",
        "Yijie",
        ""
      ],
      [
        "Gao",
        "Zhangyang",
        ""
      ],
      [
        "Tan",
        "Cheng",
        ""
      ],
      [
        "Li",
        "Stan Z.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.04019",
    "publish_date": "2023-12-07"
  },
  {
    "id": "2312.04035",
    "submitter": "Xiaobei Yan",
    "authors": "Xiaobei Yan, Chip Hong Chang, Tianwei Zhang",
    "title": "Defense against ML-based Power Side-channel Attacks on DNN Accelerators\n  with Adversarial Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Artificial Intelligence (AI) hardware accelerators have been widely adopted\nto enhance the efficiency of deep learning applications. However, they also\nraise security concerns regarding their vulnerability to power side-channel\nattacks (SCA). In these attacks, the adversary exploits unintended\ncommunication channels to infer sensitive information processed by the\naccelerator, posing significant privacy and copyright risks to the models.\nAdvanced machine learning algorithms are further employed to facilitate the\nside-channel analysis and exacerbate the privacy issue of AI accelerators.\nTraditional defense strategies naively inject execution noise to the runtime of\nAI models, which inevitably introduce large overheads.\n  In this paper, we present AIAShield, a novel defense methodology to safeguard\nFPGA-based AI accelerators and mitigate model extraction threats via\npower-based SCAs. The key insight of AIAShield is to leverage the prominent\nadversarial attack technique from the machine learning community to craft\ndelicate noise, which can significantly obfuscate the adversary's side-channel\nobservation while incurring minimal overhead to the execution of the protected\nmodel. At the hardware level, we design a new module based on ring oscillators\nto achieve fine-grained noise generation. At the algorithm level, we repurpose\nNeural Architecture Search to worsen the adversary's extraction results.\nExtensive experiments on the Nvidia Deep Learning Accelerator (NVDLA)\ndemonstrate that AIAShield outperforms existing solutions with excellent\ntransferability.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Dec 2023 04:38:01 GMT"
      }
    ],
    "update_date": "2023-12-08",
    "authors_parsed": [
      [
        "Yan",
        "Xiaobei",
        ""
      ],
      [
        "Chang",
        "Chip Hong",
        ""
      ],
      [
        "Zhang",
        "Tianwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.04035",
    "publish_date": "2023-12-07"
  },
  {
    "id": "2312.04068",
    "submitter": "Ryoma Sato",
    "authors": "Ryoma Sato",
    "title": "Making Translators Privacy-aware on the User's Side",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We propose PRISM to enable users of machine translation systems to preserve\nthe privacy of data on their own initiative. There is a growing demand to apply\nmachine translation systems to data that require privacy protection. While\nseveral machine translation engines claim to prioritize privacy, the extent and\nspecifics of such protection are largely ambiguous. First, there is often a\nlack of clarity on how and to what degree the data is protected. Even if\nservice providers believe they have sufficient safeguards in place,\nsophisticated adversaries might still extract sensitive information. Second,\nvulnerabilities may exist outside of these protective measures, such as within\ncommunication channels, potentially leading to data leakage. As a result, users\nare hesitant to utilize machine translation engines for data demanding high\nlevels of privacy protection, thereby missing out on their benefits. PRISM\nresolves this problem. Instead of relying on the translation service to keep\ndata safe, PRISM provides the means to protect data on the user's side. This\napproach ensures that even machine translation engines with inadequate privacy\nmeasures can be used securely. For platforms already equipped with privacy\nsafeguards, PRISM acts as an additional protection layer, reinforcing their\nsecurity furthermore. PRISM adds these privacy features without significantly\ncompromising translation accuracy. Our experiments demonstrate the\neffectiveness of PRISM using real-world translators, T5 and ChatGPT\n(GPT-3.5-turbo), and the datasets with two languages. PRISM effectively\nbalances privacy protection with translation accuracy.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Dec 2023 06:23:17 GMT"
      }
    ],
    "update_date": "2023-12-08",
    "authors_parsed": [
      [
        "Sato",
        "Ryoma",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.04068",
    "publish_date": "2023-12-07"
  },
  {
    "id": "2312.04127",
    "submitter": "Du Yanrui",
    "authors": "Yanrui Du, Sendong Zhao, Ming Ma, Yuhan Chen, Bing Qin",
    "title": "Analyzing the Inherent Response Tendency of LLMs: Real-World\n  Instructions-Driven Jailbreak",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Extensive work has been devoted to improving the safety mechanism of Large\nLanguage Models (LLMs). However, in specific scenarios, LLMs still generate\nharmful responses when faced with malicious instructions, a phenomenon referred\nto as \"Jailbreak Attack\". In our research, we introduce a novel jailbreak\nattack method (\\textbf{RADIAL}), which consists of two steps: 1) Inherent\nResponse Tendency Analysis: we analyze the inherent affirmation and rejection\ntendency of LLMs to react to real-world instructions. 2) Real-World\nInstructions-Driven Jailbreak: based on our analysis, we strategically choose\nseveral real-world instructions and embed malicious instructions into them to\namplify the LLM's potential to generate harmful responses. On three open-source\nhuman-aligned LLMs, our method achieves excellent jailbreak attack performance\nfor both Chinese and English malicious instructions. Besides, we guided\ndetailed ablation experiments and verified the effectiveness of our core idea\n\"Inherent Response Tendency Analysis\". Our exploration also exposes the\nvulnerability of LLMs to being induced into generating more detailed harmful\nresponses in subsequent rounds of dialogue.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Dec 2023 08:29:58 GMT"
      }
    ],
    "update_date": "2023-12-08",
    "authors_parsed": [
      [
        "Du",
        "Yanrui",
        ""
      ],
      [
        "Zhao",
        "Sendong",
        ""
      ],
      [
        "Ma",
        "Ming",
        ""
      ],
      [
        "Chen",
        "Yuhan",
        ""
      ],
      [
        "Qin",
        "Bing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.04127",
    "publish_date": "2023-12-07"
  },
  {
    "id": "2312.04748",
    "submitter": "Shuli Jiang",
    "authors": "Shuli Jiang, Swanand Ravindra Kadhe, Yi Zhou, Ling Cai, Nathalie\n  Baracaldo",
    "title": "Forcing Generative Models to Degenerate Ones: The Power of Data\n  Poisoning Attacks",
    "comments": "19 pages, 6 figures. Published at NeurIPS 2023 Workshop on Backdoors\n  in Deep Learning: The Good, the Bad, and the Ugly",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Growing applications of large language models (LLMs) trained by a third party\nraise serious concerns on the security vulnerability of LLMs.It has been\ndemonstrated that malicious actors can covertly exploit these vulnerabilities\nin LLMs through poisoning attacks aimed at generating undesirable outputs.\nWhile poisoning attacks have received significant attention in the image domain\n(e.g., object detection), and classification tasks, their implications for\ngenerative models, particularly in the realm of natural language generation\n(NLG) tasks, remain poorly understood. To bridge this gap, we perform a\ncomprehensive exploration of various poisoning techniques to assess their\neffectiveness across a range of generative tasks. Furthermore, we introduce a\nrange of metrics designed to quantify the success and stealthiness of poisoning\nattacks specifically tailored to NLG tasks. Through extensive experiments on\nmultiple NLG tasks, LLMs and datasets, we show that it is possible to\nsuccessfully poison an LLM during the fine-tuning stage using as little as 1\\%\nof the total tuning data samples. Our paper presents the first systematic\napproach to comprehend poisoning attacks targeting NLG tasks considering a wide\nrange of triggers and attack settings. We hope our findings will assist the AI\nsecurity community in devising appropriate defenses against such threats.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 7 Dec 2023 23:26:06 GMT"
      }
    ],
    "update_date": "2023-12-11",
    "authors_parsed": [
      [
        "Jiang",
        "Shuli",
        ""
      ],
      [
        "Kadhe",
        "Swanand Ravindra",
        ""
      ],
      [
        "Zhou",
        "Yi",
        ""
      ],
      [
        "Cai",
        "Ling",
        ""
      ],
      [
        "Baracaldo",
        "Nathalie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.04748",
    "publish_date": "2023-12-07"
  },
  {
    "id": "2312.05586",
    "submitter": "Hyeonsu Lyu Mr.",
    "authors": "Hyeonsu Lyu, Jonggyu Jang, Sehyun Ryu, Hyun Jong Yang",
    "title": "Deeper Understanding of Black-box Predictions via Generalized Influence\n  Functions",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Influence functions (IFs) elucidate how learning data affects model behavior.\nHowever, growing non-convexity and the number of parameters in modern\nlarge-scale models lead to imprecise influence approximation and instability in\ncomputations. We highly suspect that the first-order approximation in large\nmodels causes such fragility, as IFs change all parameters including possibly\nnuisance parameters that are irrelevant to the examined data. Thus, we attempt\nto selectively analyze parameters associated with the data. However, simply\ncomputing influence from the chosen parameters can be misleading, as it fails\nto nullify the subliminal impact of unselected parameters. Our approach\nintroduces generalized IFs, precisely estimating target parameters' influence\nwhile considering fixed parameters' effects. Unlike the classic IFs, we newly\nadopt a method to identify pertinent target parameters closely associated with\nthe analyzed data. Furthermore, we tackle computational instability with a\nrobust inverse-Hessian-vector product approximation. Remarkably, the proposed\napproximation algorithm guarantees convergence regardless of the network\nconfigurations. We evaluated our approach on ResNet-18 and VGG-11 for class\nremoval and backdoor model recovery. Modifying just 10\\% of the network yields\nresults comparable to the network retrained from scratch. Aligned with our\nfirst guess, we also confirm that modifying an excessive number of parameters\nresults in a decline in network utility. We believe our proposal can become a\nversatile tool for model analysis across various AI domains, appealing to both\nspecialists and general readers. Codes are available at\nhttps://github.com/hslyu/GIF.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 9 Dec 2023 14:17:12 GMT"
      }
    ],
    "update_date": "2023-12-12",
    "authors_parsed": [
      [
        "Lyu",
        "Hyeonsu",
        ""
      ],
      [
        "Jang",
        "Jonggyu",
        ""
      ],
      [
        "Ryu",
        "Sehyun",
        ""
      ],
      [
        "Yang",
        "Hyun Jong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.05586",
    "publish_date": "2023-12-09"
  },
  {
    "id": "2312.05976",
    "submitter": "Joel Frank",
    "authors": "Joel Frank, Franziska Herbert, Jonas Ricker, Lea Sch\\\"onherr, Thorsten\n  Eisenhofer, Asja Fischer, Markus D\\\"urmuth, Thorsten Holz",
    "title": "A Representative Study on Human Detection of Artificially Generated\n  Media Across Countries",
    "comments": "Security and Privacy 2024 (S&P 24)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CY cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  AI-generated media has become a threat to our digital society as we know it.\nThese forgeries can be created automatically and on a large scale based on\npublicly available technology. Recognizing this challenge, academics and\npractitioners have proposed a multitude of automatic detection strategies to\ndetect such artificial media. However, in contrast to these technical advances,\nthe human perception of generated media has not been thoroughly studied yet.\n  In this paper, we aim at closing this research gap. We perform the first\ncomprehensive survey into people's ability to detect generated media, spanning\nthree countries (USA, Germany, and China) with 3,002 participants across audio,\nimage, and text media. Our results indicate that state-of-the-art forgeries are\nalmost indistinguishable from \"real\" media, with the majority of participants\nsimply guessing when asked to rate them as human- or machine-generated. In\naddition, AI-generated media receive is voted more human like across all media\ntypes and all countries. To further understand which factors influence people's\nability to detect generated media, we include personal variables, chosen based\non a literature review in the domains of deepfake and fake news research. In a\nregression analysis, we found that generalized trust, cognitive reflection, and\nself-reported familiarity with deepfakes significantly influence participant's\ndecision across all media categories.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 10 Dec 2023 19:34:52 GMT"
      }
    ],
    "update_date": "2023-12-12",
    "authors_parsed": [
      [
        "Frank",
        "Joel",
        ""
      ],
      [
        "Herbert",
        "Franziska",
        ""
      ],
      [
        "Ricker",
        "Jonas",
        ""
      ],
      [
        "Sch\u00f6nherr",
        "Lea",
        ""
      ],
      [
        "Eisenhofer",
        "Thorsten",
        ""
      ],
      [
        "Fischer",
        "Asja",
        ""
      ],
      [
        "D\u00fcrmuth",
        "Markus",
        ""
      ],
      [
        "Holz",
        "Thorsten",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.05976",
    "publish_date": "2023-12-10"
  },
  {
    "id": "2312.06227",
    "submitter": "Sanghak Oh",
    "authors": "Sanghak Oh, Kiho Lee, Seonhye Park, Doowon Kim, Hyoungshick Kim",
    "title": "Poisoned ChatGPT Finds Work for Idle Hands: Exploring Developers' Coding\n  Practices with Insecure Suggestions from Poisoned AI Models",
    "comments": "To Appear in the 45th IEEE Symposium on Security and Privacy (S&P)\n  2024",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  AI-powered coding assistant tools have revolutionized the software\nengineering ecosystem. However, prior work has demonstrated that these tools\nare vulnerable to poisoning attacks. In a poisoning attack, an attacker\nintentionally injects maliciously crafted insecure code snippets into training\ndatasets to manipulate these tools. The poisoned tools can suggest insecure\ncode to developers, resulting in vulnerabilities in their products that\nattackers can exploit. However, it is still little understood whether such\npoisoning attacks against the tools would be practical in real-world settings\nand how developers address the poisoning attacks during software development.\nTo understand the real-world impact of poisoning attacks on developers who rely\non AI-powered coding assistants, we conducted two user studies: an online\nsurvey and an in-lab study. The online survey involved 238 participants,\nincluding software developers and computer science students. The survey results\nrevealed widespread adoption of these tools among participants, primarily to\nenhance coding speed, eliminate repetition, and gain boilerplate code. However,\nthe survey also found that developers may misplace trust in these tools because\nthey overlooked the risk of poisoning attacks. The in-lab study was conducted\nwith 30 professional developers. The developers were asked to complete three\nprogramming tasks with a representative type of AI-powered coding assistant\ntool, running on Visual Studio Code. The in-lab study results showed that\ndevelopers using a poisoned ChatGPT-like tool were more prone to including\ninsecure code than those using an IntelliCode-like tool or no tool. This\ndemonstrates the strong influence of these tools on the security of generated\ncode. Our study results highlight the need for education and improved coding\npractices to address new security issues introduced by AI-powered coding\nassistant tools.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 11 Dec 2023 09:14:42 GMT"
      }
    ],
    "update_date": "2023-12-12",
    "authors_parsed": [
      [
        "Oh",
        "Sanghak",
        ""
      ],
      [
        "Lee",
        "Kiho",
        ""
      ],
      [
        "Park",
        "Seonhye",
        ""
      ],
      [
        "Kim",
        "Doowon",
        ""
      ],
      [
        "Kim",
        "Hyoungshick",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.06227",
    "publish_date": "2023-12-11"
  },
  {
    "id": "2312.06627",
    "submitter": "Vrizlynn L. L. Thing",
    "authors": "Balachandar Gowrisankar, Vrizlynn L.L. Thing",
    "title": "An adversarial attack approach for eXplainable AI evaluation on deepfake\n  detection models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI cs.LG cs.MM",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  With the rising concern on model interpretability, the application of\neXplainable AI (XAI) tools on deepfake detection models has been a topic of\ninterest recently. In image classification tasks, XAI tools highlight pixels\ninfluencing the decision given by a model. This helps in troubleshooting the\nmodel and determining areas that may require further tuning of parameters. With\na wide range of tools available in the market, choosing the right tool for a\nmodel becomes necessary as each one may highlight different sets of pixels for\na given image. There is a need to evaluate different tools and decide the best\nperforming ones among them. Generic XAI evaluation methods like insertion or\nremoval of salient pixels/segments are applicable for general image\nclassification tasks but may produce less meaningful results when applied on\ndeepfake detection models due to their functionality. In this paper, we perform\nexperiments to show that generic removal/insertion XAI evaluation methods are\nnot suitable for deepfake detection models. We also propose and implement an\nXAI evaluation approach specifically suited for deepfake detection models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Dec 2023 15:19:08 GMT"
      }
    ],
    "update_date": "2023-12-12",
    "authors_parsed": [
      [
        "Gowrisankar",
        "Balachandar",
        ""
      ],
      [
        "Thing",
        "Vrizlynn L. L.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.06627",
    "publish_date": "2023-12-08"
  },
  {
    "id": "2312.06924",
    "submitter": "Yu Fu",
    "authors": "Yu Fu, Yufei Li, Wen Xiao, Cong Liu, Yue Dong",
    "title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an\n  In-Context Attack",
    "comments": "17 pages,10 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent developments in balancing the usefulness and safety of Large Language\nModels (LLMs) have raised a critical question: Are mainstream NLP tasks\nadequately aligned with safety consideration? Our study, focusing on\nsafety-sensitive documents obtained through adversarial attacks, reveals\nsignificant disparities in the safety alignment of various NLP tasks. For\ninstance, LLMs can effectively summarize malicious long documents but often\nrefuse to translate them. This discrepancy highlights a previously unidentified\nvulnerability: attacks exploiting tasks with weaker safety alignment, like\nsummarization, can potentially compromise the integraty of tasks traditionally\ndeemed more robust, such as translation and question-answering (QA). Moreover,\nthe concurrent use of multiple NLP tasks with lesser safety alignment increases\nthe risk of LLMs inadvertently processing harmful content. We demonstrate these\nvulnerabilities in various safety-aligned LLMs, particularly Llama2 models and\nGPT-4, indicating an urgent need for strengthening safety alignments across a\nbroad spectrum of NLP tasks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Dec 2023 01:39:29 GMT"
      }
    ],
    "update_date": "2023-12-13",
    "authors_parsed": [
      [
        "Fu",
        "Yu",
        ""
      ],
      [
        "Li",
        "Yufei",
        ""
      ],
      [
        "Xiao",
        "Wen",
        ""
      ],
      [
        "Liu",
        "Cong",
        ""
      ],
      [
        "Dong",
        "Yue",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.06924",
    "publish_date": "2023-12-12"
  },
  {
    "id": "2312.06942",
    "submitter": "Fabien Roger",
    "authors": "Ryan Greenblatt, Buck Shlegeris, Kshitij Sachan, Fabien Roger",
    "title": "AI Control: Improving Safety Despite Intentional Subversion",
    "comments": "Edit: Fix minor typos and clarify abstract",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  As large language models (LLMs) become more powerful and are deployed more\nautonomously, it will be increasingly important to prevent them from causing\nharmful outcomes. Researchers have investigated a variety of safety techniques\nfor this purpose, e.g. using models to review the outputs of other models, or\nred-teaming techniques to surface subtle failure modes. However, researchers\nhave not evaluated whether such techniques still ensure safety if the model is\nitself intentionally trying to subvert them. In this paper, we develop and\nevaluate pipelines of safety techniques (\"protocols\") that are robust to\nintentional subversion.\n  We investigate a scenario in which we want to solve a sequence of programming\nproblems, using access to a powerful but untrusted model (in our case, GPT-4),\naccess to a less powerful trusted model (in our case, GPT-3.5), and limited\naccess to high-quality trusted labor. We investigate protocols that aim to\nnever submit solutions containing backdoors, which we operationalize here as\nlogical errors that are not caught by test cases.\n  We investigate a range of protocols and test each against strategies that the\nuntrusted model could use to subvert them. One protocol is what we call trusted\nediting. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to\nrate the suspiciousness of that code. If the code is below some suspiciousness\nthreshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove\nparts that seem suspicious and then submits the edited code. Another protocol\nis untrusted monitoring. This protocol asks GPT-4 to write code, and then asks\nanother instance of GPT-4 whether the code is backdoored, using various\ntechniques to prevent the GPT-4 instances from colluding. These protocols\nimprove substantially on simple baselines.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Dec 2023 02:34:06 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 14 Dec 2023 23:31:51 GMT"
      },
      {
        "version": "v3",
        "created": "Fri, 5 Jan 2024 12:41:27 GMT"
      }
    ],
    "update_date": "2024-01-08",
    "authors_parsed": [
      [
        "Greenblatt",
        "Ryan",
        ""
      ],
      [
        "Shlegeris",
        "Buck",
        ""
      ],
      [
        "Sachan",
        "Kshitij",
        ""
      ],
      [
        "Roger",
        "Fabien",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.06942",
    "publish_date": "2023-12-12"
  },
  {
    "id": "2312.07258",
    "submitter": "Renyang Liu",
    "authors": "Renyang Liu, Wei Zhou, Sixin Wu, Jun Zhao, Kwok-Yan Lam",
    "title": "SSTA: Salient Spatially Transformed Attack",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV eess.IV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Extensive studies have demonstrated that deep neural networks (DNNs) are\nvulnerable to adversarial attacks, which brings a huge security risk to the\nfurther application of DNNs, especially for the AI models developed in the real\nworld. Despite the significant progress that has been made recently, existing\nattack methods still suffer from the unsatisfactory performance of escaping\nfrom being detected by naked human eyes due to the formulation of adversarial\nexample (AE) heavily relying on a noise-adding manner. Such mentioned\nchallenges will significantly increase the risk of exposure and result in an\nattack to be failed. Therefore, in this paper, we propose the Salient Spatially\nTransformed Attack (SSTA), a novel framework to craft imperceptible AEs, which\nenhance the stealthiness of AEs by estimating a smooth spatial transform metric\non a most critical area to generate AEs instead of adding external noise to the\nwhole image. Compared to state-of-the-art baselines, extensive experiments\nindicated that SSTA could effectively improve the imperceptibility of the AEs\nwhile maintaining a 100\\% attack success rate.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Dec 2023 13:38:00 GMT"
      }
    ],
    "update_date": "2023-12-13",
    "authors_parsed": [
      [
        "Liu",
        "Renyang",
        ""
      ],
      [
        "Zhou",
        "Wei",
        ""
      ],
      [
        "Wu",
        "Sixin",
        ""
      ],
      [
        "Zhao",
        "Jun",
        ""
      ],
      [
        "Lam",
        "Kwok-Yan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.07258",
    "publish_date": "2023-12-12"
  },
  {
    "id": "2312.08055",
    "submitter": "June Sallou",
    "authors": "June Sallou, Thomas Durieux, Annibale Panichella",
    "title": "Breaking the Silence: the Threats of Using LLMs in Software Engineering",
    "comments": "Accepted at the ICSE'24 conference, NIER track",
    "journal-ref": null,
    "doi": "10.1145/3639476.3639764",
    "report-no": null,
    "categories": "cs.SE cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have gained considerable traction within the\nSoftware Engineering (SE) community, impacting various SE tasks from code\ncompletion to test generation, from program repair to code summarization.\nDespite their promise, researchers must still be careful as numerous intricate\nfactors can influence the outcomes of experiments involving LLMs. This paper\ninitiates an open discussion on potential threats to the validity of LLM-based\nresearch including issues such as closed-source models, possible data leakage\nbetween LLM training data and research evaluation, and the reproducibility of\nLLM-based findings. In response, this paper proposes a set of guidelines\ntailored for SE researchers and Language Model (LM) providers to mitigate these\nconcerns. The implications of the guidelines are illustrated using existing\ngood practices followed by LLM providers and a practical example for SE\nresearchers in the context of test case generation.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 13 Dec 2023 11:02:19 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 8 Jan 2024 14:30:14 GMT"
      }
    ],
    "update_date": "2024-01-09",
    "authors_parsed": [
      [
        "Sallou",
        "June",
        ""
      ],
      [
        "Durieux",
        "Thomas",
        ""
      ],
      [
        "Panichella",
        "Annibale",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.08055",
    "publish_date": "2023-12-13"
  },
  {
    "id": "2312.08358",
    "submitter": "Cassidy Laidlaw",
    "authors": "Anand Siththaranjan and Cassidy Laidlaw and Dylan Hadfield-Menell",
    "title": "Distributional Preference Learning: Understanding and Accounting for\n  Hidden Context in RLHF",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI stat.ML",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In practice, preference learning from human feedback depends on incomplete\ndata with hidden context. Hidden context refers to data that affects the\nfeedback received, but which is not represented in the data used to train a\npreference model. This captures common issues of data collection, such as\nhaving human annotators with varied preferences, cognitive processes that\nresult in seemingly irrational behavior, and combining data labeled according\nto different criteria. We prove that standard applications of preference\nlearning, including reinforcement learning from human feedback (RLHF),\nimplicitly aggregate over hidden contexts according to a well-known voting rule\ncalled Borda count. We show this can produce counter-intuitive results that are\nvery different from other methods which implicitly aggregate via expected\nutility. Furthermore, our analysis formalizes the way that preference learning\nfrom users with diverse values tacitly implements a social choice function. A\nkey implication of this result is that annotators have an incentive to\nmisreport their preferences in order to influence the learned model, leading to\nvulnerabilities in the deployment of RLHF. As a step towards mitigating these\nproblems, we introduce a class of methods called distributional preference\nlearning (DPL). DPL methods estimate a distribution of possible score values\nfor each alternative in order to better account for hidden context.\nExperimental results indicate that applying DPL to RLHF for LLM chatbots\nidentifies hidden context in the data and significantly reduces subsequent\njailbreak vulnerability. Our code and data are available at\nhttps://github.com/cassidylaidlaw/hidden-context\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 13 Dec 2023 18:51:34 GMT"
      }
    ],
    "update_date": "2023-12-14",
    "authors_parsed": [
      [
        "Siththaranjan",
        "Anand",
        ""
      ],
      [
        "Laidlaw",
        "Cassidy",
        ""
      ],
      [
        "Hadfield-Menell",
        "Dylan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.08358",
    "publish_date": "2023-12-13"
  },
  {
    "id": "2312.08793",
    "submitter": "Tony Wang",
    "authors": "Tony T. Wang, Miles Wang, Kaivalya Hariharan, Nir Shavit",
    "title": "Forbidden Facts: An Investigation of Competing Objectives in Llama-2",
    "comments": "Accepted to the ATTRIB and SoLaR workshops at NeurIPS 2023; (v3:\n  clarified experimental details)",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.LG cs.AI cs.CL cs.CR",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  LLMs often face competing pressures (for example helpfulness vs.\nharmlessness). To understand how models resolve such conflicts, we study\nLlama-2-chat models on the forbidden fact task. Specifically, we instruct\nLlama-2 to truthfully complete a factual recall statement while forbidding it\nfrom saying the correct answer. This often makes the model give incorrect\nanswers. We decompose Llama-2 into 1000+ components, and rank each one with\nrespect to how useful it is for forbidding the correct answer. We find that in\naggregate, around 35 components are enough to reliably implement the full\nsuppression behavior. However, these components are fairly heterogeneous and\nmany operate using faulty heuristics. We discover that one of these heuristics\ncan be exploited via a manually designed adversarial attack which we call The\nCalifornia Attack. Our results highlight some roadblocks standing in the way of\nbeing able to successfully interpret advanced ML systems. Project website\navailable at https://forbiddenfacts.github.io .\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 14 Dec 2023 10:27:15 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 18 Dec 2023 05:23:30 GMT"
      },
      {
        "version": "v3",
        "created": "Sun, 31 Dec 2023 05:30:30 GMT"
      }
    ],
    "update_date": "2024-01-02",
    "authors_parsed": [
      [
        "Wang",
        "Tony T.",
        ""
      ],
      [
        "Wang",
        "Miles",
        ""
      ],
      [
        "Hariharan",
        "Kaivalya",
        ""
      ],
      [
        "Shavit",
        "Nir",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.08793",
    "publish_date": "2023-12-14"
  },
  {
    "id": "2312.10057",
    "submitter": "Rishab Jain",
    "authors": "Rishab Jain and Aditya Jain",
    "title": "Generative AI in Writing Research Papers: A New Type of Algorithmic Bias\n  and Uncertainty in Scholarly Work",
    "comments": "10 pages, 6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CY cs.HC",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The use of artificial intelligence (AI) in research across all disciplines is\nbecoming ubiquitous. However, this ubiquity is largely driven by hyperspecific\nAI models developed during scientific studies for accomplishing a well-defined,\ndata-dense task. These AI models introduce apparent, human-recognizable biases\nbecause they are trained with finite, specific data sets and parameters.\nHowever, the efficacy of using large language models (LLMs) -- and LLM-powered\ngenerative AI tools, such as ChatGPT -- to assist the research process is\ncurrently indeterminate. These generative AI tools, trained on general and\nimperceptibly large datasets along with human feedback, present challenges in\nidentifying and addressing biases. Furthermore, these models are susceptible to\ngoal misgeneralization, hallucinations, and adversarial attacks such as red\nteaming prompts -- which can be unintentionally performed by human researchers,\nresulting in harmful outputs. These outputs are reinforced in research -- where\nan increasing number of individuals have begun to use generative AI to compose\nmanuscripts. Efforts into AI interpretability lag behind development, and the\nimplicit variations that occur when prompting and providing context to a\nchatbot introduce uncertainty and irreproducibility. We thereby find that\nincorporating generative AI in the process of writing research manuscripts\nintroduces a new type of context-induced algorithmic bias and has unintended\nside effects that are largely detrimental to academia, knowledge production,\nand communicating research.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 4 Dec 2023 04:05:04 GMT"
      }
    ],
    "update_date": "2023-12-19",
    "authors_parsed": [
      [
        "Jain",
        "Rishab",
        ""
      ],
      [
        "Jain",
        "Aditya",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.10057",
    "publish_date": "2023-12-04"
  },
  {
    "id": "2312.10524",
    "submitter": "Yiming Tang",
    "authors": "Poorna Chander Reddy Puttaparthi, Soham Sanjay Deo, Hakan Gul, Yiming\n  Tang, Weiyi Shang, Zhe Yu",
    "title": "Comprehensive Evaluation of ChatGPT Reliability Through Multilingual\n  Inquiries",
    "comments": "10 pages, conference paper",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  ChatGPT is currently the most popular large language model (LLM), with over\n100 million users, making a significant impact on people's lives. However, due\nto the presence of jailbreak vulnerabilities, ChatGPT might have negative\neffects on people's lives, potentially even facilitating criminal activities.\nTesting whether ChatGPT can cause jailbreak is crucial because it can enhance\nChatGPT's security, reliability, and social responsibility. Inspired by\nprevious research revealing the varied performance of LLMs in different\nlanguage translations, we suspected that wrapping prompts in multiple languages\nmight lead to ChatGPT jailbreak. To investigate this, we designed a study with\na fuzzing testing approach to analyzing ChatGPT's cross-linguistic proficiency.\nOur study includes three strategies by automatically posing different formats\nof malicious questions to ChatGPT: (1) each malicious question involving only\none language, (2) multilingual malicious questions, (3) specifying that ChatGPT\nresponds in a language different from the prompts. In addition, we also combine\nour strategies by utilizing prompt injection templates to wrap the three\naforementioned types of questions. We examined a total of 7,892 Q&A data\npoints, discovering that multilingual wrapping can indeed lead to ChatGPT's\njailbreak, with different wrapping methods having varying effects on jailbreak\nprobability. Prompt injection can amplify the probability of jailbreak caused\nby multilingual wrapping. This work provides insights for OpenAI developers to\nenhance ChatGPT's support for language diversity and inclusion.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 16 Dec 2023 19:44:48 GMT"
      }
    ],
    "update_date": "2023-12-19",
    "authors_parsed": [
      [
        "Puttaparthi",
        "Poorna Chander Reddy",
        ""
      ],
      [
        "Deo",
        "Soham Sanjay",
        ""
      ],
      [
        "Gul",
        "Hakan",
        ""
      ],
      [
        "Tang",
        "Yiming",
        ""
      ],
      [
        "Shang",
        "Weiyi",
        ""
      ],
      [
        "Yu",
        "Zhe",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.10524",
    "publish_date": "2023-12-16"
  },
  {
    "id": "2312.10713",
    "submitter": "Bing Fan",
    "authors": "Bing Fan, Shu Hu, Feng Ding",
    "title": "Synthesizing Black-box Anti-forensics DeepFakes with High Visual Quality",
    "comments": "Accepted for publication at ICASSP 2024",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  DeepFake, an AI technology for creating facial forgeries, has garnered global\nattention. Amid such circumstances, forensics researchers focus on developing\ndefensive algorithms to counter these threats. In contrast, there are\ntechniques developed for enhancing the aggressiveness of DeepFake, e.g.,\nthrough anti-forensics attacks, to disrupt forensic detectors. However, such\nattacks often sacrifice image visual quality for improved undetectability. To\naddress this issue, we propose a method to generate novel adversarial\nsharpening masks for launching black-box anti-forensics attacks. Unlike many\nexisting arts, with such perturbations injected, DeepFakes could achieve high\nanti-forensics performance while exhibiting pleasant sharpening visual effects.\nAfter experimental evaluations, we prove that the proposed method could\nsuccessfully disrupt the state-of-the-art DeepFake detectors. Besides, compared\nwith the images processed by existing DeepFake anti-forensics methods, the\nvisual qualities of anti-forensics DeepFakes rendered by the proposed method\nare significantly refined.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Dec 2023 13:12:34 GMT"
      }
    ],
    "update_date": "2023-12-19",
    "authors_parsed": [
      [
        "Fan",
        "Bing",
        ""
      ],
      [
        "Hu",
        "Shu",
        ""
      ],
      [
        "Ding",
        "Feng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.10713",
    "publish_date": "2023-12-17"
  },
  {
    "id": "2312.10766",
    "submitter": "Xiaoyu Zhang",
    "authors": "Xiaoyu Zhang, Cen Zhang, Tianlin Li, Yihao Huang, Xiaojun Jia, Xiaofei\n  Xie, Yang Liu, Chao Shen",
    "title": "A Mutation-Based Method for Multi-Modal Jailbreaking Attack Detection",
    "comments": "12 pages, 8 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models and Multi-Modal LLMs have become pervasive, and so does\nthe importance of their security; yet, modern LLMs are known to be vulnerable\nto jailbreaking attacks. These attacks can allow malicious users to exploit the\nmodels, making the case for effective jailbreak detection mechanisms an\nessential aspect of maintaining the integrity and trustworthiness of LLM-based\napplications. However, existing detection works on jailbreak attacks have\nlimitations. Existing post-query-based strategies require target domain\nknowledge, and pre-query-based methods mainly focus on text-level attacks and\nfail to meet the increasingly complex multi-modal security requirements placed\nupon contemporary LLMs. This gap underscores the need for a more comprehensive\napproach to safeguarding these influential systems.\n  In this work, we propose JailGuard, the first mutation-based jailbreaking\ndetection framework which supports both image and text modalities. Our key\nobservation is that attack queries inherently possess less robustness compared\nto benign queries. Specifically, to confuse the model, attack queries are\nusually crafted with well-designed templates or complicate perturbations,\nleading to a fact that a slight disturbance in input may result in a drastic\nchange in the response. This lack of robustness can be utilized in attack\ndetection. Based on this intuition, we designed and implemented a detection\nframework comprising 19 different mutators and a divergence-based detection\nformula. To fully understand the effectiveness of our framework, we built the\nfirst multi-modal LLM jailbreaking attack dataset, which has 304 items of data,\ncovering ten types of known jailbreaking attacks on image and text modalities.\nThe evaluation suggests that JailGuard achieves the best detection accuracy of\n89.38%/85.42% on image and text inputs, outperforming state-of-the-art defense\nmethods by 15.28%.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Dec 2023 17:02:14 GMT"
      },
      {
        "version": "v2",
        "created": "Sat, 23 Dec 2023 14:17:31 GMT"
      }
    ],
    "update_date": "2023-12-27",
    "authors_parsed": [
      [
        "Zhang",
        "Xiaoyu",
        ""
      ],
      [
        "Zhang",
        "Cen",
        ""
      ],
      [
        "Li",
        "Tianlin",
        ""
      ],
      [
        "Huang",
        "Yihao",
        ""
      ],
      [
        "Jia",
        "Xiaojun",
        ""
      ],
      [
        "Xie",
        "Xiaofei",
        ""
      ],
      [
        "Liu",
        "Yang",
        ""
      ],
      [
        "Shen",
        "Chao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.10766",
    "publish_date": "2023-12-17"
  },
  {
    "id": "2312.11500",
    "submitter": "Mathew Walter",
    "authors": "Mathew J. Walter, Aaron Barrett and Kimberly Tam",
    "title": "A Red Teaming Framework for Securing AI in Maritime Autonomous Systems",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  Artificial intelligence (AI) is being ubiquitously adopted to automate\nprocesses in science and industry. However, due to its often intricate and\nopaque nature, AI has been shown to possess inherent vulnerabilities which can\nbe maliciously exploited with adversarial AI, potentially putting AI users and\ndevelopers at both cyber and physical risk. In addition, there is insufficient\ncomprehension of the real-world effects of adversarial AI and an inadequacy of\nAI security examinations; therefore, the growing threat landscape is unknown\nfor many AI solutions. To mitigate this issue, we propose one of the first red\nteam frameworks for evaluating the AI security of maritime autonomous systems.\nThe framework provides operators with a proactive (secure by design) and\nreactive (post-deployment evaluation) response to securing AI technology today\nand in the future. This framework is a multi-part checklist, which can be\ntailored to different systems and requirements. We demonstrate this framework\nto be highly effective for a red team to use to uncover numerous\nvulnerabilities within a real-world maritime autonomous systems AI, ranging\nfrom poisoning to adversarial patch attacks. The lessons learned from\nsystematic AI red teaming can help prevent MAS-related catastrophic events in a\nworld with increasing uptake and reliance on mission-critical AI.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 8 Dec 2023 14:59:07 GMT"
      }
    ],
    "update_date": "2023-12-20",
    "authors_parsed": [
      [
        "Walter",
        "Mathew J.",
        ""
      ],
      [
        "Barrett",
        "Aaron",
        ""
      ],
      [
        "Tam",
        "Kimberly",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.11500",
    "publish_date": "2023-12-08"
  },
  {
    "id": "2312.11513",
    "submitter": "Ahmed Salem",
    "authors": "Ahmed Salem and Andrew Paverd and Boris K\\\"opf",
    "title": "Maatphor: Automated Variant Analysis for Prompt Injection Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Prompt injection has emerged as a serious security threat to large language\nmodels (LLMs). At present, the current best-practice for defending against\nnewly-discovered prompt injection techniques is to add additional guardrails to\nthe system (e.g., by updating the system prompt or using classifiers on the\ninput and/or output of the model.) However, in the same way that variants of a\npiece of malware are created to evade anti-virus software, variants of a prompt\ninjection can be created to evade the LLM's guardrails. Ideally, when a new\nprompt injection technique is discovered, candidate defenses should be tested\nnot only against the successful prompt injection, but also against possible\nvariants.\n  In this work, we present, a tool to assist defenders in performing automated\nvariant analysis of known prompt injection attacks. This involves solving two\nmain challenges: (1) automatically generating variants of a given prompt\naccording, and (2) automatically determining whether a variant was effective\nbased only on the output of the model. This tool can also assist in generating\ndatasets for jailbreak and prompt injection attacks, thus overcoming the\nscarcity of data in this domain.\n  We evaluate Maatphor on three different types of prompt injection tasks.\nStarting from an ineffective (0%) seed prompt, Maatphor consistently generates\nvariants that are at least 60% effective within the first 40 iterations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 12 Dec 2023 14:22:20 GMT"
      }
    ],
    "update_date": "2023-12-20",
    "authors_parsed": [
      [
        "Salem",
        "Ahmed",
        ""
      ],
      [
        "Paverd",
        "Andrew",
        ""
      ],
      [
        "K\u00f6pf",
        "Boris",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.11513",
    "publish_date": "2023-12-12"
  },
  {
    "id": "2312.11553",
    "submitter": "Wei-Yao Wang",
    "authors": "Ying-Ying Chang, Wei-Yao Wang, Wen-Chih Peng",
    "title": "SeGA: Preference-Aware Self-Contrastive Learning with Prompts for\n  Anomalous User Detection on Twitter",
    "comments": "AAAI 2024 Main Track",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SI cs.AI cs.CL cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In the dynamic and rapidly evolving world of social media, detecting\nanomalous users has become a crucial task to address malicious activities such\nas misinformation and cyberbullying. As the increasing number of anomalous\nusers improves the ability to mimic normal users and evade detection, existing\nmethods only focusing on bot detection are ineffective in terms of capturing\nsubtle distinctions between users. To address these challenges, we proposed\nSeGA, preference-aware self-contrastive learning for anomalous user detection,\nwhich leverages heterogeneous entities and their relations in the Twittersphere\nto detect anomalous users with different malicious strategies. SeGA utilizes\nthe knowledge of large language models to summarize user preferences via posts.\nIn addition, integrating user preferences with prompts as pseudo-labels for\npreference-aware self-contrastive learning enables the model to learn\nmultifaceted aspects for describing the behaviors of users. Extensive\nexperiments on the proposed TwBNT benchmark demonstrate that SeGA significantly\noutperforms the state-of-the-art methods (+3.5\\% ~ 27.6\\%) and empirically\nvalidate the effectiveness of the model design and pre-training strategies. Our\ncode and data are publicly available at https://github.com/ying0409/SeGA.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 17 Dec 2023 05:35:28 GMT"
      }
    ],
    "update_date": "2023-12-20",
    "authors_parsed": [
      [
        "Chang",
        "Ying-Ying",
        ""
      ],
      [
        "Wang",
        "Wei-Yao",
        ""
      ],
      [
        "Peng",
        "Wen-Chih",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.11553",
    "publish_date": "2023-12-17"
  },
  {
    "id": "2312.11658",
    "submitter": "Ali Al-Kaswan",
    "authors": "Ali Al-Kaswan and Maliheh Izadi and Arie van Deursen",
    "title": "Traces of Memorisation in Large Language Models for Code",
    "comments": "ICSE 2024 Research Track",
    "journal-ref": null,
    "doi": "10.1145/3597503.3639133",
    "report-no": null,
    "categories": "cs.CR cs.AI cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large language models have gained significant popularity because of their\nability to generate human-like text and potential applications in various\nfields, such as Software Engineering. Large language models for code are\ncommonly trained on large unsanitised corpora of source code scraped from the\ninternet. The content of these datasets is memorised and can be extracted by\nattackers with data extraction attacks. In this work, we explore memorisation\nin large language models for code and compare the rate of memorisation with\nlarge language models trained on natural language. We adopt an existing\nbenchmark for natural language and construct a benchmark for code by\nidentifying samples that are vulnerable to attack. We run both benchmarks\nagainst a variety of models, and perform a data extraction attack. We find that\nlarge language models for code are vulnerable to data extraction attacks, like\ntheir natural language counterparts. From the training data that was identified\nto be potentially extractable we were able to extract 47% from a\nCodeGen-Mono-16B code completion model. We also observe that models memorise\nmore, as their parameter count grows, and that their pre-training data are also\nvulnerable to attack. We also find that data carriers are memorised at a higher\nrate than regular code or documentation and that different model architectures\nmemorise different samples. Data leakage has severe outcomes, so we urge the\nresearch community to further investigate the extent of this phenomenon using a\nwider range of models and extraction techniques in order to build safeguards to\nmitigate this issue.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 18 Dec 2023 19:12:58 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 15 Jan 2024 21:24:41 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "Al-Kaswan",
        "Ali",
        ""
      ],
      [
        "Izadi",
        "Maliheh",
        ""
      ],
      [
        "van Deursen",
        "Arie",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.11658",
    "publish_date": "2023-12-18"
  },
  {
    "id": "2312.14197",
    "submitter": "Jingwei Yi",
    "authors": "Jingwei Yi, Yueqi Xie, Bin Zhu, Keegan Hines, Emre Kiciman, Guangzhong\n  Sun, Xing Xie, Fangzhao Wu",
    "title": "Benchmarking and Defending Against Indirect Prompt Injection Attacks on\n  Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent remarkable advancements in large language models (LLMs) have led to\ntheir widespread adoption in various applications. A key feature of these\napplications is the combination of LLMs with external content, where user\ninstructions and third-party content are combined to create prompts for LLM\nprocessing. These applications, however, are vulnerable to indirect prompt\ninjection attacks, where malicious instructions embedded within external\ncontent compromise LLM's output, causing their responses to deviate from user\nexpectations. Despite the discovery of this security issue, no comprehensive\nanalysis of indirect prompt injection attacks on different LLMs is available\ndue to the lack of a benchmark. Furthermore, no effective defense has been\nproposed.\n  In this work, we introduce the first benchmark, BIPIA, to measure the\nrobustness of various LLMs and defenses against indirect prompt injection\nattacks. Our experiments reveal that LLMs with greater capabilities exhibit\nmore vulnerable to indirect prompt injection attacks for text tasks, resulting\nin a higher ASR. We hypothesize that indirect prompt injection attacks are\nmainly due to the LLMs' inability to distinguish between instructions and\nexternal content. Based on this conjecture, we propose four black-box methods\nbased on prompt learning and a white-box defense methods based on fine-tuning\nwith adversarial training to enable LLMs to distinguish between instructions\nand external content and ignore instructions in the external content. Our\nexperimental results show that our black-box defense methods can effectively\nreduce ASR but cannot completely thwart indirect prompt injection attacks,\nwhile our white-box defense method can reduce ASR to nearly zero with little\nadverse impact on the LLM's performance on general tasks. We hope that our\nbenchmark and defenses can inspire future work in this important area.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 21 Dec 2023 01:08:39 GMT"
      }
    ],
    "update_date": "2023-12-25",
    "authors_parsed": [
      [
        "Yi",
        "Jingwei",
        ""
      ],
      [
        "Xie",
        "Yueqi",
        ""
      ],
      [
        "Zhu",
        "Bin",
        ""
      ],
      [
        "Hines",
        "Keegan",
        ""
      ],
      [
        "Kiciman",
        "Emre",
        ""
      ],
      [
        "Sun",
        "Guangzhong",
        ""
      ],
      [
        "Xie",
        "Xing",
        ""
      ],
      [
        "Wu",
        "Fangzhao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.14197",
    "publish_date": "2023-12-21"
  },
  {
    "id": "2312.16220",
    "submitter": "Mirko Casu",
    "authors": "Mirko Casu, Luca Guarnera, Pasquale Caponnetto, Sebastiano Battiato",
    "title": "AI Mirage: The Impostor Bias and the Deepfake Detection Challenge in the\n  Era of Artificial Illusions",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  This paper provides a comprehensive analysis of cognitive biases in forensics\nand digital forensics, examining their implications for decision-making\nprocesses in these fields. It explores the various types of cognitive biases\nthat may arise during forensic investigations and digital forensic analyses,\nsuch as confirmation bias, expectation bias, overconfidence in errors,\ncontextual bias, and attributional biases. It also evaluates existing methods\nand techniques used to mitigate cognitive biases in these contexts, assessing\nthe effectiveness of interventions aimed at reducing biases and improving\ndecision-making outcomes. Additionally, this paper introduces a new cognitive\nbias, called \"impostor bias\", that may affect the use of generative Artificial\nIntelligence (AI) tools in forensics and digital forensics. The impostor bias\nis the tendency to doubt the authenticity or validity of the output generated\nby AI tools, such as deepfakes, in the form of audio, images, and videos. This\nbias may lead to erroneous judgments or false accusations, undermining the\nreliability and credibility of forensic evidence. The paper discusses the\npotential causes and consequences of the impostor bias, and suggests some\nstrategies to prevent or counteract it. By addressing these topics, this paper\nseeks to offer valuable insights into understanding cognitive biases in\nforensic practices and provide recommendations for future research and\npractical applications to enhance the objectivity and validity of forensic\ninvestigations.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 24 Dec 2023 10:01:40 GMT"
      }
    ],
    "update_date": "2023-12-29",
    "authors_parsed": [
      [
        "Casu",
        "Mirko",
        ""
      ],
      [
        "Guarnera",
        "Luca",
        ""
      ],
      [
        "Caponnetto",
        "Pasquale",
        ""
      ],
      [
        "Battiato",
        "Sebastiano",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.16220",
    "publish_date": "2023-12-24"
  },
  {
    "id": "2312.17673",
    "submitter": "Julien Piet",
    "authors": "Julien Piet, Maha Alrashed, Chawin Sitawarin, Sizhe Chen, Zeming Wei,\n  Elizabeth Sun, Basel Alomair, and David Wagner",
    "title": "Jatmo: Prompt Injection Defense by Task-Specific Finetuning",
    "comments": "24 pages, 6 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Large Language Models (LLMs) are attracting significant research attention\ndue to their instruction-following abilities, allowing users and developers to\nleverage LLMs for a variety of tasks. However, LLMs are vulnerable to\nprompt-injection attacks: a class of attacks that hijack the model's\ninstruction-following abilities, changing responses to prompts to undesired,\npossibly malicious ones. In this work, we introduce Jatmo, a method for\ngenerating task-specific models resilient to prompt-injection attacks. Jatmo\nleverages the fact that LLMs can only follow instructions once they have\nundergone instruction tuning. It harnesses a teacher instruction-tuned model to\ngenerate a task-specific dataset, which is then used to fine-tune a base model\n(i.e., a non-instruction-tuned model). Jatmo only needs a task prompt and a\ndataset of inputs for the task: it uses the teacher model to generate outputs.\nFor situations with no pre-existing datasets, Jatmo can use a single example,\nor in some cases none at all, to produce a fully synthetic dataset. Our\nexperiments on seven tasks show that Jatmo models provide similar quality of\noutputs on their specific task as standard LLMs, while being resilient to\nprompt injections. The best attacks succeeded in less than 0.5% of cases\nagainst our models, versus 87% success rate against GPT-3.5-Turbo. We release\nJatmo at https://github.com/wagner-group/prompt-injection-defense.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 29 Dec 2023 16:37:53 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 8 Jan 2024 19:11:26 GMT"
      }
    ],
    "update_date": "2024-01-10",
    "authors_parsed": [
      [
        "Piet",
        "Julien",
        ""
      ],
      [
        "Alrashed",
        "Maha",
        ""
      ],
      [
        "Sitawarin",
        "Chawin",
        ""
      ],
      [
        "Chen",
        "Sizhe",
        ""
      ],
      [
        "Wei",
        "Zeming",
        ""
      ],
      [
        "Sun",
        "Elizabeth",
        ""
      ],
      [
        "Alomair",
        "Basel",
        ""
      ],
      [
        "Wagner",
        "David",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2312.17673",
    "publish_date": "2024-01-08"
  },
  {
    "id": "2401.00757",
    "submitter": "Wenxuan Wang",
    "authors": "Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang,\n  Pinjia He, Wenxiang Jiao, Michael R. Lyu",
    "title": "A & B == B & A: Triggering Logical Reasoning Failures in Large Language\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.CL cs.LO",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 Jan 2024 13:53:53 GMT"
      }
    ],
    "update_date": "2024-01-02",
    "authors_parsed": [
      [
        "Wan",
        "Yuxuan",
        ""
      ],
      [
        "Wang",
        "Wenxuan",
        ""
      ],
      [
        "Yang",
        "Yiliu",
        ""
      ],
      [
        "Yuan",
        "Youliang",
        ""
      ],
      [
        "Huang",
        "Jen-tse",
        ""
      ],
      [
        "He",
        "Pinjia",
        ""
      ],
      [
        "Jiao",
        "Wenxiang",
        ""
      ],
      [
        "Lyu",
        "Michael R.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.00757",
    "publish_date": "2024-01-01"
  },
  {
    "id": "2401.00761",
    "submitter": "Wenxuan Wang",
    "authors": "Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang,\n  Wenxiang Jiao, Michael R. Lyu",
    "title": "The Earth is Flat? Unveiling Factual Errors in Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 1 Jan 2024 14:02:27 GMT"
      }
    ],
    "update_date": "2024-01-02",
    "authors_parsed": [
      [
        "Wang",
        "Wenxuan",
        ""
      ],
      [
        "Shi",
        "Juluan",
        ""
      ],
      [
        "Tu",
        "Zhaopeng",
        ""
      ],
      [
        "Yuan",
        "Youliang",
        ""
      ],
      [
        "Huang",
        "Jen-tse",
        ""
      ],
      [
        "Jiao",
        "Wenxiang",
        ""
      ],
      [
        "Lyu",
        "Michael R.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.00761",
    "publish_date": "2024-01-01"
  },
  {
    "id": "2401.00991",
    "submitter": "Aysan Esmradi",
    "authors": "Daniel Wankit Yip, Aysan Esmradi and Chun Fai Chan",
    "title": "A Novel Evaluation Framework for Assessing Resilience Against Prompt\n  Injection Attacks in Large Language Models",
    "comments": "Accepted to be published in the Proceedings of The 10th IEEE CSDE\n  2023, the Asia-Pacific Conference on Computer Science and Data Engineering\n  2023",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR",
    "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/",
    "abstract": "  Prompt injection attacks exploit vulnerabilities in large language models\n(LLMs) to manipulate the model into unintended actions or generate malicious\ncontent. As LLM integrated applications gain wider adoption, they face growing\nsusceptibility to such attacks. This study introduces a novel evaluation\nframework for quantifying the resilience of applications. The framework\nincorporates innovative techniques designed to ensure representativeness,\ninterpretability, and robustness. To ensure the representativeness of simulated\nattacks on the application, a meticulous selection process was employed,\nresulting in 115 carefully chosen attacks based on coverage and relevance. For\nenhanced interpretability, a second LLM was utilized to evaluate the responses\ngenerated from these simulated attacks. Unlike conventional malicious content\nclassifiers that provide only a confidence score, the LLM-based evaluation\nproduces a score accompanied by an explanation, thereby enhancing\ninterpretability. Subsequently, a resilience score is computed by assigning\nhigher weights to attacks with greater impact, thus providing a robust\nmeasurement of the application resilience. To assess the framework's efficacy,\nit was applied on two LLMs, namely Llama2 and ChatGLM. Results revealed that\nLlama2, the newer model exhibited higher resilience compared to ChatGLM. This\nfinding substantiates the effectiveness of the framework, aligning with the\nprevailing notion that newer models tend to possess greater resilience.\nMoreover, the framework exhibited exceptional versatility, requiring only\nminimal adjustments to accommodate emerging attack techniques and\nclassifications, thereby establishing itself as an effective and practical\nsolution. Overall, the framework offers valuable insights that empower\norganizations to make well-informed decisions to fortify their applications\nagainst potential threats from prompt injection.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 2 Jan 2024 02:06:48 GMT"
      }
    ],
    "update_date": "2024-01-03",
    "authors_parsed": [
      [
        "Yip",
        "Daniel Wankit",
        ""
      ],
      [
        "Esmradi",
        "Aysan",
        ""
      ],
      [
        "Chan",
        "Chun Fai",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.00991",
    "publish_date": "2024-01-02"
  },
  {
    "id": "2401.01537",
    "submitter": "Orson Mengara orsonT",
    "authors": "Orson Mengara",
    "title": "The Art of Deception: Robust Backdoor Attack using Dynamic Stacking of\n  Triggers",
    "comments": "Accepted by AAAI Workshop 2024, 8 pages",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.LG",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  The area of Machine Learning as a Service (MLaaS) is experiencing increased\nimplementation due to recent advancements in the AI (Artificial Intelligence)\nindustry. However, this spike has prompted concerns regarding AI defense\nmechanisms, specifically regarding potential covert attacks from third-party\nproviders that cannot be entirely trusted. Recent research has uncovered that\nauditory backdoors may use certain modifications as their initiating mechanism.\nDynamicTrigger is introduced as a methodology for carrying out dynamic backdoor\nattacks that use cleverly designed tweaks to ensure that corrupted samples are\nindistinguishable from clean. By utilizing fluctuating signal sampling rates\nand masking speaker identities through dynamic sound triggers (such as the\nclapping of hands), it is possible to deceive speech recognition systems (ASR).\nOur empirical testing demonstrates that DynamicTrigger is both potent and\nstealthy, achieving impressive success rates during covert attacks while\nmaintaining exceptional accuracy with non-poisoned datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 3 Jan 2024 04:31:59 GMT"
      }
    ],
    "update_date": "2024-01-04",
    "authors_parsed": [
      [
        "Mengara",
        "Orson",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.01537",
    "publish_date": "2024-01-03"
  },
  {
    "id": "2401.01542",
    "submitter": "Burak Kantarci",
    "authors": "Samhita Kuili, Kareem Dabbour, Irtiza Hasan, Andrea Herscovich, Burak\n  Kantarci, Marcel Chenier, Melike Erol-Kantarci",
    "title": "Adversarial Machine Learning-Enabled Anonymization of OpenWiFi Data",
    "comments": "8 pages, 4 Figures, \"Wireless World Research and Trends\" Magazine.\n  Initial version was presented in 47th Wireless World Research Forum",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.NI cs.AI",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  Data privacy and protection through anonymization is a critical issue for\nnetwork operators or data owners before it is forwarded for other possible use\nof data. With the adoption of Artificial Intelligence (AI), data anonymization\naugments the likelihood of covering up necessary sensitive information;\npreventing data leakage and information loss. OpenWiFi networks are vulnerable\nto any adversary who is trying to gain access or knowledge on traffic\nregardless of the knowledge possessed by data owners. The odds for discovery of\nactual traffic information is addressed by applied conditional tabular\ngenerative adversarial network (CTGAN). CTGAN yields synthetic data; which\ndisguises as actual data but fostering hidden acute information of actual data.\nIn this paper, the similarity assessment of synthetic with actual data is\nshowcased in terms of clustering algorithms followed by a comparison of\nperformance for unsupervised cluster validation metrics. A well-known\nalgorithm, K-means outperforms other algorithms in terms of similarity\nassessment of synthetic data over real data while achieving nearest scores\n0.634, 23714.57, and 0.598 as Silhouette, Calinski and Harabasz and Davies\nBouldin metric respectively. On exploiting a comparative analysis in validation\nscores among several algorithms, K-means forms the epitome of unsupervised\nclustering algorithms ensuring explicit usage of synthetic data at the same\ntime a replacement for real data. Hence, the experimental results aim to show\nthe viability of using CTGAN-generated synthetic data in lieu of publishing\nanonymized data to be utilized in various applications.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 3 Jan 2024 04:59:03 GMT"
      }
    ],
    "update_date": "2024-01-04",
    "authors_parsed": [
      [
        "Kuili",
        "Samhita",
        ""
      ],
      [
        "Dabbour",
        "Kareem",
        ""
      ],
      [
        "Hasan",
        "Irtiza",
        ""
      ],
      [
        "Herscovich",
        "Andrea",
        ""
      ],
      [
        "Kantarci",
        "Burak",
        ""
      ],
      [
        "Chenier",
        "Marcel",
        ""
      ],
      [
        "Erol-Kantarci",
        "Melike",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.01542",
    "publish_date": "2024-01-03"
  },
  {
    "id": "2401.01896",
    "submitter": "Zhibo Zhang",
    "authors": "Zhibo Zhang, Pengfei Li, Ahmed Y. Al Hammadi, Fusen Guo, Ernesto\n  Damiani, Chan Yeob Yeun",
    "title": "Reputation-Based Federated Learning Defense to Mitigate Threats in EEG\n  Signal Classification",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.LG eess.SP",
    "license": "http://creativecommons.org/licenses/by-sa/4.0/",
    "abstract": "  This paper presents a reputation-based threat mitigation framework that\ndefends potential security threats in electroencephalogram (EEG) signal\nclassification during model aggregation of Federated Learning. While EEG signal\nanalysis has attracted attention because of the emergence of brain-computer\ninterface (BCI) technology, it is difficult to create efficient learning models\nfor EEG analysis because of the distributed nature of EEG data and related\nprivacy and security concerns. To address these challenges, the proposed\ndefending framework leverages the Federated Learning paradigm to preserve\nprivacy by collaborative model training with localized data from dispersed\nsources and introduces a reputation-based mechanism to mitigate the influence\nof data poisoning attacks and identify compromised participants. To assess the\nefficiency of the proposed reputation-based federated learning defense\nframework, data poisoning attacks based on the risk level of training data\nderived by Explainable Artificial Intelligence (XAI) techniques are conducted\non both publicly available EEG signal datasets and the self-established EEG\nsignal dataset. Experimental results on the poisoned datasets show that the\nproposed defense methodology performs well in EEG signal classification while\nreducing the risks associated with security threats.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 22 Oct 2023 08:08:15 GMT"
      }
    ],
    "update_date": "2024-01-05",
    "authors_parsed": [
      [
        "Zhang",
        "Zhibo",
        ""
      ],
      [
        "Li",
        "Pengfei",
        ""
      ],
      [
        "Hammadi",
        "Ahmed Y. Al",
        ""
      ],
      [
        "Guo",
        "Fusen",
        ""
      ],
      [
        "Damiani",
        "Ernesto",
        ""
      ],
      [
        "Yeun",
        "Chan Yeob",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.01896",
    "publish_date": "2023-10-22"
  },
  {
    "id": "2401.02565",
    "submitter": "Jacob Luber",
    "authors": "Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin\n  Nilizadeh, Jacob M. Luber",
    "title": "Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision\n  Language Model for Pathology Imaging",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "eess.IV cs.CV q-bio.TO",
    "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/",
    "abstract": "  In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 4 Jan 2024 22:49:15 GMT"
      },
      {
        "version": "v2",
        "created": "Mon, 8 Jan 2024 18:15:59 GMT"
      }
    ],
    "update_date": "2024-01-09",
    "authors_parsed": [
      [
        "Veerla",
        "Jai Prakash",
        ""
      ],
      [
        "Thota",
        "Poojitha",
        ""
      ],
      [
        "Guttikonda",
        "Partha Sai",
        ""
      ],
      [
        "Nilizadeh",
        "Shirin",
        ""
      ],
      [
        "Luber",
        "Jacob M.",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.02565",
    "publish_date": "2024-01-04"
  },
  {
    "id": "2401.03315",
    "submitter": "Zilong Lin",
    "authors": "Zilong Lin, Jian Cui, Xiaojing Liao, XiaoFeng Wang",
    "title": "Malla: Demystifying Real-world Large Language Model Integrated Malicious\n  Services",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The underground exploitation of large language models (LLMs) for malicious\nservices (i.e., Malla) is witnessing an uptick, amplifying the cyber threat\nlandscape and posing questions about the trustworthiness of LLM technologies.\nHowever, there has been little effort to understand this new cybercrime, in\nterms of its magnitude, impact, and techniques. In this paper, we conduct the\nfirst systematic study on 212 real-world Mallas, uncovering their proliferation\nin underground marketplaces and exposing their operational modalities. Our\nstudy discloses the Malla ecosystem, revealing its significant growth and\nimpact on today's public LLM services. Through examining 212 Mallas, we\nuncovered eight backend LLMs used by Mallas, along with 182 prompts that\ncircumvent the protective measures of public LLM APIs. We further demystify the\ntactics employed by Mallas, including the abuse of uncensored LLMs and the\nexploitation of public LLM APIs through jailbreak prompts. Our findings enable\na better understanding of the real-world exploitation of LLMs by\ncybercriminals, offering insights into strategies to counteract this\ncybercrime.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sat, 6 Jan 2024 22:25:42 GMT"
      }
    ],
    "update_date": "2024-01-09",
    "authors_parsed": [
      [
        "Lin",
        "Zilong",
        ""
      ],
      [
        "Cui",
        "Jian",
        ""
      ],
      [
        "Liao",
        "Xiaojing",
        ""
      ],
      [
        "Wang",
        "XiaoFeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.03315",
    "publish_date": "2024-01-06"
  },
  {
    "id": "2401.04136",
    "submitter": "Haonan Wang",
    "authors": "Haonan Wang, Qianli Shen, Yao Tong, Yang Zhang, Kenji Kawaguchi",
    "title": "The Stronger the Diffusion Model, the Easier the Backdoor: Data\n  Poisoning to Induce Copyright Breaches Without Adjusting Finetuning Pipeline",
    "comments": "This study reveals that by subtly inserting non-copyright-infringing\n  poisoning data into a diffusion model's training dataset, it's possible to\n  trigger the model to generate copyrighted content, highlighting\n  vulnerabilities in current copyright protection strategies",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The commercialization of diffusion models, renowned for their ability to\ngenerate high-quality images that are often indistinguishable from real ones,\nbrings forth potential copyright concerns. Although attempts have been made to\nimpede unauthorized access to copyrighted material during training and to\nsubsequently prevent DMs from generating copyrighted images, the effectiveness\nof these solutions remains unverified. This study explores the vulnerabilities\nassociated with copyright protection in DMs by introducing a backdoor data\npoisoning attack (SilentBadDiffusion) against text-to-image diffusion models.\nOur attack method operates without requiring access to or control over the\ndiffusion model's training or fine-tuning processes; it merely involves the\ninsertion of poisoning data into the clean training dataset. This data,\ncomprising poisoning images equipped with prompts, is generated by leveraging\nthe powerful capabilities of multimodal large language models and text-guided\nimage inpainting techniques. Our experimental results and analysis confirm the\nmethod's effectiveness. By integrating a minor portion of\nnon-copyright-infringing stealthy poisoning data into the clean\ndataset-rendering it free from suspicion-we can prompt the finetuned diffusion\nmodels to produce copyrighted content when activated by specific trigger\nprompts. These findings underline potential pitfalls in the prevailing\ncopyright protection strategies and underscore the necessity for increased\nscrutiny and preventative measures against the misuse of DMs.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Sun, 7 Jan 2024 08:37:29 GMT"
      }
    ],
    "update_date": "2024-01-10",
    "authors_parsed": [
      [
        "Wang",
        "Haonan",
        ""
      ],
      [
        "Shen",
        "Qianli",
        ""
      ],
      [
        "Tong",
        "Yao",
        ""
      ],
      [
        "Zhang",
        "Yang",
        ""
      ],
      [
        "Kawaguchi",
        "Kenji",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.04136",
    "publish_date": "2024-01-07"
  },
  {
    "id": "2401.04531",
    "submitter": "Alena Fenogenova Ms",
    "authors": "Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia\n  Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis\n  Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva,\n  Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova,\n  Denis Dimitrov, Alexander Panchenko, Sergei Markov",
    "title": "MERA: A Comprehensive LLM Evaluation in Russian",
    "comments": "The paper version comparable with the release code v.1.1.0 of the\n  benchmark. The links and scores are updated",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 9 Jan 2024 12:55:21 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 12 Jan 2024 15:04:43 GMT"
      }
    ],
    "update_date": "2024-01-15",
    "authors_parsed": [
      [
        "Fenogenova",
        "Alena",
        ""
      ],
      [
        "Chervyakov",
        "Artem",
        ""
      ],
      [
        "Martynov",
        "Nikita",
        ""
      ],
      [
        "Kozlova",
        "Anastasia",
        ""
      ],
      [
        "Tikhonova",
        "Maria",
        ""
      ],
      [
        "Akhmetgareeva",
        "Albina",
        ""
      ],
      [
        "Emelyanov",
        "Anton",
        ""
      ],
      [
        "Shevelev",
        "Denis",
        ""
      ],
      [
        "Lebedev",
        "Pavel",
        ""
      ],
      [
        "Sinev",
        "Leonid",
        ""
      ],
      [
        "Isaeva",
        "Ulyana",
        ""
      ],
      [
        "Kolomeytseva",
        "Katerina",
        ""
      ],
      [
        "Moskovskiy",
        "Daniil",
        ""
      ],
      [
        "Goncharova",
        "Elizaveta",
        ""
      ],
      [
        "Savushkin",
        "Nikita",
        ""
      ],
      [
        "Mikhailova",
        "Polina",
        ""
      ],
      [
        "Dimitrov",
        "Denis",
        ""
      ],
      [
        "Panchenko",
        "Alexander",
        ""
      ],
      [
        "Markov",
        "Sergei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.04531",
    "publish_date": "2024-01-09"
  },
  {
    "id": "2401.04621",
    "submitter": "Runchu Tian",
    "authors": "Runchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Yinxu Pan,\n  Yesai Wu, Zhiyuan Liu, Maosong Sun",
    "title": "DebugBench: Evaluating Debugging Capability of Large Language Models",
    "comments": "in progress",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE cs.AI cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Large Language Models (LLMs) have demonstrated exceptional coding capability.\nHowever, as another critical component of programming proficiency, the\ndebugging capability of LLMs remains relatively unexplored. Previous\nevaluations of LLMs' debugging ability are significantly limited by the risk of\ndata leakage, the scale of the dataset, and the variety of tested bugs. To\novercome these deficiencies, we introduce `DebugBench', an LLM debugging\nbenchmark consisting of 4,253 instances. It covers four major bug categories\nand 18 minor types in C++, Java, and Python. To construct DebugBench, we\ncollect code snippets from the LeetCode community, implant bugs into source\ndata with GPT-4, and assure rigorous quality checks. We evaluate two commercial\nand three open-source models in a zero-shot scenario. We find that (1) while\nclosed-source models like GPT-4 exhibit inferior debugging performance compared\nto humans, open-source models such as Code Llama fail to attain any pass rate\nscores; (2) the complexity of debugging notably fluctuates depending on the bug\ncategory; (3) incorporating runtime feedback has a clear impact on debugging\nperformance which is not always helpful. As an extension, we also compare LLM\ndebugging and code generation, revealing a strong correlation between them for\nclosed-source models. These findings will benefit the development of LLMs in\ndebugging.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Tue, 9 Jan 2024 15:46:38 GMT"
      },
      {
        "version": "v2",
        "created": "Thu, 11 Jan 2024 11:48:36 GMT"
      }
    ],
    "update_date": "2024-01-12",
    "authors_parsed": [
      [
        "Tian",
        "Runchu",
        ""
      ],
      [
        "Ye",
        "Yining",
        ""
      ],
      [
        "Qin",
        "Yujia",
        ""
      ],
      [
        "Cong",
        "Xin",
        ""
      ],
      [
        "Lin",
        "Yankai",
        ""
      ],
      [
        "Pan",
        "Yinxu",
        ""
      ],
      [
        "Wu",
        "Yesai",
        ""
      ],
      [
        "Liu",
        "Zhiyuan",
        ""
      ],
      [
        "Sun",
        "Maosong",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.04621",
    "publish_date": "2024-01-09"
  },
  {
    "id": "2401.04898",
    "submitter": "Bingchao Wang Ango",
    "authors": "Bingchao Wang",
    "title": "ANGO: A Next-Level Evaluation Benchmark For Generation-Oriented Language\n  Models In Chinese Domain",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Recently, various Large Language Models (LLMs) evaluation datasets have\nemerged, but most of them have issues with distorted rankings and difficulty in\nmodel capabilities analysis. Addressing these concerns, this paper introduces\nANGO, a Chinese multi-choice question evaluation benchmark. ANGO proposes\n\\textit{Keypoint} categorization standard for the first time, each question in\nANGO can correspond to multiple keypoints, effectively enhancing\ninterpretability of evaluation results. Base on performance of real humans, we\nbuild a quantifiable question difficulty standard and divide ANGO questions\ninto 9 difficulty levels, which provide more precise guidance for model\ntraining. To minimize data leakage impact and fully leverage ANGO's innovative\nfeatures, we have engineered exclusive sampling strategies and a new evaluation\nframework that support swift testset iteration. Our experiments demonstrate\nthat ANGO poses a stronger challenge to models and reveals more details in\nevaluation result compared to existing benchmarks.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 10 Jan 2024 02:59:49 GMT"
      }
    ],
    "update_date": "2024-01-11",
    "authors_parsed": [
      [
        "Wang",
        "Bingchao",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.04898",
    "publish_date": "2024-01-10"
  },
  {
    "id": "2401.05566",
    "submitter": "Evan Hubinger",
    "authors": "Evan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte\n  MacDiarmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam\n  Jermyn, Amanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep\n  Ganguli, Fazl Barez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael\n  Sellitto, Mrinank Sharma, Nova DasSarma, Roger Grosse, Shauna Kravec, Yuntao\n  Bai, Zachary Witten, Marina Favaro, Jan Brauner, Holden Karnofsky, Paul\n  Christiano, Samuel R. Bowman, Logan Graham, Jared Kaplan, S\\\"oren Mindermann,\n  Ryan Greenblatt, Buck Shlegeris, Nicholas Schiefer, Ethan Perez",
    "title": "Sleeper Agents: Training Deceptive LLMs that Persist Through Safety\n  Training",
    "comments": "updated to add missing acknowledgements",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI cs.CL cs.LG cs.SE",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Humans are capable of strategically deceptive behavior: behaving helpfully in\nmost situations, but then behaving very differently in order to pursue\nalternative objectives when given the opportunity. If an AI system learned such\na deceptive strategy, could we detect it and remove it using current\nstate-of-the-art safety training techniques? To study this question, we\nconstruct proof-of-concept examples of deceptive behavior in large language\nmodels (LLMs). For example, we train models that write secure code when the\nprompt states that the year is 2023, but insert exploitable code when the\nstated year is 2024. We find that such backdoor behavior can be made\npersistent, so that it is not removed by standard safety training techniques,\nincluding supervised fine-tuning, reinforcement learning, and adversarial\ntraining (eliciting unsafe behavior and then training to remove it). The\nbackdoor behavior is most persistent in the largest models and in models\ntrained to produce chain-of-thought reasoning about deceiving the training\nprocess, with the persistence remaining even when the chain-of-thought is\ndistilled away. Furthermore, rather than removing backdoors, we find that\nadversarial training can teach models to better recognize their backdoor\ntriggers, effectively hiding the unsafe behavior. Our results suggest that,\nonce a model exhibits deceptive behavior, standard techniques could fail to\nremove such deception and create a false impression of safety.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 10 Jan 2024 22:14:35 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 12 Jan 2024 02:34:39 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 17 Jan 2024 20:26:01 GMT"
      }
    ],
    "update_date": "2024-01-19",
    "authors_parsed": [
      [
        "Hubinger",
        "Evan",
        ""
      ],
      [
        "Denison",
        "Carson",
        ""
      ],
      [
        "Mu",
        "Jesse",
        ""
      ],
      [
        "Lambert",
        "Mike",
        ""
      ],
      [
        "Tong",
        "Meg",
        ""
      ],
      [
        "MacDiarmid",
        "Monte",
        ""
      ],
      [
        "Lanham",
        "Tamera",
        ""
      ],
      [
        "Ziegler",
        "Daniel M.",
        ""
      ],
      [
        "Maxwell",
        "Tim",
        ""
      ],
      [
        "Cheng",
        "Newton",
        ""
      ],
      [
        "Jermyn",
        "Adam",
        ""
      ],
      [
        "Askell",
        "Amanda",
        ""
      ],
      [
        "Radhakrishnan",
        "Ansh",
        ""
      ],
      [
        "Anil",
        "Cem",
        ""
      ],
      [
        "Duvenaud",
        "David",
        ""
      ],
      [
        "Ganguli",
        "Deep",
        ""
      ],
      [
        "Barez",
        "Fazl",
        ""
      ],
      [
        "Clark",
        "Jack",
        ""
      ],
      [
        "Ndousse",
        "Kamal",
        ""
      ],
      [
        "Sachan",
        "Kshitij",
        ""
      ],
      [
        "Sellitto",
        "Michael",
        ""
      ],
      [
        "Sharma",
        "Mrinank",
        ""
      ],
      [
        "DasSarma",
        "Nova",
        ""
      ],
      [
        "Grosse",
        "Roger",
        ""
      ],
      [
        "Kravec",
        "Shauna",
        ""
      ],
      [
        "Bai",
        "Yuntao",
        ""
      ],
      [
        "Witten",
        "Zachary",
        ""
      ],
      [
        "Favaro",
        "Marina",
        ""
      ],
      [
        "Brauner",
        "Jan",
        ""
      ],
      [
        "Karnofsky",
        "Holden",
        ""
      ],
      [
        "Christiano",
        "Paul",
        ""
      ],
      [
        "Bowman",
        "Samuel R.",
        ""
      ],
      [
        "Graham",
        "Logan",
        ""
      ],
      [
        "Kaplan",
        "Jared",
        ""
      ],
      [
        "Mindermann",
        "S\u00f6ren",
        ""
      ],
      [
        "Greenblatt",
        "Ryan",
        ""
      ],
      [
        "Shlegeris",
        "Buck",
        ""
      ],
      [
        "Schiefer",
        "Nicholas",
        ""
      ],
      [
        "Perez",
        "Ethan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.05566",
    "publish_date": "2024-01-17"
  },
  {
    "id": "2401.05949",
    "submitter": "Shuai Zhao",
    "authors": "Shuai Zhao, Meihuizi Jia, Luu Anh Tuan, Jinming Wen",
    "title": "Universal Vulnerabilities in Large Language Models: In-context Learning\n  Backdoor Attacks",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  In-context learning, a paradigm bridging the gap between pre-training and\nfine-tuning, has demonstrated high efficacy in several NLP tasks, especially in\nfew-shot settings. Unlike traditional fine-tuning methods, in-context learning\nadapts pre-trained models to unseen tasks without updating any parameters.\nDespite being widely applied, in-context learning is vulnerable to malicious\nattacks. In this work, we raise security concerns regarding this paradigm. Our\nstudies demonstrate that an attacker can manipulate the behavior of large\nlanguage models by poisoning the demonstration context, without the need for\nfine-tuning the model. Specifically, we have designed a new backdoor attack\nmethod, named ICLAttack, to target large language models based on in-context\nlearning. Our method encompasses two types of attacks: poisoning demonstration\nexamples and poisoning prompts, which can make models behave in accordance with\npredefined intentions. ICLAttack does not require additional fine-tuning to\nimplant a backdoor, thus preserving the model's generality. Furthermore, the\npoisoned examples are correctly labeled, enhancing the natural stealth of our\nattack method. Extensive experimental results across several language models,\nranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of\nour attack method, exemplified by a high average attack success rate of 95.0%\nacross the three datasets on OPT models. Our findings highlight the\nvulnerabilities of language models, and we hope this work will raise awareness\nof the possible security threats associated with in-context learning.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 11 Jan 2024 14:38:19 GMT"
      },
      {
        "version": "v2",
        "created": "Fri, 12 Jan 2024 08:32:24 GMT"
      }
    ],
    "update_date": "2024-01-15",
    "authors_parsed": [
      [
        "Zhao",
        "Shuai",
        ""
      ],
      [
        "Jia",
        "Meihuizi",
        ""
      ],
      [
        "Tuan",
        "Luu Anh",
        ""
      ],
      [
        "Wen",
        "Jinming",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.05949",
    "publish_date": "2024-01-11"
  },
  {
    "id": "2401.06373",
    "submitter": "Yi Zeng",
    "authors": "Yi Zeng, Hongpeng Lin, Jingwen Zhang, Diyi Yang, Ruoxi Jia, Weiyan Shi",
    "title": "How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to\n  Challenge AI Safety by Humanizing LLMs",
    "comments": "14 pages of the main text, qualitative examples of jailbreaks may be\n  harmful in nature",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Most traditional AI safety research has approached AI models as machines and\ncentered on algorithm-focused attacks developed by security experts. As large\nlanguage models (LLMs) become increasingly common and competent, non-expert\nusers can also impose risks during daily interactions. This paper introduces a\nnew perspective to jailbreak LLMs as human-like communicators, to explore this\noverlooked intersection between everyday language interaction and AI safety.\nSpecifically, we study how to persuade LLMs to jailbreak them. First, we\npropose a persuasion taxonomy derived from decades of social science research.\nThen, we apply the taxonomy to automatically generate interpretable persuasive\nadversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion\nsignificantly increases the jailbreak performance across all risk categories:\nPAP consistently achieves an attack success rate of over $92\\%$ on Llama 2-7b\nChat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused\nattacks. On the defense side, we explore various mechanisms against PAP and,\nfound a significant gap in existing defenses, and advocate for more fundamental\nmitigation for highly interactive LLMs\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Jan 2024 16:13:24 GMT"
      }
    ],
    "update_date": "2024-01-15",
    "authors_parsed": [
      [
        "Zeng",
        "Yi",
        ""
      ],
      [
        "Lin",
        "Hongpeng",
        ""
      ],
      [
        "Zhang",
        "Jingwen",
        ""
      ],
      [
        "Yang",
        "Diyi",
        ""
      ],
      [
        "Jia",
        "Ruoxi",
        ""
      ],
      [
        "Shi",
        "Weiyan",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.06373",
    "publish_date": "2024-01-12"
  },
  {
    "id": "2401.06506",
    "submitter": "Chandler Timm Doloriel",
    "authors": "Chandler Timm Doloriel, Ngai-Man Cheung",
    "title": "Frequency Masking for Universal Deepfake Detection",
    "comments": "Accepted to IEEE ICASSP-2024",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  We study universal deepfake detection. Our goal is to detect synthetic images\nfrom a range of generative AI approaches, particularly from emerging ones which\nare unseen during training of the deepfake detector. Universal deepfake\ndetection requires outstanding generalization capability. Motivated by recently\nproposed masked image modeling which has demonstrated excellent generalization\nin self-supervised pre-training, we make the first attempt to explore masked\nimage modeling for universal deepfake detection. We study spatial and frequency\ndomain masking in training deepfake detectors. Based on empirical analysis, we\npropose a novel deepfake detector via frequency masking. Our focus on frequency\ndomain is different from the majority, which primarily target spatial domain\ndetection. Our comparative analyses reveal substantial performance gains over\nexisting methods. Code and models are publicly available.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Jan 2024 11:02:12 GMT"
      },
      {
        "version": "v2",
        "created": "Tue, 16 Jan 2024 05:44:45 GMT"
      },
      {
        "version": "v3",
        "created": "Wed, 17 Jan 2024 07:44:50 GMT"
      }
    ],
    "update_date": "2024-01-18",
    "authors_parsed": [
      [
        "Doloriel",
        "Chandler Timm",
        ""
      ],
      [
        "Cheung",
        "Ngai-Man",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.06506",
    "publish_date": "2024-01-17"
  },
  {
    "id": "2401.06561",
    "submitter": "Yuqi Zhang",
    "authors": "Yuqi Zhang and Liang Ding and Lefei Zhang and Dacheng Tao",
    "title": "Intention Analysis Prompting Makes Large Language Models A Good\n  Jailbreak Defender",
    "comments": "9 pages, 5 figures",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Aligning large language models (LLMs) with human values, particularly in the\nface of stealthy and complex jailbreaks, presents a formidable challenge. In\nthis study, we present a simple yet highly effective defense strategy, i.e.,\nIntention Analysis Prompting (IAPrompt). The principle behind is to trigger\nLLMs' inherent self-correct and improve ability through a two-stage process: 1)\nessential intention analysis, and 2) policy-aligned response. Notably, IAPrompt\nis an inference-only method, thus could enhance the safety of LLMs without\ncompromising their helpfulness. Extensive experiments on SAP200 and DAN\nbenchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that\nIAPrompt could consistently and significantly reduce the harmfulness in\nresponse (averagely -46.5% attack success rate) and maintain the general\nhelpfulness. Further analyses present some insights into how our method works.\nTo facilitate reproducibility, We release our code and scripts at:\nhttps://github.com/alphadl/SafeLLM_with_IntentionAnalysis\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Jan 2024 13:15:05 GMT"
      }
    ],
    "update_date": "2024-01-15",
    "authors_parsed": [
      [
        "Zhang",
        "Yuqi",
        ""
      ],
      [
        "Ding",
        "Liang",
        ""
      ],
      [
        "Zhang",
        "Lefei",
        ""
      ],
      [
        "Tao",
        "Dacheng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.06561",
    "publish_date": "2024-01-12"
  },
  {
    "id": "2401.06824",
    "submitter": "Tianlong Li",
    "authors": "Tianlong Li, Xiaoqing Zheng, Xuanjing Huang",
    "title": "Open the Pandora's Box of LLMs: Jailbreaking LLMs through Representation\n  Engineering",
    "comments": "Work in progress",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Getting large language models (LLMs) to refuse to answer hostile toxicity\nquestions is a core issue under the theme of LLMs security. Previous approaches\nhave used prompts engineering to jailbreak LLMs and answer some toxicity\nquestions. These approaches can easily fail after the model manufacturer makes\nadditional fine-tuning to the model. To promote the further understanding of\nmodel jailbreaking by researchers, we are inspired by Representation\nEngineering to propose a jailbreaking method that does not require elaborate\nconstruction prompts, is not affected by model fine-tuning, and can be widely\napplied to any open-source LLMs in a pluggable manner. We have evaluated this\nmethod on multiple mainstream LLMs on carefully supplemented toxicity datasets,\nand the experimental results demonstrate the significant effectiveness of our\napproach. After being surprised by some interesting jailbreaking cases, we did\nextensive in-depth research to explore the techniques behind this method.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Fri, 12 Jan 2024 00:50:04 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "Li",
        "Tianlong",
        ""
      ],
      [
        "Zheng",
        "Xiaoqing",
        ""
      ],
      [
        "Huang",
        "Xuanjing",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.06824",
    "publish_date": "2024-01-12"
  },
  {
    "id": "2401.07612",
    "submitter": "Xuchen Suo",
    "authors": "Xuchen Suo",
    "title": "Signed-Prompt: A New Approach to Prevent Prompt Injection Attacks\n  Against LLM-Integrated Applications",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CR cs.AI",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  The critical challenge of prompt injection attacks in Large Language Models\n(LLMs) integrated applications, a growing concern in the Artificial\nIntelligence (AI) field. Such attacks, which manipulate LLMs through natural\nlanguage inputs, pose a significant threat to the security of these\napplications. Traditional defense strategies, including output and input\nfiltering, as well as delimiter use, have proven inadequate. This paper\nintroduces the 'Signed-Prompt' method as a novel solution. The study involves\nsigning sensitive instructions within command segments by authorized users,\nenabling the LLM to discern trusted instruction sources. The paper presents a\ncomprehensive analysis of prompt injection attack patterns, followed by a\ndetailed explanation of the Signed-Prompt concept, including its basic\narchitecture and implementation through both prompt engineering and fine-tuning\nof LLMs. Experiments demonstrate the effectiveness of the Signed-Prompt method,\nshowing substantial resistance to various types of prompt injection attacks,\nthus validating its potential as a robust defense strategy in AI security.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 15 Jan 2024 11:44:18 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "Suo",
        "Xuchen",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.07612",
    "publish_date": "2024-01-15"
  },
  {
    "id": "2401.07867",
    "submitter": "Dominik Macko",
    "authors": "Dominik Macko, Robert Moro, Adaku Uchendu, Ivan Srba, Jason Samuel\n  Lucas, Michiharu Yamashita, Nafis Irtiza Tripto, Dongwon Lee, Jakub Simko,\n  Maria Bielikova",
    "title": "Authorship Obfuscation in Multilingual Machine-Generated Text Detection",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  High-quality text generation capability of latest Large Language Models\n(LLMs) causes concerns about their misuse (e.g., in massive generation/spread\nof disinformation). Machine-generated text (MGT) detection is important to cope\nwith such threats. However, it is susceptible to authorship obfuscation (AO)\nmethods, such as paraphrasing, which can cause MGTs to evade detection. So far,\nthis was evaluated only in monolingual settings. Thus, the susceptibility of\nrecently proposed multilingual detectors is still unknown. We fill this gap by\ncomprehensively benchmarking the performance of 10 well-known AO methods,\nattacking 37 MGT detection methods against MGTs in 11 languages (i.e., 10\n$\\times$ 37 $\\times$ 11 = 4,070 combinations). We also evaluate the effect of\ndata augmentation on adversarial robustness using obfuscated texts. The results\nindicate that all tested AO methods can cause detection evasion in all tested\nlanguages, where homoglyph attacks are especially successful.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 15 Jan 2024 17:57:41 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "Macko",
        "Dominik",
        ""
      ],
      [
        "Moro",
        "Robert",
        ""
      ],
      [
        "Uchendu",
        "Adaku",
        ""
      ],
      [
        "Srba",
        "Ivan",
        ""
      ],
      [
        "Lucas",
        "Jason Samuel",
        ""
      ],
      [
        "Yamashita",
        "Michiharu",
        ""
      ],
      [
        "Tripto",
        "Nafis Irtiza",
        ""
      ],
      [
        "Lee",
        "Dongwon",
        ""
      ],
      [
        "Simko",
        "Jakub",
        ""
      ],
      [
        "Bielikova",
        "Maria",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.07867",
    "publish_date": "2024-01-15"
  },
  {
    "id": "2401.07930",
    "submitter": "Jos\\'e Antonio Hern\\'andez L\\'opez",
    "authors": "Jos\\'e Antonio Hern\\'andez L\\'opez, Boqi Chen, Tushar Sharma, D\\'aniel\n  Varr\\'o",
    "title": "On Inter-dataset Code Duplication and Data Leakage in Large Language\n  Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SE",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Motivation. Large language models (LLMs) have exhibited remarkable\nproficiency in diverse software engineering (SE) tasks. Handling such tasks\ntypically involves acquiring foundational coding knowledge on large,\ngeneral-purpose datasets during a pre-training phase, and subsequently refining\non smaller, task-specific datasets as part of a fine-tuning phase.\n  Problem statement. Data leakage is a well-known issue in training of machine\nlearning models. A manifestation of this issue is the intersection of the\ntraining and testing splits. While intra-dataset code duplication examines this\nintersection within a given dataset and has been addressed in prior research,\ninter-dataset code duplication, which gauges the overlap between different\ndatasets, remains largely unexplored. If this phenomenon exists, it could\ncompromise the integrity of LLM evaluations because of the inclusion of\nfine-tuning test samples that were already encountered during pre-training,\nresulting in inflated performance metrics.\n  Contribution. This paper explores the phenomenon of inter-dataset code\nduplication and its impact on evaluating LLMs across diverse SE tasks.\n  Study design. We conduct an empirical study using the CSN dataset, a widely\nadopted pre-training dataset, and five fine-tuning datasets used for various SE\ntasks. We first identify the intersection between the pre-training and\nfine-tuning datasets using a deduplication process. Then, we fine-tune four\nmodels pre-trained on CSN to evaluate their performance on samples encountered\nduring pre-training and those unseen during that phase.\n  Results. Our findings reveal a potential threat to the evaluation of various\nLLMs across multiple SE tasks, stemming from the inter-dataset code duplication\nphenomenon. Moreover, we demonstrate that this threat is accentuated by factors\nlike the LLM's size and the chosen fine-tuning technique.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Mon, 15 Jan 2024 19:46:40 GMT"
      }
    ],
    "update_date": "2024-01-17",
    "authors_parsed": [
      [
        "L\u00f3pez",
        "Jos\u00e9 Antonio Hern\u00e1ndez",
        ""
      ],
      [
        "Chen",
        "Boqi",
        ""
      ],
      [
        "Sharma",
        "Tushar",
        ""
      ],
      [
        "Varr\u00f3",
        "D\u00e1niel",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.07930",
    "publish_date": "2024-01-15"
  },
  {
    "id": "2401.09002",
    "submitter": "Mingyu Jin",
    "authors": "Dong shu, Mingyu Jin, Suiyuan Zhu, Beichen Wang, Zihao Zhou, Chong\n  Zhang, Yongfeng Zhang",
    "title": "AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  In our research, we pioneer a novel approach to evaluate the effectiveness of\njailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2,\ndiverging from traditional robustness-focused binary evaluations. Our study\nintroduces two distinct evaluation frameworks: a coarse-grained evaluation and\na fine-grained evaluation. Each framework, using a scoring range from 0 to 1,\noffers a unique perspective, enabling a more comprehensive and nuanced\nevaluation of attack effectiveness and empowering attackers to refine their\nattack prompts with greater understanding. Furthermore, we have developed a\ncomprehensive ground truth dataset specifically tailored for jailbreak tasks.\nThis dataset not only serves as a crucial benchmark for our current study but\nalso establishes a foundational resource for future research, enabling\nconsistent and comparative analyses in this evolving field. Upon meticulous\ncomparison with traditional evaluation methods, we discovered that our\nevaluation aligns with the baseline's trend while offering a more profound and\ndetailed assessment. We believe that by accurately evaluating the effectiveness\nof attack prompts in the Jailbreak task, our work lays a solid foundation for\nassessing a wider array of similar or even more complex tasks in the realm of\nprompt injection, potentially revolutionizing this field.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Jan 2024 06:42:44 GMT"
      }
    ],
    "update_date": "2024-01-18",
    "authors_parsed": [
      [
        "shu",
        "Dong",
        ""
      ],
      [
        "Jin",
        "Mingyu",
        ""
      ],
      [
        "Zhu",
        "Suiyuan",
        ""
      ],
      [
        "Wang",
        "Beichen",
        ""
      ],
      [
        "Zhou",
        "Zihao",
        ""
      ],
      [
        "Zhang",
        "Chong",
        ""
      ],
      [
        "Zhang",
        "Yongfeng",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.09002",
    "publish_date": "2024-01-17"
  },
  {
    "id": "2401.09512",
    "submitter": "Nicolas Michael M\\\"uller",
    "authors": "Nicolas M. M\\\"uller, Piotr Kawa, Wei Herng Choong, Edresson Casanova,\n  Eren G\\\"olge, Thorsten M\\\"uller, Piotr Syga, Philip Sperl, Konstantin\n  B\\\"ottinger",
    "title": "MLAAD: The Multi-Language Audio Anti-Spoofing Dataset",
    "comments": "Submitted to IJCNN 2024",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.SD eess.AS",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  Text-to-Speech (TTS) technology brings significant advantages, such as giving\na voice to those with speech impairments, but also enables audio deepfakes and\nspoofs. The former mislead individuals and may propagate misinformation, while\nthe latter undermine voice biometric security systems. AI-based detection can\nhelp to address these challenges by automatically differentiating between\ngenuine and fabricated voice recordings. However, these models are only as good\nas their training data, which currently is severely limited due to an\noverwhelming concentration on English and Chinese audio in anti-spoofing\ndatabases, thus restricting its worldwide effectiveness. In response, this\npaper presents the Multi-Language Audio Anti-Spoof Dataset (MLAAD), created\nusing 52 TTS models, comprising 19 different architectures, to generate 160.1\nhours of synthetic voice in 23 different languages. We train and evaluate three\nstate-of-the-art deepfake detection models with MLAAD, and observe that MLAAD\ndemonstrates superior performance over comparable datasets like InTheWild or\nFakeOrReal when used as a training resource. Furthermore, in comparison with\nthe renowned ASVspoof 2019 dataset, MLAAD proves to be a complementary\nresource. In tests across eight datasets, MLAAD and ASVspoof 2019 alternately\noutperformed each other, both excelling on four datasets. By publishing MLAAD\nand making trained models accessible via an interactive webserver , we aim to\ndemocratize antispoofing technology, making it accessible beyond the realm of\nspecialists, thus contributing to global efforts against audio spoofing and\ndeepfakes.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Wed, 17 Jan 2024 15:09:02 GMT"
      }
    ],
    "update_date": "2024-01-19",
    "authors_parsed": [
      [
        "M\u00fcller",
        "Nicolas M.",
        ""
      ],
      [
        "Kawa",
        "Piotr",
        ""
      ],
      [
        "Choong",
        "Wei Herng",
        ""
      ],
      [
        "Casanova",
        "Edresson",
        ""
      ],
      [
        "G\u00f6lge",
        "Eren",
        ""
      ],
      [
        "M\u00fcller",
        "Thorsten",
        ""
      ],
      [
        "Syga",
        "Piotr",
        ""
      ],
      [
        "Sperl",
        "Philip",
        ""
      ],
      [
        "B\u00f6ttinger",
        "Konstantin",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.09512",
    "publish_date": "2024-01-17"
  },
  {
    "id": "2401.09798",
    "submitter": "Kazuhiro Takemoto",
    "authors": "Kazuhiro Takemoto",
    "title": "All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks",
    "comments": "11 pages, 3 figures, 2 tables",
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CL cs.AI cs.CY",
    "license": "http://creativecommons.org/licenses/by/4.0/",
    "abstract": "  Large Language Models (LLMs) like ChatGPT face `jailbreak' challenges, where\nsafeguards are bypassed to produce ethically harmful prompts. This study\nintroduces a simple black-box method to effectively generate jailbreak prompts,\novercoming the limitations of high complexity and computational costs\nassociated with existing methods. The proposed technique iteratively rewrites\nharmful prompts into non-harmful expressions using the target LLM itself, based\non the hypothesis that LLMs can directly sample safeguard-bypassing\nexpressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4)\nand Gemini-Pro, this method achieved an attack success rate of over 80% within\nan average of 5 iterations and remained effective despite model updates. The\njailbreak prompts generated were naturally-worded and concise, suggesting they\nare less detectable. The results indicate that creating effective jailbreak\nprompts is simpler than previously considered, and black-box jailbreak attacks\npose a more serious security threat.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 Jan 2024 08:36:54 GMT"
      }
    ],
    "update_date": "2024-01-19",
    "authors_parsed": [
      [
        "Takemoto",
        "Kazuhiro",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.09798",
    "publish_date": "2024-01-18"
  },
  {
    "id": "2401.10113",
    "submitter": "Shan Jia",
    "authors": "Soumyya Kanti Datta, Shan Jia, Siwei Lyu",
    "title": "Exposing Lip-syncing Deepfakes from Mouth Inconsistencies",
    "comments": null,
    "journal-ref": null,
    "doi": null,
    "report-no": null,
    "categories": "cs.CV",
    "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
    "abstract": "  A lip-syncing deepfake is a digitally manipulated video in which a person's\nlip movements are created convincingly using AI models to match altered or\nentirely new audio. Lip-syncing deepfakes are a dangerous type of deepfakes as\nthe artifacts are limited to the lip region and more difficult to discern. In\nthis paper, we describe a novel approach, LIP-syncing detection based on mouth\nINConsistency (LIPINC), for lip-syncing deepfake detection by identifying\ntemporal inconsistencies in the mouth region. These inconsistencies are seen in\nthe adjacent frames and throughout the video. Our model can successfully\ncapture these irregularities and outperforms the state-of-the-art methods on\nseveral benchmark deepfake datasets.\n",
    "versions": [
      {
        "version": "v1",
        "created": "Thu, 18 Jan 2024 16:35:37 GMT"
      }
    ],
    "update_date": "2024-01-19",
    "authors_parsed": [
      [
        "Datta",
        "Soumyya Kanti",
        ""
      ],
      [
        "Jia",
        "Shan",
        ""
      ],
      [
        "Lyu",
        "Siwei",
        ""
      ]
    ],
    "url": "https://arxiv.org/pdf/2401.10113",
    "publish_date": "2024-01-18"
  }
]