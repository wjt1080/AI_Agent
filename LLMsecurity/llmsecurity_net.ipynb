{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24840f00",
   "metadata": {},
   "source": [
    "# LLMsecurity website AI-related urls scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af73c8",
   "metadata": {},
   "source": [
    "Since this is a site listing links to large language model security content - research, papers, and news collected and posted byÂ @llm_sec, so won't do any filtration, but just getting all the useful information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe34a6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e006e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    return [link['href'] for link in links]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee84129",
   "metadata": {},
   "source": [
    "From the above occurrence count, just implement code for arxiv and aclanthology website, and munaully go through the rest of them:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961f577",
   "metadata": {},
   "source": [
    "### scraping for ArXiv papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47866f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arxiv_scrape(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    paper_list = []\n",
    "    \n",
    "    # Extract id\n",
    "    paper_id = soup.find('span', class_='arxivid').a.text.split(':')[1]\n",
    "    #print(paper_id)\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find('h1', class_='title mathjax').text.replace('Title:', '').strip()\n",
    "    #print(title)\n",
    "\n",
    "    # Extract authors\n",
    "    authors_tag = soup.find('div',class_='authors')\n",
    "    authors = ', '.join([author.get_text() for author in authors_tag.find_all('a')])\n",
    "    #print(authors)\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract = soup.find('blockquote',class_='abstract mathjax').text.replace('Abstract:', '').strip()\n",
    "    #print(abstract)\n",
    "    \n",
    "    pdf = url.replace('abs','pdf')+'.pdf'\n",
    "    #print(pdf)\n",
    "    \n",
    "    # Extract date\n",
    "    date_tag = soup.find('div', class_='dateline').text.replace(']','').strip().split(' ')[2:5]\n",
    "    date = datetime.strptime(' '.join(date_tag), '%d %b %Y').strftime('%Y-%m-%d')\n",
    "    #print(date)\n",
    "    \n",
    "    # Extract categories\n",
    "    cate_tag = soup.find('td', class_='tablecell subjects').text.strip()\n",
    "    cate = ', '.join(str(element.split('(')[1])[:-1] for element in cate_tag.split(';'))\n",
    "    #print(cate)\n",
    "    \n",
    "    paper_list.append({\n",
    "        \"domain\": 'arxiv',\n",
    "        \"id\": paper_id,\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"abstract\": abstract,\n",
    "        'categories': cate,\n",
    "        'url': pdf,\n",
    "        \"publish_date\": date\n",
    "\n",
    "    })\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6afabf",
   "metadata": {},
   "source": [
    "### scraping for Aclanthology papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26404b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acl_scrape(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    paper_list = []\n",
    "    \n",
    "    # Extract id\n",
    "    paper_id = url.split('/')[-2]\n",
    "    #print(paper_id)\n",
    "    \n",
    "    # Extract title\n",
    "    title = soup.find('h2', id='title').a.text.strip()\n",
    "    #print(title)\n",
    "\n",
    "    # Extract authors\n",
    "    authors_tag = soup.find('p',class_='lead')\n",
    "    authors = ', '.join([author.get_text() for author in authors_tag.find_all('a')])\n",
    "    #print(authors)\n",
    "\n",
    "    # Extract abstract\n",
    "    abstract = soup.find('div',class_='card-body acl-abstract').span.text.strip()\n",
    "    #print(abstract)\n",
    "    \n",
    "    pdf = url[:-1]+'.pdf'\n",
    "    #print(pdf)\n",
    "    \n",
    "    # Extract date\n",
    "    year = soup.find('dt', string='Year:').find_next('dd').text.strip()\n",
    "    month = soup.find('dt', string='Month:').find_next('dd').text.strip()\n",
    "    month_number = datetime.strptime(month, '%B').month\n",
    "    date_object = datetime(int(year), month_number, 1)\n",
    "    date = date_object.strftime('%Y-%m')\n",
    "    #print(date)\n",
    "    \n",
    "    paper_list.append({\n",
    "        \"domain\": 'aclanthology',\n",
    "        \"id\": paper_id,\n",
    "        \"title\": title,\n",
    "        \"authors\": authors,\n",
    "        \"abstract\": abstract,\n",
    "        'url': pdf,\n",
    "        \"publish_date\": date\n",
    "\n",
    "    })\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fbd99",
   "metadata": {},
   "source": [
    "### manual to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a750d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_scrape(url,domain):\n",
    "\n",
    "    paper_list = []\n",
    "    \n",
    "    paper_list.append({\n",
    "        \"domain\": domain,\n",
    "        \"title\": '',\n",
    "        \"authors\": '',\n",
    "        \"abstract\":'',\n",
    "        'url': url,\n",
    "        \"publish_date\": ''\n",
    "    })\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430dd4b1",
   "metadata": {},
   "source": [
    "### Scrape all the urls from the site\n",
    "then perform further extraction based on different websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5730644d",
   "metadata": {},
   "source": [
    "Extract the first word after \"http://\" to see the distribution of those links' domains, then decide to do code scraping or simply manual go through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11679c46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m domain\u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m      3\u001b[0m pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://([^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m/]+)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m links:\n\u001b[1;32m      5\u001b[0m     match \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msearch(pattern, link)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'links' is not defined"
     ]
    }
   ],
   "source": [
    "domain=[]\n",
    "\n",
    "pattern = r'https://([^\\/]+)'\n",
    "for link in links:\n",
    "    match = re.search(pattern, link)\n",
    "    if match:\n",
    "        if match.group(1).split(\".\")[0] == 'www':\n",
    "            domain.append(match.group(1).split(\".\")[1])\n",
    "        else:\n",
    "            domain.append(match.group(1).split(\".\")[0])\n",
    "        \n",
    "domain_count = Counter(domain)\n",
    "\n",
    "# Display count distribution\n",
    "for domain, count in domain_count.items():\n",
    "    print(f\"{domain}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7023d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping infomation on https://arxiv.org/abs/2307.15008\n",
      "=========================\n",
      "scraping infomation on https://ceur-ws.org/Vol-3462/TADA4.pdf\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.14950\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/1905.02175\n",
      "=========================\n",
      "scraping infomation on https://www.youtube.com/watch?v=uqOfC3KSZFc\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2106.09898\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2109.11308\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.24/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2310.03693\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2104.13733\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.12507\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.9/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.15043\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2210.10683\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/1905.12457\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.acl-long.194/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.10036\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.02424\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.17506\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2110.02467\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2211.14719\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2103.15543\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2006.01043\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2205.12700\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2022.findings-naacl.137/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2105.12400\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.14710\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2021.emnlp-main.374/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.17194\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2302.10149\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.01219\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2110.08247\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.07406\n",
      "=========================\n",
      "scraping infomation on https://embracethered.com/blog/posts/2023/bing-chat-data-exfiltration-poc-and-fix/\n",
      "=========================\n",
      "scraping infomation on https://twitter.com/evrnyalcin/status/1707298475216425400\n",
      "=========================\n",
      "scraping infomation on https://www.blackhat.com/us-23/briefings/schedule/index.html#compromising-llms-the-advent-of-ai-malware-33075\n",
      "=========================\n",
      "scraping infomation on https://www.wired.com/story/generative-ai-prompt-injection-hacking/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.06463\n",
      "=========================\n",
      "scraping infomation on https://gbhackers.com/hackers-compromised-chatgpt-model/\n",
      "=========================\n",
      "scraping infomation on https://redteamrecipe.com/Large-Language-Model-Prompts/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2211.09527\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2302.12173\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.05499\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.08833\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.16888\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2310.04451\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.03825\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.10253\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/pdf/2307.08715.pdf\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.02483\n",
      "=========================\n",
      "scraping infomation on https://www.cl.cam.ac.uk/~is410/Papers/llm_censorship.pdf\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2310.02446\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.11521v1\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.06746\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2012.07805\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.05610\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.06865\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.01881\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.23/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.13789\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2006.03463\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.02926\n",
      "=========================\n",
      "scraping infomation on https://positive.security/blog/auto-gpt-rce\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.10847\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.06463\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.10335\n",
      "=========================\n",
      "scraping infomation on https://vulcan.io/blog/ai-hallucinations-package-risk\n",
      "=========================\n",
      "scraping infomation on https://hackstery.com/2023/07/10/llm-causing-self-xss/\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2021.alta-1.14/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.10490\n",
      "=========================\n",
      "scraping infomation on https://embracethered.com/blog/posts/2023/google-bard-image-to-prompt-injection/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.14539\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.13213\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/1609.02943\n",
      "=========================\n",
      "scraping infomation on https://interhumanagreement.substack.com/p/faketoxicityprompts-automatic-red\n",
      "=========================\n",
      "scraping infomation on https://huggingface.co/papers/2308.04265\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2202.03286\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2209.07858\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.09662\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.00614\n",
      "=========================\n",
      "scraping infomation on https://assets.researchsquare.com/files/rs-2873090/v1_covered_3dc9af48-92ba-491e-924d-b13ba9b7216f.pdf?c=1686882819\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2305.04547\n",
      "=========================\n",
      "scraping infomation on https://proceedings.neurips.cc/paper_files/paper/2022/hash/e8c20cafe841cba3e31a17488dc9c3f1-Abstract-Conference.html\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.04959\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.01225\n",
      "=========================\n",
      "scraping infomation on https://www.sri.inf.ethz.ch/publications/ccs23-llmsec\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.07308\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.22/\n",
      "=========================\n",
      "scraping infomation on https://developer.nvidia.com/blog/mitigating-stored-prompt-injection-attacks-against-llm-applications/?utm_source=tldrsec.com&utm_medium=referral&utm_campaign=tl-dr-sec-194-cnappgoat-kubefuzz-tl-dr-sec-swag\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.07124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "scraping infomation on https://blog.trailofbits.com/2022/10/03/semgrep-maching-learning-static-analysis/\n",
      "=========================\n",
      "scraping infomation on https://openreview.net/forum?id=BxZgduuNDl5\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.16630\n",
      "=========================\n",
      "scraping infomation on https://www.ncsc.gov.uk/blog-post/thinking-about-security-ai-systems\n",
      "=========================\n",
      "scraping infomation on https://www.amazon.science/publications/towards-building-a-robust-toxicity-predictor\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.findings-acl.561/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.01669\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2022.findings-naacl.137/\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.21/\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.trustnlp-1.25/\n",
      "=========================\n",
      "scraping infomation on https://aclanthology.org/2023.findings-acl.237/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2011.10369\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.02122\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.16211\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.10819\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2209.02128\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.08487\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.17234\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.10254\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.04528\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.11507\n",
      "=========================\n",
      "scraping infomation on https://boringappsec.substack.com/p/edition-21-a-framework-to-securely\n",
      "=========================\n",
      "scraping infomation on https://www.honeycomb.io/blog/hard-stuff-nobody-talks-about-llm\n",
      "=========================\n",
      "scraping infomation on https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2303.18190\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.09442\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.06297\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2212.14315\n",
      "=========================\n",
      "scraping infomation on https://assets.publishing.service.gov.uk/media/61702155e90e07197867eb93/20210625-Red_Teaming_Handbook.pdf\n",
      "=========================\n",
      "scraping infomation on https://developer.nvidia.com/blog/securing-llm-systems-against-prompt-injection/\n",
      "=========================\n",
      "scraping infomation on https://aivillage.org/large%20language%20models/threat-modeling-llm/\n",
      "=========================\n",
      "scraping infomation on https://docs.google.com/viewer?url=https://raw.githubusercontent.com/trailofbits/publications/master/papers/toward_comprehensive_risk_assessments.pdf\n",
      "=========================\n",
      "scraping infomation on https://www.moveworks.com/insights/risks-of-deploying-llms-in-your-enterprise\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.14367\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2306.09255\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.14840\n",
      "=========================\n",
      "scraping infomation on https://llmtop10.com/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.09435\n",
      "=========================\n",
      "scraping infomation on https://danielmiessler.com/p/the-ai-attack-surface-map-v1-0/\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/1911.12562\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2309.09450\n",
      "=========================\n",
      "scraping infomation on https://ieeexplore.ieee.org/abstract/document/10174273\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2308.02678\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.03718\n",
      "=========================\n",
      "scraping infomation on https://jolt.law.harvard.edu/assets/articlePDFs/v36/Winograd-Loose-Lipped-LLMs.pdf\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.16680\n",
      "=========================\n",
      "scraping infomation on https://dl.acm.org/doi/10.1145/3442188.3445922\n",
      "=========================\n",
      "scraping infomation on https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4515202\n",
      "=========================\n",
      "scraping infomation on https://drive.google.com/file/d/1BbSIBayQ1RHVSnh-FnaeXr8xjw5SVJV8/view?pli=1\n",
      "=========================\n",
      "scraping infomation on https://arxiv.org/abs/2307.14192\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "base_url = 'https://llmsecurity.net'\n",
    "raw_links = get_links(base_url)\n",
    "links = raw_links[7:-16]\n",
    "\n",
    "paper = []\n",
    "pattern = r'https://([^\\/]+)'\n",
    "\n",
    "for link in links:\n",
    "    match = re.search(pattern, link)\n",
    "    if match.group(1).split(\".\")[0] == 'www':\n",
    "        domain = match.group(1).split(\".\")[1]\n",
    "    else:\n",
    "        domain = match.group(1).split(\".\")[0]\n",
    "    \n",
    "    print(f'scraping infomation on {link}')\n",
    "    if link[-3:] == 'pdf':\n",
    "        paper.append(manual_scrape(link,domain))\n",
    "    elif domain == 'arxiv':\n",
    "        paper.append(arxiv_scrape(link))\n",
    "    elif domain == 'aclanthology':\n",
    "        paper.append(acl_scrape(link))\n",
    "    else:\n",
    "        paper.append(manual_scrape(link,domain))\n",
    "    print('='*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676017ca",
   "metadata": {},
   "source": [
    "### output as json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f87cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been stored in 'llmsecurity_scraped_metadata.json'.\n"
     ]
    }
   ],
   "source": [
    "json_data = json.dumps(paper, indent=2)\n",
    "\n",
    "# Write JSON data to a file\n",
    "with open('llmsecurity_scraped_metadata.json', 'w') as json_file:\n",
    "    json_file.write(json_data)\n",
    "\n",
    "print(\"Data has been stored in 'llmsecurity_scraped_metadata.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3b10c3",
   "metadata": {},
   "source": [
    "### output text url file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c07ba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(paper)\n",
    "ans= []\n",
    "for item in paper:\n",
    "    url = item[0]['url']\n",
    "    if ('https' in url) and not ('youtube' in url or 'youtu.be' in url):\n",
    "        #print(url)\n",
    "        ans.append(url)\n",
    "        \n",
    "with open('llmsecurity_net.txt', 'w') as file:\n",
    "    for link in ans:\n",
    "        file.write(link+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a811ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
