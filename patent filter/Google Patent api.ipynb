{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2bd60979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from serpapi import GoogleSearch\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "843b1aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_meta(mode,start_date,end_date,data):\n",
    "    if not os.path.exists('raw_metadata'):\n",
    "        os.makedirs('raw_metadata')\n",
    "\n",
    "    s_label = start_date[:4] if mode == \"previous\" else start_date\n",
    "    e_label = end_date[:4] if mode == \"previous\" else end_date\n",
    "\n",
    "    metadata_filename = f'raw_metadata/raw_metadata_{s_label}_{e_label}.json'\n",
    "    with open(metadata_filename, 'w') as metadata_file:\n",
    "        json.dump(data, metadata_file, indent=4)\n",
    "    print(f'Raw metadata saved to {metadata_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db67d742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_extracted_data(mode,start_date,end_date,data):\n",
    "    \n",
    "    if not os.path.exists('extracted_data'):\n",
    "        os.makedirs('extracted_data')\n",
    "    \n",
    "    s_label = start_date[:4] if mode == \"previous\" else start_date\n",
    "    e_label = end_date[:4] if mode == \"previous\" else end_date\n",
    "    \n",
    "    extracted_filename = f'extracted_data/extracted_data_{s_label}_{e_label}.json'\n",
    "    with open(extracted_filename, 'w') as extracted_file:\n",
    "        json.dump(data, extracted_file, indent=4)\n",
    "    print(f'Extracted data saved to {extracted_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ca6c4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_para(mode,query,start_date,end_date,page_num):\n",
    "\n",
    "    if mode == \"autorun\":\n",
    "        #cover date for autorun\n",
    "        yesterday = datetime.now() - timedelta(days=1)\n",
    "        start_date = yesterday.strftime(\"%Y%m%d\")\n",
    "        end_date = yesterday.strftime(\"%Y%m%d\")\n",
    "        \n",
    "    API_KEY = os.getenv(\"SERPAPI_KEY\")\n",
    "    \n",
    "    if API_KEY is None:\n",
    "        print(\"Please set the SERPAPI_KEY environment variable.\")\n",
    "    \n",
    "    params = {\n",
    "        \"engine\": \"google_patents\",\n",
    "        \"q\": query,\n",
    "        \"num\": \"100\",\n",
    "        \"page\": page_num,\n",
    "        \"after\": f\"priority:{start_date}\",\n",
    "        \"before\": f\"priority:{end_date}\",\n",
    "        \"api_key\": API_KEY\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7b7e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_patent(mode,q,start_d,end_d):\n",
    "    \n",
    "    if mode not in [\"previous\", \"autorun\"]:\n",
    "    raise ValueError(\"Invalid mode. Mode must be 'previous' or 'autorun'.\")\n",
    "    \n",
    "    print(\"Running scraping mode: \",mode)\n",
    "    page_num = 1\n",
    "    raw_metadata = []\n",
    "    all_extracted_data = []\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        params = get_para(mode,q,start_d,end_d,page_num)\n",
    "\n",
    "        search = GoogleSearch(params)\n",
    "        results = search.get_dict()\n",
    "        extracted_data = []\n",
    "\n",
    "        if results['search_metadata']['status'] == 'Success':\n",
    "            raw_metadata.append(results)\n",
    "            total_page = results['search_information']['total_pages']\n",
    "            print(f'successfully scraped, extracting information from page {page_num} out of {total_page}')\n",
    "        else:\n",
    "            print('Failed to scrape data, debug and try again!')\n",
    "            break\n",
    "\n",
    "        organic_results = results[\"organic_results\"] \n",
    "\n",
    "        # Extract data from each result\n",
    "        for result in organic_results:\n",
    "            if \"country_status\" in result and result[\"country_status\"] is not None:\n",
    "                active_statuses = [country for country, status in result.get(\"country_status\").items() if status == \"ACTIVE\"]\n",
    "                active_country = ', '.join(active_statuses)\n",
    "\n",
    "            extracted_data.append({\n",
    "                \"patent_id\": result.get(\"patent_id\").split('/')[1],\n",
    "                \"title\": result.get(\"title\").strip(),\n",
    "                \"snippet\": result.get(\"snippet\").strip(),\n",
    "                \"grant_date\": result.get(\"grant_date\"),\n",
    "                \"publication_date\": result.get(\"publication_date\"),\n",
    "                \"inventor\": result.get(\"inventor\"),\n",
    "                \"assignee\": result.get(\"assignee\"),\n",
    "                \"language\": result.get(\"language\"),\n",
    "                \"active_country\": active_country,\n",
    "                \"pdf\": result.get(\"pdf\")\n",
    "            })\n",
    "            \n",
    "        all_extracted_data.extend(extracted_data)   \n",
    "        \n",
    "        if \"next\" in results[\"serpapi_pagination\"]:\n",
    "            page_num +=1 \n",
    "        else:\n",
    "            break     \n",
    "            \n",
    "    save_raw_meta(mode,start_d,end_d,raw_metadata)\n",
    "    save_extracted_data(mode,start_d,end_d,all_extracted_data)\n",
    "    \n",
    "    return raw_metadata,all_extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4280444d",
   "metadata": {},
   "source": [
    "## main code\n",
    "define the keywords we want to search for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97c00c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_word =[\n",
    "    '\"artificial intelligence\"',\n",
    "    '\"large language model\"',\n",
    "    '\"chatgpt\"'\n",
    "]\n",
    "\n",
    "offensive_word = [\n",
    "    '\"prompt injection\"',\n",
    "    '\"backdoor attack\"',\n",
    "    '\"extraction attack\"',\n",
    "    '\"jailbreak attack\"',\n",
    "    '\"poisoning attack\"',\n",
    "    '\"adversarial attack\"',\n",
    "    '\"privacy attack\"',\n",
    "    '\"evasion attack\"',\n",
    "    '\"robustness attack\"',\n",
    "    '\"deepfake\"',\n",
    "    '\"ransomware\"'\n",
    "]\n",
    "general_str = ' OR '.join(general_word)\n",
    "offensive_str = ' OR '.join(offensive_word)\n",
    "query = f\"({general_str}) AND ({offensive_str})\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21215285",
   "metadata": {},
   "source": [
    "we have two modes for scraping: \n",
    "1. \"previous\" = scrape the historical data, need to specify start date and end date in \"yyyymmdd\" format;\n",
    "2. another mode is designed for \"autorun\", will automatically scrape data from yesterday only(need to input start_date and end_date, but they will be automatically replaced, so can put any date you want).<br>\n",
    "\n",
    "modify start_date and end_date if want to do \"previous\" mode scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b286d94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running scraping mode:  previous\n",
      "successfully scraped, extracting information from page 1 out of 6\n",
      "successfully scraped, extracting information from page 2 out of 6\n",
      "successfully scraped, extracting information from page 3 out of 6\n",
      "successfully scraped, extracting information from page 4 out of 6\n",
      "successfully scraped, extracting information from page 5 out of 6\n",
      "successfully scraped, extracting information from page 6 out of 6\n",
      "Raw metadata saved to raw_metadata/raw_metadata_2022_2024.json\n",
      "Extracted data saved to extracted_data/extracted_data_2022_2024.json\n"
     ]
    }
   ],
   "source": [
    "start_date='20220101'\n",
    "end_date='20241231'\n",
    "raw_meta,extra_data = scrape_patent(\"previous\",query,start_date,end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2aca2",
   "metadata": {},
   "source": [
    "all the previous patent data is scraped, now combine them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ca0426f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data saved to all_extracted_data.json\n"
     ]
    }
   ],
   "source": [
    "#join all the file together\n",
    "directory = 'extracted_data'\n",
    "\n",
    "# Initialize an empty list to store combined data\n",
    "combined_data = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.json'):\n",
    "        # Load the JSON file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        with open(filepath, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            # Append the data to the combined list\n",
    "            combined_data.extend(data)\n",
    "\n",
    "# Define the output file path\n",
    "output_file = 'all_extracted_data.json'\n",
    "\n",
    "# Save the combined data to a single JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(combined_data, file, indent=4)\n",
    "\n",
    "print(f'Combined data saved to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "38e679e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved combined data as csv format to csv_all_extracted_data.csv\n"
     ]
    }
   ],
   "source": [
    "csv_file = \"csv_all_extracted_data.csv\" # Assign .csv file name to a variable\n",
    "csv_columns = [                 # Define list of columns for your .csv file\n",
    "    \"patent_id\",\n",
    "    \"title\",  \n",
    "    \"snippet\", \n",
    "    \"grant_date\", \n",
    "    \"publication_date\", \n",
    "    \"inventor\",\n",
    "    \"assignee\",\n",
    "    \"language\",\n",
    "    \"active_country\",\n",
    "    \"pdf\"\n",
    "]   \n",
    "\n",
    "\n",
    "# Save all extracted data to a CSV file\n",
    "pd.DataFrame(data=combined_data).to_csv(\n",
    "    csv_file, \n",
    "    columns=csv_columns, \n",
    "    encoding=\"utf-8\", \n",
    "    index=False\n",
    "    )\n",
    "\n",
    "print(f'saved combined data as csv format to {csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ab26c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146\n"
     ]
    }
   ],
   "source": [
    "filepath = output_file\n",
    "with open(filepath, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    # Append the data to the combined list\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a09759",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
